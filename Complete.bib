@techreport{1957RosenblattPerceptron,
number={85-460-1},
type={techreport},
volume={1957},
institution={Cornell Aeronautical Laboratory},
abstract={First publication about the perceptron},
publisher={Cornell Aeronautical Laboratory},
pages={460--1},
id={1},
note={(Project PARA)},
date={1957},
author={Rosenblatt, Frank},
mendeley-tags={seminal},
title={The Perceptron: A Perceiving and Recognizing Automaton},
groups={Seminal},
booktitle={Report 85, Cornell Aeronautical Laboratory},
url={https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf},
keywords={seminal},
}

@article{1999YaoEvolving,
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
year={1999},
groups={Seminal},
pages={25},
title={Evolving Artiﬁcial Neural Networks},
doi={10.1109/5.784219},
journaltitle={PROCEEDINGS OF THE IEEE},
date={1999},
author={Yao, Xin},
volume={87},
journal={Proceedings of the IEEE},
number={9},
langid={english},
}

@article{1956RosenblattRemarks,
pages={832--837},
journaltitle={The Annals of Mathematical Statistics},
number={3},
title={Remarks on Some Nonparametric Estimates of a Density Function},
volume={27},
issn={0003-4851},
date={1956},
publisher={Institute of Mathematical Statistics},
url={http://www.jstor.org/stable/2237390},
author={Rosenblatt, Murray},
abstract={This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymptotic mean square error of a particular class of estimates is evaluated.},
groups={Seminal},
}

@article{1943McCullochlogical,
groups={Seminal},
pages={115--133},
volume={5},
title={A logical calculus of the ideas immanent in nervous activity},
doi={10.1007/bf02478259},
number={4},
author={McCulloch, Warren S. and Pitts, Walter},
date={1943-12},
journaltitle={The Bulletin of Mathematical Biophysics},
}

@article{1962ParzenEstimation,
groups={Seminal},
issn={0003-4851},
title={On Estimation of a Probability Density Function and Mode},
author={Parzen, Emanuel},
number={3},
pages={1065--1076},
date={1962},
url={https://www.jstor.org/stable/2237880},
urldate={2019-03-28},
volume={33},
journaltitle={The Annals of Mathematical Statistics},
}

@article{2012HoogenboomHow,
langid={english},
journaltitle={The International Journal of Sports Physical Therapy},
title={How To Write A Scientific Article},
pages={6},
author={Hoogenboom, Barbara J. and Manske, Robert C.},
groups={Writing},
date={2012},
abstract={Successful production of a written product for submission to a peer-reviewed scientific journal requires substantial effort. Such an effort can be maximized by following a few simple suggestions when composing/ creating the product for submission. By following some suggested guidelines and avoiding common errors, the process can be streamlined and success realized for even beginning/novice authors as they negotiate the publication process. The purpose of this invited commentary is to offer practical suggestions for achieving success when writing and submitting manuscripts to The International Journal of Sports Physical Therapy and other professional journals.},
}

@unpublished{2018Elsevier4,
date={2018},
publisher={Emerald Publishing},
groups={Writing},
title={4 step guide to writing a literature review},
author={Elsevier},
url={http://www.emeraldgrouppublishing.com/authors/guides/write/literature.htm},
}

@inproceedings{2013KartofelevModeling,
groups={Acoustics},
langid={english},
url={http://homes.ioc.ee/stulov/smac13.pdf},
title={Modeling a vibrating string terminated against a bridge with arbitrary geometry},
date={2013},
pages={7},
booktitle={4th Stockholm Music Acoustics Conference},
author={Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-Maria and Va, Vesa},
}

@article{2009TorresInfluence,
doi={10.1016/j.apacoust.2009.07.002},
pages={1371--1377},
volume={70},
abstract={In this work the effect of two different bridge conﬁgurations on the vibrations of the top plate of a classical guitar is presented. Experimental harmonic analysis and visualization techniques, in addition to detailed damped simulations using the ﬁnite element method, were used to obtain mobility functions and operating deﬂection shapes of a top plate. The mobility functions were obtained in the following sequence. First the mobility functions were obtained on the top plate without a bridge attached. Then a bridge was glued to the top plate and new measurements were made. Finally the already attached bridge was cross-cut in situ without detaching it from the top plate, and the measurements were repeated. Those speciﬁc designs were chosen on the grounds of simplicity of construction both experimentally and in FEM simulations (no particular preference for those designs is implied). The assembly and the speciﬁc design of the bridge have shown to have considerable inﬂuence on the response of the top plate regarding the mode shapes above 300 Hz. Depending on the geometry of the bridge, its deﬂections can either be comparable to the maximum deﬂections of the top plate or can have amplitudes so small that the bridge effectively creates a nodal zone on the plate vibrations. This suggests that the shape, the stiffness, and the mass properties of the bridge may play a role in the sound quality of the instrument. Ó 2009 Elsevier Ltd. All rights reserved.},
title={Influence of the bridge on the vibrations of the top plate of a classical guitar},
groups={Acoustics},
author={Torres, Jesús Alejandro and Boullosa, Ricardo R.},
number={11},
langid={english},
issn={0003-682X},
url={https://linkinghub.elsevier.com/retrieve/pii/S0003682X09001613},
urldate={2019-04-05},
journaltitle={Applied Acoustics},
date={2009-12},
}

@collection{2009BruneauMaterials,
editor={Bruneau, Michel and Potel, Catherine},
isbn={978-1-84821-074-5},
location={London, UK},
keywords={Handbooks, manuals, etc, Acoustical engineering, Acoustical materials, Materials, Testing},
title={Materials and acoustics handbook},
langid={english},
publisher={ISTE ; Hoboken, NJ : J. Wiley},
groups={Acoustics},
date={2009},
pagetotal={919},
}

@article{2006WagnerLarge,
title={Large-Eddy Simulation for Acoustics},
date={2006},
author={Wagner, Claus and Huttl, Thomas and Sagaut, Pierre},
pages={471},
langid={english},
groups={Acoustics},
}

@inproceedings{2003KarjalainenCompilation,
groups={Acoustics},
pages={V–433–6},
doi={10.1109/ICASSP.2003.1199999},
publisher={IEEE},
location={Hong Kong, China},
url={http://ieeexplore.ieee.org/document/1199999/},
langid={english},
urldate={2019-03-31},
date={2003},
author={Karjalainen, M. and Erkut, C. and Savioja, L.},
title={Compilation of unified physical models for efficient sound synthesis},
volume={5},
abstract={This paper describes a systematic approach to specification and compilation of different physical modeling schemes particularly for sound synthesis studies. First we formulate theoretically a unified way of constructing physical interaction models which include elements that use both wave variables and Kirchhoff variables. These elements can be applied to build 1-D and multidimensional structures as well as lumped element models. In addition, typical signal processing algorithms are supported. A software environment (Block Compiler) has been developed, allowing for high-level object-based specification of physical models and their compilation to efficient code for execution.},
isbn={978-0-7803-7663-2},
booktitle={2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).},
eventtitle={International Conference on Acoustics, Speech and Signal Processing (ICASSP'03)},
}

@article{2002BoullosaVibration,
date={2002-03},
issn={0003-682X},
title={Vibration measurements in the classical guitar},
doi={10.1016/S0003-682X(01)00037-8},
author={Boullosa, Ricardo R.},
pages={311--322},
urldate={2019-04-05},
url={http://linkinghub.elsevier.com/retrieve/pii/S0003682X01000378},
number={3},
journaltitle={Applied Acoustics},
volume={63},
abstract={Some measurements are described that pertain to the high frequency behaviour of the vibrations of the string and soundboard of the guitar.Three isolated soundboards of diﬀerent woods were compared with respect to their early decay times (by means of measurements of their vibrational impulse responses), in order to assess its possible use as an objective measure of the reverberation time of the plates. The results of all these measurements show that high frequency vibrations and radiation occurs signiﬁcantly from 1 to 14 kHz, but that the sound radiation has harmonics at frequencies up to 20 kHz. This report does not produce any experimental evidence as to the importance of the high frequency region from the subjective point of view, or to the correlation of the subjective impression of reverberation of the plates with measured data of early decay times, as these aspects were not addressed at this point.},
langid={english},
groups={Acoustics},
}

@article{2001JaervelaeinenAudibility,
date={2001-07},
number={3},
author={Järveläinen, Hanna and Välimäki, Vesa and Karjalainen, Matti},
urldate={2019-03-28},
pages={79--84},
groups={Acoustics},
volume={2},
langid={english},
title={Audibility of the timbral effects of inharmonicity in stringed instrument tones},
doi={10.1121/1.1374756},
issn={1529-7853},
journaltitle={Acoustics Research Letters Online},
}

@article{1995CaldersmithDesigning,
journaltitle={Applied Acoustics},
date={1995},
issn={0003-682X},
url={http://linkinghub.elsevier.com/retrieve/pii/0003682X9593949I},
urldate={2019-04-05},
volume={46},
pages={3--17},
langid={english},
doi={10.1016/0003-682X(95)93949-I},
title={Designing a guitar family},
number={1},
groups={Acoustics},
abstract={When the standard classical guitar is scaled up a musical fourth to a treble guitar, down a musical fifth to a baritone guitar, and down an octave to a bass guitar, design compromises are necessary to maintain playability and favourable tone qulality. The resulting instruments exhibit interesting natural vibration mode and sound radiation physics. The tone qualities of the new instruments suggest relations between the guitar response envelope and human sound perception. Translating the principle natural modes of the guitar up or down with string frequencies does not necessarily produce pleasing tone qualities nor optimal playing dynamics. However, designing bracing configurations for both classical and folk baritone and bass guitars to maximise low frequency radiation eficiency does seem to produce new instruments of musical appeal, Frequency response records of standard and guitar family variants illustrate the physical behaviour of the difierent designs. Experience of musicians with the guitar family instruments indicates that creative new guitar territory is available.},
author={Caldersmith, Graham},
}

@collection{2007BeauchampAnalysis,
title={Analysis, synthesis, and perception of musical sounds: the sound of music},
series={Modern acoustics and signal processing},
publisher={Springer},
editor={Beauchamp, James W.},
isbn={978-0-387-32496-8},
groups={Acoustics},
note={OCLC: 237018036},
shorttitle={Analysis, synthesis, and perception of musical sounds},
pagetotal={325},
date={2007},
location={New York, NY},
langid={english},
}

@article{2018MartinMartinGoogle,
shorttitle={Google Scholar, Web of Science, and Scopus},
abstract={Despite citation counts from Google Scholar (GS), Web of Science (WoS), and Scopus being widely consulted by researchers and sometimes used in research evaluations, there is no recent or systematic evidence about the differences between them. In response, this paper investigates 2,448,055 citations to 2299 English-language highly-cited documents from 252 GS subject categories published in 2006, comparing GS, the WoS Core Collection, and Scopus. GS consistently found the largest percentage of citations across all areas (93/////// %–96/////// %), far ahead of Scopus (35/////// %–77/////// %) and WoS (27/////// %–73/////// %). GS found nearly all the WoS (95/////// %) and Scopus (92/////// %) citations. Most citations found only by GS were from non-journal sources (48/////// %–65/////// %), including theses, books, conference papers, and unpublished materials. Many were non-English (19/////// %–38/////// %), and they tended to be much less cited than citing sources that were also in Scopus or WoS. Despite the many unique GS citing sources, Spearman correlations between citation counts in GS and WoS or Scopus are high (0.78-0.99). They are lower in the Humanities, and lower between GS and WoS than between GS and Scopus. The results suggest that in all areas GS citation data is essentially a superset of WoS and Scopus, with substantial extra coverage.},
groups={Bibliometry},
journaltitle={Journal of Informetrics},
title={Google Scholar, Web of Science, and Scopus: A systematic comparison of citations in 252 subject categories},
number={4},
url={https://linkinghub.elsevier.com/retrieve/pii/S1751157718303249},
doi={10.1016/j.joi.2018.09.002},
author={Martín-Martín, Alberto and Orduna-Malea, Enrique and Thelwall, Mike and Delgado López-Cózar, Emilio},
issn={1751-1577},
date={2018-11},
urldate={2019-04-07},
langid={english},
volume={12},
pages={1160--1177},
}

@article{2017ZatorskiYoung,
volume={5},
groups={Bibliometry},
author={Zatorski, Hubert and Fichna, Jakub},
title={Young GI angle: The role of bibliometrics in scientist’s career development},
number={8},
pages={1151--1152},
urldate={2019-04-07},
date={2017-12},
journaltitle={United European Gastroenterology Journal},
doi={10.1177/2050640617744497},
issn={2050-6406},
langid={english},
shorttitle={Young GI angle},
}

@article{2016PerianesRodriguezConstructing,
urldate={2019-04-07},
author={Perianes-Rodriguez, Antonio and Waltman, Ludo and van Eck, Nees Jan},
journaltitle={Journal of Informetrics},
volume={10},
title={Constructing bibliometric networks: A comparison between full and fractional counting},
doi={10.1016/j.joi.2016.10.006},
issn={1751-1577},
pages={1178--1195},
number={4},
groups={Bibliometry},
shorttitle={Constructing bibliometric networks},
date={2016-11},
url={https://linkinghub.elsevier.com/retrieve/pii/S1751157716302036},
langid={english},
}

@article{2016Mongeonjournal,
doi={10.1007/s11192-015-1765-5},
langid={english},
urldate={2019-04-07},
groups={Bibliometry},
date={2016-01},
volume={106},
number={1},
author={Mongeon, Philippe and Paul-Hus, Adèle},
issn={0138-9130},
shorttitle={The journal coverage of Web of Science and Scopus},
abstract={Bibliometric methods are used in multiple ﬁelds for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some ﬁeld, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different ﬁelds, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientiﬁc output that are not covered in WoS or Scopus, such as ﬁeld-speciﬁc and national citation indexes.},
journaltitle={Scientometrics},
title={The journal coverage of Web of Science and Scopus: a comparative analysis},
pages={213--228},
}

@article{2016HarzingGoogle,
pages={787--804},
journaltitle={Scientometrics},
date={2016-02},
doi={10.1007/s11192-015-1798-9},
issn={0138-9130},
urldate={2019-04-07},
volume={106},
abstract={This article aims to provide a systematic and comprehensive comparison of the coverage of the three major bibliometric databases: Google Scholar, Scopus and the Web of Science. Based on a sample of 146 senior academics in ﬁve broad disciplinary areas, we therefore provide both a longitudinal and a cross-disciplinary comparison of the three databases. Our longitudinal comparison of eight data points between 2013 and 2015 shows a consistent and reasonably stable quarterly growth for both publications and citations across the three databases. This suggests that all three databases provide sufﬁcient stability of coverage to be used for more detailed cross-disciplinary comparisons. Our cross-disciplinary comparison of the three databases includes four key research metrics (publications, citations, h-index, and hI, annual, an annualised individual h-index) and ﬁve major disciplines (Humanities, Social Sciences, Engineering, Sciences and Life Sciences). We show that both the data source and the speciﬁc metrics used change the conclusions that can be drawn from cross-disciplinary comparisons.},
shorttitle={Google Scholar, Scopus and the Web of Science},
title={Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison},
groups={Bibliometry},
langid={english},
number={2},
author={Harzing, Anne-Wil and Alakangas, Satu},
}

@article{2014ZahediHow,
author={Zahedi, Zohreh and Costas, Rodrigo and Wouters, Paul},
volume={101},
date={2014-11},
doi={10.1007/s11192-014-1264-0},
pages={1491--1513},
number={2},
abstract={In this paper an analysis of the presence and possibilities of altmetrics for bibliometric and performance analysis is carried out. Using the web based tool Impact Story, we collected metrics for 20,000 random publications from the Web of Science. We studied both the presence and distribution of altmetrics in the set of publications, across ﬁelds, document types and over publication years, as well as the extent to which altmetrics correlate with citation indicators. The main result of the study is that the altmetrics source that provides the most metrics is Mendeley, with metrics on readerships for 62.6 /////// % of all the publications studied, other sources only provide marginal information. In terms of relation with citations, a moderate spearman correlation (r = 0.49) has been found between Mendeley readership counts and citation indicators. Other possibilities and limitations of these indicators are discussed and future research lines are outlined.},
urldate={2019-04-07},
shorttitle={How well developed are altmetrics?},
langid={english},
groups={Bibliometry},
journaltitle={Scientometrics},
issn={0138-9130},
title={How well developed are altmetrics? A cross-disciplinary analysis of the presence of ‘alternative metrics’ in scientific publications},
}

@article{2014Winterexpansion,
doi={10.1007/s11192-013-1089-2},
langid={english},
journaltitle={Scientometrics},
shorttitle={The expansion of Google Scholar versus Web of Science},
abstract={Web of Science (WoS) and Google Scholar (GS) are prominent citation services with distinct indexing mechanisms. Comprehensive knowledge about the growth patterns of these two citation services is lacking. We analyzed the development of citation counts in WoS and GS for two classic articles and 56 articles from diverse research ﬁelds, making a distinction between retroactive growth (i.e., the relative difference between citation counts up to mid-2005 measured in mid-2005 and citation counts up to mid-2005 measured in April 2013) and actual growth (i.e., the relative difference between citation counts up to mid-2005 measured in April 2013 and citation counts up to April 2013 measured in April 2013). One of the classic articles was used for a citation-by-citation analysis. Results showed that GS has substantially grown in a retroactive manner (median of 170 /////// % across articles), especially for articles that initially had low citations counts in GS as compared to WoS. Retroactive growth of WoS was small, with a median of 2 /////// % across articles. Actual growth percentages were moderately higher for GS than for WoS (medians of 54 vs. 41 /////// %). The citation-by-citation analysis showed that the percentage of citations being unique in WoS was lower for more recent citations (6.8 /////// % for citations from 1995 and later vs. 41 /////// % for citations from before 1995), whereas the opposite was noted for GS (57 vs. 33 /////// %). It is concluded that, since its inception, GS has shown substantial expansion, and that the majority of recent works indexed in WoS are now also retrievable via GS. A discussion is provided on quantity versus quality of citations, threats for WoS, weaknesses of GS, and implications for literature research and research evaluation.},
date={2014-02},
title={The expansion of Google Scholar versus Web of Science: a longitudinal study},
pages={1547--1565},
urldate={2019-04-07},
volume={98},
groups={Bibliometry},
issn={0138-9130},
author={de Winter, Joost C. F. and Zadpoor, Amir A. and Dodou, Dimitra},
number={2},
}

@article{2014HausteinCoverage,
groups={Bibliometry},
journaltitle={Scientometrics},
langid={english},
date={2014-11},
volume={101},
issn={0138-9130},
pages={1145--1163},
author={Haustein, Stefanie and Peters, Isabella and Bar-Ilan, Judit and Priem, Jason and Shema, Hadas and Terliesner, Jens},
number={2},
urldate={2019-04-07},
title={Coverage and adoption of altmetrics sources in the bibliometric community},
doi={10.1007/s11192-013-1221-3},
}

@misc{2014HarinarayanaData,
author={Harinarayana, N.},
url={https://epgp.inflibnet.ac.in/epgpdata/uploads/epgp///////\\_content/library///////\\_and///////\\_information///////\\_science/informetrics///////\\_///////\\&///////\\_scientometrics/data///////\\_sources///////\\_and///////\\_software///////\\_tools///////\\_for///////\\_bibliometric///////\\_studies/et/333///////\\_et///////\\_m2.pdf},
date={2014},
title={Data sources and software tools for bibliometric studies},
groups={Bibliometry},
}

@article{2014BornmannValidity,
journaltitle={Journal of Informetrics},
date={2014-10},
author={Bornmann, Lutz},
issn={1751-1577},
langid={english},
title={Validity of altmetrics data for measuring societal impact: A study using data from Altmetric and F1000Prime},
urldate={2019-04-07},
shorttitle={Validity of altmetrics data for measuring societal impact},
doi={10.1016/j.joi.2014.09.007},
pages={935--950},
volume={8},
url={https://linkinghub.elsevier.com/retrieve/pii/S1751157714000881},
abstract={Can altmetric data be validly used for the measurement of societal impact? The current study seeks to answer this question with a comprehensive dataset (about 100,000 records) from very disparate sources (F1000, Altmetric, and an in-house database based on Web of Science). In the F1000 peer review system, experts attach particular tags to scientiﬁc papers which indicate whether a paper could be of interest for science or rather for other segments of society. The results show that papers with the tag “good for teaching” do achieve higher altmetric counts than papers without this tag – if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is speciﬁcally scientiﬁcally oriented (“new ﬁnding”). The ﬁndings indicate that papers tailored for a readership outside the area of research should lead to societal impact.},
groups={Bibliometry},
number={4},
}

@article{2014BornmannDo,
journaltitle={Journal of Informetrics},
title={Do altmetrics point to the broader impact of research? An overview of benefits and disadvantages of altmetrics},
author={Bornmann, Lutz},
date={2014-10},
doi={10.1016/j.joi.2014.09.005},
volume={8},
groups={Bibliometry},
url={https://linkinghub.elsevier.com/retrieve/pii/S1751157714000868},
langid={english},
urldate={2019-04-07},
shorttitle={Do altmetrics point to the broader impact of research?},
issn={1751-1577},
pages={895--903},
number={4},
}

@article{2013ThelwallDo,
journaltitle={PLoS ONE},
langid={english},
editor={Bornmann, Lutz},
date={2013-05-28},
title={Do Altmetrics Work? Twitter and Ten Other Social Web Services},
urldate={2019-04-07},
doi={10.1371/journal.pone.0064841},
author={Thelwall, Mike and Haustein, Stefanie and Larivière, Vincent and Sugimoto, Cassidy R.},
shorttitle={Do Altmetrics Work?},
number={5},
groups={Bibliometry},
pages={e64841},
volume={8},
abstract={Altmetric measurements derived from the social web are increasingly advocated and used as early indicators of article impact and usefulness. Nevertheless, there is a lack of systematic scientific evidence that altmetrics are valid proxies of either impact or utility although a few case studies have reported medium correlations between specific altmetrics and citation rates for individual journals or fields. To fill this gap, this study compares 11 altmetrics with Web of Science citations for 76 to 208,739 PubMed articles with at least one altmetric mention in each case and up to 1,891 journals per metric. It also introduces a simple sign test to overcome biases caused by different citation and usage windows. Statistically significant associations were found between higher metric scores and higher citations for articles with positive altmetric scores in all cases with sufficient evidence (Twitter, Facebook wall posts, research highlights, blogs, mainstream media and forums) except perhaps for Google+ posts. Evidence was insufficient for LinkedIn, Pinterest, question and answer sites, and Reddit, and no conclusions should be drawn about articles with zero altmetric scores or the strength of any correlation between altmetrics and citations. Nevertheless, comparisons between citations and metric values for articles published at different times, even within the same year, can remove or reverse this association and so publishers and scientometricians should consider the effect of time when using altmetrics to rank articles. Finally, the coverage of all the altmetrics except for Twitter seems to be low and so it is not clear if they are prevalent enough to be useful in practice.},
issn={1932-6203},
}

@article{2013S.AdriaanseWeb,
issn={0264-0473},
number={6},
pages={727--744},
title={Web of Science, Scopus and Google Scholar: A content comprehensiveness comparison},
date={2013-11-18},
langid={english},
urldate={2019-04-07},
volume={31},
author={S. Adriaanse, Leslie and Rensleigh, Chris},
shorttitle={Web of Science, Scopus and Google Scholar},
journaltitle={The Electronic Library},
doi={10.1108/EL-12-2011-0174},
abstract={Purpose – The research aim for this study was to compare three citation resources with one another to identify the citation resource with the most representative South African scholarly environmental sciences citation coverage. This paper focuses on the results of the content veriﬁcation process which measured amongst others the citation counts, multiple copies and inconsistencies encountered across the three citation resources ISI Web of Science, Scopus and Google Scholar.},
groups={Bibliometry},
}

@article{2013GalliganAltmetrics,
pages={56--61},
groups={Bibliometry},
number={1},
author={Galligan, Finbar and Dyas-Correia, Sharon},
doi={10.1016/j.serrev.2013.01.003},
title={Altmetrics: Rethinking the Way We Measure},
journaltitle={Serials Review},
langid={english},
urldate={2019-04-07},
shorttitle={Altmetrics},
date={2013-03},
url={http://linkinghub.elsevier.com/retrieve/pii/S009879131300004X},
issn={0098-7913},
volume={39},
abstract={Altmetrics is the focus for this edition of “Balance Point.” The column editor invited Finbar Galligan who has gained considerable knowledge of altmetrics to co-author the column. Altmetrics, their relationship to traditional metrics, their importance, uses, potential impacts, and possible future directions are examined. The authors conclude that altmetrics have an important future role to play and that they offer the potential to revolutionize the analysis of the value and impact of scholarly work.},
}

@article{2012VanclayImpact,
author={Vanclay, Jerome K.},
journaltitle={Scientometrics},
volume={92},
abstract={A review of Garﬁeld’s journal impact factor and its speciﬁc implementation as the Thomson Reuters impact factor reveals several weaknesses in this commonly-used indicator of journal standing. Key limitations include the mismatch between citing and cited documents, the deceptive display of three decimals that belies the real precision, and the absence of conﬁdence intervals. These are minor issues that are easily amended and should be corrected, but more substantive improvements are needed. There are indications that the scientiﬁc community seeks and needs better certiﬁcation of journal procedures to improve the quality of published science. Comprehensive certiﬁcation of editorial and review procedures could help ensure adequate procedures to detect duplicate and fraudulent submissions.},
number={2},
pages={211--238},
langid={english},
issn={0138-9130},
title={Impact factor: outdated artefact or stepping-stone to journal certification?},
urldate={2019-04-07},
shorttitle={Impact factor},
date={2012-08},
doi={10.1007/s11192-011-0561-0},
groups={Bibliometry},
}

@article{2006BakkalbasiThree,
title={Three options for citation tracking: Google Scholar, Scopus and Web of Science},
groups={Bibliometry},
abstract={Background: Researchers turn to citation tracking to find the most influential articles for a particular topic and to see how often their own published papers are cited. For years researchers looking for this type of information had only one resource to consult: the Web of Science from Thomson Scientific. In 2004 two competitors emerged – Scopus from Elsevier and Google Scholar from Google. The research reported here uses citation analysis in an observational study examining these three databases; comparing citation counts for articles from two disciplines (oncology and condensed matter physics) and two years (1993 and 2003) to test the hypothesis that the different scholarly publication coverage provided by the three search tools will lead to different citation counts from each. Methods: Eleven journal titles with varying impact factors were selected from each discipline (oncology and condensed matter physics) using the Journal Citation Reports (JCR). All articles published in the selected titles were retrieved for the years 1993 and 2003, and a stratified random sample of articles was chosen, resulting in four sets of articles. During the week of November 7–12, 2005, the citation counts for each research article were extracted from the three sources. The actual citing references for a subset of the articles published in 2003 were also gathered from each of the three sources. Results: For oncology 1993 Web of Science returned the highest average number of citations, 45.3. Scopus returned the highest average number of citations (8.9) for oncology 2003. Web of Science returned the highest number of citations for condensed matter physics 1993 and 2003 (22.5 and 3.9 respectively). The data showed a significant difference in the mean citation rates between all pairs of resources except between Google Scholar and Scopus for condensed matter physics 2003. For articles published in 2003 Google Scholar returned the largest amount of unique citing material for oncology and Web of Science returned the most for condensed matter physics. Conclusion: This study did not identify any one of these three resources as the answer to all citation tracking needs. Scopus showed strength in providing citing literature for current (2003) oncology articles, while Web of Science produced more citing material for 2003 and 1993 condensed matter physics, and 1993 oncology articles. All three tools returned some unique material. Our data indicate that the question of which tool provides the most complete set of citing literature may depend on the subject and publication year of a given article.},
langid={english},
date={2006-12},
journaltitle={Biomedical Digital Libraries},
volume={3},
issn={1742-5581},
urldate={2019-04-07},
shorttitle={Three options for citation tracking},
number={1},
doi={10.1186/1742-5581-3-7},
author={Bakkalbasi, Nisa and Bauer, Kathleen and Glover, Janis and Wang, Lei},
}

@collection{2018ErdtAltmetrics,
urldate={2019-04-07},
publisher={Springer Singapore},
title={Altmetrics for Research Outputs Measurement and Scholarly Information Management},
doi={10.1007/978-981-13-1053-9},
langid={english},
isbn={9789811310522},
location={Singapore},
groups={Bibliometry},
date={2018},
series={Communications in Computer and Information Science},
editor={Erdt, Mojisola and Sesagiri Raamkumar, Aravind and Rasmussen, Edie and Theng, Yin-Leng},
volume={856},
}

@misc{2017EbrahimHow,
urldate={2019-04-07},
publisher={Figshare},
abstract={Bibliometrics can be defined as the statistical analysis of publications. Bibliometrics has focused on the quantitative analysis of citations and citation counts which is complex. It is so complex and specialized that personal knowledge and experience are insufficient for understanding trends and then making decisions. We need tools for analysis of bibliometrics information to recognize the research trends and evaluate scientific/institution/country’s research productivity. This presentation will provide procedure to write a Bibliometrics paper.},
date={2017},
langid={english},
doi={10.6084/m9.figshare.5374615.v1},
author={Ebrahim, Nader Ale},
url={https://figshare.com/articles/How///////\\_to///////\\_Write///////\\_a///////\\_Bibliometric///////\\_Paper/5374615/1},
title={How to Write a Bibliometric Paper},
groups={Writing},
}

@article{HendryInharmonicity,
groups={Piano},
pages={32},
author={Hendry, Simon},
langid={english},
title={Inharmonicity of Piano Strings},
abstract={Piano partials deviate from integer harmonics due to the stiffness and linear density of the strings. Values for the inharmonicity coefficient of six strings were determined experimentally and compared with calculated values. An investigation into stretched tuning was also performed with detailed readings taken from a well tuned Steinway Model D grand piano. These results are compared with Railsback‟s predictions of 1938.},
}

@article{2016Graciawave,
date={2016},
journaltitle={Reports@SCM},
volume={3},
annotation={preprint},
arxivid={1603.05516},
eprinttype={arXiv},
url={http://revistes.iec.cat/index.php/reports/article/view/142136/141068},
keywords={inharmonic spectrum,musical,stiffness,vibrating string,wave equation},
groups={Piano},
eprint={1603.05516},
pages={1--16},
number={1},
title={The wave equation for stiff strings and piano tuning.},
author={Gracia, X. and Sanz-perela, T.},
}

@article{2015GiordanoExplaining,
doi={10.1121/1.4931439},
date={2015-10},
journaltitle={The Journal of the Acoustical Society of America},
title={Explaining the Railsback stretch in terms of the inharmonicity of piano tones and sensory dissonance},
issn={0001-4966},
pages={2359--2366},
urldate={2019-03-26},
volume={138},
groups={Piano},
author={Giordano, N.},
langid={english},
number={4},
}

@inproceedings{2015ChengModelling,
pages={594--598},
doi={10.1109/ICASSP.2015.7178038},
date={2015-04},
abstract={We investigate piano acoustics and compare the theoretical temporal decay of individual partials to recordings of real-world piano notes from the RWC Music Database. We ﬁrst describe the theory behind double decay and beats, known phenomena caused by the interaction between strings and soundboard. Then we ﬁt the decay of the ﬁrst 30 partials to a standard linear model and two physically-motivated non-linear models that take into account the coupling of strings and soundboard. We show that the use of non-linear models provides a better ﬁt to the data. We use these estimated decay rates to parameterise the characteristic decay response (decay rates along frequencies) of the piano under investigation. The results also show that dynamics have no signiﬁcant effect on the decay rate.},
langid={english},
booktitle={2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn={978-1-4673-6997-8},
eventtitle={ICASSP 2015 - 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
url={http://ieeexplore.ieee.org/document/7178038/},
location={South Brisbane, Queensland, Australia},
publisher={IEEE},
title={Modelling the decay of piano sounds},
groups={Piano},
author={Cheng, Tian and Dixon, Simon and Mauch, Matthias},
urldate={2019-03-28},
}

@report{2004BurredAcoustics,
title={The Acoustics of the Piano},
groups={Piano},
date={2004},
institution={Professional Conservatory of Music Arturo Soria},
type={resreport},
pages={43},
langid={english},
year={2004},
author={Burred, Juan Jose},
}

@report{1992ChaigneNumerical,
author={Chaigne, A. and Askenfelt, A.},
date={1992},
abstract={Thefirst attempt to generate musical sounds by solving the equations of vibrating strings by means of Finite Difference Methods (FDM)was made by Hiller /& Ruiz (].AudioEng.Soc. 19, pp. 462-472, 19711. It is shown here how their numerical approach and the underlying physical model can be improved in order to simulate the motion of the piano string with a high d e g e e of realism. Starting from the fundamental equations of a damped, stlff string interacting with a nonlinear hammer, a numericalfinite difference scheme is derived,from which the time and spatial dependence of string displacement, velocity, and interactingforce between hammer and string, as well as theforce acting on the bridge, are computed in the time-domain. The strength of the model is illustrated by comparisons between measured and simulated piano tones. After this verification of the accuracy of the method, the model is used as a tool for systematically exploring the influence of string stiffness, relative strikin, position, and hammer-string mass ratio on string waveforms and spectra.},
institution={KTH},
title={Numerical simulations of piano strings},
groups={Piano},
langid={english},
year={1992},
type={resreport},
pages={24},
}

@article{1964FletcherNormal,
author={Fletcher, Harvey},
doi={10.1121/1.1918933},
title={Normal Vibration Frequencies of a Stiff Piano String},
number={1},
month={jan,publisher    = {Acoustical Society of America (ASA)},
urldate={2019-03-28},
journal={The Journal Of The Acoustical Society Of America},
volume={36},
issn={0001-4966},
langid={english},
groups={Piano},
year={1964},
journaltitle={The Journal of the Acoustical Society of America},
pages={203--209},
date={1964-01},
}

@article{1961MartinSubjective,
number={5},
date={1961},
publisher={ASA},
volume={33},
groups={Piano},
author={Martin, Daniel W. and Ward, W. D.},
journaltitle={The Journal of the Acoustical Society of America},
title={Subjective evaluation of musical scale temperament in pianos},
pages={582--585},
}

@article{2016ChabassierTime,
abstract={This article is the second of a series of two papers devoted to the numerical simulation of the piano. It concerns the numerical aspects of the work, the implementation of a piano code and the presentation of corresponding simulations. The main diﬃculty is time discretisation and stability is achieved via energy methods. Numerical illustrations are provided for a realistic piano and compared to experimental recordings.},
langid={english},
journaltitle={ESAIM: Mathematical Modelling and Numerical Analysis},
issn={0764-583X},
title={Time domain simulation of a piano. Part 2: numerical aspects},
groups={Piano},
pages={93--133},
shorttitle={Time domain simulation of a piano. Part 2},
author={Chabassier, Juliette and Duruflé, Marc and Joly, Patrick},
number={1},
date={2016-01},
doi={10.1051/m2an/2015007},
urldate={2019-03-26},
volume={50},
}

@article{2014ChabassierTime,
groups={Piano},
shorttitle={Time domain simulation of a piano. Part 1},
pages={1241--1278},
number={5},
langid={english},
volume={48},
title={Time domain simulation of a piano. Part 1: model description},
urldate={2019-03-26},
date={2014-09},
doi={10.1051/m2an/2013136},
author={Chabassier, J. and Chaigne, A. and Joly, P.},
abstract={The purpose of this study is the time domain modeling of a piano. We aim at explaining the vibratory and acoustical behavior of the piano, by taking into account the main elements that contribute to sound production. The soundboard is modeled as a bidimensional thick, orthotropic, heterogeneous, frequency dependant damped plate, using Reissner Mindlin equations. The vibroacoustics equations allow the soundboard to radiate into the surrounding air, in which we wish to compute the complete acoustical ﬁeld around the perfectly rigid rim. The soundboard is also coupled to the strings at the bridge, where they form a slight angle from the horizontal plane. Each string is modeled by a one dimensional damped system of equations, taking into account not only the transversal waves excited by the hammer, but also the stiﬀness thanks to shear waves, as well as the longitudinal waves arising from geometric nonlinearities. The hammer is given an initial velocity that projects it towards a choir of strings, before being repelled. The interacting force is a nonlinear function of the hammer compression. The ﬁnal piano model is a coupled system of partial diﬀerential equations, each of them exhibiting speciﬁc diﬃculties (nonlinear nature of the string system of equations, frequency dependant damping of the soundboard, great number of unknowns required for the acoustic propagation), in addition to couplings’ inherent diﬃculties.},
issn={0764-583X},
journaltitle={ESAIM: Mathematical Modelling and Numerical Analysis},
}

@article{2009BrynerStiff,
groups={Piano},
author={Bryner, John C.},
journaltitle={Physics Today},
title={Stiff-string theory: Richard Feynman on piano tuning},
urldate={2019-03-26},
langid={english},
shorttitle={Stiff-string theory},
number={12},
doi={10.1063/1.3273016},
issn={0031-9228},
date={2009-12},
pages={46--49},
volume={62},
}

@unpublished{2009TaoFourier,
author={Tao, Terence},
title={Fourier Transform},
url={https://www.math.ucla.edu//textasciitilde tao/preprints/fourier.pdf},
groups={Harmonic Analysis},
date={2009},
}

@misc{2004FesslerDiscrete,
note={chapter 5},
title={The Discrete Fourier Transform},
groups={Harmonic Analysis},
url={https://web.eecs.umich.edu//textasciitilde fessler/course/451/l/pdf/c5.pdf},
date={2004},
author={Fessler, Jeffrey},
}

@unpublished{2009MorinFourier,
author={Morin, David},
title={Fourier analysis.pdf},
date={2009},
groups={Harmonic Analysis},
year={2009},
note={Chapter 3},
url={http://www.people.fas.harvard.edu//textasciitilde djmorin/waves/Fourier.pdf},
}

@article{2015CourtneyMore,
langid={english},
author={Courtney, Elya and Courtney, Michael},
title={A More Accurate Fourier Transform},
date={2015-07-06},
eprint={1507.01832},
eprintclass={physics},
abstract={Fourier transform methods are used to analyze functions and data sets to provide frequencies, amplitudes, and phases of underlying oscillatory components. Fast Fourier transform (FFT) methods oﬀer speed advantages over evaluation of explicit integrals (EI) that deﬁne Fourier transforms. This paper compares frequency, amplitude, and phase accuracy of the two methods for well resolved peaks over a wide array of data sets including cosine series with and without random noise and a variety of physical data sets, including atmospheric CO2 concentrations, tides, temperatures, sound waveforms, and atomic spectra. The FFT uses MIT’s FFTW3 library. The EI method uses the rectangle method to compute the areas under the curve via complex math. Results support the hypothesis that EI methods are more accurate than FFT methods. Errors range from 5 to 10 times higher when determining peak frequency by FFT, 1.4 to 60 times higher for peak amplitude, and 6 to 10 times higher for phase under a peak. The ability to compute more accurate Fourier transforms has promise for improved data analysis in many ﬁelds, including more sensitive assessment of hypotheses in the environmental sciences related to CO2 concentrations and temperature. Other methods are available to address diﬀerent weaknesses in FFTs; however, the EI method always produces the most accurate output possible for a given data set. On the 2011 Lenovo ThinkPad used in this study, an EI transform on a 10,000 point data set took 31 seconds to complete. Source code (C) and Windows executable for the EI method are available at https://sourceforge.net/projects/amoreaccuratefouriertransform/.},
groups={Harmonic Analysis},
eprinttype={arxiv},
}

@article{1987SorensenReal,
date={1987-06},
number={6},
doi={10.1109/TASSP.1987.1165220},
keywords={Signal processing algorithms, Algorithm design and analysis, Application software, Convolutional codes, Digital images, Discrete Fourier transforms, Discrete transforms, Fast Fourier transforms, NASA},
issn={0096-3518},
volume={35},
author={Sorensen, H. and Jones, D. and Heideman, M. and Burrus, C.},
journaltitle={IEEE Transactions on Acoustics, Speech, and Signal Processing},
pages={849--863},
title={Real-valued fast Fourier transform algorithms},
groups={Harmonic Analysis},
}

@inproceedings{2015LiaoAnalytical,
langid={english},
title={Analytical solution of DFT interpolated frequency estimator for Hanning windowed signal},
eventtitle={2015 15th International Symposium on Communications and Information Technologies (ISCIT)},
doi={10.1109/ISCIT.2015.7458336},
pages={177--180},
urldate={2019-03-26},
abstract={Frequency estimation from discrete Fourier transform (DFT) coefﬁcients of a rectangular windowed signal under the inﬂuence of additive white noise is a well studied problem in signal processing. In its simplest form, the process involves ﬁnding the spectral peaks. When higher frequency resolution is required, a frequency offset can be found from the interpolation of DFT coefﬁcients. However, most of the past researches focus on monotonic cisoid signals. In practical situations where multiple harmonics are present, the sidelobes from other harmonics interfere with the estimation of the harmonic being considered. In this case, windows with smaller sidelobes such as Hanning window are preferred over rectangular window. Given the increased mathematical complexity of Hanning window, analytical solution has not yet been available for DFT interpolation. In this paper, we derive an exact analytical solution of the estimated frequency from DFT interpolation of Hanning windowed signal. In experiments, we show that the new analytical solution is accurate for monotonic cisoid signal and can considerably reduce the effect of interharmonic interference as compared to previous rectangular windowed methods.},
url={http://ieeexplore.ieee.org/document/7458336/},
publisher={IEEE},
booktitle={2015 15th International Symposium on Communications and Information Technologies (ISCIT)},
isbn={978-1-4673-6820-9},
author={Liao, Jan-Ray},
date={2015-10},
groups={Harmonic Analysis},
location={Nara, Japan},
}

@unpublished{2012MorseFourier,
author={Morse, Bryan},
pages={4},
groups={Harmonic Analysis},
langid={english},
title={The Fourier Transform: Examples, Properties, Common Pairs},
date={2012},
}

@collection{1979CarmonaNon,
pagetotal={244},
series={Lecture notes in mathematics ; 728},
location={Berlin ; New York},
keywords={Congresses, Harmonic analysis, Lie algebras, Lie groups},
title={Non-commutative harmonic analysis: proceedings, Marseille-Luminy, France, June 26 to 30, 1978: actes du Colloque d'analyse harmonique non commutative},
editor={Carmona, Jacques and Vergne, Michèle},
publisher={Springer-Verlag},
groups={Harmonic Analysis},
date={1979},
shorttitle={Non-commutative harmonic analysis},
isbn={978-0-387-09516-5},
}

@collection{2010SzoekefalviNagyHarmonic,
editor={Szőkefalvi-Nagy, Béla},
title={Harmonic analysis of operators on Hilbert space},
pagetotal={474},
isbn={978-1-4419-6093-1},
edition={2},
publisher={Springer},
groups={Harmonic Analysis},
series={Universitext},
location={New York, NY},
langid={english},
note={OCLC: 845818617},
date={2010},
}

@unpublished{2007AnandBrief,
abstract={This paper studies the mathematical machinery underlying the Discrete and Fast Fourier Transforms, algorithmic processes widely used in quantum mechanics, signal analysis, options pricing, and other diverse ﬁelds. Beginning with the basic properties of Fourier Transform, we proceed to study the derivation of the Discrete Fourier Transform, as well as computational considerations that necessitate the development of a faster way to calculate the DFT. With these considerations in mind, we study the construction of the Fast Fourier Transform, as proposed by Cooley and Tukey [7].},
langid={english},
title={A Brief Study Of Discrete And Fast Fourier Transforms},
author={Anand, Aashirwad Viswanathan},
date={2007},
groups={Harmonic Analysis},
pages={11},
year={2007},
}

@unpublished{2003KornerFirst,
date={2003},
title={A First Look at Fourier Analysis},
groups={Harmonic Analysis},
langid={english},
author={Korner, T. W.},
pages={60},
}

@misc{2017BattyDiscrete,
date={2017},
groups={Harmonic Analysis},
title={Discrete Fourier Transform Derivation},
author={Batty, C.},
}

@unpublished{2017DeSerioAddendum,
author={DeSerio, Robert},
langid={english},
groups={Harmonic Analysis},
date={2017},
title={Addendum : The Fourier transform of decaying oscillations},
number={6},
pages={12},
}

@article{2014SerraSpectral,
title={Spectral Modeling Synthesis: Past and Present},
langid={english},
groups={Harmonic Analysis},
pages={26},
date={2014},
author={Serra, Xavier},
number={November},
}

@misc{2017OliehoekGANGs,
author={Oliehoek, Frans A. and Savani, Rahul and Gallego-Posada, Jose and van der Pol, Elise and de Jong, Edwin D. and Gross, Roderich},
groups={Game Theory},
title={GANGs: Generative Adversarial Network Games},
eprinttype={arXiv},
langid={english},
journaltitle={arXiv},
eprint={1712.00679},
date={2017-12-02},
abstract={Generative Adversarial Networks (GAN) have become one of the most successful frameworks for unsupervised generative modeling. As GANs are difficult to train much research has focused on this. However, very little of this research has directly exploited gametheoretic techniques. We introduce Generative Adversarial Network Games (GANGs), which explicitly model a finite zero-sum game between a generator (G) and classifier (C) that use mixed strategies. The size of these games precludes exact solution methods, therefore we define resource-bounded best responses (RBBRs), and a resourcebounded Nash Equilibrium (RB-NE) as a pair of mixed strategies such that neither G or C can find a better RBBR. The RB-NE solution concept is richer than the notion of ‘local Nash equilibria’ in that it captures not only failures of escaping local optima of gradient descent, but applies to any approximate best response computations, including methods with random restarts. To validate our approach, we solve GANGs with the Parallel Nash Memory algorithm, which provably monotonically converges to an RB-NE. We compare our results to standard GAN setups, and demonstrate that our method deals well with typical GAN problems such as mode collapse, partial mode coverage and forgetting.},
shorttitle={GANGs},
}

@incollection{2017AnStackelberg,
groups={Game Theory},
booktitle={Improving Homeland Security Decisions},
urldate={2019-03-28},
doi={10.1017/9781316676714.021},
abstract={Security is a critical concern around the world, whether it is the challenge of protecting ports, airports and other critical infrastructure, interdicting the illegal ﬂow of drugs, weapons and money, protecting endangered species, forests and ﬁsheries, suppressing urban crime or security in cyberspace. Unfortunately, limited security resources prevent full security coverage at all times; instead, we must optimize the use of limited security resources. To that end, we founded the “security games” framework to build decision-aids for security agencies. Security games is a novel area of research that is based on computational and behavioral game theory, while also incorporating elements of AI planning under uncertainty and machine learning. We have deployed securitygames based decision aids for infrastructure security such as at the ports and ferry trafﬁc with the US coast guard (in the ports of New York, Boston, Los Angeles/Long Beach, Houston and others), for security of airports and air trafﬁc with the US Federal Air Marshals and the Los Angeles World Airport (LAX) police, and tested this framework for security of metro trains with the Los Angeles Sheriff’s Department. Moreover, recent work on “green security games” has led to testing our decision aids for protection of ﬁsheries with the US Coast Guard and protection of wildlife at sites in multiple countries, and opportunistic crime security games have focused on suppressing urban crime. This chapter will discuss applications of security games, and outline research challenges in security games including algorithms for scaling up security games as well as for handling signiﬁcant adversarial uncertainty and learning models of human adversary behaviors.},
edition={1},
langid={english},
isbn={978-1-316-67671-4},
author={An, Bo and Tambe, Milind},
title={Stackelberg Security Games (SSG) Basics and Application Overview},
editor={Abbas, Ali E. and Tambe, Milind and von Winterfeldt, Detlof},
url={https://www.cambridge.org/core/product/identifier/9781316676714///////\\%23CN-bp-21/type/book///////\\_part},
date={2017-11-02},
publisher={Cambridge University Press},
pages={485--507},
}

@collection{2016RotheEconomics,
isbn={978-3-662-47903-2},
date={2016},
series={Springer texts in business and economics},
publisher={Springer},
groups={Game Theory},
langid={english},
shorttitle={Economics and computation},
editor={Rothe, Jörg},
note={OCLC: 923541416},
pagetotal={612},
title={Economics and computation: an introduction to algorithmic game theory, computational social choice, and fair division},
location={Berlin},
}

@article{2016PerolatLearning,
eprint={1606.08718},
eprintclass={cs},
abstract={This paper addresses the problem of learning a Nash equilibrium in γ-discounted multiplayer general-sum Markov Games (MGs) in a batch setting. As the number of players increases in MG, the agents may either collaborate or team apart to increase their ﬁnal rewards. One solution to address this problem is to look for a Nash equilibrium. Although, several techniques were found for the subcase of two-player zero-sum MGs, those techniques fail to ﬁnd a Nash equilibrium in general-sum Markov Games. In this paper, we introduce a new deﬁnition of ǫ-Nash equilibrium in MGs which grasps the strategy’s quality for multiplayer games. We prove that minimizing the norm of two Bellmanlike residuals implies to learn such an ǫ-Nash equilibrium. Then, we show that minimizing an empirical estimate of the Lp norm of these Bellman-like residuals allows learning for general-sum games within the batch setting. Finally, we introduce a neural network architecture that successfully learns a Nash equilibrium in generic multiplayer generalsum turn-based MGs.},
date={2016-06-28},
title={Learning Nash Equilibrium for General-Sum Markov Games from Batch Data},
groups={Game Theory},
langid={english},
eprinttype={arxiv},
author={Pérolat, Julien and Strub, Florian and Piot, Bilal and Pietquin, Olivier},
}

@incollection{2016KrawczykMultistage,
pages={1--57},
urldate={2019-03-28},
editor={Basar, Tamer and Zaccour, Georges},
doi={10.1007/978-3-319-27335-8///////\\_3-1},
isbn={978-3-319-27335-8},
date={2016},
publisher={Springer International Publishing},
location={Cham},
author={Krawczyk, Jacek B. and Petkov, Vladimir},
groups={Game Theory},
booktitle={Handbook of Dynamic Game Theory},
abstract={In this chapter, we build on the concept of a repeated game discussed in Chap. Repeated Games and introduce the notion of a multistage game. In both types of games, several antagonistic agents interact with each other over time. The difference is that, in a multistage game, there is a dynamic system whose state keeps changing: the controls chosen by the agents in the current period affect the system’s future. In contrast with repeated games, the agents’ payoffs in multistage games depend directly on the state of this system. Examples of such settings range from a microeconomic dynamic model of a ﬁsh biomass exploited by several agents to a macroeconomic interaction between the government and the business sector. In some multistage games, physically different decisionmakers engage in simultaneous-move competition. In others, agents execute their actions sequentially rather than simultaneously. We also study hierarchical games, where a leader moves ahead of a follower. This chapter concludes with an example of memory-based strategies that can support Pareto-efﬁcient outcomes.},
title={Multistage Games},
langid={english},
}

@inproceedings{2015LiuComplex,
title={Complex Information Game Problem Based on Artificial Neural Network},
doi={10.2991/lemcs-15.2015.34},
url={http://www.atlantis-press.com/php/paper-details.php?id=25838084},
date={2015},
urldate={2019-03-28},
author={Liu, Xingfeng and Zhou, Tiansong and Zheng, Zhongxia},
location={Shenyang, China},
booktitle={Proceedings of the International Conference on Logistics, Engineering, Management and Computer Science},
abstract={The assumption of game theory is that the players in game must be rational. In the game of incomplete information, participants are not completely clear about the game. Therefore, usually there is a probability distribution of strategy selection in game. It is very complicated to know the real game information of the social and economic problems. In fact, the actual situation for many problems is that game players are irrational, or the probability distribution of game players’ strategies cannot be gotten, even the strategy sets are not complete (infinite strategy sets).There are many limitations in application of the traditional game theory. In this paper, the concept of complex information game and its Nash equilibrium are presented. It is proved that the complex information game problem can be solved by artificial neural network. An example on how to solve the complex information game problem with artificial neural network is given as well. Researchers hope that more and more scholars can use artificial intelligence theory to analyze the game theory problem. Therefore, the complex information game problems can be dealt more efficiently.},
groups={Game Theory},
eventtitle={International Conference on Logistics Engineering, Management and Computer Science (LEMCS 2015)},
publisher={Atlantis Press},
langid={english},
isbn={978-94-6252-102-5},
}

@inproceedings{2014BhatiaRecurrent,
groups={Game Theory},
abstract={We describe the properties of a connectionist network that is able to make decisions in strategic games. We use the structure of Bidirectional Associative Memory (BAM), a minimal two-layer recurrent neural network with binary activation functions and binary connection weights. We apply BAM to finite-strategy two-player games, and show that network activation in the long run is restricted to the set of rationalizable strategies. The network is not guaranteed to reach a stable activation state, but any pure strategy profile that constitutes a stable state in the network must be a pure strategy Nash equilibrium.},
date={2014},
langid={english},
volume={36},
author={Bhatia, Sudeep and Golman, Russell},
booktitle={Proceedings of the Annual Meeting of the Cognitive Science Society},
title={A Recurrent Neural Network for Game Theoretic Decision Making},
pages={6},
}

@incollection{2012NoweGame,
editor={Wiering, Marco and van Otterlo, Martijn},
date={2012},
volume={12},
isbn={978-3-642-27644-6},
urldate={2019-04-16},
groups={Game Theory},
author={Nowé, Ann and Vrancx, Peter and De Hauwere, Yann-Michaël},
pages={441--470},
booktitle={Reinforcement Learning},
location={Berlin, Heidelberg},
doi={10.1007/978-3-642-27645-3///////\\_14},
publisher={Springer Berlin Heidelberg},
title={Game Theory and Multi-agent Reinforcement Learning},
}

@article{2012FriedrichSpike,
editor={Sporns, Olaf},
groups={Game Theory},
issn={1553-7358},
pages={e1002691},
urldate={2019-03-28},
volume={8},
journaltitle={PLoS Computational Biology},
doi={10.1371/journal.pcbi.1002691},
date={2012-09-27},
number={9},
langid={english},
title={Spike-based Decision Learning of Nash Equilibria in Two-Player Games},
author={Friedrich, Johannes and Senn, Walter},
abstract={Humans and animals face decision tasks in an uncertain multi-agent environment where an agent’s strategy may change in time due to the co-adaptation of others strategies. The neuronal substrate and the computational algorithms underlying such adaptive decision making, however, is largely unknown. We propose a population coding model of spiking neurons with a policy gradient procedure that successfully acquires optimal strategies for classical game-theoretical tasks. The suggested population reinforcement learning reproduces data from human behavioral experiments for the blackjack and the inspector game. It performs optimally according to a pure (deterministic) and mixed (stochastic) Nash equilibrium, respectively. In contrast, temporal-difference(TD)-learning, covariance-learning, and basic reinforcement learning fail to perform optimally for the stochastic strategy. Spike-based population reinforcement learning, shown to follow the stochastic reward gradient, is therefore a viable candidate to explain automated decision learning of a Nash equilibrium in two-player games.},
}

@article{2010SchusterApplication,
langid={english},
pages={1--12},
issn={1687-7470},
date={2010},
volume={2010},
author={Schuster, Alfons and Yamaguchi, Yoko},
urldate={2019-03-28},
title={Application of Game Theory to Neuronal Networks},
journaltitle={Advances in Artificial Intelligence},
doi={10.1155/2010/521606},
groups={Game Theory},
}

@article{2010ElkindAlgorithmic,
title={Algorithmic Game Theory and Artificial Intelligence},
url={https://aaai.org/ojs/index.php/aimagazine/article/view/2310},
issn={0738-4602},
doi={10.1609/aimag.v31i4.2310},
pages={9},
volume={31},
author={Elkind, Edith and Leyton-Brown, Kevin},
date={2010-09-20},
journaltitle={AI Magazine},
urldate={2019-03-26},
groups={Game Theory},
abstract={We brieﬂy survey the rise of game theory as a topic of study in artiﬁcial intelligence, and explain the term algorithmic game theory. We then describe three broad areas of current inquiry by AI researchers in algorithmic game theory: game playing, social choice, and mechanism design. Finally, we give short summaries of each of the six articles appearing in this issue.},
number={4},
langid={english},
}

@article{2009SgroiLearning,
issn={0167-2681},
number={1},
urldate={2019-03-28},
shorttitle={Learning to play games},
title={Learning to play games: Neural networks as bounded-rational players},
date={2009-01},
journaltitle={Journal of Economic Behavior & Organization},
groups={Game Theory},
langid={english},
pages={27--38},
url={https://linkinghub.elsevier.com/retrieve/pii/S0167268108001959},
volume={69},
abstract={We present a neural network methodology for learning game-playing rules in general. Existing research suggests learning to …nd a Nash equilibrium in a new game is too di¢ cult a task for a neural network, but says little about what it will do instead. We observe that a neural network trained to …nd Nash equilibria in a known subset of games, will use self-taught rules developed endogenously when facing new games. These rules are close to payo¤ dominance and its best response. Our …ndings are consistent with existing experimental results, both in terms of subject’s methodology and success rates.},
doi={10.1016/j.jebo.2008.09.008},
author={Sgroi, Daniel and Zizzo, Daniel John},
}

@collection{2007NisanAlgorithmic,
groups={Game Theory},
location={Cambridge ; New York},
editor={Nisan, Noam},
langid={english},
pagetotal={754},
title={Algorithmic game theory},
publisher={Cambridge University Press},
note={OCLC: ocn122526907},
keywords={Algorithms, Game theory},
date={2007},
isbn={978-0-521-87282-9},
}

@misc{2006BlumMachine,
date={2006},
groups={Game Theory},
langid={english},
pages={18},
title={Machine Learning, Game Theory, and Mechanism Design for a Networked World},
author={Blum, A. and Blum, M. and Kearns, M. and Sandholm, T. and Hajiaghayi, M. T.},
}

@article{2005DarmonConvergence,
title={Convergence of reinforcement learning to Nash equilibrium: A search-market experiment},
langid={english},
date={2005-09},
urldate={2019-03-28},
doi={10.1016/j.physa.2005.02.074},
number={1},
pages={119--130},
author={Darmon, Eric and Waldeck, Roger},
issn={0378-4371},
groups={Game Theory},
volume={355},
url={https://linkinghub.elsevier.com/retrieve/pii/S0378437105002839},
journaltitle={Physica A: Statistical Mechanics and its Applications},
abstract={Since the introduction of Reinforcement Learning (RL) in Game Theory, a growing literature is concerned with the theoretical convergence of RL-driven outcomes towards Nash equilibrium. In this paper, we apply this issue to a search-theoretic framework (posted-price market) where sellers are confronted with a population of imperfectly informed buyers and take one decision per period (posted prices) with no direct interactions between sellers. We focus on three different scenarios with varying buyers’ characteristics. For each of these scenarios, we quantitatively and qualitatively test whether the learned variable (price strategy) converges to the Nash equilibrium. We also study the impact of the temperature parameter (deﬁning the exploitation/exploration trade off) on these results.},
shorttitle={Convergence of reinforcement learning to Nash equilibrium},
}

@unpublished{2005AgrawalWhen,
date={2005},
title={When Machine Learning Meets AI and Game Theory},
langid={english},
abstract={We study the problem of development of intelligent machine learning applications to exploit the problems of adaptation that arise in multi-agent systems, for expected-long-termproﬁt maximization. We present two results. First, we propose a learning algorithm for the Iterated Prisoners Dilemma (IPD) problem. Using numerical analysis we show that it performs strictly better than the tit-for-tat algorithm and many other adaptive and non-adaptive strategies. Second, we study the same problem from the aspect of zero-sum games. We discuss how AI and Machine Learning techniques work closely to give our agent a ’mind-reading’ capability.},
groups={Game Theory},
pages={5},
author={Agrawal, Anurag and Jaiswal, Deepak},
}

@inproceedings{2003WangReinforcement,
abstract={Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.},
date={2003},
title={Reinforcement learning to play an optimal Nash equilibrium in team Markov games},
booktitle={Advances in neural information processing systems},
groups={Game Theory},
author={Wang, Xiaofeng and Sandholm, Tuomas},
pages={1603--1610},
}

@incollection{2002TennenholtzGame,
date={2002},
pages={49--58},
urldate={2019-03-26},
doi={10.1007/3-540-45634-1///////\\_4},
publisher={Springer Berlin Heidelberg},
volume={2403},
author={Tennenholtz, Moshe},
editorb={Goos, G. and Hartmanis, J. and van Leeuwen, J.},
isbn={978-3-540-43962-2},
groups={Game Theory},
editorbtype={redactor},
location={Berlin, Heidelberg},
booktitle={Foundations and Applications of Multi-Agent Systems},
langid={english},
abstract={Game Theory and Artiﬁcial Intelligence are two mature areas of research, originating from similar roots, which have taken diﬀerent research directions in the last 50 years. Recent research however shows that the connections between these areas are deep, and that the time had come for bridging the gap between these research disciplines. In this paper we concentrate on basic issues in representation, reasoning, and learning, and discuss work that lies in the intersection of Artiﬁcial Intelligence and Game Theory, for each of these subjects.},
title={Game Theory and Artificial Intelligence},
editor={d’Inverno, Mark and Luck, Michael and Fisher, Michael and Preist, Chris},
}

@inproceedings{2014GoodfellowGenerative,
journaltitle={Advances in Neural Information Processing Systems},
keywords={seminal},
booktitle={Procedings},
eprint={arXiv:1406.2661v1},
url={http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
date={2014},
langid={english},
doi={10.1001/jamainternmed.2016.8245},
abstract={We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title={Generative Adversarial Nets},
pages={9},
mendeley-tags={seminal},
arxivid={arXiv:1406.2661v1},
groups={Seminal},
eprinttype={arXiv},
issn={1049-5258},
}

@thesis{2011EroecalAlgebraic,
institution={Johannes Kepler University Linz},
title={Algebraic Extensions for Symbolic Summation},
type={phdthesis},
author={Eröcal, Burçin},
groups={Mathematics},
abstract={The main result of this thesis is an eﬀective method to extend Karr’s symbolic summation framework to algebraic extensions. These arise, for example, when working with expressions involving (−1)n. An implementation of this method, including a modernised version of Karr’s algorithm is also presented. Karr’s algorithm is the summation analogue of the Risch algorithm for indeﬁnite integration. In the summation case, towers of specialized diﬀerence ﬁelds called ΠΣ-ﬁelds are used to model nested sums and products. This is similar to the way elementary functions involving nested logarithms and exponentials are represented in diﬀerential ﬁelds in the integration case. In contrast to the integration framework, only transcendental extensions are allowed in Karr’s construction. Algebraic extensions of ΠΣ-ﬁelds can even be rings with zero divisors. Karr’s methods rely heavily on the ability to solve ﬁrst-order linear diﬀerence equations and they are no longer applicable over these rings. Based on Bronstein’s formulation of a method used by Singer for the solution of diﬀerential equations over algebraic extensions, we transform a ﬁrst-order linear equation over an algebraic extension to a system of ﬁrst-order equations over a purely transcendental extension ﬁeld. However, this domain is not necessarily a ΠΣ-ﬁeld. Using a structure theorem by Singer and van der Put, we reduce this system to a single ﬁrst-order equation over a ΠΣ-ﬁeld, which can be solved by Karr’s algorithm. We also describe how to construct towers of diﬀerence ring extensions on an algebraic extension, where the same reduction methods can be used. A common bottleneck for symbolic summation algorithms is the computation of nullspaces of matrices over rational function ﬁelds. We present a fast algorithm for matrices over Q(x) which uses fast arithmetic at the hardware level with calls to BLAS subroutines after modular reduction. This part is joint work with Arne Storjohann.},
date={2011},
langid={english},
}

@misc{Kotlin,
title={Kotlin Language Documentation},
groups={Programming},
}

@article{LowtherTeaching,
langid={english},
abstract={This paper describes the authors’ approach of introducing important concepts and algorithms of B-splines to junior computer science students with the help of a pedagogical tool DesignMentor. This approach is non-mathematical and intuitive, and has been used and reﬁned in the past six years.},
pages={5},
title={Teaching B-splines Is Not Difficult!},
groups={Computer Graphics},
author={Lowther, John and Shene, Ching-Kuang},
}

@unpublished{2008LycheSpline,
groups={Computer Graphics},
author={Lyche, Tom and Mørken, Knut},
year={2008},
title={Spline Methods.pdf},
date={2008},
doi={10/undervisningsmateriale},
note={draft},
}

@incollection{1999PollockSmoothing,
date={1999},
booktitle={Handbook of Time Series Analysis, Signal Processing, and Dynamics},
author={Pollock, D. S. G.},
title={Smoothing with Cubic Splines},
urldate={2019-03-28},
langid={english},
publisher={Elsevier},
pages={293--322},
url={https://linkinghub.elsevier.com/retrieve/pii/B9780125609906500130},
groups={Computer Graphics},
doi={10.1016/B978-012560990-6/50013-0},
isbn={978-0-12-560990-6},
}

@article{1996Loesinusoidal,
pages={383--393},
url={http://linkinghub.elsevier.com/retrieve/pii/0377042795002413},
urldate={2019-03-28},
journaltitle={Journal of Computational and Applied Mathematics},
author={Loe, K. F.},
number={2},
title={A sinusoidal polynomial spline and its Bezier blended interpolant},
abstract={Functional polynomials composed of sinusoidal functions are introduced as basis functions to construct an interpolatory spline. An interpolant constructed in this way does not require solving a system of linear equations as many approaches do. However there are vanishing tangent vectors at the interpolating points. By blending with a Bezier curve using the data points as the control points, the blended curve is a proper smooth interpolant. The blending factor has the effect similar to the "tension" control of tension splines. Piecewise interpolants can be constructed in an analogous way as a connection of Bezier curve segments to achieve C 1 continuity at the connecting points. Smooth interpolating surface patches can also be defined by blending sinusoidal polynomial tensor surfaces and Bezier tensor surfaces. The interpolant can very efficiently be evaluated by tabulating the sinusoidal function.},
groups={Computer Graphics},
langid={english},
date={1996-07},
doi={10.1016/0377-0427(95)00241-3},
issn={0377-0427},
volume={71},
}

@article{1986HobbySmooth,
abstract={We present a system of interpolating splines with first and approximate second order geometric continuity. The curves are easily computed in linear time by solving a system of linear equations without the need to resort to any kind of successive approximation scheme. Emphasis is placed on the need to find aesthetically pleasing curves in a wide range of circumstances; favorable results are obtained even when the knots are very unequally spaced or widely separated. The curves are invariant under scaling, rotation, and reflection, and the effects of a local change fall off exponentially as one moves away from the disturbed knot.},
groups={Computer Graphics},
year={1986},
journaltitle={Discrete & Computational Geometry},
pages={123--140},
number={2},
volume={1},
date={1986},
month={6},
publisher={Springer Nature},
title={Smooth, easy to compute interpolating splines},
journal={Discrete & Computational Geometry},
issn={0179-5376},
langid={english},
urldate={2019-03-31},
author={Hobby, John D.},
doi={10.1007/BF02187690},
}

@phdthesis{2009LevienSpiral,
keywords={splines, elastica, euler spirals, curves},
author={Raph Levien},
institution={Berkeley},
date={2009},
title={From Spiral to Spline: Optimal Techniques in Interactive Curve Design},
groups={Computer Graphics},
}

@report{1974Smithsmoothing,
date={1974-02-01},
title={A smoothing algorithm using cubic spline functions},
groups={Computer Graphics, tesse:1},
abstract={Two algorithms are presented for smoothing arbitrary sets of data. They are the explicit variable algorithm and the parametric variable algorithm. The former would be used where large gradients are not encountered because of the smaller amount of calculation required. The latter would be used if the data being smoothed were double valued or experienced large gradients. Both algorithms use a least-squares technique to obtain a cubic spline fit to the data. The advantage of the spline fit is that the first and second derivatives are continuous. This method is best used in an interactive graphics environment so that the junction values for the spline curve can be manipulated to improve the fit.},
urldate={2019-03-31},
month={feb,year     = {1974},
url={https://ntrs.nasa.gov/search.jsp?R=19740008165},
author={R. E. Smith},
}

@inproceedings{2009Brunettereview,
date={2009-02},
keywords={agent-based artificial intelligence, artificial consciousness, artificial intelligence, Artificial intelligence, consciouness, embodied artificial intelligence, embodied intelligence, Geometry, Intelligent agent, Intelligent robots, Intelligent structures, Logic, machine intelligence, Machine intelligence, Natural language processing, Neurons, Strips},
author={Brunette, E. S. and Flemmer, R. C. and Flemmer, C. L.},
booktitle={Proceedings of the 4th International Conference on Autonomous Robots and Agents},
groups={tesse:5},
pages={385--392},
publisher={IEEE},
doi={10.1109/icara.2000.4804025},
title={A review of artificial intelligence},
}

@article{2017Costaevaluation,
groups={tesse:5},
date={2017-03},
pages={28--38},
volume={52},
journaltitle={Applied Soft Computing},
doi={10.1016/j.asoc.2016.12.024},
author={Costa, Yandre M. G. and Oliveira, Luiz S. and Silla, Carlos N.},
title={An evaluation of Convolutional Neural Networks for music classification using spectrograms},
}

@book{2014HaganNeural,
url={http://hagan.okstate.edu/nnd.html},
date={2014},
isbn={0-9717321-1-6},
groups={tesse:4},
author={Hagan, Martin T. and Demuth, Howard B. and Beale, Mark H. and Jesús, Orlando De},
publisher={Martin Hagan},
edition={2},
title={Neural Network Design},
}

@article{1989HornikMultilayer,
author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
groups={tesse:5},
volume={2},
title={Multilayer feedforward networks are universal approximators},
date={1989-01},
journaltitle={Neural Networks},
pages={359--366},
number={5},
doi={10.1016/0893-6080(89)90020-8},
}

@inproceedings{2017HutchingsTalkinga,
url={https://arxiv.org/abs/1706.09558},
author={Hutchings, P.},
groups={tesse:5},
booktitle={Proceedings of the First International Workshop on Deep Learning and Music joint with IJCNN},
date={2017},
title={Talking Drums: Generating drum grooves with neural networks.},
pages={43--47},
}

@article{1998LeCunGradient,
number={11},
title={Gradient-based learning applied to document recognition},
author={LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
doi={10.1109/5.726791},
pages={2278--2324},
volume={86},
groups={tesse:5},
date={1998-11},
journaltitle={Proceedings of the IEEE},
keywords={Hidden Markov models, Feature extraction, Neural networks, 2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, Machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
issn={0018-9219},
}

@book{2017MinskyPerceptrons,
publisher={MIT Press},
title={Perceptrons: An Introduction to Computational Geometry},
isbn={0-262-53477-0},
author={Minsky, Marvin and Papert, Seymour A.},
location={MA},
date={2017-09-22},
keywords={linear-classification neural-networks seminal},
url={https://www.ebook.de/de/product/28875910/marvin///////\\_minsky///////\\_seymour///////\\_a///////\\_papert///////\\_perceptrons///////\\_an///////\\_introduction///////\\_to///////\\_computational///////\\_geometry.html},
pagetotal={316},
}

@article{2006ReiffensteinCodification,
volume={50},
groups={tesse:5},
title={Codification, patents and the geography of knowledge transfer in the electronic musical instrument industry},
pages={298--318},
number={3},
abstract={Recent research in economic geography has emphasized tacit knowledge as the basis of industrial learning. In contrast, codification and the practices of industrial writing have received little attention for the roles they play in mobilizing knowledge across space. This paper offers insight into the geographies of codification through an examination of technology transfer in the electronic musical instrument industry between 1965 and 1995. The research draws on a variety of primary and secondary data that include interviews with inventors, biographical accounts and patent analysis. These sources offer perspective on the career trajectories of three U.S. inventors who transferred knowledge from various contexts in California's high-tech industry to the Japanese firm, Yamaha. Conceptually, the paper draws on the actor–network theory and Latour's idea of translation to highlight the detours inventors must take to register novelty. The analysis reveals the problematic nature of codified knowledge and its transfer; in this case codified knowledge was mobile internationally but not locally, at least until it reached Japan. The paper argues for the need to understand how texts such as patents are produced—the context of their authorship, the geographies of their circulation and their efficacy for shaping further innovative practice.},
journaltitle={The Canadian Geographer / Le Géographe canadien},
date={2006-09},
doi={10.1111/j.1541-0064.2006.00143.x},
author={Reiffenstein, Tim},
}

@article{2015SchmidhuberDeep,
date={2015-01},
doi={10.1016/j.neunet.2014.09.003},
volume={61},
title={Deep learning in neural networks: An overview},
pages={85--117},
author={Schmidhuber, Jürgen},
journaltitle={Neural Networks},
}

@article{2016SigtiaEnd,
author={Sigtia, S. and Benetos, E. and Dixon, S.},
number={5},
volume={24},
doi={10.1109/TASLP.2016.2533858},
groups={tesse:5},
title={An End-to-End Neural Network for Polyphonic Piano Music Transcription},
date={2016-05},
keywords={Hidden Markov models, recurrent neural network, Recurrent neural networks, Acoustics, recurrent neural networks, music information retrieval, Spectrogram, Feature extraction, Computational modeling, deep learning, Deep Learning, Recurrent Neural Networks, acoustic model, acoustic models, audio frame, Automatic music transcription, Automatic Music Transcription, beam search algorithm, end-to-end neural network, evaluation metrics, language model predictions, MIR, music language model, music language model predictions, music language models, Music Language Models, polyphonic music, polyphonic piano music transcription, probabilistic graphical model, speech recognition systems, supervised neural network model, unsupervised acoustic models},
journaltitle={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
issn={2329-9290},
pages={927--939},
}

@article{2007StanleyCompositional,
title={Compositional pattern producing networks: A novel abstraction of development},
volume={8},
journaltitle={Genetic Programming and Evolvable Machines},
number={2},
pages={131--162},
author={Stanley, Kenneth O.},
date={2007-05},
doi={10.1007/s10710-007-9028-8},
groups={tesse:5},
}

@article{2019Wang3D,
langid={english},
eprinttype={arxiv},
title={3D Human Pose Machines with Self-supervised Learning},
author={Wang, Keze and Lin, Liang and Jiang, Chenhan and Qian, Chen and Wei, Pengxu},
journaltitle={arXiv:1901.03798 [cs]},
url={http://arxiv.org/abs/1901.03798},
abstract={Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions.},
urldate={2019-03-26},
groups={tesse:5},
date={2019-01-11},
eprint={1901.03798},
}

@phdthesis{2004TraubeInterdisciplinary,
institution={McGill University},
abstract={This dissertation proposes an interdisciplinary approach for the study of the timbre of the classical guitar. We start by identifying the static control parameters of timbre, relating to the structural components of the guitar and the dynamic control parameters of timbre, relating to the gestures applied by the performer on the instrument. From the plucked string physical model (obtained from the tranverse wave equation), we derive a digital signal interpretation of the plucking eﬀect which is a comb ﬁltering. Then we investigate how subjective characteristics of sound, like timbre, are related to gesture parameters. The starting point for exploration is an inventory of verbal descriptors commonly used by professional musicians to describe the brightness, the colour, the shape and the texture of the sounds they produce on their instruments. An explanation for the voice-like nature of guitar tones is proposed based on the observation that the maxima of the comb-ﬁltershaped magnitude spectrum of guitar tones are located at frequencies similar to the formant frequencies of a subset of identiﬁable vowels. These analogies at the spectral level might account for the origin of some timbre descriptors such as open, oval, round, thin, closed, nasal and hollow, that seem to refer to phonetic gestures. In a experiment conducted to conﬁrm these analogies, participants were asked to associate a consonant to the attack and a vowel to the decay of guitar tones. The results of this study support the idea that some perceptual dimensions of the guitar timbre space can be borrowed from phonetics. Finally, we address the problem of the indirect acquisition of instrumental gesture parameters. Pursuing previous research on the estimation of the plucking position from a recording, we propose a new estimation method based on an iterative weighted least-square algorithm, starting from a ﬁrst approximation derived from a variant of the autocorrelation function of the signal.},
langid={english},
title={An Interdisciplinary Study of the Timbre of the Classical Guitar.},
pages={237},
author={Traube, Caroline},
date={2004},
groups={tesse:5},
}

@inproceedings{2005Florianreinforcement,
eventtitle={Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)},
doi={10.1109/SYNASC.2005.13},
booktitle={Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)},
date={2005},
publisher={IEEE},
isbn={978-0-7695-2453-5},
abstract={The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-ﬁre neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the ﬁring of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efﬁcacy of the algorithm is veriﬁed in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spiketiming-dependent synaptic changes of one sign with nonassociative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologicallyplausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
urldate={2019-03-26},
langid={english},
author={Florian, R. V.},
groups={tesse:5},
location={Timisoara, Romania},
url={http://ieeexplore.ieee.org/document/1595864/},
title={A reinforcement learning algorithm for spiking neural networks},
pages={8},
}

@article{2015MnihHuman,
url={http://www.nature.com/articles/nature14236},
doi={10.1038/nature14236},
journaltitle={Nature},
urldate={2019-03-26},
volume={518},
groups={tesse:5},
langid={english},
number={7540},
issn={0028-0836, 1476-4687},
author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
date={2015-02},
pages={529--533},
title={Human-level control through deep reinforcement learning},
}

@misc{2013MnihPlaying,
author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
eprint={1312.5602},
eprintclass={cs.LG},
title={Playing Atari with Deep Reinforcement Learning},
date={2013},
eprinttype={arXiv},
groups={tesse:5},
}

@article{1999ChellapillaEvolution,
abstract={Intelligence pertains to the ability to make appropriate decisions in light of specific goals and to adapt behavior to meet those goals in a range of environments. Mathematical games provide a framework for studying intelligent behavior in models of real-world settings or restricted domains. The behavior of alternative strategies in these games is defined by each individual’s stimulus-response mapping. Limiting these behaviors to linear functions of the environmental conditions renders the results to be little more than a façade: Effective decision making in any complex environment almost always requires nonlinear stimulus-response mappings. The obstacle then comes in choosing the appropriate representation and learning algorithm. Neural networks and evolutionary algorithms provide useful means for addressing these issues. This paper describes efforts to hybridize neural and evolutionary computation to learn appropriate strategies in zeroand nonzero-sum games, including the iterated prisoner’s dilemma, tic-tac-toe, and checkers. With respect to checkers, the evolutionary algorithm was able to discover a neural network that can be used to play at a near-expert level without injecting expert knowledge about how to play the game. The implications of evolutionary learning with respect to machine intelligence are also discussed. It is argued that evolution provides the framework for explaining naturally occurring intelligent entities and can be used to design machines that are also capable of intelligent behavior.},
langid={english},
journaltitle={Proceedings of the IEEE},
groups={tesse:5},
date={1999-09},
doi={10.1109/5.784222},
author={Chellapilla, K. and Fogel, D. B.},
number={9},
issn={0018-9219},
pages={1471--1496},
url={http://ieeexplore.ieee.org/document/784222/},
title={Evolution, neural networks, games, and intelligence},
urldate={2019-03-26},
volume={87},
}

@article{2014Sigtiarnn,
pages={6},
abstract={In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a signiﬁcant 3 /file     = {:done/2014SigtiaRnn An Rnn Based Music Language Model for Improving Automatic Music Transcription.pdf:application/pdf},
author={Sigtia, Siddharth and Benetos, Emmanouil and Cherla, Srikanth and Weyde, Tillman},
groups={tesse:2},
langid={english},
title={An rnn-based music language model for improving automatic music transcription},
date={2014},
}

@article{2006BagozziOpen,
author={Bagozzi, Richard P. and Dholakia, Utpal M.},
date={2006-07},
volume={52},
doi={10.1287/mnsc.1060.0545},
number={7},
urldate={2019-03-26},
langid={english},
shorttitle={Open Source Software User Communities},
issn={0025-1909, 1526-5501},
journaltitle={Management Science},
title={Open Source Software User Communities: A Study of Participation in Linux User Groups},
pages={1099--1115},
}

@incollection{2015Daskinp,
publisher={Springer International Publishing},
date={2015},
pages={21--45},
urldate={2019-03-26},
doi={10.1007/978-3-319-13111-5///////\\_2},
location={Cham},
booktitle={Location Science},
editor={Laporte, Gilbert and Nickel, Stefan and Saldanha da Gama, Francisco},
title={The p-Median Problem},
langid={english},
isbn={978-3-319-13110-8 978-3-319-13111-5},
groups={tesse:5},
author={Daskin, Mark S. and Maass, Kayse Lee},
abstract={The p-median problem is central to much of discrete location modeling and theory. While the p-median problem is NP-hard on a general graph, it can be solved in polynomial time on a tree. A linear time algorithm for the 1-median problem on a tree is described. We also present a classical formulation of the problem. Basic construction and improvement algorithms are outlined. Results from the literature using various metaheuristics including tabu search, heuristic concentration, genetic algorithms, and simulated annealing are summarized. A Lagrangian relaxation approach is presented and used for computational results on 40 classical test instances as well as a 500-node instance derived from the most populous counties in the contiguous United States. We conclude with a discussion of multi-objective extensions of the p-median problem.},
}

@book{2014GanderScientific,
location={Cham},
publisher={Springer International Publishing},
title={Scientific Computing - An Introduction using Maple and MATLAB},
author={Gander, Walter and Gander, Martin J. and Kwok, Felix},
volume={11},
langid={english},
groups={tesse:5},
series={Texts in Computational Science and Engineering},
urldate={2019-03-26},
date={2014},
isbn={978-3-319-04324-1 978-3-319-04325-8},
doi={10.1007/978-3-319-04325-8},
}

@incollection{2011VerterUncapacitated,
editor={Eiselt, H. A. and Marianov, Vladimir},
publisher={Springer US},
booktitle={Foundations of Location Analysis},
volume={155},
urldate={2019-03-26},
isbn={978-1-4419-7571-3 978-1-4419-7572-0},
title={Uncapacitated and Capacitated Facility Location Problems},
doi={10.1007/978-1-4419-7572-0///////\\_2},
location={Boston, MA},
pages={25--37},
groups={tesse:5},
langid={english},
author={Verter, Vedat},
date={2011},
}

@inproceedings{2018FukumotoGeneration,
abstract={We propose a modiﬁcation of generative adversarial networks (GANs) that generate illustrations of human ﬁgures from given poses represented by stick ﬁgures. In recent years, while various methods that generate images of characters using GANs have been proposed, it is not yet possible for users to freely designate poses of human ﬁgures. When generating an image of a character, the pose of the character takes is an important component of its composition. Thus it is necessary fora user who wants to create an illustration to be able to specify the pose easily. We collected a set of illustrations of human ﬁgures from the internet, and for each illustration, a simple line drawing that speciﬁes the pose was drawn manually. We constructed a GAN that takes a line drawing as its input and creates an illustration of a person in a pose that matches the line drawing. These networks are learned using the data set we prepared. In this paper, we propose a new network architecture. After constructing two networks both of which have almost the same structure as pix2pix, which is a variant model of GANs, we stack up those networks based on the idea of stack GAN. The experimental results show that, from stick ﬁgures representing common poses such as a standing pose, our methods was able to successfully generate images of characters. However, in the case of stick ﬁgures having rare poses that were not in the dataset, such as ﬁgures raising a hand or lying down, the generated images were blurred and not of a high-quality but still had the desired shapes. By expanding the dataset to include various poses, it is possible to generate diverse poses more precisely.},
isbn={978-1-5386-2666-5},
publisher={IEEE},
location={Tokyo, Japan},
url={https://ieeexplore.ieee.org/document/8377853/},
langid={english},
doi={10.1109/COMPSAC.2018.10225},
eventtitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},
title={Generation of Character Illustrations from Stick Figures Using a Modification of Generative Adversarial Network},
urldate={2019-03-26},
date={2018-07},
author={Fukumoto, Yuuya and Shimizu, Daiki and Shibata, Chihiro},
pages={183--186},
booktitle={2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},
}

@article{2018ChanEverybody,
date={2018-08-22},
title={Everybody Dance Now},
url={http://arxiv.org/abs/1808.07371},
langid={english},
groups={tesse:5},
eprint={1808.07371},
abstract={This paper presents a simple method for “do as I do" motion transfer: given a source video of a person dancing we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We pose this problem as a per-frame image-to-image translation with spatio-temporal smoothing. Using pose detections as an intermediate representation between source and target, we learn a mapping from pose images to a target subject’s appearance. We adapt this setup for temporally coherent video generation including realistic face synthesis. Our video demo can be found at https://youtu.be/PCBTZh41Ris.},
author={Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros, Alexei A.},
journaltitle={arXiv:1808.07371 [cs]},
urldate={2019-03-26},
eprinttype={arxiv},
}

@article{2018CaoReview,
pages={278--287},
urldate={2019-03-26},
abstract={In big data ﬁelds, with increasing computing capability, artiﬁcial neural networks have shown great strength in solving data classiﬁcation and regression problems. The traditional training of neural networks depends generally on the error back propagation method to iteratively tune all the parameters. When the number of hidden layers increases, this kind of training has many problems such as slow convergence, time consuming, and local minima. To avoid these problems, neural networks with random weights (NNRW) are proposed in which the weights between the hidden layer and input layer are randomly selected and the weights between the output layer and hidden layer are obtained analytically. Researchers have shown that NNRW has much lower training complexity in comparison with the traditional training of feed-forward neural networks. This paper objectively reviews the advantages and disadvantages of NNRW model, tries to reveal the essence of NNRW, gives our comments and remarks on NNRW, and provides some useful guidelines for users to choose a mechanism to train a feed-forward neural network.},
volume={275},
issn={0925-2312},
groups={tesse:5},
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S0925231217314613},
title={A review on neural networks with random weights},
date={2018-01},
doi={10.1016/j.neucom.2017.08.040},
journaltitle={Neurocomputing},
author={Cao, Weipeng and Wang, Xizhao and Ming, Zhong and Gao, Jinzhu},
}

@article{2017LinRecurrent,
author={Lin, Mude and Lin, Liang and Liang, Xiaodan and Wang, Keze and Cheng, Hui},
urldate={2019-03-26},
groups={tesse:5},
eprint={1707.09695},
date={2017-07-30},
langid={english},
title={Recurrent 3D Pose Sequence Machines},
abstract={3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.},
eprinttype={arxiv},
url={http://arxiv.org/abs/1707.09695},
journaltitle={arXiv:1707.09695 [cs]},
}

@article{2016GiryesDeep,
volume={64},
title={Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?},
pages={3444--3457},
doi={10.1109/TSP.2016.2546221},
eprint={1504.08291},
urldate={2019-03-26},
langid={english},
eprinttype={arxiv},
shorttitle={Deep Neural Networks with Random Gaussian Weights},
issn={1053-587X, 1941-0476},
journaltitle={IEEE Transactions on Signal Processing},
number={13},
url={http://arxiv.org/abs/1504.08291},
abstract={Three important properties of a classiﬁcation machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisﬁed by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
author={Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
date={2016-07-01},
}

@article{2014RingbauerExperimental,
number={1},
urldate={2019-03-26},
volume={5},
langid={english},
groups={tesse:5},
doi={10.1038/ncomms5145},
url={http://www.nature.com/articles/ncomms5145},
issn={2041-1723},
date={2014-12},
title={Experimental simulation of closed timelike curves},
journaltitle={Nature Communications},
author={Ringbauer, Martin and Broome, Matthew A. and Myers, Casey R. and White, Andrew G. and Ralph, Timothy C.},
}

@inproceedings{2014ToshevDeeppose,
groups={tesse:5},
abstract={We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-ofart or better performance on four academic benchmarks of diverse real-world images.},
pages={1653--1660},
isbn={978-1-4799-5118-5},
doi={10.1109/CVPR.2014.214},
location={Columbus, OH, USA},
booktitle={2014 IEEE Conference on Computer Vision and Pattern Recognition},
urldate={2019-03-26},
author={Toshev, Alexander and Szegedy, Christian},
title={DeepPose: Human Pose Estimation via Deep Neural Networks},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909610},
publisher={IEEE},
date={2014-06},
shorttitle={DeepPose},
eventtitle={2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
langid={english},
}

@article{2017JanochaLoss,
abstract={Deep neural networks are currently among the most commonly used classiﬁers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design – one can conveniently adapt their architecture to speciﬁc needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can ﬁnd impressively wide spread of various conﬁgurations of almost every aspect of the deep nets, one element is, in authors’ opinion, underrepresented – while solving classiﬁcation problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions aﬀect deep models and their learning dynamics, as well as resulting classiﬁers robustness to various eﬀects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justiﬁed classiﬁcation objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassiﬁcation. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.},
langid={english},
eprinttype={arxiv},
title={On Loss Functions for Deep Neural Networks in Classification},
eprint={1702.05659},
url={http://arxiv.org/abs/1702.05659},
date={2017-02-18},
urldate={2019-03-26},
journaltitle={arXiv:1702.05659 [cs]},
groups={tesse:5},
author={Janocha, Katarzyna and Czarnecki, Wojciech Marian},
}

@article{2016RoszkowskaApplication,
journaltitle={Optimum. Studia Ekonomiczne},
doi={10.15290/ose.2016.02.80.11},
publisher={University of Bialystok},
urldate={2019-03-26},
langid={english},
year={2016},
date={2016},
author={Roszkowska, Ewa},
pages={144--162},
url={http://repozytorium.uwb.edu.pl/jspui/handle/11320/4640},
groups={tesse:5},
issn={1506-7637},
title={The Application of UTA Method for Support Evaluation Negotiation Offers},
abstract={The MCDA technique has been extensively and successfully applied for supporting decision making in negotiation processes. The mostly used techniques SAW, AHP or TOPSIS are based on direct preference information which requires from negotiator a clear and precise definition all the parameters of the preference model (e.g. issue weights, option rates, aspiration and reservation values etc.), so those techniques can be successfully applied in well-structured negotiation problems. But, many real negotiation problems are illstructured, that means that the negotiation space is imprecisely defined, and the negotiator’s preferences the vagueness or imperfect.},
number={2},
journal={Optimum. Studia Ekonomiczne},
}

@article{2018StierAnalysing,
urldate={2019-03-26},
groups={tesse:5, Game Theory},
langid={english},
title={Analysing Neural Network Topologies: a Game Theoretic Approach},
date={2018},
shorttitle={Analysing Neural Network Topologies},
volume={126},
journaltitle={Procedia Computer Science},
abstract={Abstract Artiﬁcial Neural Networks have shown impressive success in very diﬀerent application cases. Choosing a proper network architectAurrteiﬁisciaacl rNiteicuarladl eNceistwioonrkfosrhaavneetswhoorwkn’sismupccreessssi,vuesusuacllcyedssoninevineraymdiaﬀneuraelnmt aapnpnleirc.aAtisona cstarsaeisg.hCtfhoorwosairndgsatrpatreogpye,rlnaregtwe,omrkosatrlcyhfiutellcytcuorneniescatecdriatirccahlidteecctiusrieosnafroer saenleectwteodr,kth’sesruebccyersesl,yuinsugaollnyadognoeodinoaptmimanizuaatliomnasntnraetre. gAystaosﬁtrnadigphrtofpoerwr waredigsthrtastewghyi,llearagtet,hme soasmtlye ftuimllye acvoonindeicntgedovaercrﬁhittteincgtu.rHesowareevseerl,elcatregde, pthaerrtseboyf rtehleyiﬁnngaol nneatwgooorkd oarpetirmedizuantidoannts.trIantetghye tboesﬁtncdasper,oplaerrgwe epiagrhtstsowf hthileenaetttwheorskambeectoimmee asivmoipdliyngirroevleevrﬁantttinfogr. Hlaotewreivnefer,relanrcgiengp.aIrntsthoef twhoerﬁstncaal snee,thwiogrhklyaprearraemduentedraiznet.dIanrcthheitebcetsutrceasshei,nldaergr eprpoapretsr oofpttihmeinzaettiwoonraknbdeaclolmowe tshime pealysyircrereleavtiaonnt ofofraldavteerrsienrfiearleenxcaimngp.lIens fthoeolwinogrstht ecanseet,whoigrkh.ly parameterized architectures hinder proper optimization and allow theAeaﬁsrystcsrteeaptioinn roefmaodvvienrgseirriraelleevxaanmtpalrecshfioteocltiunrgalthpearntestwlieosrkin. identifying those parts, which requires measuring the contribution of Aindﬁirvsitdsutaelp cionmrepmonoevnintsg siurrcehleavsannteaurrcohniste. cItnurparlepvaiortussliwesorikn,idheenutriifsytiicnsg bthasoesde poanrtuss, iwnghitchherweqeuigirhets dmisetarsibuuritniognthoef caonnetruibrountioans coof nitnrdibivuitdiounalmceoamsuproenheanvtse ssuhcohwnassonmeuerosuncsc. eIsns,pbruevt idoousnowt oprrko,vihdeeuraisptriocspebratsheedoroenticuaslinugndtehrestwanediginhgt.distribution of a neuron as conTthriebruetfioorne,mineaosuurrwe ohrakvewsehionwvenstsiogmatee gsuacmceestsh,ebourettdicomnoetaspurroevsi,dneaamperloyptehretShheaopreletiycavlauluned(eSrsVta),nidninogrd. er to separate relevant from irreTlheveraenftoprea,rtisn oofurawn oarrktiﬁwceiainl vneesutirgaal tneegtwamorekt.hWeoerebtiecgminebaysudreess,ignnaimngelya tchoeaSlihtiaopnlaelygvaamluee f(oSrVa)n, inarotirﬁdceiratlonseeupraarlanteetrweloervka,nwt fhroemre inrereulreovnasntfopramrtscooafliatnioanrstiaﬁncdialthneeuavraelrangeetwcoornkt.riWbuetiboengsinofbnyeduerosingsnitnogcoaacloitaiolintisonyaielldgatmoethfeorSahnapalretyiﬁcviaalluen.euInraol rndeetrwtoorkm, ewahsuerree nhoeuwrownesllfothrme Schoaapllietiyonvsaluanedmtehaesuarveesrathgee ccoonnttrriibbuuttiioonnsofoifnndeivuirdounasl ntoeucrooanlsit,iownesryemielodvetolotwhe-cSohnatrpibleuytinvgalunee.urIonnosradnedr mtoemaseuarseuirtes himowpawcteollnththeeSnheatpwloeyrkvpaelurfeomrmeaansucree.s the contribution of individual neurons, we remove low-contributing neurons and measure its imIpnacotuornexthpeernimetwenotrskwpeersfhoorwmathnacte.the Shapley value outperforms other heuristics for measuring the contribution of neurons.},
url={https://linkinghub.elsevier.com/retrieve/pii/S187705091831233X},
author={Stier, Julian and Gianini, Gabriele and Granitzer, Michael and Ziegler, Konstantin},
pages={234--243},
doi={10.1016/j.procs.2018.07.257},
issn={1877-0509},
}

@article{ShohamMultiagent,
title={Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
langid={english},
groups={tesse:2, Game Theory},
author={Shoham, Yoav},
pages={532},
}

@book{2018SuttonReinforcement,
edition={Second edition},
pagetotal={526},
isbn={978-0-262-03924-6},
title={Reinforcement learning: an introduction},
author={Sutton, Richard S. and Barto, Andrew G.},
date={2018},
publisher={The MIT Press},
abstract={"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
groups={tesse:5},
location={Cambridge, Massachusetts},
series={Adaptive computation and machine learning series},
keywords={Reinforcement learning},
langid={english},
shorttitle={Reinforcement learning},
}

@inproceedings{2017Pike-burkeOptimistic,
pages={9},
title={Optimistic Planning for the Stochastic Knapsack Problem},
booktitle={Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
author={Pike-Burke, Ciara and Grunewalder, Steﬀen},
langid={english},
date={2017},
abstract={The stochastic knapsack problem is a stochastic resource allocation problem that arises frequently and yet is exceptionally hard to solve. We derive and study an optimistic planning algorithm speciﬁcally designed for the stochastic knapsack problem. Unlike other optimistic planning algorithms for MDPs, our algorithm, OpStoK, avoids the use of discounting and is adaptive to the amount of resources available. We achieve this behavior by means of a concentration inequality that simultaneously applies to capacity and reward estimates. Crucially, we are able to guarantee that the aforementioned conﬁdence regions hold collectively over all time steps by an application of Doob’s inequality. We demonstrate that the method returns an -optimal solution to the stochastic knapsack problem with high probability. To the best of our knowledge, our algorithm is the ﬁrst which provides such guarantees for the stochastic knapsack problem. Furthermore, our algorithm is an anytime algorithm and will return a good solution even if stopped prematurely. This is particularly important given the diﬃculty of the problem. We also provide theoretical conditions to guarantee OpStoK does not expand all policies and demonstrate favorable performance in a simple experimental setting.},
groups={tesse:5},
}

@book{2015ChrupalaLearning,
title={Learning language through pictures},
date={2015},
groups={tesse:5},
langid={english},
isbn={978-1-941643-73-0},
author={Chrupala, Grzegorz and Kadar, Akos and Alishahi, Afra and Zong, Chengqing and Strube, Michael},
publisher={Association for Computational Linguistics},
abstract={We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Like humans, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.},
note={OCLC: 6893507677},
}

@article{DeanApproximating,
author={Dean, Brian C. and Goemans, Michel X. and Vondrak, Jan},
abstract={We consider a stochastic variant of the NP-hard 0/1 knapsack problem in which item values are deterministic and item sizes are independent random variables with known, arbitrary distributions. Items are placed in the knapsack sequentially, and the act of placing an item in the knapsack instantiates its size. Our goal is to compute a solution “policy” that maximizes the expected value of items placed in the knapsack, and we consider both non-adaptive policies (that designate a priori a ﬁxed sequence of items to insert) and adaptive policies (that can make dynamic choices based on the instantiated sizes of items placed in the knapsack thus far). We show that adaptivity provides only a constant-factor improvement by demonstrating a greedy non-adaptive algorithm that approximates the optimal adaptive policy within a factor of 7. We also design an adaptive polynomial-time algorithm which approximates the optimal adaptive policy within a factor of 5 + , for any constant > 0.},
groups={tesse:2},
langid={english},
title={Approximating the Stochastic Knapsack Problem: The Beneﬁt of Adaptivity},
pages={10},
}

@book{1999LevineNew,
date={1999-07},
doi={10.1596/1813-9450-2146},
author={Levine, Ross and Beck, Thorsten},
publisher={The World Bank},
groups={tesse:5},
langid={english},
series={Policy Research Working Papers},
title={A New Database on Financial Development and Structure},
abstract={This paper introduces a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a wide variety of indicators that measure the size, activity and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of nonbank financial institutions and by presenting measures of the size of bond and primary equity markets. This paper describes the sources, the construction and the intuition for the different indicators and presents descriptive statistics.},
urldate={2019-03-26},
}

@article{1995LacherNeural,
doi={10.1016/0377-2217(93)E0274-2},
journaltitle={European Journal of Operational Research},
urldate={2019-03-26},
date={1995-08},
author={Lacher, R. C. and Coats, Pamela K. and Sharma, Shanker C. and Fant, L. Franklin},
title={A neural network for classifying the financial health of a firm},
url={http://linkinghub.elsevier.com/retrieve/pii/0377221793E02742},
volume={85},
abstract={We present here a neural network applied to a universal business problem: the estimation of the future fiscal health of a corporation. The commonly used accounting and financial tool for such classification and prediction is a multiple discriminant analysis (MDA) of financial ratios. But the MDA technique has limitations based on its assumptions of linear separability, multivariate normality, and independence of the predictive variableS. A neural network, being free from such constraining assumptions, is able to achieve superior results. Our neural network model is the Cascade-Correlation architecture recently developed by Scott E. Fahlman and Christian Lebiere at Carnegie Mellon University. This new approach solves the hidden architecture enigma encountered using other types of neural networks. Also, Cascade-Correlation manages error signals in a manner which significantly improves execution speed. Our research is the first to use Cascade-Correlation for corporate health estimation.},
number={1},
groups={tesse:5},
pages={53--65},
langid={english},
issn={0377-2217},
}

@article{2015TeshniziComparison,
doi={10.5455/aim.2015.23.296-300},
issn={0353-8109},
volume={23},
author={Teshnizi, Saeed and Ayatollahi, Sayyed},
journaltitle={Acta Informatica Medica},
url={http://www.scopemed.org/fulltextpdf.php?mno=203919},
abstract={Background and objective: Artificial Neural Networks (ANNs) have recently been applied in situations where an analysis based on the logistic regression (LR) is a standard statistical approach; direct comparisons of the results, however, are seldom attempted. In this study, we compared both logistic regression models and feed-forward neural networks on the academic failure data set. Methods: The data for this study included 18 questions about study situation of 275 undergraduate students selected randomly from among nursing and midwifery and paramedic schools of Hormozgan University of Medical Sciences in 2013. Logistic regression with forward method and feed forward Artificial Neural Network with 15 neurons in hidden layer were fitted to the dataset. The accuracy of the models in predicting academic failure was compared by using ROC (Receiver Operating Characteristic) and classification accuracy. Results: Among nine ANNs, the ANN with 15 neurons in hidden layer was a better ANN compared with LR. The Area Under Receiver Operating Characteristics (AUROC) of the LR model and ANN with 15 neurons in hidden layers, were estimated as 0.55 and 0.89, respectively and ANN was significantly greater than the LR. The LR and ANN models respectively classified 77.5///////\\% and 84.3///////\\% of the students correctly. Conclusion: Based on this dataset, it seems the classification of the students in two groups with and without academic failure by using ANN with 15 neurons in the hidden layer is better than the LR model.},
date={2015},
number={5},
title={A Comparison of Logistic Regression Model and Artificial Neural Networks in Predicting of Student's Academic Failure},
urldate={2019-03-26},
langid={english},
pages={296},
}

@article{2018LehtinenNoise2noise,
langid={english},
journaltitle={arXiv:1803.04189 [cs, stat]},
eprint={1803.04189},
eprinttype={arxiv},
urldate={2019-03-26},
abstract={We apply basic statistical reasoning to signal reconstruction by machine learning — learning to map corrupted observations to clean signals —with a simple and powerful conclusion: under certain common circumstances, it is possible to learn to restore signals without ever observing clean ones, at performance close or equal to training using clean exemplars. We show applications in photographic noise removal, denoising of synthetic Monte Carlo images, and reconstruction of MRI scans from undersampled inputs, all based on only observing corrupted data.},
date={2018-03-12},
title={Noise2Noise: Learning Image Restoration without Clean Data},
groups={tesse:5},
shorttitle={Noise2Noise},
url={http://arxiv.org/abs/1803.04189},
author={Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
}

@inproceedings{2018MasuyamaModal,
abstract={For a musical instrument sound containing partials, or modes, the behavior of modes around the attack time is particularly important. However, accurately decomposing it around the attack time is not an easy task, especially when the onset is sharp. This is because spectra of the modes are peaky while the sharp onsets need a broad one. In this paper, an optimization-based method of modal decomposition is proposed to achieve accurate decomposition around the attack time. The proposed method is formulated as a constrained optimization problem to enforce the perfect reconstruction property which is important for accurate decomposition. For optimization, the alternating direction method of multipliers (ADMM) is utilized, where the update of variables is calculated in closed form. The proposed method realizes accurate modal decomposition in the simulation and real piano sounds.},
author={Masuyama, Yoshiki and Kusano, Tsubasa and Yatabe, Kohei and Oikawa, Yasuhiro},
url={https://ieeexplore.ieee.org/document/8462350/},
urldate={2019-03-26},
booktitle={2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
title={Modal Decomposition of Musical Instrument Sound Via Alternating Direction Method of Multipliers},
location={Calgary, AB},
eventtitle={ICASSP 2018 - 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi={10.1109/ICASSP.2018.8462350},
isbn={978-1-5386-4658-8},
publisher={IEEE},
langid={english},
date={2018-04},
pages={631--635},
}

@article{2018EphratLooking,
issn={0730-0301},
number={4},
pages={1--11},
title={Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation},
eprint={1804.03619},
urldate={2019-03-26},
abstract={We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
date={2018-07-30},
eprinttype={arxiv},
volume={37},
journaltitle={ACM Transactions on Graphics},
groups={tesse:5},
shorttitle={Looking to Listen at the Cocktail Party},
author={Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
langid={english},
url={http://arxiv.org/abs/1804.03619},
doi={10.1145/3197517.3201357},
}

@inproceedings{2017NecasovaSolving,
date={2017},
doi={10.1063/1.4992649},
title={Solving wave equation using finite differences and Taylor series},
pages={480013},
booktitle={AIP Conference Proceedings},
urldate={2019-03-26},
publisher={Author(s)},
abstract={The paper deals with the numerical solution of partial diﬀerential equations (PDEs), especially wave equation. Two methods are used to obtain numerical solution of the wave equation. The Finite Diﬀerence Method (FDM) is used for transformation of wave equation to the system of ordinary diﬀerential equations (ODEs), diﬀerent types of diﬀerence formulas are used. The inﬂuence of arithmetic to higher order diﬀerence formulas is also presented. The Modern Taylor Series Method (MTSM) allows to solve ODEs numerically with extremely high precision. An important feature of this method is an automatic integration order setting, i.e. using as many Taylor series terms as the deﬁned accuracy requires.},
langid={english},
eventtitle={INTERNATIONAL CONFERENCE OF NUMERICAL ANALYSIS AND APPLIED MATHEMATICS (ICNAAM 2016)},
location={Rhodes, Greece},
groups={tesse:5, Finite Difference Methods},
year={2017},
author={Nečasová, Gabriela and Kocina, Filip and Veigend, Petr and Chaloupka, Jan and Šátek, Václav and Kunovský, Jiří},
}

@article{zappi_shader-based_nodate,
author={Zappi, Victor and Allen, Andrew and Fels, Sidney},
title={Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments},
groups={tesse:4},
langid={english},
pages={6},
abstract={Physical modelling is a sophisticated synthesis technique, often used in the design of Digital Musical Instruments (DMIs). Some of the most precise physical simulations of sound propagation are based on Finite-Diﬀerence TimeDomain (FDTD) methods, which are stable, highly parameterizable but characterized by an extremely heavy computational load. This drawback hinders the spread of FDTD from the domain of oﬀ-line simulations to the one of DMIs. With this paper, we present a novel approach to real-time physical modelling synthesis, which implements a 2D FDTD solver as a shader program running on the GPU directly within the graphics pipeline. The result is a system capable of running fully interactive, massively sized simulation domains, suitable for novel DMI design. With the help of diagrams and code snippets, we provide the implementation details of a ﬁrst interactive application, a drum head simulator whose source code is available online. Finally, we evaluate the proposed system, showing how this new approach can work as a valuable alternative to classic GPGPU modelling.},
}

@report{2017EisenachNonparametrically,
date={2017},
pages={23},
year={2017},
groups={tesse:5},
langid={english},
title={Nonparametrically learning activation functions in deep neural nets},
author={Eisenach, Carson and Wang, Zhaoran and Liu, Han},
institution={Princeton University},
abstract={We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justiﬁcation for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15 /\\% relative increase in test performance compared to the baseline.},
type={techreport},
}

@article{2017LouizosLearning,
author={Louizos, Christos and Welling, Max and Kingma, Diederik P.},
urldate={2019-03-26},
eprinttype={arxiv},
langid={english},
url={http://arxiv.org/abs/1712.01312},
abstract={We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by “stretching” a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efﬁcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
eprint={1712.01312},
title={Learning Sparse Neural Networks through $L0$ Regularization},
journaltitle={arXiv:1712.01312 [cs, stat]},
date={2017-12-04},
}

@article{2018IshikawaGeometric,
date={2018-07},
title={Geometric-integration tools for the simulation of musical sounds},
pages={511--540},
journaltitle={Japan Journal of Industrial and Applied Mathematics},
urldate={2019-03-26},
number={2},
doi={10.1007/s13160-017-0292-6},
author={Ishikawa, Ai and Michels, Dominik L. and Yaguchi, Takaharu},
volume={35},
abstract={During the last decade, much attention has been given to sound rendering and the simulation of acoustic phenomena by solving appropriate models described by Hamiltonian partial differential equations. In this contribution, we introduce a procedure to develop appropriate tools inspired from geometric integration in order to simulate musical sounds. Geometric integrators are numerical integrators of excellent quality that are designed exclusively for Hamiltonian ordinary differential equations. The introduced procedure is a combination of two techniques in geometric integration: the semi-discretization method by Celledoni et al. (J Comput Phys 231:6770–6789, 2012) and symplectic partitioned Runge–Kutta methods. This combination turns out to be a right procedure that derives numerical schemes that are effective and suitable for computation of musical sounds. By using this procedure we derive a series of explicit integration algorithms for a simple model describing piano sounds as a representative example for virtual instruments. We demonstrate the advantage of the numerical methods by evaluating a variety of numerical test cases.},
groups={tesse:5},
langid={english},
issn={0916-7005, 1868-937X},
}

@article{2016JaderbergDecoupled,
abstract={Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
urldate={2019-03-26},
url={http://arxiv.org/abs/1608.05343},
author={Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
eprinttype={arxiv},
langid={english},
date={2016-08-18},
title={Decoupled Neural Interfaces using Synthetic Gradients},
groups={tesse:5},
journaltitle={arXiv:1608.05343 [cs]},
eprint={1608.05343},
}

@article{2018HayashiNew,
journaltitle={Network Science},
eprinttype={arxiv},
langid={english},
urldate={2019-03-26},
eprint={1706.03910},
author={Hayashi, Yukio},
doi={10.1017/nws.2017.25},
issn={2050-1242, 2050-1250},
number={1},
pages={54--70},
abstract={Today’s economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading eﬃciency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures.},
title={A new design principle of robust onion-like networks self-organized in growth},
volume={6},
url={http://arxiv.org/abs/1706.03910},
date={2018-03},
groups={tesse:5},
}

@article{2016MoriseWorld,
pages={1877--1884},
langid={english},
shorttitle={WORLD},
author={Morise, Masanori and Yokomori, Fumiya and Ozawa, Kenji},
journaltitle={IEICE Transactions on Information and Systems},
volume={E99.D},
doi={10.1587/transinf.2015EDP7457},
title={WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications},
urldate={2019-03-26},
url={https://www.jstage.jst.go.jp/article/transinf/E99.D/7/E99.D///////\\_2015EDP7457////////\\_article},
date={2016},
abstract={A vocoder-based speech synthesis system, named WORLD, was developed in an eﬀort to improve the sound quality of realtime applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been diﬃcult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The eﬀectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.},
issn={0916-8532, 1745-1361},
number={7},
}

@article{2016SerafinVirtual,
shorttitle={Virtual Reality Musical Instruments},
issn={0148-9267, 1531-5169},
title={Virtual Reality Musical Instruments: State of the Art, Design Principles, and Future Directions},
abstract={The rapid development and availability of low-cost technologies have created a wide interest in virtual reality. In the field of computer music, the term “virtual musical instruments” has been used for a long time to describe software simulations, extensions of existing musical instruments, and ways to control them with new interfaces for musical expression. Virtual reality musical instruments (VRMIs) that include a simulated visual component delivered via a head-mounted display or other forms of immersive visualization have not yet received much attention. In this article, we present a field overview of VRMIs from the viewpoint of the performer. We propose nine design guidelines, describe evaluation methods, analyze case studies, and consider future challenges.},
author={Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C. and Nordahl, Rolf},
doi={10.1162/COMJ///////\\_a///////\\_00372},
groups={tesse:5},
volume={40},
journaltitle={Computer Music Journal},
date={2016-09},
number={3},
pages={22--40},
urldate={2019-03-26},
langid={english},
}

@inproceedings{2016LiaoHow,
langid={english},
title={How Important Is Weight Symmetry in Backpropagation?},
groups={tesse:5},
pages={8},
author={Liao, Qianli and Leibo, Joel Z. and Poggio, Tomaso},
abstract={Gradient backpropagation (BP) requires symmetric feedforward and feedback connections—the same weights must be used for forward and backward passes. This “weight transport problem” (Grossberg 1987) is thought to be one of the main reasons to doubt BP’s biologically plausibility. Using 15 different classiﬁcation datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.’s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter—the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100 /\\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a “Batch Manhattan” (BM) update rule.},
booktitle={Thirtieth AAAI Conference on Artificial Intelligence},
date={2016},
}

@misc{2006DongFinite,
groups={tesse:5, Finite Difference Methods},
author={Dong, Shuonan},
date={2006},
url={https://w3.pppl.gov/m3d/1dwave/2006-04-12///////\\_18.085///////\\_Wave.pdf},
pages={27},
langid={english},
title={Finite Difference Methods for the Hyperbolic Wave Partial Differential Equations},
}

@article{2016BalleEnd,
groups={tesse:5},
abstract={We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear ﬁlters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate–distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate–distortion curve, as speciﬁed by a trade-off parameter. Across an independent set of test images, we ﬁnd that the optimized method generally exhibits better rate–distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
date={2016-11-05},
eprinttype={arxiv},
title={End-to-end Optimized Image Compression},
journaltitle={arXiv:1611.01704 [cs, math]},
eprint={1611.01704},
url={http://arxiv.org/abs/1611.01704},
urldate={2019-03-26},
langid={english},
author={Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
}

@article{2016HighlanderVery,
date={2016-01-25},
langid={english},
author={Highlander, Tyler and Rodriguez, Andres},
title={Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add},
journaltitle={arXiv:1601.06815 [cs]},
url={http://arxiv.org/abs/1601.06815},
eprinttype={arxiv},
urldate={2019-03-26},
eprint={1601.06815},
abstract={Convolutional neural networks (CNNs) are currently state-of-the-art for various classiﬁcation tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to O(N2n2) per kernel per layer where the inputs are N × N arrays and the kernels are n × n arrays. Convolution can be efﬁciently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of O(N2 log2 N) using the fast Fourier transform (FFT). However, the increase in efﬁciency is less signiﬁcant when N n as is the case in CNNs. We mitigate this by using the “overlap-and-add” technique reducing the computational complexity to O(N2 log2 n) per kernel. This method increases the algorithm’s efﬁciency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 × 8 kernel and a 224 × 224 image.},
}

@article{2015ReyesSupersymmetrica,
date={2015-10-13},
url={http://arxiv.org/abs/1510.03735},
title={Supersymmetric features of the Error and Dawson's functions},
groups={tesse:5},
urldate={2019-03-26},
eprint={1510.03735},
langid={english},
eprinttype={arxiv},
author={Reyes, Marco A. and Arcos-Olalla, Rafael},
abstract={Following a letter by Bassett, we show ﬁrst that it is possible to ﬁnd an analytical approximation to the error function in terms of a ﬁnite series of hyperbolic tangents from the supersymmetric (SUSY) solution of the Po¨schl-Teller eigenvalue problem in quantum mechanics (QM). Afterwards, we show that the second order diﬀerential equation for the derivatives of Dawson’s function can be found in another SUSY related eigenvalue problem, where the factorization of the simple harmonic oscillator Hamiltonian renders the wrong-sign Hermite diﬀerential equation, and that Dawson’s second order diﬀerential equation possess a singular SUSY type relation to this equation.},
journaltitle={arXiv:1510.03735 [math-ph, physics:quant-ph]},
}

@article{2015SouzaFilhoMusica,
abstract={The production of sound by acoustic musical instruments is caused by the vibration of a resonant structure that can be described by signal corresponding to the temporal evolution of the vibration associated with the sound pressure. The fact that the sound can be characterized by a set of signals, suggests that a device can generate sound and therefore imitate sounds of acoustic instruments. Such device is called a synthesizer, its main component in sound production is an oscillator. This work presents the synthesis of a classic theme of West Coast Jazz that has as peculiarity an odd metric. All the notes were identiﬁed in the score and synthesized on computer.},
author={Souza Filho, N. E. and Gonçalves, B. A. and Oliveira, V. T.},
urldate={2019-03-26},
date={2015-06},
volume={37},
langid={portuguese},
groups={tesse:5},
title={Música para estudantes de engenharia: Síntese sonora de tema de jazz},
number={2},
doi={10.1590/S1806-11173721804},
journaltitle={Revista Brasileira de Ensino de Física},
issn={1806-1117},
pages={2313--1--2313--10},
url={http://www.scielo.br/scielo.php?script=sci///////\\_arttext///////\\&pid=S1806-11172015000200014///////\\&lng=pt///////\\&tlng=pt},
shorttitle={Música para estudantes de engenharia},
}

@article{2015YoungHci,
urldate={2019-03-26},
abstract={Here we present an analysis of literature relating to the evaluation methodologies of Digital Musical Instruments (DMIs) derived from the field of Human Computer Interaction (HCI). We then apply choice aspects from these existing evaluation models and apply them to an optimized evaluation for assessing new DMIs.},
groups={tesse:5},
journaltitle={Unpublished},
title={HCI Models for Digital Musical Instruments: Methodologies for Rigorous Testing of Digital Musical Instruments},
langid={english},
doi={10.13140/rg.2.1.3949.9364},
shorttitle={HCI Models for Digital Musical Instruments},
date={2015},
author={Young, Gareth and Murphy, David},
}

@article{2014VincentEfficient,
abstract={An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D × d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efﬁcient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields aofsptheeedcuopmopfut4Dadt,ioi.nes.},
title={Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets},
author={Vincent, Pascal and de Brébisson, Alexandre and Bouthillier, Xavier},
journaltitle={arXiv:1412.7091 [cs]},
eprint={1412.7091},
eprinttype={arxiv},
url={http://arxiv.org/abs/1412.7091},
date={2014-12-22},
langid={english},
urldate={2019-03-26},
}

@inproceedings{2015HarrisonAlgorithm,
language={English},
author={Harrison, Reginald L. and Bilbao, Stefan and Perry, James},
date={2015-11-30},
day={30},
title={An algorithm for a valved brass instrument synthesis environment using finite-difference time-domain methods with performance optimisation},
booktitle={Proceedings of the 18th International Conference on Digital Audio Effects},
abstract={This paper presents a physical modelling sound synthesis environment for the production of valved brass instrument sounds. The governing equations of the system are solved using finite-difference time-domain (FDTD) methods and the environment is implemented in the C programming language. Users of the environment can create their own custom instruments and are able to control player parameters such as lip frequency, mouth pressure and valve openings through the use of instrument and score files.The algorithm for sound synthesis is presented in detail along with a discussion of optimisation methods used to reduce run time. Binaries for the environment are available for download online for multiple platforms.},
groups={tesse:5, Finite Difference Methods},
}

@article{2014ChoProperties,
author={Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
eprint={1409.1259},
abstract={Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a ﬁxed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we ﬁnd that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
langid={english},
title={On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
groups={tesse:5},
shorttitle={On the Properties of Neural Machine Translation},
url={http://arxiv.org/abs/1409.1259},
date={2014-09-03},
journaltitle={arXiv:1409.1259 [cs, stat]},
eprinttype={arxiv},
urldate={2019-03-26},
}

@article{2015HilleHow,
doi={10.1016/j.econedurev.2014.10.007},
author={Hille, Adrian and Schupp, Jürgen},
langid={english},
groups={tesse:5},
url={https://linkinghub.elsevier.com/retrieve/pii/S0272775714000995},
title={How learning a musical instrument affects the development of skills},
volume={44},
journaltitle={Economics of Education Review},
date={2015-02},
pages={56--82},
issn={0272-7757},
abstract={Despite numerous studies on skill development, we know little about the effects of extracurricular music activities on cognitive and non-cognitive skills. This study examines how music training during childhood and youth affects the development of cognitive skills, school grades, personality, time use and ambition using data from the German Socio-Economic Panel (SOEP). Our ﬁndings suggest that adolescents with music training have better school grades, are more conscientious, open and ambitious. These effects are stronger among adolescents from lower socio-economic status. In order to address the non-random selection into playing music, we take into account detailed information on the child and its parents, which may determine both the decision to pursue music lessons and educational outcomes. While lacking truly exogenous variations in music activities, our results are robust to a large range of sensitivity tests. We thereby approach causality better than previous observational studies.},
urldate={2019-03-26},
}

@article{2015ChatziioannouEnergy,
urldate={2019-03-26},
pages={262--279},
volume={339},
abstract={Collisions are an innate part of the function of many musical instruments. Due to the nonlinear nature of contact forces, special care has to be taken in the construction of numerical schemes for simulation and sound synthesis. Finite difference schemes and other time-stepping algorithms used for musical instrument modelling purposes are normally arrived at by discretising a Newtonian description of the system. However because impact forces are nonanalytic functions of the phase space variables, algorithm stability can rarely be established this way. This paper presents a systematic approach to deriving energy conserving schemes for frictionless impact modelling. The proposed numerical formulations follow from discretising Hamilton's equations of motion, generally leading to an implicit system of nonlinear equations that can be solved with Newton's method. The approach is first outlined for point mass collisions and then extended to distributed settings, such as vibrating strings and beams colliding with rigid obstacles. Stability and other relevant properties of the proposed approach are discussed and further demonstrated with simulation examples. The methodology is exemplified through a case study on tanpura string vibration, with the results confirming the main findings of previous studies on the role of the bridge in sound generation with this type of string instrument.},
journaltitle={Journal of Sound and Vibration},
groups={tesse:5, Finite Difference Methods},
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S0022460X14009146},
date={2015-03},
title={Energy conserving schemes for the simulation of musical instrument contact dynamics},
author={Chatziioannou, Vasileios and van Walstijn, Maarten},
issn={0022-460X},
doi={10.1016/j.jsv.2014.11.017},
}

@article{2013ParsaeianComparison,
langid={english},
abstract={Background: The purpose of this investigation was to compare empirically predictive ability of an artificial neural network with a logistic regression in prediction of low back pain. Methods: Data from the second national health survey were considered in this investigation. This data includes the information of low back pain and its associated risk factors among Iranian people aged 15 years and older. Artificial neural network and logistic regression models were developed using a set of 17294 data and they were validated in a test set of 17295 data. Hosmer and Lemeshow recommendation for model selection was used in fitting the logistic regression. A three-layer perceptron with 9 inputs, 3 hidden and 1 output neurons was employed. The efficiency of two models was compared by receiver operating characteristic analysis, root mean square and -2 Loglikelihood criteria. Results: The area under the ROC curve (SE), root mean square and -2Loglikelihood of the logistic regression was 0.752 (0.004), 0.3832 and 14769.2, respectively. The area under the ROC curve (SE), root mean square and -2Loglikelihood of the artificial neural network was 0.754 (0.004), 0.3770 and 14757.6, respectively. Conclusions: Based on these three criteria, artificial neural network would give better performance than logistic regression. Although, the difference is statistically significant, it does not seem to be clinically significant.},
groups={tesse:5},
author={Parsaeian, M. and Mohammad, K. and Mahmoudi, M. and Zeraati, H.},
date={2013},
journaltitle={Iranian Journal of Public Health},
title={Comparison of Logistic Regression and Artificial Neural Network in Low Back Pain Prediction: Second National Health Survey},
pages={8},
}

@article{2012HinrichsenEntropy,
abstract={The human sense of hearing perceives a combination of sounds ‘in tune’ if the corresponding harmonic spectra are correlated, meaning that the neuronal excitation pattern in the inner ear exhibits some kind of order. Based on this observation it is suggested that musical instruments such as pianos can be tuned by minimizing the Shannon entropy of suitably preprocessed Fourier spectra. This method reproduces not only the correct stretch curve but also similar pitch ﬂuctuations as in the case of high-quality aural tuning.},
pages={1--8},
issn={1806-1117},
number={2},
doi={10.1590/S1806-11172012000200004},
date={2012-06},
journaltitle={Revista Brasileira de Ensino de Física},
url={http://www.scielo.br/scielo.php?script=sci///////\\_arttext///////\\&pid=S1806-11172012000200004///////\\&lng=en///////\\&tlng=en},
groups={tesse:5},
langid={english},
title={Entropy-based tuning of musical instruments},
author={Hinrichsen, Haye},
urldate={2019-03-26},
volume={34},
}

@article{2011OmodhrainFramework,
journaltitle={Computer Music Journal},
number={1},
author={O'Modhrain, Sile},
groups={tesse:5},
date={2011-03},
urldate={2019-03-26},
title={A Framework for the Evaluation of Digital Musical Instruments},
issn={0148-9267},
langid={english},
doi={10.1162/COMJ///////\\_a///////\\_00038},
pages={28--42},
volume={35},
}

@article{2004SmithVirtual,
author={Smith, Julius O.},
pages={283--304},
issn={0929-8215, 1744-5027},
urldate={2019-03-26},
journaltitle={Journal of New Music Research},
number={3},
langid={english},
volume={33},
title={Virtual Acoustic Musical Instruments: Review and Update},
doi={10.1080/0929821042000317859},
abstract={This article1 gives an overview of selected developments in musical sound synthesis based on physical models of musical instruments—sometimes called “virtual acoustic” sound synthesis. Emphasis is placed on techniques which yield the highest playability and sound quality in real time at a reasonable computational expense.},
date={2004-09},
groups={tesse:5},
shorttitle={Virtual Acoustic Musical Instruments},
}

@article{2010Bioucas-diasMultiplicative,
groups={tesse:5},
title={Multiplicative Noise Removal Using Variable Splitting and Constrained Optimization},
issn={1057-7149, 1941-0042},
eprint={0912.1845},
doi={10.1109/TIP.2010.2045029},
number={7},
urldate={2019-03-26},
journaltitle={IEEE Transactions on Image Processing},
volume={19},
author={Bioucas-Dias, José M. and Figueiredo, Mário A. T.},
abstract={Multiplicative noise (also known as speckle noise) models are central to the study of coherent imaging systems, such as synthetic aperture radar and sonar, and ultrasound and laser imaging. These models introduce two additional layers of difﬁculties with respect to the standard Gaussian additive noise scenario: (1) the noise is multiplied by (rather than added to) the original image; (2) the noise is not Gaussian, with Rayleigh and Gamma being commonly used densities. These two features of multiplicative noise models preclude the direct application of most state-of-the-art algorithms, which are designed for solving unconstrained optimization problems where the objective has two terms: a quadratic data term (log-likelihood), reﬂecting the additive and Gaussian nature of the noise, plus a convex (possibly nonsmooth) regularizer (e.g., a total variation or wavelet-based regularizer/prior). In this paper, we address these difﬁculties by: (1) converting the multiplicative model into an additive one by taking logarithms, as proposed by some other authors; (2) using variable splitting to obtain an equivalent constrained problem; and (3) dealing with this optimization problem using the augmented Lagrangian framework. A set of experiments shows that the proposed method, which we name MIDAL (multiplicative image denoising by augmented Lagrangian), yields state-of-the-art results both in terms of speed and denoising performance.},
pages={1720--1730},
url={http://arxiv.org/abs/0912.1845},
langid={english},
date={2010-07},
eprinttype={arxiv},
}

@article{2010MignotDigital,
urldate={2019-03-26},
date={2010-05},
langid={english},
doi={10.1109/TASL.2009.2038671},
author={Mignot, Rémi and Helie, Thomas and Matignon, Denis},
url={http://ieeexplore.ieee.org/document/5446589/},
volume={18},
number={4},
pages={843--854},
abstract={This paper deals with digital waveguide modeling of wind instruments. It presents the application of state–space representations for the reﬁned acoustic model of Webster–Lokshin. This acoustic model describes the propagation of longitudinal waves in axisymmetric acoustic pipes with a varying cross-section, visco-thermal losses at the walls, and without assuming planar or spherical waves. Moreover, three types of discontinuities of the shape can be taken into account (radius, slope, and curvature). The purpose of this work is to build low-cost digital simulations in the time domain based on the Webster–Lokshin model. First, decomposing a resonator into independent elementary parts and isolating delay operators lead to a Kelly–Lochbaum network of input/output systems and delays. Second, for a systematic assembling of elements, their state–space representations are derived in discrete time. Then, standard tools of automatic control are used to reduce the complexity of digital simulations in the time domain. The method is applied to a real trombone, and results of simulations are presented and compared with measurements. This method seems to be a promising approach in term of modularity, complexity of calculation, and accuracy, for any acoustic resonators based on tubes.},
journaltitle={IEEE Transactions on Audio, Speech, and Language Processing},
shorttitle={Digital Waveguide Modeling for Wind Instruments},
title={Digital Waveguide Modeling for Wind Instruments: Building a State–Space Representation Based on the Webster–Lokshin Model},
issn={1558-7916, 1558-7924},
}

@article{2009SchneiderNeural,
title={The Neural Basis of Individual Holistic and Spectral Sound Perception},
pages={315--328},
date={2009-06},
journaltitle={Contemporary Music Review},
number={3},
urldate={2019-03-26},
volume={28},
groups={tesse:5},
issn={0749-4467, 1477-2256},
author={Schneider, Peter and Wengenroth, Martina},
langid={english},
doi={10.1080/07494460903404402},
}

@article{2009EmbrechtsPanjer,
issn={1432-2994, 1432-5217},
journaltitle={Mathematical Methods of Operations Research},
title={Panjer recursion versus FFT for compound distributions},
pages={497--508},
urldate={2019-03-26},
groups={tesse:5},
date={2009-07},
doi={10.1007/s00186-008-0249-2},
number={3},
volume={69},
author={Embrechts, Paul and Frei, Marco},
langid={english},
abstract={Numerical evaluation of compound distributions is an important task in insurance mathematics and quantitative risk management (QRM). In practice, both recursive methods as well as transform based techniques are widely used. We give a survey of these tools, point out the respective merits and provide some numerical examples.},
}

@article{2008CharlesTutorial,
groups={tesse:5},
langid={english},
number={3},
author={Charles, Jean-François},
date={2008-09},
doi={10.1162/comj.2008.32.3.87},
volume={32},
issn={0148-9267, 1531-5169},
journaltitle={Computer Music Journal},
title={A Tutorial on Spectral Sound Processing Using Max/MSP and Jitter},
pages={87--102},
urldate={2019-03-26},
}

@article{2007ParentCase,
doi={10.1007/s10551-007-9533-y},
issn={0167-4544},
journaltitle={Journal of Business Ethics},
author={Parent, Milena M. and Deephouse, David L.},
pages={1--23},
volume={75},
number={1},
urldate={2019-03-26},
groups={tesse:5},
langid={english},
title={A Case Study of Stakeholder Identification and Prioritization by Managers},
date={2007-08-27},
abstract={The purpose of this article is to examine stakeholder identification and prioritization by managers using the power, legitimacy, and urgency framework of Mitchell et al. (Academy of Management Review 22, 853–886; 1997). We use a multi-method, comparative case study of two large-scale sporting event organizing committees, with a particular focus on interviews with managers at three hierarchical levels. We support the positive relationship between number of stakeholder attributes and perceived stakeholder salience. Managers’ hierarchical level and role have direct and moderating effects on stakeholder identiﬁcation and perceived salience. We also found that most stakeholders were deﬁnitive, dominant, or dormant types –the other ﬁve types were rare. Power has the most important effect on salience, followed by urgency and legitimacy. Based on our case study, we offer several ways to advance the theory of stakeholder identiﬁcation and salience.},
}

@inproceedings{2016JackEffect,
title={Effect of latency on performer interaction and subjective quality assessment of a digital musical instrument},
location={Norrkoping, Sweden},
pages={116--123},
author={Jack, Robert H. and Stockman, Tony and McPherson, Andrew},
doi={10.1145/2986416.2986428},
publisher={ACM Press},
url={http://dl.acm.org/citation.cfm?doid=2986416.2986428},
abstract={When designing digital musical instruments the importance of low and consistent action-to-sound latency is widely accepted. This paper investigates the eﬀects of latency (020ms) on instrument quality evaluation and performer interaction. We present ﬁndings from an experiment conducted with musicians who performed on an percussive digital musical instrument with variable amounts of latency. Three latency conditions were tested against a zero latency condition, 10ms, 20ms and 10ms ± 3ms jitter. The zero latency condition was signiﬁcantly rated more positively than the 10ms with jitter and 20ms latency conditions in six quality measures, emphasising the importance of not only low, but stable latency in digital musical instruments. There was no signiﬁcant diﬀerence in rating between the zero latency condition and 10ms condition. A quantitative analysis of timing accuracy in a metronome task under latency conditions showed no signiﬁcant diﬀerence in mean synchronisation error. This suggests that the 20ms and 10ms with jitter latency conditions degrade subjective impressions of an instrument, but without signiﬁcantly aﬀecting the timing performance of our participants. These ﬁndings are discussed in terms of control intimacy and instrument transparency.},
langid={english},
urldate={2019-03-26},
eventtitle={the Audio Mostly 2016},
isbn={978-1-4503-4822-5},
booktitle={Proceedings of the Audio Mostly 2016 on - AM '16},
date={2016},
}

@article{kandus_fisica_nodate,
langid={portuguese},
author={Kandus, Alejandra and Gutmann, Friedrich Wolfgang},
journaltitle={A f},
pages={7},
title={A f´ısica das oscilac¸˜oes mecˆanicas em instrumentos musicais: Exemplo do berimbau},
abstract={In this work it is discussed the wave propagation in an elastic medium, particularly along strings and the air. The basic principles of the wave propagation are presented, taking as an example case a typical instrument from the state of Bahia, well known by all Brazilians, the berimbau.},
}

@article{erkut_finite_nodate,
abstract={The one-dimensional digital waveguides, combined with the commuted synthesis method, allow modeling and high-quality synthesis of plucked string instrument tones in a very eﬃcient manner. However, the increasing computational power of the modern processors makes it feasible to experiment with more complex algorithms also for real-time sound synthesis purposes. By certain simpliﬁcations, time-domain methods based on ﬁnite diﬀerences (FDTD) are eﬃcient enough to run in real time on modern processors and yet they are more ﬂexible than the computationally less expensive commuted synthesis. The resulting structures, which are called 1-D FDTD waveguides, have previously been shown to be equal to or to approximate many properties of digital waveguides, including lossless and lossy propagation, input and output ports, terminations, and scattering junctions. The numerical stability of the 1-D FDTD waveguides, as well as their initialization and formation of the traveling waves are well understood. However, a careful comparison between 1-D FDTD waveguides and conventional digital waveguides in terms of their limitations, computational eﬃciency, accuracy, and interaction has not been carried out. The aim of this paper is to ﬁll this gap by highlighting important properties of both methods side by side in string instrument modeling and synthesis. We then try to combine the best properties of both methods and discuss the interaction of the two model structures. Synthetic tones and short musical phrases obtained by both synthesis models will be demonstrated during the presentation. These sound examples will also be available at http://www.acoustics.hut.fi/demos/.},
author={Erkut, Cumhur and Karjalainen, Matti},
title={Finite difference method vs. digital waveguide method in string instrument modeling and synthesis},
langid={english},
pages={8},
}

@article{monti_department_2000,
langid={english},
abstract={This paper describes an algorithm, which performs monophonic music transcription. A pitch tracker calculates the fundamental frequency of the signal from the autocorrelation function. A continuity-restoration block takes the extracted pitch and determines the score corresponding to the original performance. The signal envelope analysis completes the transcription system, calculating attack-sustain-decay-release times, which improves the synthesis process. Attention is also paid to the extraction of timbre and wavetable synthesis.},
title={Department of Electronic Engineering, King’s College London, Strand, London WC2R 2LS, UK},
date={2000},
author={Monti, Giuliano and Sandler, Mark},
pages={4},
}

@article{1999AgleWho,
langid={english},
pages={19},
title={WHO MATTERS TO CEOS? AN INVESTIGATION OF STAKEHOLDER ATTRIBUTES AND SALIENCE, CORPORATE PERFORMANCE, AND CEO VALUES},
journaltitle={Academy of Management Journal},
groups={tesse:5},
date={1999},
author={Agle, Bradley R.},
}

@article{1999RowlandMissing,
number={5},
pages={378--388},
title={The missing wave momentum mystery},
author={Rowland, David R. and Pask, Colin},
urldate={2019-03-26},
groups={tesse:5},
langid={english},
date={1999-05},
doi={10.1119/1.19272},
volume={67},
journaltitle={American Journal of Physics},
issn={0002-9505, 1943-2909},
}

@article{2004Costa-giomiEffects,
urldate={2019-03-26},
langid={english},
volume={32},
journaltitle={Psychology of Music},
date={2004-04},
issn={0305-7356, 1741-3087},
groups={tesse:5},
author={Costa-Giomi, Eugenia},
title={Effects of Three Years of Piano Instruction on Children’s Academic Achievement, School Performance and Self-Esteem},
doi={10.1177/0305735604041491},
number={2},
pages={139--152},
}

@article{1999SmithBark,
volume={7},
publisher={Institute of Electrical and Electronics Engineers (IEEE)},
pages={697--708},
journaltitle={IEEE Transactions on Speech and Audio Processing},
url={http://ieeexplore.ieee.org/document/799695/},
author={Smith, J. O. and Abel, J. S.},
date={1999-11},
number={6},
journal={IEEE Transactions on Speech and Audio Processing},
year={1999},
langid={english},
groups={tesse:5},
urldate={2019-03-26},
abstract={Use of a bilinear conformal map to achieve a frequency warping nearly identical to that of the Bark frequency scale is described. Because the map takes the unit circle to itself, its form is that of the transfer function of a ﬁrst-order allpass ﬁlter. Since it is a ﬁrst-order map, it preserves the model order of rational systems, making it a valuable frequency warping technique for use in audio ﬁlter design. A closed-form weighted-equation-error method is derived which computes the optimal mapping coeﬃcient as a function of sampling rate, and the solution is shown to be generally indistinguishable from the optimal least-squares solution. The optimal Chebyshev mapping is also found to be essentially identical to the optimal least-squares solution. The expression 0.8517*sqrt(atan(0.06583*Fs))-0.1916 is shown to accurately approximate the optimal allpass coeﬃcient as a function of sampling rate Fs in kHz for sampling rates greater than 1 kHz. A ﬁlter design example is included which illustrates improvements due to carrying out the design over a Bark scale. Corresponding results are also given and compared for approximating the related “equivalent rectangular bandwidth (ERB) scale” of Moore and Glasberg using a ﬁrst-order allpass transformation. Due to the higher frequency resolution called for by the ERB scale, particularly at low frequencies, the ﬁrst-order conformal map is less able to follow the desired mapping, and the error is two to three times greater than the Bark-scale case, depending on the sampling rate.},
doi={10.1109/89.799695},
title={Bark and ERB bilinear transforms},
}

@report{2000SerraSound,
pages={6},
groups={tesse:5},
title={Sound Transformations Based on the SMS High Level Attributes},
type={resreport},
date={2000},
institution={Pompeu Fabra University},
author={Serra, Xavier and Bonada, Jordi},
abstract={The basic Spectral Modeling Synthesis (SMS) technique models sounds as the sum of sinusoids plus a residual. Though this analysis/synthesis system has proved to be successful in transforming sounds, more powerful and intuitive musical transformations can be achieved by moving into the SMS high-level attribute plane. In this paper we describe how to extract high level sound attributes from the basic representation, modify them, and add them back before the synthesis stage. In this process new problems come up for which we propose some initial solutions.},
langid={english},
}

@article{1991BrownMusical,
date={1991-05},
journaltitle={The Journal of the Acoustical Society of America},
title={Musical frequency tracking using the methods of conventional and “narrowed” autocorrelation},
issn={0001-4966},
number={5},
author={Brown, Judith C. and Zhang, Bin},
urldate={2019-03-26},
volume={89},
pages={2346--2354},
groups={tesse:5},
langid={english},
doi={10.1121/1.400923},
}

@article{1976ProvencherFourier,
date={1976-01},
number={1},
pages={27--41},
author={Provencher, Stephen},
langid={english},
url={https://www.cell.com/biophysj/pdf/S0006-3495(76)85660-3.pdf},
abstract={A method based on the Fourier convolution theorem is developed for the analysis of data composed of random noise, plus an unknown constant "base line," plus a sum of (or an integral over a continuous spectrum of) exponential decay functions. The Fourier method's usual serious practical limitation of needing high accuracy data over a very wide time range is eliminated by the introduction of convergence parameters and a Gaussian taper window. A computer program is described for the analysis of discrete spectra, where the data involves only a sum of exponentials. The program is completely automatic in that the only necessary inputs are the raw data (not necessarily in equal intervals of time); no potentially biased initial guesses concerning either the number or the values of the components are needed. The outputs include the number of components, the amplitudes and time constants together with their estimated errors, and a spectral plot of the solution. The limiting resolving power of the method is studied by analyzing a wide range of simulated two-, three-, and four-component data. The results seem to indicate that the method is applicable over a considerably wider range of conditions than nonlinear least squares or the method of moments.},
title={A Fourier method for the analysis of exponential decay curves},
groups={tesse:5, Acoustics},
journal={Biophysical Journal},
year={1976},
month={January},
journaltitle={Biophysical Journal},
urldate={2019-03-26},
publisher={Elsevier BV},
doi={10.1016/s0006-3495(76)85660-3},
volume={16},
}

@article{2015LixinConstruction,
urldate={2019-03-26},
langid={english},
number={1},
journaltitle={The Open Cybernetics ////////\\\& Systemics Journal},
doi={10.2174/1874110X01509012055},
volume={9},
title={Construction of the Logistic Regression Estimation Model in Early Warning on Pure Financial Indicators},
abstract={In order to establish a reasonable and effective financial crisis early warning model, the article chooses some financial indicators and uses factor analysis to get the common factors to conduct the most salient financial indicators. In the case that multirole linearity is not significant, the article uses the logistic regression to analyse eligible financial data E and obtains the financial crisis warning model. Then the article found that this model has a high predictive accuracy.},
author={Lixin, Zhang and Long, Wang},
date={2015-10-08},
issn={1874-110X},
pages={2055--2059},
url={http://benthamopen.com/ABSTRACT/TOCSJ-9-2055},
}

@book{2017SchmidEvaluating,
author={Schmid, Gian-Marco},
isbn={978-3-658-18419-3 978-3-658-18420-9},
doi={10.1007/978-3-658-18420-9},
location={Wiesbaden},
publisher={Springer Fachmedien Wiesbaden},
title={Evaluating the Experiential Quality of Musical Instruments},
urldate={2019-03-26},
langid={english},
date={2017},
groups={tesse:5},
}

@book{2011JensenComputational,
doi={10.1007/978-1-4419-8678-8},
urldate={2019-03-26},
date={2011},
publisher={Springer New York},
langid={english},
location={New York, NY},
groups={tesse:5},
author={Jensen, Finn B. and Kuperman, William A. and Porter, Michael B. and Schmidt, Henrik},
title={Computational Ocean Acoustics},
isbn={978-1-4419-8677-1 978-1-4419-8678-8},
}

@article{2018LokkiSpecial,
pages={518},
author={Lokki, Tapio and Müller, Meinard and Serafin, Stefania and Välimäki, Vesa},
groups={tesse:5},
issn={2076-3417},
number={4},
doi={10.3390/app8040518},
date={2018-03-28},
url={http://www.mdpi.com/2076-3417/8/4/518},
volume={8},
langid={english},
journaltitle={Applied Sciences},
title={Special Issue on “Sound and Music Computing”},
urldate={2019-03-26},
}

@article{SalihSecond,
author={Salih, A.},
langid={english},
title={Second-Order Wave Equation},
pages={24},
groups={Acoustics},
}

@misc{2005KnutWave,
pages={48},
date={2005},
langid={english},
year={2005},
author={Knut, Andreas Lie},
groups={tesse:5},
title={The Wave Equation in 1D and 2D},
}

@article{2012SimicInsolvency,
volume={20},
date={2012-06-01},
groups={tesse:5},
urldate={2019-03-26},
langid={english},
abstract={The prediction of corporate ﬁnancial failure, crucial for the prevention and mitigation of economic downturns in a national economy, requires the categorization of healthy and unhealthy companies. This study examines the case of Serbia and applies multivariant statistical methods and speciﬁc artiﬁcial neural network architectures—the self-organizing map (SOM)—to assess the corporate ﬁnancial health of various companies. Financial ratios drawn from corporate balance sheets become the independent variables in a multivariate discriminant analysis (MDA). These ﬁnancial ratios and the discriminant Z-score in the MDA form the input for the SOM, which creates a hybrid MDA-SOM model that is capable of predicting corporate ﬁnancial insolvency. The experimental results of this research correctly estimate company ﬁnancial health in 95 /\\% of cases. These are reliable predictions that are comparable with similar studies in other countries.},
issn={1367-0751, 1368-9894},
journaltitle={Logic Journal of IGPL},
author={Simic, D. and Kovacevic, I. and Simic, S.},
title={Insolvency prediction for assessing corporate financial health},
doi={10.1093/jigpal/jzr009},
number={3},
pages={536--549},
}

@article{2011MokhatabRafieiFinancial,
issn={0957-4174},
volume={38},
number={8},
author={Mokhatab Rafiei, F. and Manzari, S. M. and Bostanian, S.},
date={2011-08},
pages={10210--10217},
urldate={2019-03-26},
abstract={The purpose of this study is to design a model to predict ﬁnancial health of companies. Financial ratios for 180 manufacturing companies quoted in Tehran Stock Exchange for one year (year ended March 21, 2008) have been used. Three models; based on artiﬁcial neural networks (ANN), genetic algorithm (GA), and multiple discriminant analysis (MDA) are utilized to classify the bankrupt from non bankrupt corporations. ANN model achieved 98.6 /\\% and 96.3 /\\% accuracy rates in training and holdout samples, respectively. To evaluate the reliability of the model, the data were examined with genetic algorithm and Multivariate discriminate analysis method. GA model attained only 92.5 /\\% and 91.5 /\\% accuracy rates and MDA reached 80.6 /\\% and 79.9 in training and holdout samples, respectively.},
doi={10.1016/j.eswa.2011.02.082},
groups={tesse:5},
title={Financial health prediction models using artificial neural networks, genetic algorithm and multivariate discriminant analysis: Iranian evidence},
shorttitle={Financial health prediction models using artificial neural networks, genetic algorithm and multivariate discriminant analysis},
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S0957417411002880},
journaltitle={Expert Systems with Applications},
}

@article{freitas_universidade_nodate,
langid={portuguese},
author={Freitas, Jéssica Galdino De},
title={UNIVERSIDADE FEDERAL FLUMINENSE ESCOLA DE ENGENHARIA DEPARTAMENTO DE ENGENHARIA DE PRODUÇÃO MESTRADO EM ENGENHARIA DE PRODUÇÃO},
pages={133},
}

@article{ghosh_comparative_nodate,
pages={7},
title={Comparative study of Financial Time Series Prediction By Artificial Neural Network with Gradient Descent Learning},
abstract={Financial forecasting is an example of a signal processing problem which is challenging due to Small sizes, high noise, non-stationarity, and non-linearity,but fast forecasting of stock market price is very important for strategic business planning.Present study is aimed to develop a comparative predictive model with Feedforward Multilayer Artificial Neural Network ///////\\& Recurrent Time Delay Neural Network for the Financial Timeseries Prediction.This study is developed with the help of historical stockprice dataset made available by GoogleFinance.To develop this prediction model Backpropagation method with Gradient Descent learning has been implemented.Finally the Neural Net ,learned with said algorithm is found to be skillful predictor for non-stationary noisy Financial Timeseries.},
author={Ghosh, Arka},
langid={english},
}

@article{2015VakhtinaCapital,
author={Vakhtina, Elena and Wosnitza, Jan Henrik},
urldate={2019-03-26},
url={https://linkinghub.elsevier.com/retrieve/pii/S0378437114005937},
volume={417},
journaltitle={Physica A: Statistical Mechanics and its Applications},
date={2015-01},
abstract={In this investigation, we examine the univariate as well as the multivariate capabilities of the log-periodic [super-exponential] power law (LPPL) for the prediction of bank runs. The research is built upon daily CDS spreads of 40 international banks for the period from June 2007 to March 2010, i.e. at the heart of the global financial crisis. For this time period, 20 of the financial institutions received federal bailouts and are labeled as defaults while the remaining institutions are categorized as non-defaults. The employed multivariate pattern recognition approach represents a modification of the CORA3 algorithm. The approach is found to be robust regardless of reasonable changes of its inputs. Despite the fact that distinct alarm indices for banks do not clearly demonstrate predictive capabilities of the LPPL, the synchronized alarm indices confirm the multivariate discriminative power of LPPL patterns in CDS spread developments acknowledged by bootstrap intervals with 70 /\\% confidence level.},
title={Capital market based warning indicators of bank runs},
pages={304--320},
issn={0378-4371},
groups={tesse:5},
langid={english},
doi={10.1016/j.physa.2014.07.024},
}

@article{2013AliajBank’s,
abstract={Financial Transactions have increased and have become more complex over this 20 years. This paper examines the importance of credit ratings assigned to banks in Europe and the positive and negative impacts. We will also see Albanian Banking system over the last 5 years focusing on the risks it faces. The aim of this article is to analyze and identify the importance of Banks’ Rating. Actually, banks are the dominant sector within Albanian financial system, managing more than 95.5 /\\% of total financial assets. Although banks sometimes use internal models as a substitute for credit ratings for their credit assessments, the internal models themselves often tend to rely heavily on ratings for actual or methodological input. Investors’ reliance on credit ratings has increased over the past 30 years. Acquiring information is costly, particularly for fixed income investors, given collective action problems. Thus investors seek to outsource creditworthiness assessments to rating agencies. Finally we will see if there is a need of Banks Rating in Albanian`s Banks, the requirements of Basel III and the regulatory of the Bank of Albania.},
journaltitle={Mediterranean Journal of Social Sciences},
groups={tesse:5},
title={Bank’s Rating a Need or Necessity in Albanian Banking System},
author={Aliaj, Ada and Hoti, Ilir},
date={2013-10-01},
url={http://www.mcser.org/journal/index.php/mjss/article/view/1219},
issn={20399340, 20392117},
urldate={2019-03-26},
langid={english},
doi={10.5901/mjss.2013.v4n10p490},
}

@article{los_atendimento_nodate,
title={Atendimento de instituições financeiras às recomendações de evidenciação ambiental da Global Reporting Initiative (GRI)},
langid={portuguese},
pages={23},
author={Los, Geovana Zimmermann},
}

@article{esumo_alise_nodate,
langid={portuguese},
title={ANÁLISE COMPARATIVA DOS RELATÓRIOS DE SUSTENTABILIDADE DO GLOBAL REPORTING INITIATIVE COM ÊNFASE NAS EMPRESAS DE CAPITAL ABERTO COM ATUAÇÃO NO BRASIL},
pages={14},
abstract={The environmental matter has been studied in several areas of the scientific knowledge due the scenery of development,. In this sense, the Global Reporting Initiative (GRI), an independent organization, is developing a strategic world-wide accepted model through guidelines based on principles. The goal of this article is identify the behavioral diversities regarding to the economic and environmental aspects of the partner capital open companies acting in Brazil that adopt GRI's Guidelines. Besides advancing, this global net which is ruled in sustainability allow that the companies choose the indicative they want to report, and also it does not demand the submission of reports to the external audit. The methodology of the research is data rising and bibliographical, with exploratory purpose. It was identified that GRI's Indicators contributes for the sustainable development as well as guarantee the inevitable tendency consisting in what is demonstrated, corresponding to the reality of the open capital companies with performance in Brazil, country that prioritizes GRI's Principles.},
author={Esumo, R},
}

@book{2009ClarkePrinciples,
keywords={Machine learning, Data mining, Data Mining, Maschinelles Lernen, Statistical methods, Statistik},
series={Springer series in statistics},
pagetotal={781},
location={Dordrecht ; New York},
note={OCLC: ocn440103793},
isbn={978-0-387-98134-5 978-0-387-98135-2},
date={2009},
publisher={Springer},
langid={english},
title={Principles and theory for data mining and machine learning},
author={Clarke, Bertrand S. and Fokoué, Ernest and Zhang, Hao Helen},
groups={tesse:5},
}

@book{2002AgrestiCategorical,
publisher={Wiley-Interscience},
title={Categorical data analysis},
series={Wiley series in probability and statistics},
keywords={Multivariate analysis},
groups={tesse:5},
pagetotal={710},
author={Agresti, Alan},
langid={english},
isbn={978-0-471-36093-3},
date={2002},
location={New York},
edition={2nd ed},
}

@book{2015YeeVector,
pagetotal={589},
keywords={Datenverarbeitung, Hochleistungsrechnen, Linear models (Statistics), Lineares Modell, Regression analysis, Statistisches Modell, Vector spaces, Wahrscheinlichkeitstheorie},
location={New York, NY},
groups={tesse:5},
note={OCLC: ocn907271683},
series={Springer series in statistics},
shorttitle={Vector generalized linear and additive models},
author={Yee, Thomas W.},
langid={english},
isbn={978-1-4939-2817-0},
date={2015},
title={Vector generalized linear and additive models: with an implementation in R},
publisher={Springer},
}

@article{1979DeMeersmanLeast,
pages={277--281},
issn={0377-0427},
journaltitle={Journal of Computational and Applied Mathematics},
url={https://linkinghub.elsevier.com/retrieve/pii/0771050X79900445},
urldate={2019-03-28},
abstract={It is shown how a least squares problem subject to equality constraints can be replaced by an unconstrained least squares problem. Constraints and equations may be non linear. Results seem to be too complicated to be applied to general cases but can quite successfully be used for special problems like the closing of balances for instance.},
volume={5},
doi={10.1016/0771-050X(79)90044-5},
langid={english},
number={4},
groups={tesse:5},
title={Least squares with non-linear equality constraints Application to closing of balances},
author={De Meersman, R.},
date={1979-12},
}

@book{2014PoularikasAdaptive,
isbn={978-1-4822-5335-1 978-1-4822-5336-8},
langid={english},
url={https://www.taylorfrancis.com/books/9781482253368},
date={2014-09-30},
publisher={CRC Press},
shorttitle={Adaptive Filtering},
title={Adaptive Filtering: Fundamentals of Least Mean Squares with MATLAB®},
urldate={2019-03-28},
author={Poularikas, Alexander},
groups={tesse:5},
doi={10.1201/b17464},
}

@article{2014StankovicInstantaneous,
doi={10.1016/j.dsp.2014.09.008},
journaltitle={Digital Signal Processing},
groups={tesse:5, DSP},
date={2014-12},
issn={1051-2004},
pages={1--13},
author={Stanković, Ljubiša and Djurović, Igor and Stanković, Srdjan and Simeunović, Marko and Djukanović, Slobodan and Daković, Miloš},
title={Instantaneous frequency in time–frequency analysis: Enhanced concepts and performance of estimation algorithms},
urldate={2019-03-28},
url={https://linkinghub.elsevier.com/retrieve/pii/S1051200414002838},
volume={35},
langid={english},
abstract={The instantaneous frequency (IF) is a very important feature of nonstationary signals in numerous applications. The ﬁrst overview of the concept and application of the IF estimators is presented in seminal papers by Boashash. Since then, a signiﬁcant knowledge has been gained about the performance of the IF estimators. This knowledge has been used not only for development of various IF estimators but also for introduction of novel time–frequency (TF) representations. The IF estimation in environments characterized by low signal-to-noise (SNR) has achieved signiﬁcant beneﬁts from these theoretical developments. In this paper, we review some of the most important developments in the last two decades related to the concept of the IF, performance analysis of IF estimators, and development of IF estimators for low SNR environments.},
shorttitle={Instantaneous frequency in time–frequency analysis},
}

@book{2004StrikwerdaFinite,
keywords={Differential equations, Partial, Finite differences, Numerical solutions},
langid={english},
groups={tesse:5, Finite Difference Methods},
date={2004},
title={Finite difference schemes and partial differential equations},
isbn={978-0-89871-567-5},
location={Philadelphia},
author={Strikwerda, John C.},
pagetotal={435},
publisher={Society for Industrial and Applied Mathematics},
edition={2nd ed},
}

@article{zhdanov_solving_nodate,
langid={english},
pages={7},
journaltitle={Solving Least Squares Problems},
abstract={This article is devoted to a new algorithm for solving least squares problems with linear equality constraints. The presented algorithm can help solve large dimension ill-conditioned problems e¢ ciently.},
author={Zhdanov, Aleksandr Ivanovich and Gogoleva, Sofya Yuryevna},
title={Solving Least Squares Problems With Equality Constraints Based On Augmented Regularized Normal Equations},
}

@article{2013RiskusImproved,
abstract={In this paper an improved version of an earlier proposed algorithm for approximating cubic Bezier curve by a set of circular arcs is presented. It is investigated how the improved algorithm fits for approximation of quadratic Bezier curves. These issues occur in CAD/CAM systems during data exchange into data formats which do not support Bezier curves. Experimental results on examples, widely used in the sources enlisted in references, are presented. Two typographical errors, made in the previous article, are corrected.},
author={Riškus, Aleksas and Liutkus, Giedrius},
urldate={2019-03-28},
date={2013-12-12},
langid={english},
number={4},
title={An Improved Algorithm for the Approximation of a Cubic Bezier Curve and its Application for Approximating Quadratic Bezier Curve},
journaltitle={Information Technology And Control},
doi={10.5755/j01.itc.42.4.1707},
issn={2335-884X, 1392-124X},
volume={42},
url={http://www.itc.ktu.lt/index.php/ITC/article/view/1707},
}

@thesis{torin_percussion_2015,
langid={english},
author={Torin, Alberto},
title={Percussion Instrument Modelling In 3D: Sound Synthesis Through Time Domain Numerical Simulation},
type={phdthesis},
date={2015},
}

@book{2011BeckSociedade,
langid={portuguese},
title={Sociedade de risco: rumo a uma outra modernidade},
shorttitle={Sociedade de risco},
note={OCLC: 753356287},
groups={tesse:5},
isbn={978-85-7326-450-0},
location={São Paulo},
publisher={Editora 34},
date={2011},
author={Beck, Ulrich and Nascimento, Sebastião},
}

@book{2017HorstmannScala,
keywords={Computer programming, Functional programming (Computer science), Programming languages (Electronic computers), Scala (Computer program language)},
date={2017},
groups={tesse:5, Programming},
edition={Second edition},
location={Boston},
author={Horstmann, Cay S.},
note={OCLC: ocn964820811},
pagetotal={359},
publisher={Addison-Wesley},
langid={english},
isbn={978-0-13-454056-6},
title={Scala for the impatient},
}

@book{1997DodgeComputer,
pagetotal={455},
publisher={Schirmer Books [u.a.]},
shorttitle={Computer music},
edition={2. ed},
author={Dodge, Charles and Jerse, Thomas A.},
langid={english},
note={OCLC: 247058281},
date={1997},
isbn={978-0-02-864682-4},
location={New York},
title={Computer music: synthesis, composition, and performance},
}

@book{2014KoenigSpectral,
groups={tesse:5},
isbn={978-0-19-872290-8},
month={nov},
year={2014},
publisher={Oxford University Press},
title={Spectral Analysis of Musical Sounds with Emphasis on the Piano},
author={David M. Koenig},
doi={10.1093/acprof:oso/9780198722908.001.0001},
langid={english},
date={2014-11-13},
urldate={2019-03-28},
}

@book{2008MontgomeryIntroduction,
title={Introduction to Time Series Analysis and Forecasting},
date={2008},
langid={english},
publisher={Wiley},
groups={tesse:5},
isbn={978-0-471-65397-4},
url={https://www.amazon.com/Introduction-Analysis-Forecasting-Douglas-Montgomery/dp/0471653977?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0471653977},
author={Montgomery, Douglas C. and Jennings, Cheryl L. and Kulahci, Murat},
pages={469},
}

@collection{2018KarasavvoglouEconomy,
series={Springer Proceedings in Business and Economics},
isbn={978-3-319-70376-3 978-3-319-70377-0},
langid={english},
title={Economy, Finance and Business in Southeastern and Central Europe},
doi={10.1007/978-3-319-70377-0},
editor={Karasavvoglou, Anastasios and Goić, Srećko and Polychronidou, Persefoni and Delias, Pavlos},
publisher={Springer International Publishing},
urldate={2019-03-28},
location={Cham},
groups={tesse:5},
date={2018},
}

@collection{2006KomaraEncyclopedia,
title={Encyclopedia of the blues},
date={2006},
pagetotal={2},
keywords={Biography Dictionaries, Blues (Music), Blues musicians, Encyclopedias},
langid={english},
publisher={Routledge},
groups={tesse:5},
editor={Komara, Edward M.},
location={New York},
isbn={978-0-415-92699-7 978-0-415-92700-0 978-0-415-92701-7},
}

@article{1969HansonExtensions,
title={Extensions and applications of the Householder algorithm for solving linear least squares problems},
journal={Mathematics of Computation},
pages={787},
author={Hanson, Richard J. and Lawson, Charles L.},
publisher={American Mathematical Society (AMS)},
groups={tesse:5},
langid={english},
doi={10.2307/2004965},
url={https://www.jstor.org/stable/2004965?origin=crossref},
year={1969},
urldate={2019-03-28},
date={1969-10},
number={108},
volume={23},
journaltitle={Mathematics of Computation},
}

@article{GallierFundamentals,
author={Gallier, Jean and Quaintance, Jocelyn},
title={Fundamentals of Linear Algebra and Optimization},
pages={947},
groups={tesse:2, Linear Algebra},
langid={english},
}

@book{2009GesserLibras?,
author={Gesser, Audrei},
title={Libras? que língua é essa?: crenças e preconceitos em torno da língua de sinais e da realidade surda},
note={OCLC: 817147074},
date={2009},
shorttitle={Libras?},
publisher={Parábola Ed.},
location={São Paulo},
isbn={978-85-7934-001-7},
langid={portuguese},
}

@article{myers_smarter_nodate,
title={A Smarter Way to Learn JavaScript: The new approach that uses technology to cut your effort in half},
author={Myers, Mark},
langid={english},
pages={288},
}

@book{2013GolubMatrix,
groups={tesse:5},
langid={english},
location={Baltimore},
title={Matrix computations},
publisher={The Johns Hopkins University Press},
edition={Fourth edition},
series={Johns Hopkins studies in the mathematical sciences},
keywords={Data processing, Matrices},
author={Golub, Gene H. and Van Loan, Charles F.},
note={OCLC: ocn824733531},
date={2013},
pagetotal={756},
isbn={978-1-4214-0794-4},
}

@incollection{2013RossFront,
year={2013},
isbn={978-0-12-415825-2},
date={2013},
ean={9780124158252},
booktitle={Simulation},
pagetotal={310},
publisher={Elsevier},
url={https://linkinghub.elsevier.com/retrieve/pii/B9780124158252000140},
urldate={2019-03-28},
langid={english},
author={Ross, Sheldon M.},
title={Front Matter},
doi={10.1016/B978-0-12-415825-2.00014-0},
groups={tesse:5},
pages={i--ii},
}

@article{1959GardnerMethod,
title={Method for the Analysis of Multicomponent Exponential Decay Curves},
volume={31},
author={Gardner, Donald G. and Gardner, Jeanne C. and Laush, George and Meinke, W. Wayne},
issn={0021-9606, 1089-7690},
doi={10.1063/1.1730560},
qualityassured={qualityAssured},
keywords={qualityAssured},
number={4},
pages={978--986},
urldate={2019-03-28},
journaltitle={The Journal of Chemical Physics},
groups={tesse:5},
date={1959},
langid={english},
}

@article{1983KarplusDigital,
volume={7},
author={Karplus, Kevin and Strong, Alex},
urldate={2019-03-28},
title={Digital Synthesis of Plucked-String and Drum Timbres},
issn={0148-9267},
url={https://www.jstor.org/stable/3680062?origin=crossref},
journaltitle={Computer Music Journal},
langid={english},
groups={tesse:5},
number={2},
doi={10.2307/3680062},
pages={43},
date={1983},
}

@article{1994ChaigneNumericala,
langid={english},
journaltitle={The Journal of the Acoustical Society of America},
author={Chaigne, Antoine and Askenfelt, Anders},
issn={0001-4966},
volume={95},
urldate={2019-03-28},
doi={10.1121/1.408459},
pages={1112--1118},
date={1994-02},
number={2},
title={Numerical simulations of piano strings. A physical model for a struck string using finite difference methods},
}

@article{1998FontanaPhysical,
author={Fontana, Federico and Rocchesso, Davide},
date={1998},
pages={15},
title={Physical Modeling of Membranes for Percussion Instruments},
groups={tesse:5, Acoustics, Digital Waveguides},
journaltitle={acta acustica},
volume={84},
abstract={Recent research on Physical Modeling has led to 2-D discrete-time structures based on the Digital Waveguides. These structures are well suited for efficient yet accurate simulation of wave propagation in an ideal membrane. Nevertheless, real membranes exhibit a different behaviour, due to the environmental conditions and to the material they are made of. In this work we consider some aspects, crucial for the audio signal, of the physical phenomena concerning real membranes, and we will develop a 2-D waveguide model encompassing the effects of these aspects. In order to excite the simulated membrane, we will consider a hammer model previously developed for piano strings, and here adapted to fit the hammer-membrane interaction.},
langid={english},
}

@article{2004MoltenoExperimental,
pages={1157--1169},
author={Molteno, Timothy C. and Tufillaro, Nicholas B.},
date={2004-09},
journaltitle={American Journal of Physics},
title={An experimental investigation into the dynamics of a string},
number={9},
doi={10.1119/1.1764557},
groups={tesse:5},
urldate={2019-03-28},
langid={english},
issn={0002-9505, 1943-2909},
volume={72},
}

@article{2006VaelimaekiDiscrete,
date={2006-01-01},
author={Välimäki, Vesa and Pakarinen, Jyri and Erkut, Cumhur and Karjalainen, Matti},
title={Discrete-time modelling of musical instruments},
number={1},
urldate={2019-03-28},
doi={10.1088/0034-4885/69/1/R01},
volume={69},
groups={tesse:5, Acoustics},
pages={1--78},
langid={english},
url={http://stacks.iop.org/0034-4885/69/i=1/a=R01?key=crossref.fc944552766588c9a4a07777ff78c7f1},
journaltitle={Reports on Progress in Physics},
issn={0034-4885, 1361-6633},
abstract={This article describes physical modelling techniques that can be used for simulating musical instruments. The methods are closely related to digital signal processing. They discretize the system with respect to time, because the aim is to run the simulation using a computer. The physics-based modelling methods can be classiﬁed as mass–spring, modal, wave digital, ﬁnite difference, digital waveguide and source–ﬁlter models. We present the basic theory and a discussion on possible extensions for each modelling technique. For some methods, a simple model example is chosen from the existing literature demonstrating a typical use of the method. For instance, in the case of the digital waveguide modelling technique a vibrating string model is discussed, and in the case of the wave digital ﬁlter technique we present a classical piano hammer model. We tackle some nonlinear and time-varying models and include new results on the digital waveguide modelling of a nonlinear string. Current trends and future directions in physical modelling of musical instruments are discussed.},
}

@article{2014BilbaoNumerical,
eprint={1405.2589},
title={Numerical Modeling of Collisions in Musical Instruments},
urldate={2019-03-28},
url={http://arxiv.org/abs/1405.2589},
abstract={Collisions play an important role in many aspects of the physics of musical instruments. The striking action of a hammer or mallet in keyboard and percussion instruments is perhaps the most important example, but others include reed-beating eﬀects in wind instruments, the string/neck interaction in fretted instruments such as the guitar as well as in the sitar and the wire/membrane interaction in the snare drum. From a simulation perspective, whether the eventual goal is the validation of musical instrument models or sound synthesis, such highly nonlinear problems pose various diﬃculties, not the least of which is the risk of numerical instability. In this article, a novel ﬁnite diﬀerence time domain simulation framework for such collision problems is developed, where numerical stability follows from strict numerical energy conservation or dissipation, and where a a power law formulation for collisions is employed, as a potential function within a Hamiltonian formulation. The power law serves both as a model of deformable collision, and as a mathematical penalty under perfectly rigid, non-deformable collision. This formulation solves a major problem underlying previous work, where a Hamiltonian framework was not employed for collisions, and thus stability was not ensured. Various numerical examples, illustrating the unifying features of such methods across a wide variety of systems in musical acoustics are presented, including numerical stability and energy conservation/dissipation, bounds on spurious penetration in the case of rigid collisions, as well as various aspects of musical instrument physics.},
eprinttype={arxiv},
author={Bilbao, Stefan and Torin, Alberto and Chatziioannou, Vasileios},
groups={tesse:5},
date={2014-05-11},
journaltitle={arXiv:1405.2589 [math]},
langid={english},
}

@article{2016LyubimovMathematical,
journaltitle={Acoustical Physics},
date={2016-03},
number={2},
issn={1063-7710, 1562-6865},
title={Mathematical model of acoustic speech production with mobile walls of the vocal tract},
author={Lyubimov, N. A. and Zakharov, E. V.},
doi={10.1134/S1063771016020093},
pages={225--234},
volume={62},
abstract={A mathematical speech production model is considered that describes acoustic oscillation prop agation in a vocal tract with mobile walls. The wave field function satisfies the Helmholtz equation with boundary conditions of the third kind (impedance type). The impedance mode corresponds to a three parameter pendulum oscillation model. The experimental research demonstrates the nonlinear character of how the mobility of the vocal tract walls influence the spectral envelope of a speech signal.},
langid={english},
urldate={2019-03-28},
}

@article{2016CostaGraphical,
pages={26--42},
abstract={Purpose – This study aims to use a graphical approach to highlight the differences between outranking and preference relationships. The outranking principle is based on a structure of non-dominance, which differs from the usual structures of preferences.},
number={1},
date={2016-02-08},
journaltitle={Journal of Modelling in Management},
groups={tesse:5},
langid={english},
volume={11},
shorttitle={Graphical interpretation of outranking principles},
issn={1746-5664},
title={Graphical interpretation of outranking principles: Avoiding misinterpretation results from ELECTRE I},
doi={10.1108/JM2-08-2013-0037},
author={Costa, Helder Gomes},
urldate={2019-03-28},
}

@inproceedings{1998LairdEfficient,
urldate={2019-03-28},
pages={12--12},
groups={tesse:5, Digital Waveguides},
doi={10.1049/ic:19980829},
publisher={IEE},
volume={1998},
booktitle={IEE Colloquium on Audio and Music Technology: the Challenge of Creative DSP},
abstract={Digital waveguides may be used to construct efficient physical models of musical instruments and other resonant systems. They are commonly used to model systems with simple boundary - conditions, such as strings and square membranes. Membranes are modelled by connecting digital waveguides together to form a mesh; however when modelling circular membranes, such as those found in percussive drums or microphones, the mesh does not approximate well to the boundary shape. It was found that a two-dimensional triangular mesh structure, with rimguides attached around the edge could overcome this. In this paper several rimguide strategies have been explored. The best of these involves adjusting the phase delay so that the model is accurate for frequencies close to DC. The improvement is particularly noticeable for efficient (low sample rate) implementations.},
eventtitle={IEE Colloquium on Audio and Music Technology: the Challenge of Creative DSP},
location={London, UK},
langid={english},
author={Laird, J. and Masri, P. and Canagarajah, C. N.},
date={1998},
title={Efficient and accurate synthesis of circular membranes using digital waveguides},
}

@article{2007MurphyAcoustic,
author={Murphy, D. and Kelloniemi, A. and Mullen, J. and Shelley, S.},
pages={55--66},
urldate={2019-03-28},
volume={24},
doi={10.1109/MSP.2007.323264},
langid={english},
date={2007-03},
groups={tesse:5},
title={Acoustic Modeling Using the Digital Waveguide Mesh},
url={http://ieeexplore.ieee.org/document/4117929/},
number={2},
journaltitle={IEEE Signal Processing Magazine},
issn={1053-5888},
}

@article{2010BankModal,
langid={english},
author={Bank, Balázs and Zambon, Stefano and Fontana, Federico},
doi={10.1109/TASL.2010.2040524},
title={A Modal-Based Real-Time Piano Synthesizer},
volume={18},
issn={1558-7916, 1558-7924},
number={4},
pages={809--821},
journaltitle={IEEE Transactions on Audio, Speech, and Language Processing},
url={http://ieeexplore.ieee.org/document/5446595/},
urldate={2019-03-28},
abstract={This paper presents a real-time piano synthesizer where both the transverse and longitudinal motion of the string is modeled by modal synthesis, resulting in a coherent and highly parallel model structure. The paper applies recent developments in piano modeling and focuses on the issues related to practical implementation (e.g., numerical stability, aliasing, and efﬁciency). A strong emphasis is given to modeling nonlinear string vibrations, and a new variation of earlier synthesis techniques is proposed which is particularly well suited for modal synthesis. For soundboard modeling, the possibilities of using FFT-based fast convolution and parallel second-order ﬁlters are discussed. Additionally, the paper describes the details of the software implementation and discusses the computational complexity of each model block. The piano model runs on current computer hardware with full polyphony in real time.},
groups={tesse:5},
date={2010-05},
}

@article{2012BilbaoTime,
langid={english},
issn={0001-4966},
date={2012-01},
pages={914--925},
urldate={2019-03-28},
volume={131},
author={Bilbao, Stefan},
number={1},
doi={10.1121/1.3651240},
journaltitle={The Journal of the Acoustical Society of America},
title={Time domain simulation and sound synthesis for the snare drum},
groups={tesse:5},
}

@inproceedings{2008DiukObject,
doi={10.1145/1390156.1390187},
pages={240--247},
abstract={Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the wellknown Taxi domain, plus a real-life videogame.},
langid={english},
groups={tesse:5},
date={2008},
urldate={2019-03-28},
publisher={ACM Press},
isbn={978-1-60558-205-4},
eventtitle={the 25th international conference},
author={Diuk, Carlos and Cohen, Andre and Littman, Michael L.},
url={http://portal.acm.org/citation.cfm?doid=1390156.1390187},
title={An object-oriented representation for efficient reinforcement learning},
booktitle={Proceedings of the 25th international conference on Machine learning - ICML '08},
location={Helsinki, Finland},
}

@inproceedings{2010IoanaTime,
publisher={IEEE},
doi={10.1109/ICASSP.2010.5495259},
pages={5634--5637},
abstract={One of the most challenging applications of time-frequency representations deals with the analysis of the signal issued from natural environment. Recently, the interest for passive underwater context increased, basically due to the richness of the information carried out by the natural signals. Taken into account the non-linear multi-component time-frequency behavior of such signals, their analysis is a complex problem. In this paper we introduce a new time-frequency analysis concept that aims to extract the non-linear timefrequency components. The main feature of this technique is the joint use of time-amplitude, time-frequency and timephase information. This is materialized by a short-time polynomial phase modeling and the fusion of local information according to the best locally matches of local cubic frequency modulations. Tests provided on real data illustrate the benefits of the proposed approach.},
shorttitle={Time-frequency-phase tracking approach},
title={Time-frequency-phase tracking approach : Application to underwater signals in a passive context},
isbn={978-1-4244-4295-9},
url={http://ieeexplore.ieee.org/document/5495259/},
urldate={2019-03-28},
langid={english},
author={Ioana, Cornel and Mars, Jerome I. and Serbanescu, Alexandru and Stankovic, Srdjan},
booktitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
date={2010},
eventtitle={2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
location={Dallas, TX, USA},
}

@article{2017LanctotUnified,
abstract={To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we ﬁrst observe that policies learned using InRL can overﬁt to the other agents’ policies during training, failing to sufﬁciently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and ﬁctitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
eprint={1711.00832},
groups={tesse:5},
title={A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
langid={english},
author={Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
journaltitle={arXiv:1711.00832 [cs]},
date={2017-11-02},
urldate={2019-03-28},
url={http://arxiv.org/abs/1711.00832},
eprinttype={arxiv},
}

@article{GiannakopoulosDeep,
pages={6},
author={Giannakopoulos, Petros},
abstract={We employ the Deep Q-Learning algorithm with Experience Replay to train an agent capable of achieving a high-level of play in the L-Game while selflearning from low-dimensional states. We also employ variable batch size for training in order to mitigate the loss of the rare reward signal and significantly accelerate training. Despite the large action space due to the number of possible moves, the low-dimensional state space and the rarity of rewards, which only come at the end of a game, DQL is successful in training an agent capable of strong play without the use of any search methods or domain knowledge.},
title={A Deep Q-Learning Agent for the L-Game with Variable Batch Training},
langid={english},
groups={tesse:2, Game Theory},
}

@article{hu_multiagent_nodate,
langid={english},
pages={9},
title={Multiagent Reinforcement LeAarlgnoinrigt:hmTheoretical Framework and an},
author={Hu, Junling and Wellman, Michael P},
abstract={In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under speci ed conditions. This algorithm is useful for nding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to nd optimal strategies.},
}

@article{horie_neural_nodate,
pages={6},
abstract={We propose a new mutually coupled plural Neural Networks (N.N.) modules and its application to associative memories from the view point of noncooperative game theory. First, We propose a new dynamical searching model named Parallel Steepest Descent Method with Braking operators (PSDMB) which searches the Nash Equilibrium (NE) points under [0, 11-intervalor nonnegative constraints. Second, we propose a new mutually coupled plural N.N. modules named Game Neural Networks (GNN) to realize the proposed PSDMB with quadratic objective functions. In Addition, we indicate relations between the PSDMB, the GNN and the Lotka-Volterra equation, Last, for an application of the proposed GNN, we propose two kinds of multi modular associative memories which can associate the combined patterns composed of plural partial patterns: (1) the combined patterns are stored as the NE points and robust for noisy inputs; (2) the circulative sequence of the combined patterns are stored as saddles of a heteroclinic cycle.},
author={Horie, Ryota and Aiyoshi, Eitaro},
title={Neural Networks Realization of Searching Models for Nash Equilibr' and Their Application to Associative},
langid={english},
}

@article{erev_predicting_nodate,
author={Erev, IDo and Roth, Alvin E},
pages={35},
title={Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria},
langid={english},
}

@article{HuNash,
title={Nash Q-Learning for General-Sum Stochastic Games},
groups={tesse:2, Game Theory},
abstract={We extend Q-learning to a noncooperative multiagent context, using the framework of generalsum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (deﬁned by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the ﬁrst grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of ofﬂine learning performance in both games, we ﬁnd agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance.},
author={Hu, Junling and Wellman, Michael P.},
pages={31},
langid={english},
}

@article{2016SilverMastering,
volume={529},
groups={tesse:5},
date={2016-01},
doi={10.1038/nature16961},
urldate={2019-03-28},
pages={484--489},
langid={english},
journaltitle={Nature},
url={http://www.nature.com/articles/nature16961},
number={7587},
title={Mastering the game of Go with deep neural networks and tree search},
author={Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
issn={0028-0836, 1476-4687},
}

@book{2014KamlerHelping,
author={Kamler, Barbara and Thomson, Pat},
pagetotal={189},
edition={Second edition},
isbn={978-0-415-82348-7 978-0-415-82349-4},
keywords={Dissertations, Academic, EDUCATION / General, EDUCATION / Higher, EDUCATION / Research},
shorttitle={Helping doctoral students write},
title={Helping doctoral students write: pedagogies for supervision},
date={2014},
abstract={"Helping Doctoral Students Write offers a proven approach to effective doctoral writing. By treating research as writing and writing as research, the authors offer pedagogical strategies for doctoral supervisors that will assist the production of well-argued and lively dissertations. It is clear that many doctoral candidates find research writing complicated and difficult, but the advice they receive often glosses over the complexities of writing and/or locates the problem in the writer. Kamler and Thomson provide a highly effective framework for scholarly work that is located in personal, institutional and cultural contexts. The pedagogical approach developed in the book is based on the notion of writing as a social practice. This approach allows supervisors to think of doctoral writers as novices who need to learn new ways with words as they enter the discursive practices of scholarly communities. This involves learning sophisticated writing practices with specific sets of conventions and textual characteristics. The authors offer supervisors practical advice on helping with commonly encountered writing tasks such as the proposal, the journal abstract, the literature review and constructing the dissertation argument. The first edition of this book has helped many academics and thousands of research students produce better written material. Now fully updated the second edition includes: Examples from a broader range of academic disciplines A new chapter on writing from the thesis More advice on reading and note taking, performance and conferences, Further information on developing a personal academic writing style, and Advice on the use of social media (blogs, tweets and wikis) to create trans-disciplinary and trans-national networks and conversations"--},
location={London ; New York},
publisher={Routledge},
langid={english},
groups={tesse:5},
}

@article{2007SgroiNeural,
groups={tesse:5},
issn={0378-4371},
doi={10.1016/j.physa.2006.10.026},
url={https://linkinghub.elsevier.com/retrieve/pii/S037843710601048X},
title={Neural networks and bounded rationality},
langid={english},
author={Sgroi, Daniel and Zizzo, Daniel J.},
journaltitle={Physica A: Statistical Mechanics and its Applications},
date={2007-03},
pages={717--725},
number={2},
urldate={2019-03-28},
volume={375},
abstract={Traditionally the emphasis in neural network research has been on improving their performance as a means of pattern recognition. Here we take an alternative approach and explore the remarkable similarity between the under-performance of neural networks trained to behave optimally in economic situations and observed human performance in the laboratory under similar circumstances. In particular, we show that neural networks are consistent with observed laboratory play in two very important senses. Firstly, they select a rule for behavior which appears very similar to that used by laboratory subjects. Secondly, using this rule they perform optimally only approximately 60 /\\% of the time.},
}

@article{2008MarchioriPredicting,
author={Marchiori, D. and Warglien, M.},
journaltitle={Science},
doi={10.1126/science.1151185},
issn={0036-8075, 1095-9203},
urldate={2019-03-28},
langid={english},
date={2008-02-22},
pages={1111--1113},
groups={tesse:5},
title={Predicting Human Interactive Learning by Regret-Driven Neural Networks},
volume={319},
number={5866},
}

@article{SpiliopoulosLearning,
pages={10},
title={Learning backward induction: a neural network agent approach},
author={Spiliopoulos, Leonidas},
abstract={This paper addresses the question of whether neural networks (NNs), a realistic cognitive model of human information processing, can learn to backward induce in a two-stage game with a unique subgame-perfect Nash equilibrium. The NNs were found to predict the Nash equilibrium approximately 70 /\\% of the time in new games. Similarly to humans, the neural network agents are also found to suffer from subgame and truncation inconsistency, supporting the contention that they are appropriate models of general learning in humans. The agents were found to behave in a bounded rational manner as a result of the endogenous emergence of decision heuristics. In particular a very simple heuristic socialmax, that chooses the cell with the highest social payoff explains their behavior approximately 60 /\\% of the time, whereas the ownmax heuristic that simply chooses the cell with the maximum payoff for that agent fares worse explaining behavior roughly 38 /\\%, albeit still signiﬁcantly better than chance. These two heuristics were found to be ecologically valid for the backward induction problem as they predicted the Nash equilibrium in 67 /\\% and 50 /\\% of the games respectively. Compared to various standard classiﬁcation algorithms, the NNs were found to be only slightly more accurate than standard discriminant analyses. However, the latter do not model the dynamic learning process and have an ad hoc postulated functional form. In contrast, a NN agent’s behavior evolves with experience and is capable of taking on any functional form according to the universal approximation theorem.},
groups={tesse:2},
langid={english},
}

@inproceedings{2009BabesQ,
pages={4},
groups={tesse:5},
author={Babes, Monica and Wunder, Michael and Littman, Michael},
booktitle={Proc. of 8th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2009)},
langid={english},
title={Q-learning in Two-Player Two-Action Games},
abstract={Q-learning is a simple, powerful algorithm for behavior learning. It was derived in the context of single agent decision making in Markov decision process environments, but its applicability is much broader—in experiments in multiagent environments, Q-learning has also performed well. Our preliminary analysis using dynamical systems ﬁnds that Qlearning’s indirect control of behavior via estimates of value contributes to its beneﬁcial performance in general-sum 2player games like the Prisoner’s Dilemma.},
date={2009},
}

@article{2010WierstraRecurrent,
number={5},
date={2010-10-01},
volume={18},
groups={tesse:5},
urldate={2019-03-28},
pages={620--634},
issn={1367-0751, 1368-9894},
journaltitle={Logic Journal of IGPL},
author={Wierstra, D. and Forster, A. and Peters, J. and Schmidhuber, J.},
doi={10.1093/jigpal/jzp049},
langid={english},
title={Recurrent policy gradients},
abstract={Reinforcement learning for partially observable Markov decision problems (POMDPs) is a challenge as it requires policies with an internal state. Traditional approaches suﬀer signiﬁcantly from this shortcoming and usually make strong assumptions on the problem domain such as perfect system models, state-estimators and a Markovian hidden system. Recurrent neural networks (RNNs) oﬀer a natural framework for dealing with policy learning using hidden state and require only few limiting assumptions. As they can be trained well using gradient descent, they are suited for policy gradient approaches.},
}

@inproceedings{2010VassiliadesMultiagent,
date={2010-07},
title={Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma: Fast cooperation through evolved payoffs},
author={Vassiliades, Vassilis and Christodoulou, Chris},
isbn={978-1-4244-6916-1},
eventtitle={2010 International Joint Conference on Neural Networks (IJCNN)},
location={Barcelona, Spain},
pages={1--8},
publisher={IEEE},
doi={10.1109/IJCNN.2010.5596937},
booktitle={The 2010 International Joint Conference on Neural Networks (IJCNN)},
url={http://ieeexplore.ieee.org/document/5596937/},
langid={english},
abstract={In this paper, we investigate the importance of rewards in Multiagent Reinforcement Learning in the context of the Iterated Prisoner's Dilemma. We use an evolutionary algorithm to evolve valid payoff structures with the aim of encouraging mutual cooperation. An exhaustive analysis is performed by investigating the effect of: i) the lower and upper bounds of the search space of the payoff values, ii) the reward sign, iii) the population size, and iv) the mutation operators used. Our results indicate that valid structures that encourage cooperation can quickly be obtained, while their analysis shows that: i) they should contain a mixture of positive and negative values and ii) the magnitude of the positive values should be much smaller than the magnitude of the negative values.},
groups={tesse:5},
urldate={2019-03-28},
shorttitle={Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma},
}

@article{2014HeNeural,
volume={57},
date={2014-09},
pages={73--78},
url={https://linkinghub.elsevier.com/retrieve/pii/S0893608014001348},
doi={10.1016/j.neunet.2014.06.002},
author={He, Xing and Yu, Junzhi and Huang, Tingwen and Li, Chuandong and Li, Chaojie},
urldate={2019-03-28},
issn={0893-6080},
abstract={In this paper, based on an equivalent mixed linear complementarity problem, we propose a neural network to solve multiuser power control optimization problems (MPCOP), which is modeled as the noncooperative Nash game in modern digital subscriber line (DSL). If the channel crosstalk coeﬃcients matrix is positive semideﬁnite, it is shown that the proposed neural network is stable in the sense of Lyapunov and global convergent to an Nash equilibrium, and the Nash equilibrium is unique if the channel crosstalk coeﬃcients matrix is positive deﬁnite. Finally, simulation results on two numerical examples show the eﬀectiveness and performance of the proposed neural network.},
journaltitle={Neural Networks},
title={Neural network for solving Nash equilibrium problem in application of multiuser power control},
langid={english},
}

@article{2015NarasimhanLanguage,
date={2015-06-30},
langid={english},
url={http://arxiv.org/abs/1506.08941},
eprint={1506.08941},
urldate={2019-03-28},
title={Language Understanding for Text-based Games Using Deep Reinforcement Learning},
author={Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
abstract={In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.},
groups={tesse:5},
eprinttype={arxiv},
journaltitle={arXiv:1506.08941 [cs]},
}

@article{schuurmans_deep_nodate,
author={Schuurmans, Dale and Zinkevich, Martin A},
title={Deep Learning Games},
pages={9},
abstract={We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods. For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game. We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem. Based on these connections we investigate alternative learning methods, and ﬁnd that regret matching can achieve competitive training performance while producing sparser models than current deep learning strategies.},
langid={english},
}

@article{2015HausknechtDeep,
eprint={1507.06527},
abstract={Deep Reinforcement Learning has yielded proﬁcient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the ﬁrst post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN’s performance on standard Atari games and partially observed equivalents featuring ﬂickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN’s performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN’s performance degrades less than DQN’s. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN’s input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
date={2015-07-23},
url={http://arxiv.org/abs/1507.06527},
eprinttype={arxiv},
author={Hausknecht, Matthew and Stone, Peter},
urldate={2019-03-28},
groups={tesse:5},
langid={english},
title={Deep Recurrent Q-Learning for Partially Observable MDPs},
journaltitle={arXiv:1507.06527 [cs]},
}

@article{2017UrbanGaussian,
journaltitle={arXiv:1711.11059 [cs, stat]},
eprinttype={arxiv},
langid={english},
url={http://arxiv.org/abs/1711.11059},
abstract={We propose stochastic, non-parametric activation functions that are fully learnable and individual to each neuron. Complexity and the risk of overﬁtting are controlled by placing a Gaussian process prior over these functions. The result is the Gaussian process neuron, a probabilistic unit that can be used as the basic building block for probabilistic graphical models that resemble the structure of neural networks. The proposed model can intrinsically handle uncertainties in its inputs and self-estimate the conﬁdence of its predictions. Using variational Bayesian inference and the central limit theorem, a fully deterministic loss function is derived, allowing it to be trained as efﬁciently as a conventional neural network using mini-batch gradient descent. The posterior distribution of activation functions is inferred from the training data alongside the weights of the network.},
groups={tesse:5},
eprint={1711.11059},
title={Gaussian Process Neurons Learn Stochastic Activation Functions},
date={2017-11-29},
author={Urban, Sebastian and Basalla, Marcus and van der Smagt, Patrick},
urldate={2019-03-28},
}

@article{2017HarperReinforcement,
number={12},
title={Reinforcement learning produces dominant strategies for the Iterated Prisoner’s Dilemma},
langid={english},
author={Harper, Marc and Knight, Vincent and Jones, Martin and Koutsovoulos, Georgios and Glynatsi, Nikoleta E. and Campbell, Owen},
doi={10.1371/journal.pone.0188046},
issn={1932-6203},
pages={e0188046},
journaltitle={PLOS ONE},
editor={Deng, Yong},
date={2017-12-11},
urldate={2019-03-28},
volume={12},
}

@inproceedings{2017KorukhovaTraining,
author={Korukhova, Yulia and Kuryshev, Sergey},
eventtitle={9th International Conference on Agents and Artificial Intelligence},
location={Porto, Portugal},
isbn={978-989-758-219-6 978-989-758-220-2},
urldate={2019-03-28},
groups={tesse:5},
doi={10.5220/0006242102960301},
shorttitle={Training Agents with Neural Networks in Systems with Imperfect Information},
pages={296--301},
booktitle={Proceedings of the 9th International Conference on Agents and Artificial Intelligence},
date={2017},
title={Training Agents with Neural Networks in Systems with Imperfect Information:},
publisher={SCITEPRESS - Science and Technology Publications},
langid={english},
url={http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006242102960301},
abstract={Multi-agent Systems, Neural Networks, Dominated Strategies.},
}

@article{KamraPolicy,
author={Kamra, Nitin and Gupta, Umang and Fang, Fei and Liu, Yan and Tambe, Milind},
pages={10},
groups={tesse:2},
abstract={A wealth of algorithms centered around (integer) linear programming have been proposed to compute equilibrium strategies in security games with discrete states and actions. However, in practice many domains possess continuous state and action spaces. In this paper, we consider a continuous space security game model with inﬁnite-size action sets for players and present a novel deep learning based approach to extend the existing toolkit for solving security games. Specifically, we present (i) OptGradFP, a novel and general algorithm that searches for the optimal defender strategy in a parameterized continuous search space, and can also be used to learn policies over multiple game states simultaneously; (ii) OptGradFP-NN, a convolutional neural network based implementation of OptGradFP for continuous space security games. We demonstrate the potential to predict good defender strategies via experiments and analysis of OptGradFP and OptGradFP-NN on discrete and continuous game settings.},
langid={english},
title={Policy Learning for Continuous Space Security Games Using Neural Networks},
}

@article{2018WangTowards,
date={2018-02-28},
eprint={1803.00162},
url={http://arxiv.org/abs/1803.00162},
journaltitle={arXiv:1803.00162 [cs]},
author={Wang, Weixun and Hao, Jianye and Wang, Yixi and Taylor, Matthew},
shorttitle={Towards Cooperation in Sequential Prisoner's Dilemmas},
abstract={The Iterated Prisoner’s Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner’s dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reﬂecting grades of cooperation. We introduce a Sequential Prisoner’s Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The ﬁrst phase is ofﬂine: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the ApplePear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents.},
urldate={2019-03-28},
langid={english},
eprinttype={arxiv},
title={Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent Reinforcement Learning Approach},
}

@article{2017HausknechtMachine,
url={http://ieeexplore.ieee.org/document/7393590/},
langid={english},
title={Machine Learning Capabilities of a Simulated Cerebellum},
number={3},
urldate={2019-03-28},
pages={510--522},
volume={28},
issn={2162-237X, 2162-2388},
abstract={This article describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks - eyelid conditioning, pendulum balancing, PID control, robot balancing, pattern recognition, and MNIST handwritten digit recognition. These tasks span several paradigms of machine learning including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, reinforcement learning and temporal pattern recognition both prove problematic due to the delayed nature of error signals and the simulator’s inability to solve the credit assignment problem. These results are consistent with previous ﬁndings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning while the cerebellum handles supervised learning.},
date={2017-03},
groups={tesse:5},
journaltitle={IEEE Transactions on Neural Networks and Learning Systems},
doi={10.1109/TNNLS.2015.2512838},
author={Hausknecht, Matthew and Li, Wen-Ke and Mauk, Michael and Stone, Peter},
}

@collection{1995ChauvinBackpropagation,
location={Hillsdale, N.J},
langid={english},
publisher={Lawrence Erlbaum Associates},
groups={tesse:5},
series={Developments in connectionist theory},
shorttitle={Backpropagation},
keywords={Back propagation (Artificial intelligence)},
title={Backpropagation: theory, architectures, and applications},
editor={Chauvin, Yves and Rumelhart, David E.},
pagetotal={561},
isbn={978-0-8058-1258-9 978-0-8058-1259-6},
date={1995},
}

@unpublished{2014SathyanarayanaGentle,
langid={english},
groups={tesse:5},
date={2014},
author={Sathyanarayana, Shashi},
pages={15},
title={A Gentle Introduction to Backpropagation},
}

@article{2016ZagoruykoWide,
author={Zagoruyko, Sergey and Komodakis, Nikos},
eprint={1605.07146},
urldate={2019-03-28},
abstract={Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
groups={tesse:5},
title={Wide Residual Networks},
eprinttype={arxiv},
url={http://arxiv.org/abs/1605.07146},
langid={english},
date={2016-05-23},
journaltitle={arXiv:1605.07146 [cs]},
}

@article{2018DonahueAdversarial,
eprinttype={arxiv},
author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
url={http://arxiv.org/abs/1802.04208},
urldate={2019-03-28},
journaltitle={arXiv:1802.04208 [cs]},
eprint={1802.04208},
title={Adversarial Audio Synthesis},
date={2018-02-12},
groups={tesse:5},
abstract={While Generative Adversarial Networks (GANs) have seen wide success at the problem of synthesizing realistic images, they have seen little application to audio generation. Unlike for images, a barrier to success is that the best discriminative representations for audio tend to be non-invertible, and thus cannot be used to synthesize listenable outputs. In this paper, we introduce WaveGAN, a ﬁrst attempt at applying GANs to raw audio synthesis in an unsupervised setting. Our experiments on speech demonstrate that WaveGAN can produce intelligible words from a small vocabulary of human speech, as well as synthesize audio from other domains such as bird vocalizations, drums, and piano. Qualitatively, we ﬁnd that human judges prefer the generated examples from WaveGAN over those from a method which na¨ıvely apply GANs on image-like audio feature representations.},
langid={english},
}

@inproceedings{1999SilvescuFourier,
urldate={2019-03-28},
title={Fourier neural networks},
groups={tesse:5},
volume={1},
abstract={A new kind of neuron model that has a Fourier-like IN/OUT function is introduced. The model is discussed in a general theoretical framework and some completeness theorems are presented. Current experimental results show that the new model outperforms by a large margin both in representational power and convergence speed the classical mathematical model of neuron based on weighted sum of inputs ﬁltered by a nonlinear function. The new model is also appealing from a neurophysiological point of view because it produces a more realistic representation by considering the inputs as oscillations.},
isbn={978-0-7803-5529-3},
author={Silvescu, A.},
location={Washington, DC, USA},
url={http://ieeexplore.ieee.org/document/831544/},
langid={english},
eventtitle={International Conference on Neural Networks},
publisher={IEEE},
booktitle={IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
date={1999},
doi={10.1109/IJCNN.1999.831544},
pages={488--491},
}

@article{2002StanleyEvolving,
abstract={An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best ﬁxed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efﬁciency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signiﬁcantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
groups={tesse:5},
volume={10},
doi={10.1162/106365602320169811},
langid={english},
author={Stanley, Kenneth O. and Miikkulainen, Risto},
urldate={2019-03-28},
issn={1063-6560, 1530-9304},
journaltitle={Evolutionary Computation},
number={2},
pages={99--127},
date={2002-06},
title={Evolving Neural Networks through Augmenting Topologies},
}

@article{2007YadavTime,
langid={english},
pages={1157--1163},
volume={7},
issn={1568-4946},
abstract={Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network.},
doi={10.1016/j.asoc.2006.01.003},
journaltitle={Applied Soft Computing},
date={2007-08},
author={Yadav, R. N. and Kalra, P. K. and John, J.},
title={Time series prediction with single multiplicative neuron model},
number={4},
url={https://linkinghub.elsevier.com/retrieve/pii/S156849460600007X},
urldate={2019-03-28},
groups={tesse:5},
}

@article{2013GravesGenerating,
eprinttype={arxiv},
title={Generating Sequences With Recurrent Neural Networks},
urldate={2019-03-28},
langid={english},
url={http://arxiv.org/abs/1308.0850},
groups={tesse:5},
author={Graves, Alex},
abstract={This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
eprint={1308.0850},
date={2013-08-04},
journaltitle={arXiv:1308.0850 [cs]},
}

@article{2013LinNetwork,
eprinttype={arxiv},
abstract={We propose a novel deep network structure called “Network In Network”(NIN) to enhance model discriminability for local patches within the receptive ﬁeld. The conventional convolutional layer uses linear ﬁlters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive ﬁeld. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classiﬁcation layer, which is easier to interpret and less prone to overﬁtting than traditional fully connected layers. We demonstrated the state-of-the-art classiﬁcation performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
groups={tesse:5},
langid={english},
url={http://arxiv.org/abs/1312.4400},
journaltitle={arXiv:1312.4400 [cs]},
author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
title={Network In Network},
eprint={1312.4400},
urldate={2019-03-28},
date={2013-12-16},
}

@article{hahn_expressive_nodate,
pages={193},
title={Expressive sampling synthesis. Learning extended source-filter models from instrument sound databases for expressive sample manipulations},
author={Hahn, Henrik},
langid={english},
}

@article{2016WangFinancial,
title={Financial Time Series Prediction Using Elman Recurrent Random Neural Networks},
journaltitle={Computational Intelligence and Neuroscience},
doi={10.1155/2016/4742515},
volume={2016},
pages={1--14},
langid={english},
url={http://www.hindawi.com/journals/cin/2016/4742515/},
author={Wang, Jie and Wang, Jun and Fang, Wen and Niu, Hongli},
date={2016},
urldate={2019-03-28},
groups={tesse:5},
issn={1687-5265, 1687-5273},
}

@article{NayebiGruva,
groups={tesse:2},
langid={english},
author={Nayebi, Aran and Vitelli, Matt},
title={GRUV: Algorithmic Music Generation using Recurrent Neural Networks},
pages={6},
abstract={We compare the performance of two different types of recurrent neural networks (RNNs) for the task of algorithmic music generation, with audio waveforms as input. In particular, we focus on RNNs that have a sophisticated gating mechanism, namely, the Long Short-Term Memory (LSTM) network and the recently introduced Gated Recurrent Unit (GRU). Our results indicate that the generated outputs of the LSTM network were signiﬁcantly more musically plausible than those of the GRU.},
}

@article{2016BarrowCross,
volume={32},
title={Cross-validation aggregation for combining autoregressive neural network forecasts},
abstract={This paper evaluates k-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to k-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S0169207016300188},
number={4},
pages={1120--1137},
author={Barrow, Devon K. and Crone, Sven F.},
doi={10.1016/j.ijforecast.2015.12.011},
journaltitle={International Journal of Forecasting},
date={2016-10},
issn={0169-2070},
urldate={2019-03-28},
}

@article{2016ShinDeep,
journaltitle={IEEE Transactions on Medical Imaging},
shorttitle={Deep Convolutional Neural Networks for Computer-Aided Detection},
doi={10.1109/TMI.2016.2528162},
abstract={Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet) and the revival of deep convolutional neural networks (CNN). CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufﬁcient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classiﬁcation: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised ﬁne-tuning. Another effective method is transfer learning, i.e., ﬁne-tuning CNN models (supervised) pre-trained from natural image dataset to medical image tasks (although domain transfer between two medical image datasets is also possible).},
pages={1285--1298},
volume={35},
url={http://ieeexplore.ieee.org/document/7404017/},
date={2016-05},
title={Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning},
issn={0278-0062, 1558-254X},
author={Shin, Hoo-Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
urldate={2019-03-28},
number={5},
langid={english},
}

@article{2016DingFacenet2expnet,
journaltitle={arXiv:1609.06591 [cs]},
eprint={1609.06591},
url={http://arxiv.org/abs/1609.06591},
author={Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
title={FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition},
urldate={2019-03-28},
abstract={Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although ﬁne-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We ﬁrst propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the reﬁning stage, we append fullyconnected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
langid={english},
shorttitle={FaceNet2ExpNet},
date={2016-09-21},
groups={tesse:5},
eprinttype={arxiv},
}

@article{2016WuInvestigating,
journaltitle={arXiv:1601.02539 [cs]},
eprint={1601.02539},
abstract={Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve signiﬁcantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simpliﬁed architecture. The simpliﬁed architecture has signiﬁcantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
langid={english},
groups={tesse:5},
url={http://arxiv.org/abs/1601.02539},
author={Wu, Zhizheng and King, Simon},
urldate={2019-03-28},
title={Investigating gated recurrent neural networks for speech synthesis},
eprinttype={arxiv},
date={2016-01-11},
}

@article{2017ValinHybrid,
journaltitle={arXiv:1709.08243 [cs, eess]},
langid={english},
eprint={1709.08243},
title={A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement},
urldate={2019-03-28},
url={http://arxiv.org/abs/1709.08243},
date={2017-09-24},
eprinttype={arxiv},
author={Valin, Jean-Marc},
abstract={Despite noise suppression being a mature area in signal processing, it remains highly dependent on ﬁne tuning of estimator algorithms and parameters. In this paper, we demonstrate a hybrid DSP/deep learning approach to noise suppression. A deep neural network with four hidden layers is used to estimate ideal critical band gains, while a more traditional pitch ﬁlter attenuates noise between pitch harmonics. The approach achieves signiﬁcantly higher quality than a traditional minimum mean squared error spectral estimator, while keeping the complexity low enough for real-time operation at 48 kHz on a low-power processor.},
}

@article{2017JanaiComputer,
shorttitle={Computer Vision for Autonomous Vehicles},
eprint={1704.05519},
title={Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art},
url={http://arxiv.org/abs/1704.05519},
eprinttype={arxiv},
abstract={Recent years have witnessed amazing progress in AI related ﬁelds such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing ﬁeld, however, it becomes increasingly diﬃcult to stay up-to-date or enter the ﬁeld as a beginner. While several topic speciﬁc survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several speciﬁc topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we ﬁrst provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
groups={tesse:5},
journaltitle={arXiv:1704.05519 [cs]},
date={2017-04-18},
urldate={2019-03-28},
author={Janai, Joel and Güney, Fatma and Behl, Aseem and Geiger, Andreas},
langid={english},
}

@article{2016SaxenaConvolutional,
langid={english},
groups={tesse:5},
eprinttype={arxiv},
date={2016-06-08},
url={http://arxiv.org/abs/1606.02492},
title={Convolutional Neural Fabrics},
abstract={Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a “fabric” that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on backpropagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classiﬁcation on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
journaltitle={arXiv:1606.02492 [cs]},
eprint={1606.02492},
urldate={2019-03-28},
author={Saxena, Shreyas and Verbeek, Jakob},
}

@article{ChenAccelerator,
author={Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
groups={tesse:2},
pages={4},
langid={english},
title={Accelerator for Deep Convolutional Neural Networks},
}

@article{2016ZhaoPeak,
journaltitle={arXiv:1607.06997 [cs]},
eprinttype={arxiv},
author={Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
eprint={1607.06997},
date={2016-07-24},
urldate={2019-03-28},
title={Peak-Piloted Deep Network for Facial Expression Recognition},
abstract={Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A specialpurpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper deﬁnition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
url={http://arxiv.org/abs/1607.06997},
langid={english},
groups={tesse:5},
}

@article{1983JaffeExtensions,
pages={56--69},
journaltitle={Computer Music Journal},
number={2},
author={Jaffe, David A. and Smith, Julius O.},
title={Extensions of the Karplus-Strong Plucked-String Algorithm},
issn={0148-9267},
date={1983},
urldate={2019-03-28},
doi={10.2307/3680063},
volume={7},
url={https://www.jstor.org/stable/3680063},
groups={tesse:5},
}

@inproceedings{2018FukudaStochastic,
pages={1--3},
date={2018-05},
title={Stochastic weights binary neural networks on FPGA},
doi={10.1109/ISNE.2018.8394726},
booktitle={2018 7th International Symposium on Next Generation Electronics (ISNE)},
abstract={To achieve an advanced Internet of Things (IoT), it is necessary to combine artificial intelligence (AI) with IoT. Compact circuits that can operate AI functions will be useful for this purpose. Therefore, we propose stochastic weights binary neural networks (SWBNN). SWBNNs are more accurate than binary neural networks (BNN) with small circuits. BNNs can be realized with small circuits since binary calculation needs simpler circuits than real number calculation. However, BNNs have lower accuracy than networks with real numbers. Thus, the proposed SWBNNs are BNNs that behave stochastically, which makes them more accurate than BNNs. Moreover, SWBNNs can still be achieved with small circuits since they execute binary calculation. As a result, the accuracy for the test data of SWBNNs is closer to the accuracy for learning data than the accuracy for the test data of BNNs is. Especially when using the CIFAR10 database, the difference in the identification accuracy rate between learning data and test data decreased from 6 /\\% for BNNs to 2 /\\% for SWBNNs. From results of a field-programmable gate array (FPGA) implementation, circuits of SWBNNs are sufficiently small although they are 10 /\\% bigger than those of BNNs. Therefore, SWBNNs are more accurate than BNNs, and the circuit costs ofintroducing stochastic weights are low.},
author={Fukuda, Y. and Kawahara, T.},
eventtitle={2018 7th International Symposium on Next Generation Electronics (ISNE)},
groups={tesse:5},
}

@article{2009SaitisPhysical,
pages={2684--2684},
author={Saitis, Charalampos and Orr, Sarah and van Walstijn, Maarten},
shorttitle={Physical modeling of the piano},
volume={125},
journaltitle={The Journal of the Acoustical Society of America},
langid={english},
title={Physical modeling of the piano: An investigation into the effect of string stiffness on the hammer string interaction.},
date={2009-04},
doi={10.1121/1.4784255},
urldate={2019-03-28},
issn={0001-4966},
number={4},
}

@unpublished{2001AirdTowards,
date={2001},
title={Towards material modelling in physical models using digital waveguides},
groups={tesse:5, Digital Waveguides},
author={Aird, Marc and Laird, Joel},
}

@book{2014MontgomeryApplied,
isbn={978-1-118-53971-2},
edition={Sixth edition},
abstract={"This best-selling engineering statistics text provides a practical approach that is more oriented to engineering and the chemical and physical sciences than many similar texts. It is packed with unique problem sets that reflect realistic situations engineers will encounter in their working lives. This text shows how statistics, the science of data is just as important for engineers as the mechanical, electrical, and materials sciences"--},
author={Montgomery, Douglas C. and Runger, George C.},
groups={tesse:5, Probability},
title={Applied statistics and probability for engineers},
pagetotal={811},
date={2014},
publisher={John Wiley and Sons, Inc},
location={Hoboken, NJ},
}

@thesis{1989WatkinsLearning,
institution={King's College, Cambridge},
title={Learning from delayed rewards},
type={phdthesis},
author={Watkins, Christopher John Cornish Hellaby},
date={1989},
groups={tesse:5},
}

@inproceedings{1988LecunTheoretical,
title={A theoretical framework for back-propagation},
author={LeCun, Yann and Touresky, D. and Hinton, G. and Sejnowski, T.},
pages={21--28},
booktitle={Proceedings of the 1988 connectionist models summer school},
date={1988},
volume={1},
groups={tesse:5},
publisher={CMU, Pittsburgh, Pa: Morgan Kaufmann},
}

@misc{2018PeirceSolving,
note={Introductory lecture notes on Partial Differential Equations},
url={https://www.math.ubc.ca//textasciitilde peirce/M257///////\\_316///////\\_2012///////\\_Lecture///////\\_8.pdf},
author={Peirce, Anthony},
langid={english},
date={2018},
groups={tesse:5, Finite Difference Methods},
publisher={The University of British Columbia},
abstract={In this lecture we introduce the finite difference method that is widely used for approximating PDEs using the computer. We use the definition of the derivative and Taylor series to derive finite difference approximations to the first and second derivatives of a function. We then use these finite difference quotients to approximate the derivatives in the heat equation and to derive a finite difference approximation to the heat equation. Similarly, the technique is applied to the wave equation and Laplace’s Equation. The technique is illustrated using EXCEL spreadsheets.},
title={Solving the Heat, Laplace and Wave quations using finite difference methods},
}

@article{1994MartucciSymmetric,
date={1994-05},
number={5},
volume={42},
pages={1038--1051},
doi={10.1109/78.295213},
author={Martucci, S. A.},
issn={1053-587X},
groups={tesse:5},
title={Symmetric convolution and the discrete sine and cosine transforms},
journaltitle={IEEE Transactions on Signal Processing},
abstract={This paper discusses the use of symmetric convolution and the discrete sine and cosine transforms (DSTs and DCTs) for general digital signal processing. The operation of symmetric convolution is a formalized approach to convolving symmetrically extended sequences. The result is the same as that obtained by taking an inverse discrete trigonometric transform (DTT) of the product of the forward DTTs of those two sequences. There are 16 members in the family of DTTs. Each provides a representation for a corresponding distinct type of symmetric-periodic sequence. The author defines symmetric convolution, relates the DSTs and DCTs to symmetric-periodic sequences, and then use these principles to develop simple but powerful convolution-multiplication properties for the entire family of DSTs and DCTs. Symmetric convolution can be used for discrete linear filtering when the filter is symmetric or antisymmetric. The filtering will be efficient because fast algorithms exist for all versions of the DTTs. Conventional linear convolution is possible if one first zero-pad the input data. Symmetric convolution and its fast implementation using DTTs are now an alternative to circular convolution and the DFT.<<ETX>>},
}

@article{1996SandholmMultiagent,
title={Multiagent reinforcement learning in the Iterated Prisoner's Dilemma},
abstract={Reinforcement learning (RL) is based on the idea that the tendency to produce an action should be strengthened (reinforced) if it produces favorable results, and weakened if it produces unfavorable results. Q-learning is a recent RL algorithm that does not need a model of its environment and can be used on-line. Therefore, it is well suited for use in repeated games against an unknown opponent. Most RL research has been confined to single-agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the Iterated Prisoner's Dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-For-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-For-Tat. Playing against another learner was more difficult because the adaptation of the other learner created a non-stationary environment, and because the other learner was not endowed with any a priori knowledge about the IPD game such as a policy designed to encourage cooperation. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and longer exploration schedules fared best in the IPD games.},
doi={10.1016/0303-2647(95)01551-5},
groups={tesse:5},
volume={37},
issn={0303-2647},
number={1},
shortjournal={Biosystems},
urldate={2019-03-28},
url={http://www.sciencedirect.com/science/article/pii/0303264795015515},
journaltitle={Biosystems},
pages={147--166},
author={Sandholm, Tuomas W. and Crites, Robert H.},
date={1996-01-01},
}

@book{2012HarringtonMachine,
publisher={Manning Publications Co},
title={Machine learning in action},
date={2012},
langid={english},
pagetotal={354},
isbn={978-1-61729-018-3},
location={Shelter Island, N.Y},
note={OCLC: ocn746834657},
author={Harrington, Peter},
groups={tesse:5},
keywords={Machine learning, Handbooks, manuals, etc},
}

@book{1992HollandAdaptation,
title={Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence},
publisher={MIT Press},
series={Complex adaptive systems},
groups={tesse:5},
location={Cambridge, Mass},
isbn={978-0-262-08213-6 978-0-262-58111-0},
keywords={Adaptation (Biology), Adaptive control systems, Mathematical models},
shorttitle={Adaptation in natural and artificial systems},
date={1992},
langid={english},
author={Holland, John H.},
pagetotal={211},
edition={1st MIT Press ed},
}

@article{2018WangPacking,
doi={10.1109/TPAMI.2018.2857824},
pages={1--1},
date={2018},
author={Wang, Y. and Xu, C. and Xu, C. and Tao, D.},
title={Packing Convolutional Neural Networks in the Frequency Domain},
abstract={Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present a series of approaches for compressing and speeding up CNNs in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. Furthermore, we explore a data-driven method for removing redundancies in both spatial and frequency domains, which allows us to discard more useless weights by keeping similar accuracies. After obtaining the optimal sparse CNN in the frequency domain, we relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.},
issn={0162-8828},
journaltitle={IEEE Transactions on Pattern Analysis and Machine Intelligence},
}

@book{2017McclureTensorflow,
pagetotal={370},
groups={tesse:5},
langid={english},
isbn={978-1-78646-216-9},
title={TensorFlow Machine Learning Cookbook},
location={s.l},
author={McClure, Nick},
note={OCLC: 992150633},
publisher={Packt Publishing},
date={2017},
}

@book{2016BonninBuilding,
date={2016},
title={Building Machine Learning Projects with TensorFlow},
groups={tesse:5},
langid={english},
author={Bonnin, Rodolfo},
}

@incollection{2006WerbosBackwards,
doi={10.1007/3-540-28438-9///////\\_2},
author={Werbos, Paul J.},
urldate={2019-03-28},
shorttitle={Backwards Differentiation in AD and Neural Nets},
booktitle={Automatic Differentiation: Applications, Theory, and Implementations},
date={2006},
publisher={Springer-Verlag},
abstract={Backwards calculation of derivatives – sometimes called the reverse mode, the full adjoint method, or backpropagation, has been developed and applied in many fields. This paper reviews several strands of history, advanced capabilities and types of application – particularly those which are crucial to the development of brain-like capabilities in intelligent control and artificial intelligence.},
title={Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities},
location={Berlin/Heidelberg},
langid={english},
volume={50},
editor={Bücker, Martin and Corliss, George and Naumann, Uwe and Hovland, Paul and Norris, Boyana},
isbn={978-3-540-28403-1},
pages={15--34},
groups={tesse:5},
}

@book{2008PoliField,
title={A field guide to genetic programming},
isbn={978-1-4092-0073-4},
author={Poli, Riccardo and Langdon, William B. and McPhee, Nicholas F. and Koza, John R.},
location={[Morrisville, NC},
groups={tesse:5, Programming},
date={2008},
note={OCLC: 837998350},
publisher={Lulu Press]},
langid={english},
pagetotal={233},
}

@article{2004LengLine,
url={https://linkinghub.elsevier.com/retrieve/pii/S0893608004001698},
abstract={This paper presents a new on-line algorithm for creating a self-organizing fuzzy neural network (SOFNN) from sample patterns to implement a singleton or Takagi-Sugeno (TS) type fuzzy model. The SOFNN is based on ellipsoidal basis function (EBF) neurons consisting of a center vector and a width vector. New methods of the structure learning and the parameter learning, based on new adding and pruning techniques and a recursive on-line learning algorithm, are proposed and developed. A proof of the convergence of both the estimation error and the linear network parameters is also given in the paper. The proposed methods are very simple and effective and generate a fuzzy neural model with a high accuracy and compact structure. Simulation work shows that the SOFNN has the capability of self-organization to determine the structure and parameters of the network automatically.},
groups={tesse:5},
langid={english},
journaltitle={Neural Networks},
title={An on-line algorithm for creating self-organizing fuzzy neural networks},
pages={1477--1493},
number={10},
issn={0893-6080},
volume={17},
date={2004-12},
doi={10.1016/j.neunet.2004.07.009},
author={Leng, Gang and Prasad, Girijesh and McGinnity, Thomas Martin},
urldate={2019-03-28},
}

@article{2017LiFrequency,
eprinttype={arxiv},
urldate={2019-03-28},
author={Li, Junxuan and You, Shaodi and Robles-Kelly, Antonio},
groups={tesse:5},
abstract={In this paper, we present a frequency domain neural network for image super-resolution. The network employs the convolution theorem so as to cast convolutions in the spatial domain as products in the frequency domain. Moreover, the non-linearity in deep nets, often achieved by a rectifier unit, is here cast as a convolution in the frequency domain. This not only yields a network which is very computationally efficient at testing but also one whose parameters can all be learnt accordingly. The network can be trained using back propagation and is devoid of complex numbers due to the use of the Hartley transform as an alternative to the Fourier transform. Moreover, the network is potentially applicable to other problems elsewhere in computer vision and image processing which are often cast in the frequency domain. We show results on super-resolution and compare against alternatives elsewhere in the literature. In our experiments, our network is one to two orders of magnitude faster than the alternatives with an imperceptible loss of performance.},
title={A Frequency Domain Neural Network for Fast Image Super-resolution},
eprint={1712.03037},
url={http://arxiv.org/abs/1712.03037},
journaltitle={arXiv:1712.03037 [cs]},
date={2017-12-08},
}

@inproceedings{2016ChenCompressing,
groups={tesse:5},
title={Compressing Convolutional Neural Networks in the Frequency Domain},
pages={1475--1484},
isbn={978-1-4503-4232-2},
publisher={ACM Press},
booktitle={Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
url={http://dl.acm.org/citation.cfm?doid=2939672.2939839},
author={Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
location={San Francisco, California, USA},
date={2016},
doi={10.1145/2939672.2939839},
eventtitle={the 22nd ACM SIGKDD International Conference},
urldate={2019-03-28},
langid={english},
}

@collection{1979DenkerErgodic,
isbn={978-0-387-09517-2},
location={Berlin ; New York},
publisher={Springer-Verlag},
editor={Denker, Manfred and Jacobs, Konrad},
date={1979},
title={Ergodic theory: proceedings, Oberwolfach, Germany, June 11-17, 1978},
shorttitle={Ergodic theory},
pagetotal={209},
series={Lecture notes in mathematics ; 729},
}

@article{2018MukhometzianovCapsnet,
groups={tesse:5},
journaltitle={arXiv:1805.11195 [cs, stat]},
author={Mukhometzianov, Rinat and Carrillo, Juan},
date={2018-05-28},
eprinttype={arxiv},
eprint={1805.11195},
url={http://arxiv.org/abs/1805.11195},
abstract={Image classification has become one of the main tasks in the field of computer vision technologies. In this context, a recent algorithm called CapsNet that implements an approach based on activity vectors and dynamic routing between capsules may overcome some of the limitations of the current state of the art artificial neural networks (ANN) classifiers, such as convolutional neural networks (CNN). In this paper, we evaluated the performance of the CapsNet algorithm in comparison with three well-known classifiers (Fisher-faces, LeNet, and ResNet). We tested the classification accuracy on four datasets with a different number of instances and classes, including images of faces, traffic signs, and everyday objects. The evaluation results show that even for simple architectures, training the CapsNet algorithm requires significant computational resources and its classification performance falls below the average accuracy values of the other three classifiers. However, we argue that CapsNet seems to be a promising new technique for image classification, and further experiments using more robust computation resources and re-fined CapsNet architectures may produce better outcomes.},
urldate={2019-03-28},
title={CapsNet comparative performance evaluation for image classification},
}

@article{2007FastlCritical,
langid={english},
author={Fastl, Hugo and Zwicker, Eberhard},
date={2007},
note={OCLC: 7328866217},
groups={tesse:2},
pages={149--173},
title={Critical Bands and Excitation},
}

@article{CookReal,
groups={tesse:2, DSP},
langid={english},
pages={269},
author={Cook, Perry R. (Author)},
title={Real Sound Synthesis for Interactive Applications},
}

@book{2003TrautmannDigital,
date={2003},
location={Boston, MA},
urldate={2019-03-31},
langid={english},
publisher={Springer US},
isbn={978-1-4613-4900-6 978-1-4615-0049-0},
title={Digital Sound Synthesis by Physical Modeling Using the Functional Transformation Method},
doi={10.1007/978-1-4615-0049-0},
author={Trautmann, Lutz and Rabenstein, Rudolf},
}

@book{2003JaynesProbability,
url={http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847},
abstract={The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E.T. Jaynes dispels the imaginary distinction between'probability theory'and'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary. (A cura dell'editore).},
location={Cambridge},
shorttitle={Probability theory},
publisher={Cambridge University Press},
author={Jaynes, Edwin T and Bretthorst, G. Larry and Publishing, EBSCO},
title={Probability theory: the logic of science},
date={2003},
isbn={978-0-511-06589-7},
note={OCLC: 982265136},
urldate={2019-03-31},
langid={english},
}

@book{2013FieldDiscovering,
date={2013},
edition={Reprint},
pagetotal={957},
publisher={Sage},
title={Discovering statistics using R},
note={OCLC: 854989686},
groups={tesse:5, Probability},
author={Field, Andy and Miles, Jeremy and Field, Zoë},
langid={english},
isbn={978-1-4462-0045-2 978-1-4462-0046-9},
location={Los Angeles, Calif.},
}

@book{2012TijmsUnderstanding,
isbn={978-1-107-65856-1},
note={OCLC: 803790744},
location={Cambridge},
publisher={Cambridge Univ. Press},
title={Understanding probability},
groups={tesse:5, Probability},
edition={3. ed},
pagetotal={562},
abstract={"Understanding Probability is a unique and stimulating approach to a first course in probability. The first part of the book demystifies probability and uses many wonderful probability applications from everyday life to help the reader develop a feel for probabilities. The second part, covering a wide range of topics, teaches clearly and simply the basics of probability. This fully revised third edition has been packed with even more exercises and examples, and it includes new sections on Bayesian inference, Markov chain Monte Carlo simulation, hitting probabilities in random walks and Brownian motion, and a new chapter on continuous-time Markov chains with applications. Here you will find all the material taught in an introductory probability course. The first part of the book, with its easy-going style, can be read by anybody with a reasonable background in high school mathematics. The second part of the book requires a basic course in calculus"--},
author={Tijms, Henk C.},
keywords={Probabilities, Chance, Mathematical analysis, MATHEMATICS / Probability /\\& Statistics / General},
langid={english},
date={2012},
}

@article{KennedyHilbert,
groups={tesse:2, DSP},
pages={440},
langid={english},
title={Hilbert Space Methods in Signal Processing},
author={Kennedy, Rodney A. and Sadeghi, Parastoo},
}

@book{2017FanoTwenty,
isbn={978-3-319-58731-8 978-3-319-58732-5},
publisher={Springer International Publishing},
groups={tesse:5, Quantum Mechanics},
langid={english},
author={Fano, Guido and Blinder, S. M.},
title={Twenty-First Century Quantum Mechanics: Hilbert Space to Quantum Computers},
doi={10.1007/978-3-319-58732-5},
series={UNITEXT for Physics},
location={Cham},
shorttitle={Twenty-First Century Quantum Mechanics},
date={2017},
urldate={2019-03-31},
}

@book{2017SamuelProgramming,
title={Programming Kotlin: familiarize yourself with all of Kotlin's features with this in-depth guide},
shorttitle={Programming Kotlin},
author={Samuel, Stephen and Bocutiu, Stefan},
langid={english},
date={2017},
}

@article{1982Jacquet-lagrezeAssessing,
date={1982-06},
issn={0377-2217},
urldate={2019-03-31},
number={2},
volume={10},
title={Assessing a set of additive utility functions for multicriteria decision-making, the UTA method},
groups={tesse:5},
url={https://linkinghub.elsevier.com/retrieve/pii/0377221782901552},
author={Jacquet-Lagreze, E. and Siskos, J.},
pages={151--164},
langid={english},
journaltitle={European Journal of Operational Research},
doi={10.1016/0377-2217(82)90155-2},
}

@article{2001BeutheComparative,
author={Beuthe, Michel and Scannella, Giuseppe},
title={Comparative analysis of UTA multicriteria methods q},
abstract={This paper reviews the main variants of the utility additive (UTA) multicriteria method, and systematically compares their predictive performance on two sets of data. It analyses both those cases where the model provides a ranking with errors and those without errors. First, it shows that the reference projects should be chosen carefully in order to elicit as much information as possible from the decision maker: a set of projects satisfying a fractional factorial plan is recommended. Second, it discusses the use of alternative post-optimality methods for solving the problem of multiple solutions and their dierent predictive performances. Third, it presents the results of simulations based on utility functions involving interdependence between criteria, and shows that UTA handles this problem eectively by an adjustment of its coecients. Finally, the in¯uence of the model's parameters on the predictive performance is also investigated. Ó 2001 Elsevier Science B.V. All rights reserved.},
pages={17},
date={2001},
groups={tesse:5},
langid={english},
journaltitle={European Journal of Operational Research},
}

@incollection{2005SiskosUta,
author={Siskos, Yannis and Grigoroudis, Evangelos and Matsatsinis, Nikolaos F.},
isbn={978-0-387-23067-2},
volume={78},
location={New York},
urldate={2019-03-31},
abstract={Abstract: UTA methods refer to the philosophy of assessing a set of value or utility functions, assuming the axiomatic basis of MAUT and adopting the preference disaggregation principle. UTA methodology uses linear programming techniques in order to optimally infer additive value/utility functions, so that these functions are as consistent as possible with the global decision-maker’s preferences (inference principle). The main objective of this chapter is to analytically present the UTA method and its variants and to summarize the progress made in this field. The historical background and the philosophy of the aggregation-disaggregation approach are firstly given in this chapter. The detailed presentation of the basic UTA algorithm is presented, including the discussion on the stability and sensitivity analyses. Several variants of the UTA method, which incorporate different forms of optimality criteria used in the LP formulation, are also discussed. The implementation of the UTA methods is illustrated by a general overview of UTA-based DSSs, as well as real-world decision-making applications. Finally, several potential future research developments of the UTA methodologies within the context of MCDA are discussed through this chapter.},
date={2005},
groups={tesse:5},
langid={english},
doi={10.1007/0-387-23081-5///////\\_8},
booktitle={Multiple Criteria Decision Analysis: State of the Art Surveys},
pages={297--334},
publisher={Springer-Verlag},
title={UTA Methods},
}

@book{1979HwangMultiple,
urldate={2019-03-31},
location={Berlin, Heidelberg},
year={1979},
editorbtype={redactor},
groups={tesse:5},
isbn={978-3-540-09111-0},
langid={english},
date={1979},
series={Lecture Notes in Economics and Mathematical Systems},
editorb={Beckmann, M. and Künzi, H. P.},
title={Multiple Objective Decision Making — Methods and Applications},
doi={10.1007/978-3-642-45511-7},
publisher={Springer Berlin Heidelberg},
volume={164},
author={Hwang, Ching-Lai and Masud, Abu Syed Md.},
}

@collection{2010ZopounidisHandbook,
langid={english},
publisher={Springer},
date={2010},
series={Applied optimization},
title={Handbook of multicriteria analysis},
note={OCLC: 845476314},
pagetotal={455},
groups={tesse:5},
editor={Zopounidis, Constantin},
isbn={978-3-540-92827-0 978-3-540-92828-7},
location={Heidelberg},
}

@book{2013IshizakaMulti,
groups={tesse:5},
note={OCLC: 931031284},
shorttitle={Multi-criteria decision analysis},
title={Multi-criteria decision analysis: methods and software},
langid={english},
author={Ishizaka, Alessio and Nemery, Philippe},
pagetotal={296},
date={2013},
isbn={978-1-119-97407-9 978-1-118-64490-4},
publisher={Wiley},
location={Chichester},
}

@book{2000TriantaphyllouMulti,
editorb={Pardalos, Panos M. and Hearn, Donald},
isbn={978-1-4419-4838-0 978-1-4757-3157-6},
volume={44},
shorttitle={Multi-criteria Decision Making Methods},
editorbtype={redactor},
location={Boston, MA},
series={Applied Optimization},
doi={10.1007/978-1-4757-3157-6},
publisher={Springer US},
urldate={2019-03-31},
title={Multi-criteria Decision Making Methods: A Comparative Study},
langid={english},
author={Triantaphyllou, Evangelos},
date={2000},
}

@book{2009BritainMulti,
author={Britain, Great and for Communities, Department and Government, Local},
note={OCLC: 502304922},
publisher={Communities and Local Government},
title={Multi-criteria analysis: a manual.},
urldate={2019-03-31},
langid={english},
shorttitle={Multi-criteria analysis},
date={2009},
isbn={978-1-4098-1023-0},
url={http://www.communities.gov.uk/documents/corporate/pdf/1132618.pdf},
location={Wetherby},
}

@book{2002BeltonMultiple,
langid={english},
publisher={Springer US},
isbn={978-1-4613-5582-3 978-1-4615-1495-4},
author={Belton, Valerie and Stewart, Theodor J.},
title={Multiple Criteria Decision Analysis an Integrated Approach},
note={OCLC: 903189757},
location={Boston, MA},
groups={tesse:5},
date={2002},
abstract={The field of multiple criteria decision analysis (MCDA), also termed multiple criteria decision aid, or multiple criteria decision making (MCDM), has developed rapidly over the past quarter century and in the process a number of divergent schools of thought have emerged. This can make it difficult for a new entrant into the field to develop a comprehensive appreciation of the range of tools and approaches which are available to assist decision makers in dealing with the ever-present difficulties of seeking compromise or consensus between conflicting inter ests and goals, i.e. the "multiple criteria". The diversity of philosophies and models makes it equally difficult for potential users of MCDA, i.e. management scientists and/or decision makers facing problems involving conflicting goals, to gain a clear understanding of which methodologies are appropriate to their particular context. Our intention in writing this book has been to provide a compre hensive yet widely accessible overview of the main streams of thought within MCDA. We aim to provide readers with sufficient awareness of the underlying philosophies and theories, understanding of the practi cal details of the methods, and insight into practice to enable them to implement any of the approaches in an informed manner. As the title of the book indicates, our emphasis is on developing an integrated view of MCDA, which we perceive to incorporate both integration of differ ent schools of thought within MCDA, and integration of MCDA with broader management theory, science and practice.},
}

@book{2010TrappenbergFundamentals,
note={OCLC: 699333421},
groups={tesse:5},
pagetotal={390},
publisher={Oxford University Press},
date={2010},
edition={Second edition},
isbn={978-0-19-956841-3},
location={Oxford New York Auckland},
langid={english},
title={Fundamentals of computational neuroscience},
author={Trappenberg, Thomas P.},
}

@book{2015DharaniBiology,
title={The biology of thought: a neuronal mechanism in the generation of thought - a new molecular model},
publisher={Elsevier Acad. Press},
shorttitle={The biology of thought},
author={Dharani, Krishnagopal},
location={Amsterdam},
note={OCLC: 931711206},
langid={english},
date={2015},
isbn={978-0-12-800900-0},
pagetotal={229},
}

@book{2016BearNeuroscience,
groups={tesse:5},
title={Neuroscience: exploring the brain},
author={Bear, Mark F. and Paradiso, Michael A. and Connors, Barry W.},
note={OCLC: 1090148410},
date={2016},
isbn={978-0-7817-7817-6},
langid={english},
shorttitle={Neuroscience},
location={Hagerstown, Md.},
publisher={Lippincott Williams //\\\& Wilkins},
}

@book{2015SterlingPrinciples,
date={2015},
publisher={The MIT Press},
groups={tesse:5},
langid={english},
location={Cambridge, Massachusetts},
note={OCLC: 915145186},
keywords={Brain, Learning, Neural circuitry, Neural Pathways, physiology, Physiology},
title={Principles of neural design},
isbn={978-0-262-02870-7},
pagetotal={542},
author={Sterling, Peter and Laughlin, Simon},
}

@article{LedouxHow,
author={LeDOUX, J. O. S. E. P. H.},
title={HOW OUR BRAINS BECOME WHO WE ARE},
langid={english},
groups={tesse:2},
pages={160},
}

@article{DayanTheoretical,
groups={tesse:2},
langid={english},
author={Dayan, Peter and Abbott, L. F.},
pages={432},
title={THEORETICAL NEUROSCIENCE},
}

@book{2014VanderbeiLinear,
langid={english},
date={2014},
isbn={978-1-4614-7629-0 978-1-4614-7630-6},
edition={4. ed},
title={Linear programming: foundations and extensions},
number={196},
pagetotal={414},
note={OCLC: 859535863},
publisher={Springer},
series={International series in operations research //\\\& management science},
groups={tesse:5, Optimization},
shorttitle={Linear programming},
author={Vanderbei, Robert J.},
location={New York, NY},
}

@book{2000SchrijverTheory,
author={Schrijver, Alexander},
edition={Reprinted},
date={2000},
location={Chichester},
pagetotal={471},
title={Theory of linear and integer programming},
note={OCLC: 247967491},
publisher={Wiley},
series={Wiley-Interscience series in discrete mathematics and optimization},
groups={tesse:5, Optimization},
langid={english},
isbn={978-0-471-98232-6},
}

@book{2016ChiconeInvitation,
langid={english},
title={Invitation to applied mathematics - differential equations, modeling, and c.},
date={2016},
author={Chicone, University Of Mi, Carmen (professor Of Mathematics},
note={OCLC: 1023278769},
groups={tesse:5},
isbn={978-0-12-804153-6},
}

@book{2013LukeEssentials,
location={Morrisville, N.C.},
publisher={lulu.com},
langid={english},
date={2013},
title={Essentials of metaheuristics: a set of undergraduate lecture notes},
isbn={978-1-300-54962-8},
edition={Second edition, online version 2.0},
groups={tesse:5},
shorttitle={Essentials of metaheuristics},
note={OCLC: 876159426},
pagetotal={239},
author={Luke, Sean},
}

@inproceedings{1992VemulapatiSolving,
urldate={2019-03-31},
isbn={978-0-8186-2775-0},
langid={english},
publisher={IEEE Comput. Soc. Press},
author={Vemulapati, U. B.},
title={Solving equality constrained least squares problems},
eventtitle={Scalable High Performance Computing Conference SHPCC-92.},
url={http://ieeexplore.ieee.org/document/232669/},
groups={tesse:5},
booktitle={Proceedings Scalable High Performance Computing Conference SHPCC-92.},
location={Williamsburg, VA, USA},
date={1992},
doi={10.1109/SHPCC.1992.232669},
pages={380--384},
}

@book{2015BjoerckNumerical,
author={Björck, Åke},
date={2015},
title={Numerical Methods in Matrix Computations},
publisher={Springer International Publishing},
volume={59},
doi={10.1007/978-3-319-05089-8},
groups={tesse:5},
isbn={978-3-319-05088-1 978-3-319-05089-8},
location={Cham},
series={Texts in Applied Mathematics},
langid={english},
urldate={2019-03-31},
}

@article{SelesnickLeast,
author={Selesnick, Ivan},
groups={tesse:2},
langid={english},
title={Least Squares with Examples in Signal Processing},
pages={15},
}

@collection{2017JawaidGreen,
publisher={Springer International Publishing},
langid={english},
isbn={978-3-319-49381-7 978-3-319-49382-4},
shorttitle={Green biocomposites},
date={2017},
groups={tesse:5},
series={Green energy and technology},
editor={Jawaid, Mohammad and Salit, Mohd Sapuan and Alothman, Othman Y.},
location={Cham},
title={Green biocomposites: design and applications},
pagetotal={345},
}

@collection{2017JawaidGreena,
date={2017},
pagetotal={409},
editor={Jawaid, Hohammad and Sapuan, Salit Mohd and Alothman, Othman Y.},
publisher={Springer International Publishing},
location={Cham},
title={Green biocomposites: manufacturing and properties},
series={Green energy and technology},
groups={tesse:5},
note={OCLC: 964576721},
isbn={978-3-319-46609-5 978-3-319-46610-1},
shorttitle={Green biocomposites},
langid={english},
}

@article{PhillipsBio,
author={Phillips, Steven},
groups={tesse:2},
title={BIO-COMPOSITE MATERIAL APPLICATIONS TO MUSICAL INSTRUMENTS},
langid={english},
pages={97},
}

@collection{2013EbnesajjadHandbook,
editor={Ebnesajjad, Sina},
note={OCLC: 820108309},
shorttitle={Handbook of biopolymers and biodegradable plastics},
date={2013},
langid={english},
pagetotal={462},
title={Handbook of biopolymers and biodegradable plastics: properties, processing and applications},
publisher={Elsevier},
isbn={978-1-4557-2834-3},
location={Amsterdam},
series={Plastics design library (PDL)},
}

@article{PillaHandbook,
groups={tesse:2},
langid={english},
author={Pilla, Srikanth},
title={Handbook of Bioplastics and Biocomposites Engineering Applications},
pages={622},
}

@article{2015KesslerGreen,
shorttitle={Green biorenewable biocomposites},
number={11},
month={June},
journal={Choice Reviews Online},
journaltitle={Choice Reviews Online},
urldate={2019-03-31},
pages={52--5901--52--5901},
langid={english},
title={Green biorenewable biocomposites: from knowledge to industrial applications},
year={2015},
issn={0009-4978, 1523-8253},
volume={52},
doi={10.5860/CHOICE.191046},
date={2015-07-01},
groups={tesse:5},
publisher={American Library Association},
author={Kessler, Michael R. and Thakur, Vijay Kumar},
}

@book{2016BucurHandbook,
publisher={Springer International Publishing},
doi={10.1007/978-3-319-32080-9},
author={Bucur, Voichita},
location={Cham},
date={2016},
title={Handbook of Materials for String Musical Instruments},
groups={tesse:5},
isbn={978-3-319-32078-6 978-3-319-32080-9},
langid={english},
urldate={2019-03-31},
}

@article{LewisIntroduction,
title={INTRODUCTION TO PROGRAMMING AND PROBLEM-SOLVING USING SCALA},
pages={591},
author={Lewis, Mark C. and Lacher, Lisa L.},
groups={tesse:2, Programming},
langid={english},
}

@book{2005BucklandProgramming,
isbn={978-1-55622-078-4},
groups={tesse:5, Programming},
title={Programming game AI by example},
author={Buckland, Mat},
publisher={Wordware Publ},
date={2005},
note={OCLC: 249763995},
location={Plano, Tex},
series={Wordware game developer's library},
pagetotal={495},
langid={english},
}

@article{bugnion_scala:applied_nodate,
author={Bugnion, Pascal},
langid={english},
title={Scala:Applied Machine Learning},
pages={1623},
}

@article{1997SussmanStructure,
langid={english},
pages={133},
title={Structure and interpretation of computer programs},
month={February},
issn={0898-1221},
urldate={2019-03-31},
volume={33},
journal={Computers \\\& Mathematics with Applications},
author={Sussman, Gerald Jay and Abelson, Hal},
number={4},
journaltitle={Computers //\\\& Mathematics with Applications},
date={1997-02},
doi={10.1016/S0898-1221(97)90051-1},
groups={tesse:5},
url={https://linkinghub.elsevier.com/retrieve/pii/S0898122197900511},
publisher={Elsevier BV},
year={1997},
}

@book{2017BallWriting,
author={Ball, Thorsten},
date={2017},
langid={english},
title={Writing An Interpreter In Go},
groups={tesse:5},
}

@book{2016EngQt5,
date={2016},
isbn={978-1-78328-027-8 978-1-78328-028-5},
pagetotal={283},
author={Eng, Lee Zhi},
title={Qt5 C++ GUI programming cookbook: use Qt5 to design and build a graphical user interface that is functional, appealing, and user-friendly for your software application},
location={Birmingham Mumbai},
publisher={Packt Publishing},
series={Quick answers to common problems},
langid={english},
shorttitle={Qt5 C++ GUI programming cookbook},
note={OCLC: 960408279},
}

@book{2009HaykinNeural,
publisher={Pearson},
title={Neural networks and learning machines},
langid={english},
groups={tesse:5},
author={Haykin, Simon S.},
date={2009},
pagetotal={906},
isbn={978-0-13-147139-9},
location={New York},
note={OCLC: 857737780},
edition={3. ed},
}

@article{deng_deep_nodate,
journaltitle={Deep Learning},
langid={english},
author={Deng, Li and Yu, Dong},
pages={197},
volume={7},
title={Deep Learning: Methods and Applications},
}

@collection{2014KowaliwGrowing,
date={2014},
shorttitle={Growing adaptive machines},
editor={Kowaliw, Taras and Bredeche, Nicolas and Doursat, René},
location={Heidelberg},
pagetotal={261},
publisher={Springer},
title={Growing adaptive machines: combining development and learning in artificial neural networks},
note={OCLC: 892794596},
series={Studies in computational intelligence},
langid={english},
isbn={978-3-642-55336-3 978-3-642-55337-0},
number={557},
}

@article{CilimkovicNeural,
abstract={Neural Networks (NN) are important data mining tool used for classiﬁcation and clustering. It is an attempt to build machine that will mimic brain activities and be able to learn. NN usually learns by examples. If NN is supplied with enough examples, it should be able to perform classiﬁcation and even discover new trends or patterns in data. Basic NN is composed of three layers, input, output and hidden layer. Each layer can have number of nodes and nodes from input layer are connected to the nodes from hidden layer. Nodes from hidden layer are connected to the nodes from output layer. Those connections represent weights between nodes.},
title={Neural Networks and Back Propagation Algorithm},
groups={tesse:2},
langid={english},
author={Cilimkovic, Mirza},
pages={12},
}

@book{2016ShanmuganathanArtificial,
langid={english},
urldate={2019-03-31},
isbn={978-3-319-28495-8 978-3-319-28493-4},
date={2016},
groups={tesse:5},
doi={10.1007/978-3-319-28495-8},
author={Shanmuganathan, Subana and Samarasinghe, Sandhya},
abstract={This book covers theoretical aspects as well as recent innovative applications of Artificial Neural networks (ANNs) in natural, environmental, biological, social, industrial and automated systems. It presents recent results of ANNs in modelling small, large and complex systems under three categories, namely, 1) Networks, Structure Optimisation, Robustness and Stochasticity 2) Advances in Modelling Biological and Environmental Systems and 3) Advances in Modelling Social and Economic Systems. The book aims at serving undergraduates, postgraduates and researchers in ANN computational modelling.},
title={Artificial neural network modelling},
note={OCLC: 939520392},
}

@book{2019VasilevPython,
title={Python deep learning: exploring deep learning techniques and neural network architectures with PyTorch, Keras, and TensorFlow},
urldate={2019-03-31},
isbn={978-1-78934-970-2},
url={http://proquest.safaribooksonline.com/?fpi=9781789348460},
note={OCLC: 1086398837},
shorttitle={Python deep learning},
date={2019},
author={Vasilev, Ivan},
langid={english},
}

@book{1999AnthonyNeural,
author={Anthony, Martin and Bartlett, Peter L.},
date={1999},
langid={english},
title={Neural Network Learning: Theoretical Foundations},
publisher={Cambridge University Press},
doi={10.1017/CBO9780511624216},
shorttitle={Neural Network Learning},
isbn={978-0-511-62421-6},
url={http://ebooks.cambridge.org/ref/id/CBO9780511624216},
location={Cambridge},
urldate={2019-03-31},
}

@book{2007GalushkinNeural,
isbn={978-3-540-48125-6},
groups={tesse:5},
location={Berlin, Heidelberg},
title={Neural Networks Theory},
publisher={Springer-Verlag Berlin Heidelberg},
author={Galushkin, Alexander I.},
note={OCLC: 938669307},
langid={english},
date={2007},
}

@book{2016LayLinear,
note={OCLC: 875056178},
langid={english},
abstract={With traditional linear algebra texts, the course is relatively easy for students during the early stages as material is presented in a familiar, concrete setting. However, when abstract concepts are introduced, students often hit a wall. Instructors seem to agree that certain concepts (such as linear independence, spanning, subspace, vector space, and linear transformations) are not easily understood and require time to assimilate. These concepts are fundamental to the study of linear algebra, so students' understanding of them is vital to mastering the subject. This text makes these concepts more accessible by introducing them early in a familiar, concrete Rn setting, developing them gradually, and returning to them throughout the text so that when they are discussed in the abstract, students are readily able to understand.},
date={2016},
title={Linear algebra and its applications},
author={Lay, David C. and Lay, Steven R. and McDonald, Judith},
groups={tesse:5, Linear Algebra},
isbn={978-0-321-98238-4},
}

@book{2008Leyton-brownEssentials,
pagetotal={88},
series={Synthesis lectures on artificial intelligence and machine learning},
number={Lecture 3},
langid={english},
shorttitle={Essentials of game theory},
author={Leyton-Brown, Kevin and Shoham, Yoav},
publisher={Morgan ////////\\\& Claypool Publishers},
title={Essentials of game theory: a concise, multidisciplinary introduction},
note={OCLC: 236164649},
date={2008},
isbn={978-1-59829-593-1 978-1-59829-594-8},
location={San Rafael, California},
}

@book{2009GintisGame,
shorttitle={Game theory evolving},
isbn={978-0-691-14050-6 978-0-691-14051-3},
author={Gintis, Herbert},
langid={english},
date={2009},
edition={2. ed},
location={Princeton, NJ},
note={OCLC: 845420030},
title={Game theory evolving: a problem-centered introduction to modeling strategic interaction},
publisher={Princeton Univ. Press},
pagetotal={390},
}

@book{2005HayesSchaums,
publisher={McGraw Hill},
title={Schaum's outline of theory and problems of digital signal processing},
location={New York},
date={2005},
edition={Nachdr.},
author={Hayes, Monson H.},
pagetotal={432},
series={Schaum's outline series},
groups={tesse:5, DSP},
langid={english},
note={OCLC: 255732140},
isbn={978-0-07-027389-4},
}

@book{2016AmiotMusic,
author={Amiot, Emmanuel},
doi={10.1007/978-3-319-45581-5},
location={Cham},
publisher={Springer International Publishing},
series={Computational Music Science},
urldate={2019-03-31},
date={2016},
groups={tesse:5},
isbn={978-3-319-45580-8 978-3-319-45581-5},
langid={english},
title={Music Through Fourier Space},
}

@book{2018BroughtonDiscrete,
author={Broughton, S. Allen and Bryan, Kurt},
location={Hoboken, NJ},
title={Discrete fourier analysis and wavelets: applications to signal and image processing},
date={2018},
edition={Second edition},
note={OCLC: 1030990110},
isbn={978-1-119-25824-7 978-1-119-25822-3 978-1-119-25823-0},
pagetotal={442},
abstract={Vector spaces, signals, and images -- The discrete fourier transform -- The discrete cosine transform -- Convolution and filtering -- Windowing and localization -- Frames -- Filter banks -- Lifting for filter banks and wavelets -- Wavelets},
langid={english},
shorttitle={Discrete fourier analysis and wavelets},
publisher={John Wiley},
}

@book{2014AndreescuComplex,
title={Complex numbers from A to ... Z},
langid={english},
isbn={978-0-8176-8414-3},
date={2014},
author={Andreescu, Titu and Andrica, D and Andrica, D},
note={OCLC: 868044080},
abstract={"The book reflects the unique experience of the authors. It distills a vast mathematical literature, most of which is unknown to the western public, and captures the essence of an abundant problem culture. The target audience includes undergraduates, high school students and their teachers, mathematical contestants (such as those training for Olympiads or the W.L. Putnam Mathematical Competition) and their coaches, as well as anyone interested in essential mathematics."--Page 4 of cover.},
}

@book{1998NeedhamVisual,
langid={english},
editor={Needham, Tristan},
pagetotal={592},
location={Oxford},
year={1998},
groups={tesse:5},
title={Visual complex analysis},
date={1998},
publisher={Clarendon Press},
author={Needham, Tristan},
note={OCLC: 632965949},
isbn={978-0-19-853447-1},
}

@book{2004OdianPrinciples,
note={OCLC: 249002704},
date={2004},
author={Odian, George G.},
publisher={Wiley-Interscience},
pagetotal={812},
isbn={978-0-471-27400-1},
groups={tesse:5},
edition={4. ed},
title={Principles of polymerization},
langid={english},
location={Hoboken, NJ},
}

@book{2012ClaydenOrganic,
groups={tesse:5},
langid={english},
title={Organic chemistry},
location={Oxford},
isbn={978-0-19-927029-3},
pagetotal={1234},
author={Clayden, Jonathan and Greeves, Nick and Warren, Stuart G.},
edition={Second edition},
publisher={Oxford University Press},
date={2012},
keywords={Textbooks, Chemistry, Organic},
note={OCLC: 775825174},
}

@book{2007HiemenzPolymer,
date={2007},
title={Polymer chemistry},
note={OCLC: 255638513},
pagetotal={587},
langid={english},
edition={2. ed},
author={Hiemenz, Paul C. and Lodge, Timothy},
groups={tesse:5},
isbn={978-1-57444-779-8},
location={Boca Raton, Fla.},
publisher={CRC Press},
}

@article{MorrisFundamentals,
groups={tesse:2, Calculus},
langid={english},
author={Morris, Carla C.},
title={Fundamentals of Calculus},
pages={371},
}

@book{2015KaabarFriendly,
author={Kaabar, Mohammed K. A.},
urldate={2019-03-31},
date={2015},
doi={10.13140/2.1.4168.4806},
groups={tesse:5},
langid={english},
title={A Friendly Introduction to Differential Equations},
publisher={CreateSpace},
}

@book{2014BronsonSchaums,
note={OCLC: 887191871},
publisher={McGraw-Hill Companies, The},
date={2014},
url={http://accessengineeringlibrary.com/browse/schaums-outline-of-differential-equations-fourth-edition},
langid={english},
groups={tesse:5},
author={Bronson, Richard and Costa, Gabriel},
location={Blacklick},
isbn={978-0-07-182485-9},
urldate={2019-03-31},
title={Schaum's Outline of Differential Equations, 4th Edition.},
}

@book{2008HolznerDifferential,
date={2008-05-23},
publisher={John Wiley and Sons Ltd},
author={Holzner, Steven},
title={Differential Equations For Dummies},
url={https://www.ebook.de/de/product/7119629/steven///////\\_holzner///////\\_differential///////\\_equations///////\\_for///////\\_dummies.html},
ean={9780470178140},
pages={363},
pagetotal={360},
groups={tesse:5},
isbn={978-0-470-17814-0},
langid={english},
year={2008},
}

@book{2012NagleFundamentals,
isbn={978-0-321-74773-0 978-0-321-75820-0},
langid={english},
groups={tesse:5},
publisher={Pearson Education},
note={OCLC: 701619880},
title={Fundamentals of differential equations},
date={2012},
location={Boston},
author={Nagle, R. Kent and Saff, E. B. and Snider, Arthur David},
}

@book{2016AdzievskiIntroduction,
author={Adzievski, Kuzman},
doi={10.1201/b15775},
urldate={2019-03-31},
isbn={978-1-4665-1057-9},
langid={english},
title={Introduction to Partial Differential Equations for Scientists and Engineers Using Mathematica},
url={https://www.taylorfrancis.com/books/9781466510579},
edition={1},
publisher={Chapman and Hall/CRC},
date={2016-04-19},
}

@book{2015LoganFirst,
groups={tesse:5},
title={A first course in differential equations},
note={OCLC: 915389739},
series={Undergraduate texts in mathematics},
location={Cham Heidelberg New York Dordrecht London},
author={Logan, J. David},
edition={Third edition},
date={2015},
langid={english},
publisher={Springer},
pagetotal={369},
isbn={978-3-319-17851-6 978-3-319-17852-3},
}

@article{RudnickiFundamentals,
author={Rudnicki, John W.},
groups={tesse:2},
title={Fundamentals of Continuum Mechanics},
langid={english},
pages={220},
}

@book{2008GonzalezFirst,
groups={tesse:5},
doi={10.1017/CBO9780511619571},
collection={Cambridge Texts in Applied Mathematics},
author={Gonzalez, Oscar and Stuart, Andrew M.},
publisher={Cambridge University Press},
date={2008},
series={Cambridge Texts in Applied Mathematics},
place={Cambridge},
title={A First Course in Continuum Mechanics},
}

@book{2014MuellerExpedition,
langid={english},
urldate={2019-03-31},
series={Solid Mechanics and Its Applications},
date={2014},
location={Dordrecht},
volume={210},
groups={tesse:5},
doi={10.1007/978-94-007-7799-6},
author={Müller, Wolfgang H.},
publisher={Springer Netherlands},
title={An Expedition to Continuum Theory},
isbn={978-94-007-7798-9 978-94-007-7799-6},
}

@book{2008IrgensContinuum,
author={Irgens, Fridtjov},
title={Continuum mechanics: with 4 tables},
pagetotal={661},
groups={tesse:5},
note={OCLC: 253962701},
isbn={978-3-540-74297-5 978-3-540-74298-2},
publisher={Springer},
langid={english},
location={Berlin},
shorttitle={Continuum mechanics},
date={2008},
}

@book{2003SteinComplex,
author={Stein, Elias M. and Shakarchi, Rami},
location={Princeton, N.J. Oxford},
series={Princeton lectures in analysis},
isbn={978-0-691-11385-2},
pagetotal={392},
title={Complex analysis},
date={2003},
number={2},
publisher={Princeton University Press},
langid={english},
groups={tesse:5},
}

@book{2001RaoTransform,
langid={english},
series={The electrical engineering and signal processing series},
groups={tesse:5},
author={Rao, Kamisetty Ramamohan and Yip, Pat C.},
isbn={978-0-8493-3692-8},
date={2001},
title={The transform and data compression handbook},
editor={Rao, Kamisetty Ramamohan and Yip, Pat C.},
pagetotal={388},
note={OCLC: 247573781},
location={Boca Raton, Fla.},
publisher={CRC Press},
}

@misc{2006UlrichEnvelope,
groups={tesse:5},
author={Ulrich, T. J.},
title={Envelope calculation from the Hilbert transform},
langid={english},
date={2006},
pages={6},
}

@book{2013DangeloHermitian,
isbn={978-1-4614-8525-4 978-1-4614-8526-1},
groups={tesse:5},
langid={english},
doi={10.1007/978-1-4614-8526-1},
publisher={Springer New York},
series={Cornerstones},
author={D'Angelo, John P.},
date={2013},
title={Hermitian Analysis},
location={New York, NY},
urldate={2019-03-31},
}

@incollection{2013BoschEarly,
publisher={Springer Berlin Heidelberg},
title={The Early Stage Software Startup Development Model: A Framework for Operationalizing Lean Principles in Software Startups},
booktitle={Lean Enterprise Software and Systems},
doi={10.1007/978-3-642-44930-7///////\\_1},
urldate={2019-03-31},
volume={167},
langid={english},
author={Bosch, Jan and Holmström Olsson, Helena and Björk, Jens and Ljungblad, Jens},
groups={tesse:5},
editor={Fitzgerald, Brian and Conboy, Kieran and Power, Ken and Valerdi, Ricardo and Morgan, Lorraine and Stol, Klaas-Jan},
pages={1--15},
shorttitle={The Early Stage Software Startup Development Model},
isbn={978-3-642-44929-1 978-3-642-44930-7},
editorb={van der Aalst, Wil and Mylopoulos, John and Rosemann, Michael and Shaw, Michael J. and Szyperski, Clemens},
date={2013},
editorbtype={redactor},
abstract={Software startups are more popular than ever and growing in numbers. They operate under conditions of extreme uncertainty and face many challenges. Often, agile development practices and lean principles are suggested as ways to increase the odds of succeeding as a startup, as they both advocate close customer collaboration and short feedback cycles focusing on delivering direct customer value. However, based on an interview study we see that despite guidance and support in terms of well-known and documented development methods, practitioners find it difficult to implement and apply these in practice. To explore this further, and to propose operational support for software startup companies, this study aims at investigating (1) what are the typical challenges when finding a product idea worth scaling, and (2) what solution would serve to address these challenges. To this end, we propose the ‘Early Stage Software Startup Development Model’ (ESSSDM). The model extends already existing lean principles, but offers novel support for practitioners for investigating multiple product ideas in parallel, for determining when to move forward with a product idea, and for deciding when to abandon a product idea. The model was evaluated in a software startup project, as well as with industry professionals within the software startup domain.},
location={Berlin, Heidelberg},
}

@incollection{2015GiardinoKey,
urldate={2019-03-31},
title={Key Challenges in Early-Stage Software Startups},
booktitle={Agile Processes in Software Engineering and Extreme Programming},
volume={212},
langid={english},
groups={tesse:5},
isbn={978-3-319-18611-5 978-3-319-18612-2},
date={2015},
location={Cham},
editor={Lassenius, Casper and Dingsøyr, Torgeir and Paasivaara, Maria},
author={Giardino, Carmine and Bajwa, Sohaib Shahid and Wang, Xiaofeng and Abrahamsson, Pekka},
pages={52--63},
publisher={Springer International Publishing},
doi={10.1007/978-3-319-18612-2///////\\_5},
}

@book{2001RoadsMicrosound,
title={Microsound},
date={2001},
isbn={978-0-262-18215-7},
publisher={MIT Press},
location={Cambridge, Mass.},
pagetotal={409},
groups={tesse:5},
langid={english},
note={OCLC: 834185525},
author={Roads, Curtis},
}

@misc{2002SaloFinite,
pages={12},
title={Finite Difference Method in Sound Synthesis},
date={2002},
url={http://math.aalto.fi//textasciitilde ksalo2/akusem/sem///////\\_FDM.pdf},
author={Salo, Maaria},
abstract={This paper reviews the ﬁnite difference method in the sound synthesis of string instruments. The mathematical basis for the method and the evaluation of the recursion equations are considered. Some stability conditions are discussed. Initial and boundary conditions are reviewed for piano- and guitar-like strings.},
groups={tesse:5, Finite Difference Methods},
langid={english},
}

@book{1999WoodardNation,
publisher={Univ. of North Carolina Press},
pagetotal={329},
author={Woodard, Komozi},
editoratype={collaborator},
shorttitle={A nation within a nation},
note={OCLC: 237335993},
isbn={978-0-8078-2457-3 978-0-8078-4761-9},
date={1999},
title={A nation within a nation: Amiri Baraka (LeRoi Jones) and Black Power politics},
editora={Baraka, Imamu Amiri},
location={Chapel Hill, NC},
}

@article{2012TrevinoAudio,
pages={85--89},
doi={10.1162/COMJ///////\\_r///////\\_00121},
author={Trevino, Jeffrey and Allen, Drew},
issn={0148-9267, 1531-5169},
number={2},
urldate={2019-03-31},
langid={english},
journaltitle={Computer Music Journal},
date={2012-06},
groups={tesse:5},
volume={36},
title={The Audio Programming Book, Edited by Richard Boulanger and Victor Lazzarini},
}

@collection{2009CormenIntroduction,
isbn={978-0-262-03384-8 978-0-262-53305-8},
editor={Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald Linn and Stein, Clifford},
title={Introduction to algorithms},
abstract={I. Foundations. The role of algorithms in computing -- Getting started -- Growth of functions -- Divide-and-conquer -- Probabilistic analysis and randomized algorithms -- II. Sorting and order statistics. Heapsort -- Quicksort -- Sorting in linear time -- Medians and order statistics -- III. Data structures. Elementary data structures -- Hash tables -- Binary search trees -- Red-black trees -- Augmenting data structures -- IV. Advanced design and analysis techniques. Dynamic programming -- Greedy algorithms -- Amortized analysis -- V. Advanced data structures. B-trees -- Fibonacci heaps -- van Emde Boas trees -- Data structures for disjoint sets -- VI. Graph algorithms. Elementary graph algorithms -- Minimum spanning trees -- Single-source shortest paths -- All-pairs shortest paths -- Maximun flow -- VII. Selected topics. Multithreaded algorithms -- Matrix operations -- Linear programming -- Polynomials and the FFT -- Number-theoretic algorithms -- String matching -- Computational geometry -- NP-completeness -- Approximation algorithms -- VIII. Appendix: Mathematical background. Summations -- Sets, etc. -- Counting and probability -- Matrices},
note={OCLC: 698955316},
location={Cambridge, Mass.},
pagetotal={1292},
groups={tesse:5, Programming},
langid={english},
publisher={MIT Press},
date={2009},
edition={3. ed},
}

@article{2010RabensteinTubular,
journaltitle={IEEE Transactions on Audio, Speech, and Language Processing},
abstract={Tubular bells are geometrically simple representatives of three-dimensional vibrating structures. Under certain assumptions, a tubular bell can be modeled as a rectangular plate with different types of homogeneous boundary conditions. Suitable functional transformations with respect to time and space turn the corresponding initial-boundary value problem into a two-dimensional transfer function. An algorithmic model follows according to the functional transformation method in digital sound synthesis. As with simpler vibrating structures (strings, membranes) the synthesis algorithms consist of a parallel arrangement of second-order sections. Their coefﬁcients are obtained by simple analytic expressions directly from the physical parameters of the tubular bell.},
issn={1558-7916},
author={Rabenstein, Rudolf and Koch, Tilman and Popp, Christian},
date={2010-05},
pages={881--890},
volume={18},
shorttitle={Tubular Bells},
url={http://ieeexplore.ieee.org/document/5299084/},
title={Tubular Bells: A Physical and Algorithmic Model},
doi={10.1109/TASL.2009.2035214},
urldate={2019-03-31},
number={4},
groups={tesse:5},
langid={english},
}

@article{1993ItohCurve,
groups={tesse:5},
title={A curve fitting algorithm for character fonts},
date={1993},
number={3},
journaltitle={Electronic publishing},
pages={195--205},
volume={6},
author={Itoh, Koichi and Ohno, Yoshio},
publisher={Citeseer},
}

@unpublished{2012BarendrechtGentle,
langid={english},
groups={tesse:5},
date={2012},
title={A gentle introduction to rational Bézier curves and NURBS},
author={Barendrecht, P. J.},
}

@misc{2008BertkaIntroduction,
author={Bertka, Benjamin T},
title={An Introduction to Bezier Curves, B-Splines, and Tensor Product Surfaces with History and Applications},
date={2008},
langid={english},
}

@collection{2001RogersIntroduction,
isbn={978-1-55860-669-2},
editor={Rogers, David F.},
pagetotal={324},
publisher={Morgan Kaufmann Publishers},
shorttitle={An introduction to NURBS},
location={San Francisco, Calif.},
title={An introduction to NURBS: with historical perspective},
note={OCLC: 247648741},
langid={english},
date={2001},
}

@misc{2012SederbergComputer,
author={Sederberg, Thomas W.},
groups={tesse:5},
pages={289},
langid={english},
year={2012},
date={2012},
title={Computer Aided Geometric Design},
}

@book{2006FarinCurves,
title={Curves and surfaces for CAGD: a practical guide},
series={The Morgan Kaufmann series in computer graphics and geometric modeling},
author={Farin, Gerald E.},
groups={tesse:5},
note={OCLC: 254200301},
date={2006},
isbn={978-1-55860-737-8},
location={San Francisco, Calif.},
edition={5. ed., [Nachdr.]},
pagetotal={499},
langid={english},
shorttitle={Curves and surfaces for CAGD},
publisher={Morgan Kaufmann Publ},
}

@article{2011BostockD³,
journaltitle={IEEE Transactions on Visualization and Computer Graphics},
doi={10.1109/TVCG.2011.185},
author={Bostock, M. and Ogievetsky, V. and Heer, J.},
pages={2301--2309},
url={http://ieeexplore.ieee.org/document/6064996/},
abstract={Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-speciﬁc abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efﬁciency and retaining powerful declarative components. Immediate evaluation of operators further simpliﬁes debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations.},
volume={17},
langid={english},
issn={1077-2626},
number={12},
date={2011-12},
urldate={2019-03-31},
groups={tesse:5},
title={D³ Data-Driven Documents},
}

@article{cabrinha_bezier_nodate,
abstract={The development of digital fabrication has reintroduced material processes with digital processes. There has been much discussion about the tool and the objects of the tool, but little discussion of the implication of the material process on the digital process. A brief historical review on the development of computer numerical control and the origins of the Bézier curve reveals an instrumental fact: computer numerical controlled tools necessitated advancements in computational surfaces which eventually led to NURBS (Non-Uniform Rational B-Splines) surfaces. In other words, the origins of NURBS surfaces resides in its relation to material processes, rather than many current approaches that develop free form surfaces and then force the tool onto the material without regard to the material properties.},
title={From Bézier to NURBS: Integrating Material and Digital Techniques through a Plywood Shell},
langid={english},
pages={14},
author={Cabrinha, Mark},
}

@article{RamshawMultiplying,
groups={tesse:2},
langid={english},
author={Ramshaw, Lyle},
pages={173},
title={On multiplying points: the paired algebras of forms and sites},
}

@book{2010PatrikalakisShape,
author={Patrikalakis, Nicholas M. and Maekawa, Takashi},
date={2010},
doi={10.1007/978-3-642-04074-0},
isbn={978-3-642-04073-3 978-3-642-04074-0},
publisher={Springer Berlin Heidelberg},
urldate={2019-03-31},
title={Shape Interrogation for Computer Aided Design and Manufacturing},
location={Berlin, Heidelberg},
groups={tesse:5},
langid={english},
}

@article{2012FaroukiBernstein,
number={6},
groups={tesse:5},
date={2012-08},
url={https://linkinghub.elsevier.com/retrieve/pii/S0167839612000192},
journaltitle={Computer Aided Geometric Design},
abstract={One hundred years after the introduction of the Bernstein polynomial basis, we survey the historical development and current state of theory, algorithms, and applications associated with this remarkable method of representing polynomials over ﬁnite domains. Originally introduced by Sergei Natanovich Bernstein to facilitate a constructive proof of the Weierstrass approximation theorem, the leisurely convergence rate of Bernstein polynomial approximations to continuous functions caused them to languish in obscurity, pending the advent of digital computers. With the desire to exploit the power of computers for geometric design applications, however, the Bernstein form began to enjoy widespread use as a versatile means of intuitively constructing and manipulating geometric shapes, spurring further development of basic theory, simple and eﬃcient recursive algorithms, recognition of its excellent numerical stability properties, and an increasing diversiﬁcation of its repertoire of applications. This survey provides a brief historical perspective on the evolution of the Bernstein polynomial basis, and a synopsis of the current state of associated algorithms and applications.},
title={The Bernstein polynomial basis: A centennial retrospective},
shorttitle={The Bernstein polynomial basis},
pages={379--419},
author={Farouki, Rida T.},
issn={0167-8396},
langid={english},
urldate={2019-03-31},
volume={29},
doi={10.1016/j.cagd.2012.03.001},
}

@book{1995LawsonSolving,
groups={tesse:5},
number={15},
location={Philadelphia},
date={1995},
edition={10. Dr.},
author={Lawson, Charles L. and Hanson, Richard J.},
publisher={SIAM},
pagetotal={337},
isbn={978-0-89871-356-5},
note={OCLC: 844911591},
series={Classics in applied mathematics},
title={Solving least squares problems},
}

@book{2000BloomfieldFourier,
urldate={2019-03-31},
isbn={978-0-471-72223-6 978-0-471-88948-9},
author={Bloomfield, Peter},
shorttitle={Fourier Analysis of Time Series},
location={Hoboken, NJ, USA},
series={Wiley Series in Probability and Statistics},
publisher={John Wiley ////////\\\& Sons, Inc.},
date={2000-01-21},
doi={10.1002/0471722235},
url={https://iujfk.files.wordpress.com/2013/04/bloomfield-2000-fourier-analysis-of-time-series-an-introduction-2ed.pdf},
title={Fourier Analysis of Time Series: An Introduction},
}

@thesis{2002AirdMusical,
institution={University of Bath},
author={Aird, Marc-Laurent},
type={phdthesis},
title={Musical instrument modelling using digital waveguides},
groups={tesse:5, Digital Waveguides},
date={2002},
}

@inproceedings{2002BakkerReinforcement,
booktitle={In NIPS},
title={Reinforcement Learning with Long Short-Term Memory},
url={https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.pdf},
groups={tesse:5},
date={2002},
publisher={MIT Press},
abstract={This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage /\\# /\\# /\\# learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevantevents. This is demonstrated in a T-maze task, as well as in a di /\\#cult variation of the pole balancing task. 1},
pages={1475--1482},
author={Bakker, Bram},
}

@book{2012PieglNurbs,
groups={tesse:5},
author={Piegl, Les and Tiller, Wayne},
title={The NURBS book},
publisher={Springer Science //\\\& Business Media},
date={2012},
isbn={978-3-540-61545-3},
}

@inproceedings{2005SmithViewpoints,
publisher={INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
url={https://ccrma.stanford.edu//textasciitilde jos/kna/kna.pdf},
date={2005},
author={Smith, Julius O.},
groups={tesse:5},
title={Viewpoints on the history of digital synthesis},
booktitle={Proceedings of the International Computer Music Conference},
pages={1--1},
}

@article{2004Martinez-friasSegmentation,
date={2004-07-15},
author={Martínez-Frías, M. L.},
title={Segmentation anomalies of the vertebras and ribs: one expression of the primary developmental field},
doi={10.1002/ajmg.a.30016},
journaltitle={American Journal of Medical Genetics. Part A},
pmid={15214001},
pages={127--131},
volume={128A},
issn={1552-4825},
shorttitle={Segmentation anomalies of the vertebras and ribs},
number={2},
shortjournal={Am. J. Med. Genet. A},
}

@collection{2017InstituteGuide,
abstract={"The PMBOK},
series={PMBOK guide},
date={2017},
location={Newtown Square, PA},
pagetotal={756},
publisher={Project Management Institute},
title={A guide to the project management body of knowledge / Project Management Institute},
editor={Institute, Project Management},
isbn={978-1-62825-184-5},
edition={Sixth edition},
}

@book{2017AnthonyFullstack,
isbn={978-0-9913446-2-8},
pagetotal={836},
shorttitle={Fullstack React},
author={Anthony, Accomazzo and Nathaniel, Murray and Ari, Lerner},
title={Fullstack React: The Complete Guide to ReactJS and Friends},
groups={tesse:5},
publisher={Fullstack.io},
date={2017-09-12},
abstract={Stop wasting your time learning React with incomplete and confusing tutorials. There are so many incorrect, confusing, and out-of-date blog articles One tutorial says one thing and another says something completely different. There are too many options There are fifty different boilerplates and a dozen different Flux implementations. Which one is best? What if you could master the entire framework in less time, with solid foundations, without beating your head against the wall? Imagine how quickly you can get all of your work done with the right tools and best practices. Seriously, let's stop wasting time scouring Google, searching through incorrect, out-of-date, blog posts and get everything you need to be productive in one, well-organized place, complete with both simple and complex examples to get your app up and running. You'll learn what you need to know to work professionally and build solid, well-tested, optimized apps with ReactJS. This book is your definitive guide.},
}

@book{2015GottschlingDiscovering,
publisher={Addison-Wesley Professional},
location={Boston},
edition={1 edition},
title={Discovering Modern C++: An Intensive Course for Scientists, Engineers, and Programmers},
abstract={As scientific and engineering projects grow larger and more complex, it is increasingly likely that those projects will be written in C++. With embedded hardware growing more powerful, much of its software is moving to C++, too. Mastering C++ gives you strong skills for programming at nearly every level, from “close to the hardware” to the highest-level abstractions. In short, C++ is a language that scientific and technical practitioners need to know. Peter Gottschling’s Discovering Modern C++ is an intensive introduction that guides you smoothly to sophisticated approaches based on advanced features. Gottschling introduces key concepts using examples from many technical problem domains, drawing on his extensive experience training professionals and teaching C++ to students of physics, math, and engineering. This book is designed to help you get started rapidly and then master increasingly robust features, from lambdas to expression templates. You’ll also learn how to take advantage of the powerful libraries available to C++ programmers: both the Standard Template Library (STL) and scientific libraries for arithmetic, linear algebra, differential equations, and graphs. Throughout, Gottschling demonstrates how to write clear and expressive software using object orientation, generics, metaprogramming, and procedural techniques. By the time you’re finished, you’ll have mastered all the abstractions you need to write C++ programs with exceptional quality and performance.},
date={2015-12-27},
author={Gottschling, Peter},
pagetotal={480},
isbn={978-0-13-438358-3},
shorttitle={Discovering Modern C++},
}

@book{2014DineenMultivariate,
author={Dineen, Seán},
series={Springer Undergraduate Mathematics Series},
abstract={Multivariate calculus can be understood best by combining geometric insight, intuitive arguments, detailed explanations and mathematical reasoning. This textbook not only follows this programme, but additionally provides a solid description of the basic concepts, via familiar examples, which are then tested in technically demanding situations. In this new edition the introductory chapter and two of the chapters on the geometry of surfaces have been revised. Some exercises have been replaced and others provided with expanded solutions. Familiarity with partial derivatives and a course in linear algebra are essential prerequisites for readers of this book. Multivariate Calculus and Geometry is aimed primarily at higher level undergraduates in the mathematical sciences. The inclusion of many practical examples involving problems of several variables will appeal to mathematics, science and engineering students.},
publisher={Springer-Verlag},
groups={tesse:5, Calculus},
edition={3},
title={Multivariate Calculus and Geometry},
url={https://www.springer.com/gp/book/9781447164180},
urldate={2019-03-31},
langid={english},
isbn={978-1-4471-6418-0},
location={London},
date={2014},
}

@book{1965FinneyPrinciples,
note={OCLC: 799320107},
publisher={Prentice-Hall},
location={Englewood Cliffs, N.J.},
date={1965},
isbn={978-0-13-702852-8},
author={Finney, Harry Hanson and Miller, Herbert Elmer},
title={Principles of accounting intermediate},
}

@misc{2017LessardEquality,
title={Equality constraints and tradeoffs},
date={2017},
url={https://laurentlessard.com/teaching/cs524/slides/9///////\\%20-///////\\%20equality///////\\%20constraints///////\\%20and///////\\%20tradeoffs.pdf},
author={Lessard, Laurent},
note={Introduction to Optimization},
groups={tesse:5},
}

@article{2008DonosoFisica,
journaltitle={Revista Brasileira de Ensino de Física},
volume={30},
groups={tesse:5},
pages={23051--230521},
issn={1806-1117},
author={Donoso, José Pedro and Tannús, Alberto and Guimarães, Francisco and Freitas, Thiago Corrêa de},
urldate={2019-03-31},
doi={10.1590/S1806-11172008000200006},
number={2},
url={http://www.scielo.br/scielo.php?pid=S0102-47442008000200006///////\\&script=sci///////\\_arttext},
date={2008},
title={A física do violino},
}

@article{2013SantosBalanco,
number={4},
title={BALANÇO SOCIAL: UMA ANÁLISE COMPARATIVA DOS INDICADORES SOCIAIS INTERNOS E EXTERNOS DAS INSTITUIÇÕES FINANCEIRAS CAIXA E BANCO DO BRASIL 2002 A 2011},
shorttitle={BALANÇO SOCIAL},
author={dos Santos, Anderson Pires and Rosa, Luciênio and Júnior, Silva},
date={2013},
volume={2},
journaltitle={Revista UNEMAT de Contabilidade},
}

@mvbook{2008DahlquistNumerical,
publisher={Society for Industrial and Applied Mathematics},
volume={2},
groups={tesse:5},
langid={english},
pagetotal={book},
date={2008},
author={Dahlquist, Germund},
title={Numerical Methods in Scientific Computing},
note={Google-Books-ID: 2i7TjgEACAAJ},
volumes={2},
isbn={978-1-4122-2499-4},
}

@book{2016LibbyResponsive,
pagetotal={174},
abstract={Design and deliver an optimal user experience for all devices About This Book Get to grips with the core functionality of RWD through examples Discover how to make layouts, content and media flexible, and explore why a content-first approach is more effective Maximize the performance of your web pages so that they work across all browsers and devices irrespective of the screen size Who This Book Is For This book is for web designers who are familiar with HTML and CSS, and want to begin with responsive web design. Web development experience and knowledge of HTML5, CSS3 is assumed. What You Will Learn Explore various layout options Understand what can be achieved in the browser, without the use of third-party tools Executing media queries to benefit responsive designs Understand the basics of responsive workflow and boilerplate frameworks Improve performance of responsive web design Maintain compatibility across various browsers In Detail Responsive web design (RWD) is a web design approach aimed at crafting sites to provide an optimal viewing and interaction experience—providing easy reading and navigation with minimum resizing, panning, and scrolling—and all of this across a wide range of devices from desktop computer monitors to mobile phones. Responsive web design is becoming more important as the amount of mobile traffic now accounts for more than half of the Internet’s total traffic. This book will give you in depth knowledge about the basics of responsive web design. You will embark on a journey of building effective responsive web pages that work across a range of devices, from mobile phones to smart TVs, with nothing more than standard markup and styling techniques. You'll begin by getting an understanding of what RWD is and its significance to the modern web. Building on the basics, you'll learn about layouts and media queries. Following this, we’ll dive into creating layouts using grid based templates. We’ll also cover the important topic of performance management, and discover how to tackle cross-browser challenges. Style and approach This is a practical example-based book which will delve into various elements and benefits of a responsive web design. It will help you understand the essential skills needed to create responsive web sites and guide you through the basics of building responsive web pages for any device. The topics are a blend of theoretical and practical essentials which will assist you to explore more about responsive web design.},
groups={tesse:5},
author={Libby, Alex and Gupta, Gaurav and Talesra, Asoj},
isbn={978-1-78355-307-5},
date={2016-08-29},
location={Birmingham, UK},
title={Responsive Web Design with HTML5 and CSS3 Essentials},
publisher={Packt Publishing - ebooks Account},
}

@book{2016HolecekDream,
date={2016-07-01},
title={Dream Yoga: Illuminating Your Life Through Lucid Dreaming and the Tibetan Yogas of Sleep},
abstract={Lucid dreaming—becoming fully conscious in the dream state—has attracted legions of those seeking to explore their vast inner worlds. Yet our states of sleep offer much more than entertainment. Combining modern lucid dreaming principles with the time-tested insights of Tibetan dream yoga makes this astonishing yet elusive experience both easier to access and profoundly life-changing. With Dream Yoga, Andrew Holecek presents a practical guide for meditators, lucid dreamers ready to go deeper, and complete beginners. Topics include: meditations and techniques for dream induction and lucidity, enhancing dream recall, dream interpretation, working with nightmares, and more.},
author={Holecek, Andrew and LaBerge, Stephen},
pagetotal={352},
shorttitle={Dream Yoga},
publisher={Sounds True},
}

@book{2017GulliDeep,
publisher={Packt},
pagetotal={304},
isbn={978-1-78712-842-2 978-1-78712-903-0},
title={Deep learning with Keras: implement neural networks with Keras on Theano and TensorFlow},
date={2017},
author={Gullì, Antonio and Pal, Sujit},
location={Birmingham Mumbai},
shorttitle={Deep learning with Keras},
}

@book{2004FeigenbaumTotal,
date={2004-08-01},
title={Total Quality Control},
pagetotal={896},
isbn={978-0-07-022003-4},
publisher={McGraw-Hill Professional},
groups={tesse:5},
edition={3 edition},
author={Feigenbaum, Armand V.},
location={New York; London},
abstract={Total Quality Control integrates an organization's quality development with existing business practices to produce quantifiable customer satisfaction. The heavily-revised Fourth Edition introduces new TQM methodologies and shows how to achieve productivity, market penetration, and thrive in a global economy. * New emphasis on managing, operating, and integrating the key areas of a company's quality value chain * Six Sigma and its role in Total Quality Management * Ensuring customer satisfaction and retention},
}

@article{1992WidrowBackpropagation,
groups={tesse:5},
date={1992},
url={http://www-isl.stanford.edu//textasciitilde widrow/papers/c1992backpropagationand.pdf},
journaltitle={Proceedings of the INNS Summer Workshop on Neural Network Computing for the Electric Power Industry},
author={Widrow, B. and Lehr, M.},
title={Backpropagation and its Applications},
}

@book{2015AzadCalculus,
pagetotal={87},
title={Calculus, Better Explained: A Guide To Developing Lasting Intuition},
date={2015},
shorttitle={Calculus, Better Explained},
author={Azad, Kalid},
abstract={Calculus, Better Explained is the calculus primer you wish you had in school. Learn the essential concepts using concrete analogies and vivid diagrams, not mechanical definitions. Calculus isn't a set of rules, it's a specific, practical viewpoint we can apply to everyday thinking.Frustrated With Abstract, Mechanical Lessons? I was too. Despite years of classes, I didn't have a strong understanding of calculus concepts. Sure, I could follow mechanical steps, but I had no lasting intuition.The classes I've seen are too long, taught in the wrong order, and without solid visualizations. Here's how this course is different:1) It gets to the point. A typical class plods along, saving concepts like Integrals until Week 8. I want to see what calculus can offer by Minute 8. Each compact, tightly-written lesson can be read in 15 minutes.2) Concepts are taught in their natural order. Most classes begin with the theory of limits, a technical concept discovered 150 years after calculus was invented. That's like putting a new driver into a Formula-1 racecar on day 1. We can begin with the easy-to-grasp concepts discovered 2000 years ago.3) It has vivid analogies and visualizations. Calculus is usually defined as the "study of change"... which sounds like history or geology. Instead of an abstract definition, we'll see calculus a step-by-step viewpoint to explore patterns.4) It's written by a human, for humans. I'm not a haughty professor or strict schoolmarm. I'm a friend who saw a fun way to internalize some difficult ideas. This course is a chat over coffee, not a keep-your-butt-in-your-seat lecture.The goal is to help you grasp the Aha! moments behind calculus in hours, not a painful semester (or a decade, in my case).Join Thousands Of Happy ReadersHere's a few samples of anonymous feedback as people went through the course. The material covers a variety of levels, whether you're looking for intuitive appreciation or the specifics of the rules."I've done all of this stuff before, and I do understand calculus intuitively, but this was the most fun I've had going through this kind of thing. The informal writing and multitude of great analogies really helps this become an enjoyable read and the rest is simple after that - you make this seem easy, but at the same time, you aren't doing it for us…This is what math education is supposed to be like :)""I have psychology and medicine background so I relate your ideas to my world. To me the most useful idea was what each circle production feels like. Rings are natural growth…Slices are automatable chunks and automation cheapens production… Boards in the shape on an Arch are psychologically most palatable for work (wind up, hard part, home stretch). Brilliant and kudos, from one INTP to another.""I like how you're introducing both derivatives and integrals at the same time - it's really helps with understanding the relationship between them. Also, I appreciate how you're coming from such a different angle than is traditionally taken - it's always interesting to see where you decide to go next.""That was breathtaking. Seriously, mail my air back please, I've grown used to it. Beautiful work, thank you. Lesson 15 was masterful. I am starting to feel calculus. "d/dx is good" (sorry, couldn't resist!)."},
}

@book{1979GareyComputers,
author={Garey, Michael R. and Johnson, David S.},
title={Computers and Intractability: A Guide to the Theory of NP-Completeness},
location={New York u.a},
date={1979-01-15},
edition={1st Edition edition},
isbn={978-0-7167-1045-5},
publisher={W. H. Freeman},
abstract={A readable, straightforward guide by two authors with extensive experience in the field. This text shows how to recognize NP-complete problems and offers practical suggestions for dealing with them effectively. It is suitable as a supplement to courses in algorithm design, computational complexity, operations research, or combinatorial mathematics, and as a text for seminars on approximation algorithms or computational complexity.},
pagetotal={340},
groups={tesse:5},
shorttitle={Computers and Intractability},
}

@book{1973GillespieQuantum,
title={A quantum mechanics primer},
abstract={Book by Gillespie, Daniel T},
location={Scranton, Pa},
author={Gillespie, Daniel T.},
year={1973},
publisher={International Textbook Co},
groups={tesse:5, Quantum Mechanics},
isbn={978-0-7002-2290-2},
pagetotal={137},
url={https://www.amazon.com/quantum-mechanics-primer-Daniel-Gillespie/dp/0700222901?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0700222901},
date={1973},
}

@book{1997MarcondesIniciacao,
publisher={Zahar},
title={Iniciação à história da filosofia: Dos pré-socráticos a Wittgenstein},
pagetotal={435},
abstract={Resultado de mais de quinze anos dedicados ao ensino da filosofia, este verdadeiro guia da história do pensamento ocidental situa pensadores e correntes filosóficas em seu contexto histórico, discute idéias e conceitos e, quando necessário, apresenta os textos mais relevantes dos filósofos em questão, como o mito da caverna de Platão ou a tabela dos juízos e categorias de Kant. Dividido cronologicamente em quatro partes - filosofia antiga, medieval, moderna e contemporânea -, após cada capítulo o livro traz quadros sinóticos que recapitulam a matéria estudada, uma seleção de leituras sugeridas, além de propor questões e temas para discussão em sala de aula, o que reforça e traduz o caráter expressamente didático da obra. Conta também com índice remissivo.},
author={Marcondes, Danilo},
date={1997},
shorttitle={Iniciação à história da filosofia},
}

@book{ceddia_pure_nodate,
author={Ceddia, Dave},
pagetotal={191},
shorttitle={Pure React},
abstract={Front-end development today is massively overwhelming.The React ecosystem is huge – Redux, React Router, Webpack, Babel… Where do you even start?Most people begin by reading tutorials, but sifting through them to find the one that makes it 'click' is an exercise in frustration. On top of that, they dish out pages of code and leave it up to you to “learn by osmosis.”If you've tried to learn React on your own, you've probably had the same experience I did: after sifting through countless tutorials and video, you still can’t cobble together your own app. One second you’re nodding along, sure that you understand. The next, you’re staring at a blinking cursor wondering what to type.Thousands of people are already using React in their jobs or startups, and you can too -- once you figure it out.But what if you could skip straight to being as productive with React as you are with your current framework of choice?What if you were able to code as fast as you can think, using the JavaScript you know and love?Writing apps could be fun again! And fast.Teaching yourself React can be overwhelming… but it doesn’t have to be. You can master the fundamentals in a matter of days.Here's the thing: you need real practice. Not just nodding along to tutorials, but actually building examples and exercises. That's how you gain mastery, and this book will show you how.You’ll get hands-on practice building a series of small components and micro apps – no big monolithic app here. The bite-size apps will have you reviewing concepts until they’re second nature.With the included exercises you will be writing your own code right from the start – this isn’t a “copy, paste, watch it run” tutorial.Don't worry, we won't build any "ToDo" apps. The world already has enough of those.Learn to turn mockups directly into code, how to “think in components,” and how to handle changing state in React's world of one-way data flow.Here's what the book covers:- Easy project setup with Create React App (you’ll be running code within minutes)- Debugging strategies for when things go wrong- Mastering JSX syntax, including “if”s, loops, and dynamic child components- Using props to communicate between components, and make them as reusable as possible- How PropTypes can save you time debugging and help “future you” remember how to use the components you wrote- Using the “children” prop to render dynamic content- How to write React in the modern ES6 style, with a gentle introduction to ES6 syntax (you don't need to know ES6!)- How input controls work in React (controlled and uncontrolled)- Where and how to properly use component state in a React application- Stateful vs Stateless components- The lifecycle of a component, and how to use it to your advantageThe book includes examples large and small, and 25 exercises to hone your knowledge.Buy Pure React and start learning React today!},
title={Pure React: A step-by-step guide to mastering React.},
}

@book{2014PooleLinear,
location={Stamford, CT},
abstract={David Poole's innovative LINEAR ALGEBRA: A MODERN INTRODUCTION, 4e emphasizes a vectors approach and better prepares students to make the transition from computational to theoretical mathematics. Balancing theory and applications, the book is written in a conversational style and combines a traditional presentation with a focus on student-centered learning. Theoretical, computational, and applied topics are presented in a flexible yet integrated way. Stressing geometric understanding before computational techniques, vectors and vector geometry are introduced early to help students visualize concepts and develop mathematical maturity for abstract thinking. Additionally, the book includes ample applications drawn from a variety of disciplines, which reinforce the fact that linear algebra is a valuable tool for modeling real-life problems.},
groups={tesse:5, Linear Algebra},
date={2014-01-08},
author={Poole, David},
title={Linear Algebra: A Modern Introduction},
edition={4 edition},
isbn={978-1-285-46324-7},
pagetotal={720},
publisher={Cengage Learning},
shorttitle={Linear Algebra},
}

@book{2015McfarlandCss,
groups={tesse:5},
shorttitle={CSS},
publisher={O'Reilly Media},
title={CSS: The Missing Manual},
date={2015-08-13},
edition={4 edition},
abstract={CSS lets you create professional-looking websites, but learning its finer points can be tricky—even for seasoned web developers. This fully updated edition provides the most modern and effective tips, tricks, and tutorial-based instruction on CSS available today. Learn how to use new tools such as Flexbox and Sass to build web pages that look great and run fast on any desktop or mobile device. Ideal for casual and experienced designers alike.The important stuff you need to know:Start with the basics. Write CSS-friendly HTML, including the HTML5 tags recognized by today’s browsers.Design for mobile devices. Create web pages that look great when visitors use them on the go.Make your pages work for you. Add animations that capture the imagination, and forms that get the job done.Take control of page layouts. Use professional design techniques such as floats and positioning.Make your layouts more flexible. Design websites with Flexbox that adjust to different devices and screen sizes.Work more efficiently. Write less CSS code and work with smaller files, using Syntactically Awesome Stylesheets (Sass).},
pagetotal={720},
author={McFarland, David Sawyer},
}

@book{lazyprogrammer_deep_nodate,
author={LazyProgrammer},
title={Deep Learning: Recurrent Neural Networks in Python: LSTM, GRU, and more RNN machine learning architectures in Python and Theano},
shorttitle={Deep Learning},
pagetotal={56},
abstract={LSTM, GRU, and more advanced recurrent neural networksLike Markov models, Recurrent Neural Networks are all about learning sequences - but whereas Markov Models are limited by the Markov assumption, Recurrent Neural Networks are not - and as a result, they are more expressive, and more powerful than anything we’ve seen on tasks that we haven’t made progress on in decades.In the first section of the course we are going to add the concept of time to our neural networks.I’ll introduce you to the Simple Recurrent Unit, also known as the Elman unit.We are going to revisit the XOR problem, but we’re going to extend it so that it becomes the parity problem - you’ll see that regular feedforward neural networks will have trouble solving this problem but recurrent networks will work because the key is to treat the input as a sequence.In the next section of the book, we are going to revisit one of the most popular applications of recurrent neural networks - language modeling.One popular application of neural networks for language is word vectors or word embeddings. The most common technique for this is called Word2Vec, but I’ll show you how recurrent neural networks can also be used for creating word vectors.In the section after, we’ll look at the very popular LSTM, or long short-term memory unit, and the more modern and efficient GRU, or gated recurrent unit, which has been proven to yield comparable performance.We’ll apply these to some more practical problems, such as learning a language model from Wikipedia data and visualizing the word embeddings we get as a result.All of the materials required for this course can be downloaded and installed for FREE. We will do most of our work in Numpy, Matplotlib, and Theano. I am always available to answer your questions and help you along your data science journey.See you in class!“Hold up... what’s deep learning and all this other crazy stuff you’re talking about?”If you are completely new to deep learning, you might want to check out my earlier books and courses on the subject:Deep Learning in Python https://www.amazon.com/dp/B01CVJ19E8Deep Learning in Python Prerequisities https://www.amazon.com/dp/B01D7GDRQ2Much like how IBM’s Deep Blue beat world champion chess player Garry Kasparov in 1996, Google’s AlphaGo recently made headlines when it beat world champion Lee Sedol in March 2016.What was amazing about this win was that experts in the field didn’t think it would happen for another 10 years. The search space of Go is much larger than that of chess, meaning that existing techniques for playing games with artificial intelligence were infeasible. Deep learning was the technique that enabled AlphaGo to correctly predict the outcome of its moves and defeat the world champion.Deep learning progress has accelerated in recent years due to more processing power (see: Tensor Processing Unit or TPU), larger datasets, and new algorithms like the ones discussed in this book.},
}

@book{2019RaoNatural,
edition={1 edition},
title={Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning},
pagetotal={256},
author={Rao, Delip and McMahan, Brian},
publisher={O'Reilly Media},
shorttitle={Natural Language Processing with PyTorch},
abstract={Natural Language Processing (NLP) provides boundless opportunities for solving problems in artificial intelligence, making products such as Amazon Alexa and Google Translate possible. If you’re a developer or data scientist new to NLP and deep learning, this practical guide shows you how to apply these methods using PyTorch, a Python-based deep learning library.Authors Delip Rao and Brian McMahon provide you with a solid grounding in NLP and deep learning algorithms and demonstrate how to use PyTorch to build applications involving rich representations of text specific to the problems you face. Each chapter includes several code examples and illustrations.Explore computational graphs and the supervised learning paradigmMaster the basics of the PyTorch optimized tensor manipulation libraryGet an overview of traditional NLP concepts and methodsLearn the basic ideas involved in building neural networksUse embeddings to represent words, sentences, documents, and other featuresExplore sequence prediction and generate sequence-to-sequence modelsLearn design patterns for building production NLP systems},
location={Beijing Boston Farnham},
date={2019-02-11},
isbn={978-1-4919-7823-8},
}

@book{SportoElm,
title={Elm Tutorial},
author={sporto},
url={https://github.com/sporto/elm-tutorial},
groups={tesse:2, Programming},
}

@book{1990MooreElements,
isbn={978-0-13-252552-7},
abstract={This is a general introduction to the theory of computer music, giving details on sound, digital signal processing, math, and C programming. It assumes a strong knowledge of music.},
groups={tesse:5},
title={Elements of Computer Music},
publisher={Prentice Hall},
pagetotal={560},
edition={1st edition},
location={Englewood Cliffs, N.J},
author={Moore, F. Richard},
date={1990-02-19},
}

@thesis{2017MartinApplication,
type={phdthesis},
institution={Czech Technical University in Prague},
date={2017},
title={Application of Game Theoretic Algorithms to Gomoku},
url={https://dspace.cvut.cz/bitstream/handle/10467/70078/F3-BP-2017-Muzika-Martin-Application///////\\%20of///////\\%20Game///////\\%20Theoretic///////\\%20Algorithms.pdf},
author={Martin, Muzika},
groups={tesse:5},
}

@book{2015KelleherFundamentals,
author={Kelleher, John D. and Namee, Brian Mac and D'Arcy, Aoife},
abstract={A comprehensive introduction to the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications.Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context.After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution. The book, informed by the authors' many years of teaching machine learning, and working on predictive data analytics projects, is suitable for use by undergraduates in computer science, engineering, mathematics, or statistics; by graduate students in disciplines with applications for predictive data analytics; and as a reference for professionals.},
date={2015-07-31},
title={Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies},
edition={1 edition},
pagetotal={624},
publisher={The MIT Press},
shorttitle={Fundamentals of Machine Learning for Predictive Data Analytics},
}

@book{2013SpanielGame,
author={Spaniel, William},
shorttitle={Game Theory 101},
abstract={Game Theory 101: The Complete Textbook is a no-nonsense, games-centered introduction to strategic form (matrix) and extensive form (game tree) games. From the first lesson to the last, this textbook introduces games of increasing complexity and then teaches the game theoretical tools necessary to solve them. Inside, you will find:1) All the basics fully explained, including pure strategy Nash equilibrium, mixed strategy Nash equilibrium, the mixed strategy algorithm, how to calculate payoffs, strict dominance, weak dominance, iterated elimination of strictly dominated strategies, iterated elimination of weakly dominated strategies, subgame perfect equilibrium, backward induction, forward induction, and more.2) Dozens of games solved, including the prisoner’s dilemma, stag hunt, matching pennies, zero sum games, battle of the sexes/Bach or Stravinsky, chicken/snowdrift, pure coordination, deadlock, safety in numbers, Selten's game, the escalation game, the ultimatum game, the pirate game, nim, the centipede game, the hawk-dove game, the volunteer's dilemma, and rock-paper-scissors.3) Rich descriptions of important economic concepts such as commitment problems, burning bridges, perverse incentives, and the chain store paradox.4) Advanced topics such as generalized games, comparative statics, and knife-edge conditions.5) Real world applications including wars, firm entry/exit, tournament strategy, arms races, advertising, game shows, soccer, baseball, video games, and more.6) Crystal clear, line-by-line calculations of every step, with more than 400 images so you don’t miss a thing.Quick, efficient, and to the point, Game Theory 101: The Complete Textbook is perfect for introductory game theory, intermediate microeconomics, and political science.},
pagetotal={275},
title={Game Theory 101: The Complete Textbook},
date={2013},
}

@book{2007BinmoreGame,
pagetotal={201},
isbn={978-0-19-921846-2},
abstract={Games are everywhere: Drivers manoeuvring in heavy traffic are playing a driving game. Bargain hunters bidding on eBay are playing an auctioning game. A firm negotiating next year's wage is playing a bargaining game. The opposing candidates in an election are playing a political game. The supermarket's price for corn flakes is decided by playing an economic game.Game theory is about how to play such games in a rational way. Even when the players have not thought everything out in advance, game theory often works for the same reason that mindless animals sometimes end up behaving very cleverly: evolutionary forces eliminate irrational play because it is unfit.Game theory has seen spectacular successes in evolutionary biology and economics, and is beginning to revolutionize other disciplines from psychology to political science. This Very Short Introduction introduces the fascinating world of game theory, showing how it can be understood without mathematical equations, and revealing that everything from how to play poker optimally to the sex ratio among bees can be understood by anyone willing to think seriously about the problem.ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
shorttitle={Game Theory},
location={Oxford},
publisher={OUP Oxford},
title={Game Theory: A Very Short Introduction},
author={Binmore, Ken},
date={2007-10-25},
edition={1 edition},
}

@book{2008DahlquistNumericala,
author={Dahlquist, Germund and Björck, Åke},
location={Philadelphia},
pagetotal={746},
isbn={978-0-89871-644-3},
url={http://fmipa.umri.ac.id/wp-content/uploads/2016/03/Dahlquist///////\\_G.///////\\_Bjoerck///////\\_A.///////\\_Vol.1.///////\\_Numerical///////\\_methodBookZZ.org///////\\_.pdf},
date={2008-09-04},
title={Numerical Methods in Scientific Computing: Volume 1},
shorttitle={Numerical Methods in Scientific Computing},
abstract={This new book from the authors of the classic book Numerical Methods addresses the increasingly important role of numerical methods in science and engineering. More cohesive and comprehensive than any other modern textbook in the field, it combines traditional and well-developed topics with other material that is rarely found in numerical analysis texts, such as interval arithmetic, elementary functions, operator series, convergence acceleration, and continued fractions. Although this volume is self-contained, more comprehensive treatments of matrix computations will be given in a forthcoming volume. A supplementary Website contains three appendices: an introduction to matrix computations; a description of Mulprec, a MATLAB® multiple precision package; and a guide to literature, algorithms, and software in numerical analysis. Review questions, problems, and computer exercises are also included. For use in an introductory graduate course in numerical analysis and for researchers who use numerical methods in science and engineering.},
publisher={Society for Industrial and Applied Mathematics},
}

@book{2018LazarMastering,
author={Lazar, Guillaume and Penea, Robin},
date={2018},
isbn={978-1-78899-539-9},
location={Birmingham, UK},
title={Mastering Qt 5: create stunning cross-platform applications using C++ with Qt widgets and QML with QT Quick},
publisher={Packt},
shorttitle={Mastering Qt 5},
url={http://www.arxen.com/descargas/Books/Mastering///////\\%20Qt///////\\%205///////\\%20-///////\\%20Guillaume///////\\%20Lazar,///////\\%20Robin///////\\%20Penea.pdf},
note={OCLC: 1039917696},
}

@book{2004ScheyDiv,
abstract={This new fourth edition of the acclaimed and bestselling Div, Grad, Curl, and All That has been carefully revised and now includes updated notations and seven new example exercises. Since the publication of the First Edition over thirty years ago, Div, Grad, Curl, and All That has been widely renowned for its clear and concise coverage of vector calculus, helping science and engineering students gain a thorough understanding of gradient, curl, and Laplacian operators without required knowledge of advanced mathematics.},
author={Schey, H. M.},
title={Div, Grad, Curl, and All That: An Informal Text on Vector Calculus},
date={2004-12-01},
edition={4th edition},
pagetotal={176},
publisher={W. W. Norton ////////\\\& Company},
shorttitle={Div, Grad, Curl, and All That},
location={New York},
isbn={978-0-393-92516-6},
}

@book{2015GrolemundHands,
date={2015-08-01},
langid={english},
pagetotal={247},
year={2015},
urldate={2019-04-01},
title={Hands-On Programming with R},
abstract={Learn how to program by diving into the R language, and then use your newfound skills to solve practical data science problems. With this book, you’ll learn how to load data, assemble and disassemble data objects, navigate R’s...},
author={Grolemund, Garrett},
isbn={978-1-4493-5901-0},
publisher={O'Reilly UK Ltd.},
url={https://d1b10bmlvqabco.cloudfront.net/attach/ighbo26t3ua52t/igp9099yy4v10/igz7vp4w5su9/OReilly///////\\_HandsOn///////\\_Programming///////\\_with///////\\_R///////\\_2014.pdf},
ean={9781449359010},
groups={tesse:5, Programming},
}

@book{2017PercivalTest,
isbn={978-1-4919-5870-4},
abstract={By taking you through the development of a real web application from beginning to end, the second edition of this hands-on guide demonstrates the practical advantages of test-driven development (TDD) with Python. You’ll learn how to write and run tests before building each part of your app, and then develop the minimum amount of code required to pass those tests. The result? Clean code that works.In the process, you’ll learn the basics of Django, Selenium, Git, jQuery, and Mock, along with current web development techniques. If you’re ready to take your Python skills to the next level, this book—updated for Python 3.6—clearly demonstrates how TDD encourages simple designs and inspires confidence.Dive into the TDD workflow, including the unit test/code cycle and refactoringUse unit tests for classes and functions, and functional tests for user interactions within the browserLearn when and how to use mock objects, and the pros and cons of isolated vs. integrated testsTest and automate your deployments with a staging serverApply tests to the third-party plugins you integrate into your siteRun tests automatically by using a Continuous Integration environmentUse TDD to build a REST API with a front-end Ajax interface},
author={Percival, Harry},
location={Sebastopol, CA},
shorttitle={Test-Driven Development with Python},
title={Test-Driven Development with Python: Obey the Testing Goat: Using Django, Selenium, and JavaScript},
pagetotal={624},
date={2017-08-31},
edition={2 edition},
publisher={O'Reilly Media},
url={http://www.obeythetestinggoat.com/pages/book.html///////\\#toc},
}

@book{2015EcoHow,
pagetotal={256},
author={Eco, Umberto and Erspamer, Francesco},
date={2015-03-06},
abstract={Umberto Eco's wise and witty guide to researching and writing a thesis, published in English for the first time. By the time Umberto Eco published his best-selling novel The Name of the Rose, he was one of Italy's most celebrated intellectuals, a distinguished academic and the author of influential works on semiotics. Some years before that, in 1977, Eco published a little book for his students, How to Write a Thesis, in which he offered useful advice on all the steps involved in researching and writing a thesis―from choosing a topic to organizing a work schedule to writing the final draft. Now in its twenty-third edition in Italy and translated into seventeen languages, How to Write a Thesis has become a classic. Remarkably, this is its first, long overdue publication in English. Eco's approach is anything but dry and academic. He not only offers practical advice but also considers larger questions about the value of the thesis-writing exercise. How to Write a Thesis is unlike any other writing manual. It reads like a novel. It is opinionated. It is frequently irreverent, sometimes polemical, and often hilarious. Eco advises students how to avoid “thesis neurosis” and he answers the important question “Must You Read Books?” He reminds students “You are not Proust” and “Write everything that comes into your head, but only in the first draft.” Of course, there was no Internet in 1977, but Eco's index card research system offers important lessons about critical thinking and information curating for students of today who may be burdened by Big Data.How to Write a Thesis belongs on the bookshelves of students, teachers, writers, and Eco fans everywhere. Already a classic, it would fit nicely between two other classics: Strunk and White and The Name of the Rose.ContentsThe Definition and Purpose of a Thesis • Choosing the Topic • Conducting Research • The Work Plan and the Index Cards • Writing the Thesis • The Final Draft},
title={How to Write a Thesis},
publisher={The MIT Press},
isbn={978-0-262-52713-2},
groups={tesse:5},
location={Cambridge, Massachusetts},
edition={Translation edition},
translator={Farina, Caterina Mongiat and Farina, Geoff},
}

@book{1982MichaelsonHow,
publisher={ISI Press},
author={Michaelson, Herbert B.},
title={How to write and publish engineering papers and reports},
location={Philadelphia},
series={The Professional writing series},
url={https://www.amazon.com/Publish-Engineering-Reports-Professional-writing/dp/0894950169?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0894950169},
groups={tesse:5, Writing},
date={1982},
year={1982},
pagetotal={158},
isbn={978-0-89495-016-2},
}

@book{2017PastineIntroducing,
author={Pastine, Ivan and Pastine, Tuvana},
pagetotal={176},
shorttitle={Introducing Game Theory},
publisher={Icon Books Ltd},
abstract={When should you adopt an aggressive business strategy? How do we make decisions when we don’t have all the information? What makes international environmental cooperation possible?Game theory is the study of how we make a decision when the outcome of our moves depends on the decisions of someone else. Economists Ivan and Tuvana Pastine explain why, in these situations, we sometimes cooperate, sometimes clash, and sometimes act in a way that seems completely random.Stylishly brought to life by award-winning cartoonist Tom Humberstone, Game Theory will help readers understand behaviour in everything from our social lives to business, global politics to evolutionary biology. It provides a thrilling new perspective on the world we live in.},
date={2017-03-02},
title={Introducing Game Theory: A Graphic Guide},
}

@book{2010MixermanZen,
abstract={(Technical Reference). In his first book, The Daily Adventures of Mixerman , the author detailed the frustrating and often hilarious goings on during the process of recording a major-label band. Musicians, engineers, and producers laughed and cried at the crazy goings-on they'd never imagined or recognized all too well. Now Mixerman turns his razor-sharp gaze to the art of mixing and gives followers and the uninitiated reason to hope if not for logic and civility in the recording studio then at least for a good sounding record. With a firm commitment to art over technology and to maintaining a grasp of each, Mixerman outlines his own approach to recording success, based on his years mixing records in all genres of music for all kinds of artists, often under trying circumstances. As he states in his introduction to the new volume, "Even if you're not a professional mixer, even if you're a musician trying to mix your own work or a studio owner in a smaller market, you have your own set of pressures to deal with while you're mixing. Regardless of what those pressures are, it's important to identify and recognize them, if for no other reason than so you can learn to completely ignore them." But how? "That's where the Zen comes in."},
pagetotal={350},
title={Zen and the Art of Mixing},
date={2010-10-01},
publisher={Hal Leonard},
author={Mixerman},
}

@book{2003KunderaArt,
isbn={978-0-06-009374-7},
pagetotal={176},
edition={Reprint edition},
abstract={Kundera brilliantly examines the work of such important and diverse figures as Rabelais, Cervantes, Sterne, Diderot, Flaubert, Tolstoy, and Musil. He is especially penetrating on Hermann Broch, and his exploration of the world of Kafka's novels vividly reveals the comic terror of Kafka's bureaucratized universe.Kundera's discussion of his own work includes his views on the role of historical events in fiction, the meaning of action, and the creation of character in the post-psychological novel.},
location={New York},
date={2003-04-01},
publisher={Harper Perennial Modern Classics},
author={Kundera, Milan},
title={The Art of the Novel},
}

@book{2018SinghIonic,
shorttitle={Ionic Cookbook},
date={2018-04-30},
publisher={Packt Publishing},
author={Singh, Indermohan and Phan, Hoc},
title={Ionic Cookbook: Recipes to create cutting-edge, real-time hybrid mobile apps with Ionic, 3rd Edition},
pagetotal={392},
edition={3 edition},
abstract={Solve all your Ionic-related issues through dedicated recipes that will help you get the best out of Ionic. Working with Ionic components to find out the best way to share data between them effectively.Key FeaturesLeverage Ionic 3.9 and its exciting new features to create cutting-edge, real-time apps Work through simple recipes to address your problems directly and solve them effectively Get examples at each step to guide you on your learning curve with AngularBook DescriptionIonic is the preferred choice for JavaScript developers to develop real-time hybrid applications. This book will get you started with Ionic 3.9 and help you create Angular 5 components that interact with templates.You will work with Ionic components and find out how to share data efficiently between them. You'll discover how to make the best use of the REST API to handle back-end services and then move on to animating your application to make it look pretty. You then learn to add in a local push notification in order to test the app. Then you'll work with Cordova to support native functionalities on both iOS and Android. From there, you'll get to grips with using the default themes for each platform and customizing your own. We then take you through the advanced Ionic features like lazy loading, deep linking, localizing ionic apps etc. Finally, you'll see how best to deploy your app to different platforms.This book will solve all your Ionic-related issues through dedicated recipes that will help you get the best out of Ionic.What you will learnHelp readers to jump-start Ionic appsExplore essential features of Ionic with examplesLearn how to use native device functionalitiesMake the best use of the REST API to handle back-end servicesWork with Cordova to support native functionalities on both iOS and Android.Master advanced topics in app development such as deep linking and lazy loadingWho This Book Is ForThis book targets JavaScript developers. No previous knowledge of Ionic is necessary, but prior knowledge of web development techniques would be useful.Table of ContentsCreating Our First App with Ionic 3Adding Ionic 3 ComponentsExtending Ionic 3 with Angular 4 Building BlocksValidating Forms and Making HTTP RequestsAdding AnimationUser Authentication and Push NotificationSupporting Device Functionalities Using Ionic NativeTheming the AppTaking your app to the next levelPublishing the App for Different Platforms},
}

@book{2017GalowiczC++17,
author={Galowicz, Jacek},
date={2017-06-28},
title={C++17 STL Cookbook: Discover the latest enhancements to functional programming and lambda expressions},
pagetotal={534},
edition={1 edition},
publisher={Packt Publishing},
abstract={Key FeaturesLearn the latest features of C++ and how to write better code by using the Standard Library (STL). Reduce the development time for your applications.Understand the scope and power of STL features to deal with real-world problems.Compose your own algorithms without forfeiting the simplicity and elegance of the STL way.Book DescriptionC++ has come a long way and is in use in every area of the industry. Fast, efficient, and flexible, it is used to solve many problems. The upcoming version of C++ will see programmers change the way they code. If you want to grasp the practical usefulness of the C++17 STL in order to write smarter, fully portable code, then this book is for you.Beginning with new language features, this book will help you understand the language's mechanics and library features, and offers insight into how they work. Unlike other books, ours takes an implementation-specific, problem-solution approach that will help you quickly overcome hurdles. You will learn the core STL concepts, such as containers, algorithms, utility classes, lambda expressions, iterators, and more, while working on practical real-world recipes. These recipes will help you get the most from the STL and show you how to program in a better way.By the end of the book, you will be up to date with the latest C++17 features and save time and effort while solving tasks elegantly using the STL.What you will learnLearn about the new core language features and the problems they were intended to solveUnderstand the inner workings and requirements of iterators by implementing themExplore algorithms, functional programming style, and lambda expressionsLeverage the rich, portable, fast, and well-tested set of well-designed algorithms provided in the STLWork with strings the STL way instead of handcrafting C-style codeUnderstand standard support classes for concurrency and synchronization, and how to put them to workUse the filesystem library addition available with the C++17 STLAbout the AuthorJacek Galowicz obtained his master of science in electrical engineering/computer engineering at RWTH Aachen University, Germany. While at university, he enjoyed working as a student assistant in teaching and research, and he participated in several scientific publications. During and after his studies, he worked as a freelancer and implemented applications as well as kernel drivers in C and C++, touching various areas, including 3D graphics programming, databases, network communication, and physics simulation. In recent years, he has been programming performance- and security-sensitive microkernel operating systems for Intel x86 virtualization at Intel and FireEye in Braunschweig, Germany. He has a strong passion for modern C++ implementations of low-level software, and he tries hard to combine high performance with an elegant coding style. Learning purely functional programming and Haskell in recent years triggered his drive to implement generic code with the aid of meta programming.Table of ContentsThe New C++17 FeaturesSTL ContainersIteratorsLambda ExpressionsSTL Algorithm BasicsAdvanced Use of STL AlgorithmsStrings, Stream Classes, and Regular ExpressionsUtility ClasssesParallelism and ConcurrencyFilesystem},
shorttitle={C++17 STL Cookbook},
}

@book{2014HorsleyUnlimited,
shorttitle={Unlimited Memory},
edition={2 edition},
pagetotal={189},
author={Horsley, Kevin},
date={2014-01-26},
publisher={TCK Publishing},
abstract={Kevin Horsley Broke a World Memory Record in 2013...And You're About to Learn How to Use His Memory Strategies to Learn Faster, Be More Productive and Achieve More SuccessWith over 200,000 copies sold, Unlimited Memory is a Wall Street Journal Best Seller and has been the ///////\\#1 memory book on Amazon for more than two years. It has been translated into more than a dozen languages including French, Chinese, Russian, Korean, Ukrainian, and Lithuanian.Most people never tap into 10///////\\% of their potential for memory.In this book, you're about to learn:How the World's Top Memory Experts Concentrate and Remember Any Information at Will, and How You Can TooDo you ever feel like you're too busy, too stressed or just too distracted to concentrate and get work done?In Unlimited Memory, you'll learn how the world's best memory masters get themselves to concentrate at will, anytime they want. When you can easily focus and concentrate on the task at hand, and store and recall useful information, you can easily double your productivity and eliminate wasted time, stress and mistakes at work.In this book, you'll find all the tools, strategies and techniques you need to improve your memory.Here’s just a taste of the memory methods you'll learn in this book: The 3 bad habits that keep you from easily remembering important information How a simple pattern of thinking can stop you from imprinting and remembering key facts, figures and ideas, and how to break this old pattern so you’ll never again be known as someone with a “bad memory” How to master your attention so you can focus and concentrate longer, even during challenging or stressful situations How to use your car to remember anything you want (like long lists or information you need to remember for your studies or personal life) without writing anything down Simple methods that allow you to nail down tough information or complex concepts quickly and easily How to combine your long-term memory (things you already know and will never forget) and short-term memory (information you want to remember right now) to create instant recall for tests, presentations and important projects The simple, invisible mental technique for remembering names without social awkwardness or anxiety How using your imagination to bring boring information to life can help you dramatically improve your attention span and recall An incredible strategy for remembering numbers (the same system Kevin used to remember Pi to 10,000 digits and beat the world memory record by 14 minutes) How to use a mental map to lock in and connect hundreds or even thousands of ideas in your long-term memory (this method will allow you to become a leading expert in your field faster than you ever dreamed possible)If you're ready to harness the incredible power of your mind to remember more in less time, this book is for you.About the AuthorFor over 25 years, Kevin Horsley has been analyzing the mind and memory and its capacity for brilliance. He is one of only a few people in the world to have received the title of International Grandmaster of Memory. He is a World Memory Championship medalist, and a two-time World Record holder for The Everest of memory tests. Kevin is also an author of four books, and the designer of a mathematics game with the Serious Games Institute at North-West University Vaal Campus.His work has been featured in The Oprah Magazine, Time, Forbes, Inc., The Huffington Post, ELLE, Longevity and on numerous TV and radio shows. Kevin is an International professional speaker and has spoken in many different countries. He assists organizations in improving their learning, motivation, creativity and thinking.},
title={Unlimited Memory: How to Use Advanced Learning Strategies to Learn Faster, Remember More and be More Productive},
}

@book{2015VerouCss,
abstract={In this practical guide, CSS expert Lea Verou provides 47 undocumented techniques and tips to help intermediate-to advanced CSS developers devise elegant solutions to a wide range of everyday web design problems.Rather than focus on design, CSS Secrets shows you how to solve problems with code. You'll learn how to apply Lea's analytical approach to practically every CSS problem you face to attain DRY, maintainable, flexible, lightweight, and standards-compliant results.Inspired by her popular talks at over 60 international web development conferences, Lea Verou provides a wealth of information for topics including:Backgrounds and BordersShapesVisual EffectsTypographyUser ExperienceStructure and LayoutTransitions and Animations},
shorttitle={CSS Secrets},
publisher={O'Reilly Media},
date={2015-07-03},
isbn={978-1-4493-7263-7},
author={Verou, Lea},
location={Sebastopol, CA},
pagetotal={392},
title={CSS Secrets: Better Solutions to Everyday Web Design Problems},
edition={1 edition},
groups={tesse:5},
}

@book{1997BieglerSystematic,
groups={tesse:5},
author={Biegler, Lorenz T. and Grossmann, Ignacio E. and Westerberg, Arthur W.},
title={Systematic Methods of Chemical Process Design},
date={1997-04-06},
location={Upper Saddle River, N.J},
edition={1 edition},
abstract={Brings together all the information engineers and researchers need to develop efficient, cost-effective chemical production processes. The book presents a systematic approach to chemical process design, covering both continuous and batch processes. Starting with the basics, the book then moves on to advanced topics. Among the topics covered are: flowsheet synthesis, mass and energy balances, equipment sizing and costing, economic evaluation, process simulation and optimization. The book also covers specific chemical processes such as distillation systems, reactor networks, separation, and heat exchange networks. It shows how to build more flexible processes, including multiproduct batch processes. Any researcher or practicing engineer involved in designing chemical processes.},
pagetotal={796},
publisher={Prentice Hall},
isbn={978-0-13-492422-9},
}

@book{2017MoskalaAndroid,
shorttitle={Android Development with Kotlin},
pagetotal={440},
abstract={Learn how to make Android development much faster using a variety of Kotlin features, from basics to advanced, to write better quality code.Key FeaturesLeverage specific features of Kotlin to ease Android application developmentWrite code based on both object oriented and functional programming to build robust applicationsFilled with various practical examples so you can easily apply your knowledge to real world scenariosIdentify the improved way of dealing with common Java patternsBook DescriptionNowadays, improved application development does not just mean building better performing applications. It has become crucial to find improved ways of writing code. Kotlin is a language that helps developers build amazing Android applications easily and effectively. This book discusses Kotlin features in context of Android development. It demonstrates how common examples that are typical for Android development, can be simplified using Kotlin. It also shows all the benefits, improvements and new possibilities provided by this language.The book is divided in three modules that show the power of Kotlin and teach you how to use it properly. Each module present features in different levels of advancement. The first module covers Kotlin basics. This module will lay a firm foundation for the rest of the chapters so you are able to read and understand most of the Kotlin code. The next module dives deeper into the building blocks of Kotlin, such as functions, classes, and function types. You will learn how Kotlin brings many improvements to the table by improving common Java concepts and decreasing code verbosity. The last module presents features that are not present in Java. You will learn how certain tasks can be achieved in simpler ways thanks to Kotlin.Through the book, you will learn how to use Kotlin for Android development. You will get to know and understand most important Kotlin features, and how they can be used. You will be ready to start your own adventure with Android development with Kotlin.What you will learnRun a Kotlin application and understand the integration with Android StudioIncorporate Kotlin into new/existing Android Java based projectLearn about Kotlin type system to deal with null safety and immutabilityDefine various types of classes and deal with propertiesDefine collections and transform them in functional wayDefine extensions, new behaviours to existing libraries and Android framework classesUse generic type variance modifiers to define subtyping relationship between generic typesBuild a sample applicationTable of ContentsBeginning Your Kotlin AdventureLaying a FoundationPlaying with FunctionsClasses and ObjectsFunctions as First-Class CitizensGenerics Are Your FriendsExtension Functions and PropertiesDelegatesMaking Your Marvel Gallery Application},
author={Moskala, Marcin and Wojda, Igor},
publisher={Packt Publishing},
title={Android Development with Kotlin: Enhance your skills for Android development using Kotlin},
edition={1 edition},
date={2017-08-30},
}

@book{2017BancilaModern,
date={2017-05-15},
pagetotal={590},
publisher={Packt Publishing - ebooks Account},
shorttitle={Modern C++ Programming Cookbook},
isbn={978-1-78646-518-4},
author={Bancila, Marius},
abstract={Over 100 recipes to help you overcome your difficulties with C++ programming and gain a deeper understanding of the working of modern C++About This BookExplore the most important language and library features of C++17, including containers, algorithms, regular expressions, threads, and more,Get going with unit testing frameworks Boost.Test, Google Test and Catch,Extend your C++ knowledge and take your development skills to new heights by making your applications fast, robust, and scalable.Who This Book Is ForIf you want to overcome difficult phases of development with C++ and leverage its features using modern programming practices, then this book is for you. The book is designed for both experienced C++ programmers as well as people with strong knowledge of OOP concepts.What You Will LearnGet to know about the new core language features and the problems they were intended to solveUnderstand the standard support for threading and concurrency and know how to put them on work for daily basic tasksLeverage C++'s features to get increased robustness and performanceExplore the widely-used testing frameworks for C++ and implement various useful patterns and idiomsWork with various types of strings and look at the various aspects of compilationExplore functions and callable objects with a focus on modern featuresLeverage the standard library and work with containers, algorithms, and iteratorsUse regular expressions for find and replace string operationsTake advantage of the new filesystem library to work with files and directories},
title={Modern C++ Programming Cookbook: Recipes to explore data structure, multithreading, and networking in C++17},
}

@book{2018LapanDeep,
pagetotal={546},
title={Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more},
shorttitle={Deep Reinforcement Learning Hands-On},
publisher={Packt Publishing},
author={Lapan, Maxim},
abstract={This practical guide will teach you how deep learning (DL) can be used to solve complex real-world problems.Key FeaturesExplore deep reinforcement learning (RL), from the first principles to the latest algorithmsEvaluate high-profile RL methods, including value iteration, deep Q-networks, policy gradients, TRPO, PPO, DDPG, D4PG, evolution strategies and genetic algorithmsKeep up with the very latest industry developments, including AI-driven chatbotsBook DescriptionRecent developments in reinforcement learning (RL), combined with deep learning (DL), have seen unprecedented progress made towards training agents to solve complex problems in a human-like way. Google's use of algorithms to play and defeat the well-known Atari arcade games has propelled the field to prominence, and researchers are generating new ideas at a rapid pace.Deep Reinforcement Learning Hands-On is a comprehensive guide to the very latest DL tools and their limitations. You will evaluate methods including Cross-entropy and policy gradients, before applying them to real-world environments. Take on both the Atari set of virtual games and family favorites such as Connect4. The book provides an introduction to the basics of RL, giving you the know-how to code intelligent learning agents to take on a formidable array of practical tasks. Discover how to implement Q-learning on 'grid world' environments, teach your agent to buy and trade stocks, and find out how natural language models are driving the boom in chatbots.What you will learnUnderstand the DL context of RL and implement complex DL modelsLearn the foundation of RL: Markov decision processesEvaluate RL methods including Cross-entropy, DQN, Actor-Critic, TRPO, PPO, DDPG, D4PG and othersDiscover how to deal with discrete and continuous action spaces in various environmentsDefeat Atari arcade games using the value iteration methodCreate your own OpenAI Gym environment to train a stock trading agentTeach your agent to play Connect4 using AlphaGo ZeroExplore the very latest deep RL research on topics including AI-driven chatbotsWho This Book Is ForSome fluency in Python is assumed. Basic deep learning (DL) approaches should be familiar to readers and some practical experience in DL will be helpful. This book is an introduction to deep reinforcement learning (RL) and requires no background in RL.Table of ContentsWhat is Reinforcement Learning?OpenAI GymDeep Learning with PyTorchThe Cross-Entropy MethodTabular Learning and the Bellman EquationDeep Q-NetworksDQN ExtensionsStocks Trading Using RLPolicy Gradients – An AlternativeThe Actor-Critic MethodAsynchronous Advantage Actor-CriticChatbots Training with RLWeb NavigationContinuous Action SpaceTrust Regions – TRPO, PPO, and ACKTRBlack-Box Optimization in RLBeyond Model-Free – ImaginationAlphaGo Zero},
location={Birmingham Mumbai},
date={2018-06-21},
isbn={978-1-78883-424-7},
}

@book{2016CostandiNeuroplasticity,
author={Costandi, Moheb},
date={2016-08-19},
isbn={978-0-262-52933-4},
location={Cambridge, MA},
title={Neuroplasticity},
abstract={The real story of how our brains and nervous systems change throughout our lifetimes―with or without “brain training.”Fifty years ago, neuroscientists thought that a mature brain was fixed like a fly in amber, unable to change. Today, we know that our brains and nervous systems change throughout our lifetimes. This concept of neuroplasticity has captured the imagination of a public eager for self-improvement―and has inspired countless Internet entrepreneurs who peddle dubious “brain training” games and apps. In this book, Moheb Costandi offers a concise and engaging overview of neuroplasticity for the general reader, describing how our brains change continuously in response to our actions and experiences.Costandi discusses key experimental findings, and describes how our thinking about the brain has evolved over time. He explains how the brain changes during development, and the “synaptic pruning” that takes place before brain maturity. He shows that adult brains can grow new cells (citing, among many other studies, research showing that sexually mature male canaries learn a new song every year). He describes the kind of brain training that can bring about improvement in brain function. It's not gadgets and games that promise to “rewire your brain” but such sustained cognitive tasks as learning a musical instrument or a new language. (Costandi also notes that London cabbies increase their gray matter after rigorous training in their city's complicated streets.) He tells how brains compensate after stroke or injury; describes addiction and pain as maladaptive forms of neuroplasticity; and considers brain changes that accompany childhood, adolescence, parenthood, and aging. Each of our brains is custom-built. Neuroplasticity is at the heart of what makes us human.},
publisher={The MIT Press},
groups={tesse:5},
pagetotal={192},
}

@book{2015NiouStrategy,
author={Niou, Emerson and Ordeshook, Peter C.},
edition={1 edition},
location={New York, NY},
pagetotal={432},
publisher={Routledge},
shorttitle={Strategy and Politics},
date={2015-05-28},
title={Strategy and Politics: An Introduction to Game Theory},
isbn={978-1-138-01948-5},
abstract={Strategy and Politics: An Introduction to Game Theory is designed to introduce students with no background in formal theory to the application of game theory to modeling political processes. This accessible text covers the essential aspects of game theory while keeping the reader constantly in touch with why political science as a whole would benefit from considering this method. Examining the very phenomena that power political machineries―elections, legislative and committee processes, and international conflict, the book attempts to answer fundamental questions about their nature and function in a clear, accessible manner. Included at the end of each chapter is a set of exercises designed to allow students to practice the construction and analysis of political models. Although the text assumes only an elementary-level training in algebra, students who complete a course around this text will be equipped to read nearly all of the professional literature that makes use of game theoretic analysis.},
}

@book{2016RashidMake,
date={2016-03-31},
author={Rashid, Tariq},
edition={1 edition},
abstract={A step-by-step gentle journey through the mathematics of neural networks, and making your own using the Python computer language. Neural networks are a key element of deep learning and artificial intelligence, which today is capable of some truly impressive feats. Yet too few really understand how neural networks actually work. This guide will take you on a fun and unhurried journey, starting from very simple ideas, and gradually building up an understanding of how neural networks work. You won't need any mathematics beyond secondary school, and an accessible introduction to calculus is also included. The ambition of this guide is to make neural networks as accessible as possible to as many readers as possible - there are enough texts for advanced readers already!You'll learn to code in Python and make your own neural network, teaching it to recognise human handwritten numbers, and performing as well as professionally developed networks. Part 1 is about ideas. We introduce the mathematical ideas underlying the neural networks, gently with lots of illustrations and examples. Part 2 is practical. We introduce the popular and easy to learn Python programming language, and gradually builds up a neural network which can learn to recognise human handwritten numbers, easily getting it to perform as well as networks made by professionals. Part 3 extends these ideas further. We push the performance of our neural network to an industry leading 98 /\\% using only simple ideas and code, test the network on your own handwriting, take a privileged peek inside the mysterious mind of a neural network, and even get it all working on a Raspberry Pi. All the code in this has been tested to work on a Raspberry Pi Zero.},
location={s.l.},
pagetotal={222},
publisher={CreateSpace Independent Publishing Platform},
groups={tesse:5, Neural Networks and Sustainability, Neural Networks},
title={Make Your Own Neural Network},
isbn={978-1-5308-2660-5},
}

@book{2014TalwalkarJoy,
publisher={CreateSpace Independent Publishing Platform},
shorttitle={The Joy of Game Theory},
author={Talwalkar, Presh},
title={The Joy of Game Theory: An Introduction to Strategic Thinking},
pagetotal={154},
abstract={This book is a selection of the best articles from Game Theory Tuesdays, a column from the blog Mind Your Decisions. Articles from Game Theory Tuesdays have been referenced in The Freakonomics Blog, Yahoo Finance, and CNN.com. Game theory is the study of interactive decision making--that is, in situations where each person's action affects the outcome for the whole group. Game theory is a beautiful subject and this book will teach you how to understand the theory and practically implement solutions through a series of stories and the aid of over 30 illustrations. This book has two primary objectives. (1) To help you recognize strategic games, like the Prisoner's Dilemma, Bertrand Duopoly, Hotelling's Game, the Game of Chicken, and Mutually Assured Destruction. (2) To show you how to make better decisions and change the game, a powerful concept that can transform no-win situations into mutually beneficial outcomes. You'll learn how to negotiate better by making your threats credible, sometimes limiting options or burning bridges, and thinking about new ways to create better outcomes. As these goals indicate, game theory is about more than board games and gambling. It all seems so simple, and yet that definition belies the complexity of game theory. While it may only take seconds to get a sense of game theory, it takes a lifetime to appreciate and master it. This book will get you started.},
date={2014-08-08},
isbn={978-1-5004-9744-6},
}

@book{2012RamachandranTell,
date={2012-04-05},
title={The Tell-Tale Brain: Unlocking the Mystery of Human Nature},
isbn={978-0-09-953759-5},
groups={tesse:5},
author={Ramachandran, V. S.},
abstract={John, aged sixty, suffered a stroke and recovered fully, except in one respect: although he can see perfectly, he can no longer recognise faces, even his own reflection in a mirror.Whenever Francesca touches a particular texture, she experiences a vivid emotion: denim = extreme sadness; wax = embarrassment; orange peel = shock.Jimmie, whose left arm was recently amputated, can still feel it - and it's itchy.Our brains are the most enchanting and complex things in the known universe - but what happens when they go wrong? Dr V. S. Ramachandran, 'the Sherlock Holmes of brain science' and one of the world's leading neuroscientists, has spent a lifetime working with patients who suffer from rare and baffling brain conditions. In The Tell-Tale Brain, he tells their stories, and explores what they reveal about the greatest mystery of them all: how our minds work, and what makes each of us so uniquely human.},
pagetotal={384},
shorttitle={The Tell-Tale Brain},
publisher={Windmill Books},
}

@collection{2012KandelPrinciples,
isbn={978-0-07-139011-8},
location={New York},
abstract={Publisher's Note: Products purchased from Third Party sellers are not guaranteed by the publisher for quality, authenticity, or access to any online entitlements included with the product.Now updated: the definitive neuroscience resource―from Eric R. Kandel, MD (winner of the Nobel Prize in 2000); James H. Schwartz, MD, PhD; Thomas M. Jessell, PhD; Steven A. Siegelbaum, PhD; and A. J. Hudspeth, PhDA Doody's Core Title for 2017!900 full-color illustrationsDeciphering the link between the human brain and behavior has always been one of the most intriguing―and often challenging―aspects of scientific endeavor. The sequencing of the human genome, and advances in molecular biology, have illuminated the pathogenesis of many neurological diseases and have propelled our knowledge of how the brain controls behavior.To grasp the wider implications of these developments and gain a fundamental understanding of this dynamic, fast-moving field, Principles of Neuroscience stands alone as the most authoritative and indispensible resource of its kind.In this classic text, prominent researchers in the field expertly survey the entire spectrum of neural science, giving an up-to-date, unparalleled view of the discipline for anyone who studies brain and mind. Here, in one remarkable volume, is the current state of neural science knowledge―ranging from molecules and cells, to anatomic structures and systems, to the senses and cognitive functions―all supported by more than 900 precise, full-color illustrations. In addition to clarifying complex topics, the book also benefits from a cohesive organization, beginning with an insightful overview of the interrelationships between the brain, nervous system, genes, and behavior. Principles of Neural Science then proceeds with an in-depth examination of the molecular and cellular biology of nerve cells, synaptic transmission, and the neural basis of cognition. The remaining sections illuminate how cells, molecules, and systems give us sight, hearing, touch, movement, thought, learning, memories, and emotions.The new fifth edition of Principles of Neural Science is thoroughly updated to reflect the tremendous amount of research, and the very latest clinical perspectives, that have significantly transformed the field within the last decade.Ultimately, Principles of Neural Science affirms that all behavior is an expression of neural activity, and that the future of clinical neurology and psychiatry hinges on the progress of neural science. Far exceeding the scope and scholarship of similar texts, this unmatched guide offers a commanding, scientifically rigorous perspective on the molecular mechanisms of neural function and disease―one that you’ll continually rely on to advance your comprehension of brain, mind, and behavior.FEATURESThe cornerstone reference in the field of neuroscience that explains how the nerves, brain, and mind functionClear emphasis on how behavior can be examined through the electrical activity of both individual neurons and systems of nerve cellsCurrent focus on molecular biology as a tool for probing the pathogenesis of many neurological diseases, including muscular dystrophy, Huntington disease, and certain forms of Alzheimer’s diseaseMore than 900 engaging full-color illustrations―including line drawings, radiographs, micrographs, and medical photographs clarify often-complex neuroscience conceptsOutstanding section on the development and emergence of behavior, including important coverage of},
date={2012-10-26},
publisher={McGraw-Hill Education / Medical},
editor={Kandel, Eric R. and Schwartz, James H. and Jessell, Thomas M. and Siegelbaum, Steven A. and Hudspeth, A. J.},
title={Principles of Neural Science},
edition={5th edition},
pagetotal={1760},
groups={tesse:5},
}

@book{Qt5,
groups={tesse:2, Programming},
title={Qt5 Cadaques},
url={https://qmlbook.github.io/},
}

@book{2015SusskindQuantum,
abstract={From the bestselling author of The Theoretical Minimum, a DIY introduction to the math and science of quantum mechanics.First he taught you classical mechanics. Now, physicist Leonard Susskind has teamed up with data engineer Art Friedman to present the theory and associated mathematics of the strange world of quantum mechanics.In this follow-up to the New York Times best-selling The Theoretical Minimum, Susskind and Friedman provide a lively introduction to this famously difficult field, which attempts to understand the behavior of sub-atomic objects through mathematical abstractions. Unlike other popularizations that shy away from quantum mechanics' weirdness, Quantum Mechanics embraces the utter strangeness of quantum logic. The authors offer crystal-clear explanations of the principles of quantum states, uncertainty and time dependence, entanglement, and particle and wave states, among other topics, and each chapter includes exercises to ensure mastery of each area. Like The Theoretical Minimum, this volume runs parallel to Susskind's eponymous Stanford University-hosted continuing education course.An approachable yet rigorous introduction to a famously difficult topic, Quantum Mechanics provides a tool kit for amateur scientists to learn physics at their own pace.},
date={2015-05-12},
author={Susskind, Leonard and Friedman, Art},
pagetotal={384},
publisher={Basic Books},
groups={tesse:5, Quantum Mechanics},
isbn={978-0-465-06290-4},
shorttitle={Quantum Mechanics},
location={New York},
title={Quantum Mechanics: The Theoretical Minimum},
}

@book{2006RileyMathematical,
date={2006-03-13},
edition={3 edition},
isbn={978-0-521-67971-8},
abstract={The third edition of this highly acclaimed undergraduate textbook is suitable for teaching all the mathematics for an undergraduate course in any of the physical sciences. As well as lucid descriptions of all the topics and many worked examples, it contains over 800 exercises. New stand-alone chapters give a systematic account of the 'special functions' of physical science, cover an extended range of practical applications of complex variables, and give an introduction to quantum operators. Further tabulations, of relevance in statistics and numerical integration, have been added. In this edition, half of the exercises are provided with hints and answers and, in a separate manual available to both students and their teachers, complete worked solutions. The remaining exercises have no hints, answers or worked solutions and can be used for unaided homework; full solutions are available to instructors on a password-protected web site, www.cambridge.org/9780521679718.},
publisher={Cambridge University Press},
shorttitle={Mathematical Methods for Physics and Engineering},
pagetotal={1359},
location={Cambridge ; New York},
author={Riley, K. F. and Hobson, M. P. and Bence, S. J.},
title={Mathematical Methods for Physics and Engineering: A Comprehensive Guide},
}

@misc{WangIterated,
title={Iterated Prisoners Dilemma with Reinforcement Learning.pdf},
url={http://web.stanford.edu/class/psych209/Readings/2017ProjectExamples/wangkeven///////\\_17581///////\\_1628229///////\\_psych209///////\\_paper.pdf},
author={Wang, Keven},
groups={tesse:2, Game Theory},
}

@report{2018Clarivate2018,
groups={tesse:5},
url={https://clarivate.com/wp-content/uploads/2018/06/Crv///////\\_JCR///////\\_Full-Marketing-List///////\\_A4///////\\_2018///////\\_v4.pdf},
date={2018},
publisher={Clarivate Analytics},
author={Clarivate},
institution={Clarivate Analytics},
title={2018 Journal Citation Reports},
type={techreport},
}

@book{1996HayesStatistical,
edition={1 edition},
publisher={WILEY},
title={Statistical DSP},
location={New York},
groups={tesse:5, DSP},
isbn={978-0-471-59431-4},
author={Hayes, Monson H.},
abstract={The main thrust is to provide students with a solid understanding of a number of important and related advanced topics in digital signal processing such as Wiener filters, power spectrum estimation, signal modeling and adaptive filtering. Scores of worked examples illustrate fine points, compare techniques and algorithms and facilitate comprehension of fundamental concepts. Also features an abundance of interesting and challenging problems at the end of every chapter.},
pagetotal={628},
date={1996-04-11},
}

@article{SangamMapping,
title={Mapping and Visualization Softwares tools: a review},
groups={tesse:2},
author={Sangam, S. L. and Mogali, Miss Shivaranjini S.},
url={https://edisciplinas.usp.br/pluginfile.php/4131773/mod///////\\_folder/content/0/Mapping///////\\%20and///////\\%20Visualisation///////\\%20Software///////\\%20Tools///////\\%20A///////\\%20Review///////\\%20.pdf?forcedownload=1},
shorttitle={Mapping and Visualization Softwares tools},
}

@book{2007PressNumerical,
isbn={978-0-511-33555-6},
publisher={Cambridge University Press},
location={Cambridge, UK; New York},
author={Press, William H},
note={OCLC: 212427139},
title={Numerical recipes: the art of scientific computing},
date={2007},
shorttitle={Numerical recipes},
}

@article{1996SchneiderNurb,
author={Schneider, Philip},
title={NURB Curves: A Guide for the Uninitiated},
journaltitle={Develop},
url={http://digiitalarchfab.com/portal/wp-content/uploads/2011/03/Nurbs-Curve-A-Guide-for-the-Uninitiated.pdf},
groups={tesse:5},
date={1996},
}

@book{2014PirkleDesigning,
date={2014-11-26},
publisher={Routledge},
shorttitle={Designing Software Synthesizer Plug-Ins in C++},
title={Designing Software Synthesizer Plug-Ins in C++: For RackAFX, VST3, and Audio Units},
abstract={Bridging the gap from theory to programming, Designing Software Synthesizer Plug-Ins in C++ For RackAFX, VST3 and Audio Units contains complete code for designing and implementing software synthesizers for both Windows and Mac platforms. You will learn synthesizer operation, starting with the underlying theory of each synthesizer component, and moving on to the theory of how these components combine to form fully working musical instruments that function on a variety of target digital audio workstations (DAWs). Containing some of the latest advances in theory and algorithm development, this book contains information that has never been published in textbook form, including several unique algorithms of the author’s own design. The book is broken into three parts: plug-in programming, theory and design of the central synthesizer components of oscillators, envelope generators, and filters, and the design and implementation of six complete polyphonic software synthesizer musical instruments, which can be played in real time. The instruments implement advanced concepts including a user-programmable modulation matrix. The final chapter shows you the theory and code for a suite of delay effects to augment your synthesizers, introducing you to audio effect processing. The companion website, www.focalpress.com/cw/pirkle, gives you access to free software to guide you through the application of concepts discussed in the book, and code for both Windows and Mac platforms. In addition to the software, it features bonus projects, application notes, and video tutorials. A reader forum, monitored by the author, gives you the opportunity for questions and information exchange.},
author={Pirkle, Will},
location={Burlington, MA},
pagetotal={760},
isbn={978-1-138-78707-0},
edition={1 edition},
}

@article{1992EbdonIntroduction,
issn={09598103, 10970126},
langid={english},
journaltitle={Polymer International},
date={1992},
doi={10.1002/pi.4990270217},
title={Introduction to polymers (second edition) R. J. Young and P. A. Lovell Chapman and Hall, London, 1991. pp. 443, price £16.95. ISBN 0-412-30640-9 (PB); ISBN 0–412–30630–1 (HB)},
pages={207--208},
number={2},
urldate={2019-04-01},
author={Ebdon, J. R.},
volume={27},
}

@book{1991YoungIntroduction,
author={Young, Robert J. and Lovell, P. A.},
year={1991},
location={London ; New York},
title={Introduction to polymers},
isbn={978-0-412-30630-3},
publisher={Chapman //\\\& Hall},
pagetotal={443},
url={https://www.amazon.com/Introduction-Polymers-Robert-J-Young/dp/0412306301?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0412306301},
edition={2nd ed},
date={1991},
groups={tesse:5},
}

@book{2005TemamMathematical,
date={2005-06-20},
title={Mathematical Modeling in Continuum Mechanics},
author={Temam, Roger},
isbn={978-0-521-61723-9},
abstract={Temam and Miranville present core topics within the general themes of fluid and solid mechanics. The brisk style allows the text to cover a wide range of topics including viscous flow, magnetohydrodynamics, atmospheric flows, shock equations, turbulence, nonlinear solid mechanics, solitons, and the nonlinear Schrödinger equation. This second edition will be a unique resource for those studying continuum mechanics at the advanced undergraduate and beginning graduate level whether in engineering, mathematics, physics or the applied sciences. Exercises and hints for solutions have been added to the majority of chapters, and the final part on solid mechanics has been substantially expanded. These additions have now made it appropriate for use as a textbook, but it also remains an ideal reference book for students and anyone interested in continuum mechanics.},
groups={tesse:5},
publisher={Cambridge University Press},
pagetotal={356},
edition={2 edition},
location={Cambridge, UK ; New York},
}

@book{2009NollJoint,
author={Noll, Terrie},
edition={Edição: Spi},
title={The Joint Book: The Complete Guide to Wood Joinery},
date={2009-03-21},
groups={tesse:5},
abstract={This hardcover book with internal spiral binding is 6.5in x 8in, a perfect size for carpenters and woodworkers to keep near their workbench or toolbox for quick access.The design of this book allows it to lay open flat, which allows for easy and frequent reference, and the interior photographs, illustrations and diagrams, make the learning process simple and fun for beginners, and provides useful tips for more advanced readers.The Joint Book contains easy-to-follow step-by-step instructions for creating edge and scarf joints, lapped and housed joints, mortise and tenon joints, miters and bevels, dovetails, dowels and biscuits, and provides detailed descriptions of fasteners, hardware, and knockdown joints. This book is the perfect companion for any woodworker interested in improving their joint-making skills.},
shorttitle={The Joint Book},
isbn={978-0-7858-2227-1},
location={London},
publisher={Chartwell Books},
}

@book{2018BoydIntroduction,
title={Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares},
publisher={Cambridge University Press},
pagetotal={457},
abstract={This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra – vectors, matrices, and least squares – that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB, and data sets accompanying the book online at https://web.stanford.edu//textasciitilde boyd/vmls/. Suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
edition={1 edition},
author={Boyd, Stephen and Vandenberghe, Lieven},
shorttitle={Introduction to Applied Linear Algebra},
date={2018-06-30},
groups={tesse:5, Linear Algebra},
}

@book{1999McclellandPsychological,
series={Parallel Distributed Processing. Explorations in the Microstructure of Cognition},
number={Vol. 2},
location={Cambridge},
note={OCLC: 248384622},
author={McClelland, James L. and Rumelhart, David E. and McClelland, James L.},
isbn={978-0-262-63110-5},
date={1999},
title={Psychological and Biological Models},
pagetotal={611},
}

@book{1986RumelhartParallel,
groups={tesse:5},
title={Parallel distributed processing: explorations in the microstructure of cognition},
location={Cambridge, Mass},
editora={University of California, San Diego},
editoratype={collaborator},
shorttitle={Parallel distributed processing},
publisher={MIT Press},
pagetotal={2},
series={Computational models of cognition and perception},
date={1986},
isbn={978-0-262-18120-4 978-0-262-13218-3},
author={Rumelhart, David E. and McClelland, James L.},
}

@book{2015DiezOpenintro,
abstract={NO COLOR in the text or graphs in this "budget edition". For a FULL COLOR textbook option, see openintro.org/os/amazonhc, which redirects to the proper Amazon page. The OpenIntro project was founded in 2009 to improve the quality and availability of education by producing exceptional books and teaching tools that are free to use and easy to modify. Our inaugural effort is OpenIntro Statistics. Probability is optional, inference is key, and we feature real data whenever possible. Files for the entire book are freely available at openintro.org, and anybody can purchase a paperback copy from amazon.com for about 10. OpenIntro has grown through the involvement and enthusiasm of our community. Visit our website, openintro.org. We provide videos, labs for R and SAS, teaching resources like slides, and many other helpful resources.},
title={OpenIntro Statistics},
author={Diez, David M. and Barr, Christopher D. and Çetinkaya-Rundel, Mine},
shorttitle={OpenIntro Statistics},
edition={3 edition},
groups={tesse:5, Probability},
date={2015-07-02},
pagetotal={436},
isbn={978-1-943450-04-6},
publisher={OpenIntro, Inc.},
url={https://www.openintro.org/stat/textbook.php?stat///////\\_book=os},
}

@inproceedings{2013PassonneauAutomated,
url={https://pennstate.pure.elsevier.com/en/publications/automated-pyramid-scoring-of-summaries-using-distributional-seman},
date={2013-01-01},
author={Passonneau, Rebecca Jane and Chent, Emily and Guot, Weiwei and Perin, Dolores},
urldate={2019-04-01},
eventtitle={51st Annual Meeting of the Association for Computational Linguistics, ACL 2013},
title={Automated pyramid scoring of summaries using distributional semantics},
pages={143--147},
booktitle={Short Papers},
publisher={Association for Computational Linguistics (ACL)},
}

@book{noauthor_proceedings_2013,
date={2013},
url={https://aclweb.org/anthology/P13-2000},
title={Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013},
isbn={978-1-937284-51-0},
}

@misc{farnum_review_nodate,
urldate={2019-04-01},
url={https://www.academia.edu/8924647/Review///////\\_of///////\\_differentiation///////\\_and///////\\_integration///////\\_rules///////\\_from///////\\_Calculus///////\\_I///////\\_and///////\\_II///////\\_for///////\\_Ordinary///////\\_Differential///////\\_Equations///////\\_3301},
abstract={Review of differentiation and integration rules from Calculus I and II for Ordinary Differential Equations, 3301},
author={Farnum, Larissa},
langid={english},
title={Review of differentiation and integration rules from Calculus I and II for Ordinary Differential Equations},
}

@reference{2019LearningStatistical,
editor={Learning, Hawkes},
date={2019},
groups={tesse:5, Probability},
title={Statistical Tables},
url={http://www.hawkeslearning.com/documents/statdatasets/stat///////\\_tables.pdf},
}

@article{2011IlhanAdaptive,
pages={242--248},
number={1},
date={2011},
title={The adaptive knapsack problem with stochastic rewards},
volume={59},
journaltitle={Operations research},
author={İlhan, Taylan and Iravani, Seyed M. R. and Daskin, Mark S.},
groups={tesse:5},
url={http://users.iems.northwestern.edu//textasciitilde iravani/Stochastic///////\\_Knapsack.pdf},
}

@misc{PoczosIntroduction,
title={Introduction to Machine Learning (Lecture Notes)},
groups={tesse:2},
author={Poczos, Barnabas},
}

@article{KoernerVectors,
langid={english},
pages={458},
title={Vectors, Pure and Applied},
author={Körner, T. W.},
groups={tesse:2, Mathematics},
}

@book{2009StewartProbability,
author={Stewart, William J.},
date={2009},
note={OCLC: ocn255018592},
isbn={978-0-691-14062-9},
location={Princeton, N.J},
pagetotal={758},
keywords={Computer simulation, Markov processes, Probabilities, Queuing theory},
shorttitle={Probability, Markov chains, queues, and simulation},
title={Probability, Markov chains, queues, and simulation: the mathematical basis of performance modeling},
publisher={Princeton University Press},
langid={english},
}

@article{2014FowlerSequences,
langid={english},
pages={147},
groups={tesse:2, Mathematics},
author={Fowler, Jim and Snapp, Bart},
date={2014},
title={Sequences and Series},
}

@book{2016PinedoScheduling,
note={OCLC: 945375528},
groups={tesse:5},
title={Scheduling: theory, algorithms, and systems},
date={2016},
isbn={978-3-319-26578-0 978-3-319-26580-3},
edition={Fifth Edition},
pagetotal={670},
keywords={Production scheduling},
langid={english},
location={Cham Heidelberg New York Dordrecht London},
publisher={Springer},
author={Pinedo, Michael},
shorttitle={Scheduling},
}

@book{2004BruceMastering,
shorttitle={Mastering astral projection},
author={Bruce, Robert and Mercer, Brian},
langid={english},
isbn={978-0-7387-0467-8},
title={Mastering astral projection: 90-day guide to out-of-body experience},
pagetotal={483},
date={2004},
edition={1st ed},
location={St. Paul, Minn},
publisher={Llewellyn},
keywords={Astral projection},
}

@article{2015LermanPatent,
title={Patent Strategies of Technology Startups: An Empirical Study},
urldate={2019-04-01},
abstract={How does a patent strategy affect a tech startup company’s growth? This is a fundamental question for technology entrepreneurs, investors, lawyers and the innovation system as a whole. In this study, I shed light on this issue by conducting an empirical analysis of the patenting strategies of technology startups, examining the relationship between a company’s patent applications and different events over the company’s life: rounds of investment received, company acquisition and closure. I provide the first comprehensive cross-industry analysis of this question, by analyzing the patent portfolios of United States startups listed in CrunchBase, a crowd-sourced registry of tech companies used by the startup industry. By looking into these companies’ public patent applications from the United States Patent and Trademark Office (USPTO) database between 2008 and 2012, I examine the patenting patterns of startups as they progress through funding rounds.},
groups={tesse:5},
date={2015},
journaltitle={SSRN Electronic Journal},
langid={english},
issn={1556-5068},
doi={10.2139/ssrn.2610433},
shorttitle={Patent Strategies of Technology Startups},
url={http://www.ssrn.com/abstract=2610433},
author={Lerman, Celia},
}

@book{2018NiederstRobbinsLearning,
edition={Fifth Edition},
title={Learning Web Design: a beginner's guide to HTML, CSS, Javascript, and Web Graphics},
isbn={978-1-4919-6020-2},
location={Beijing ; Sebastopol, CA},
publisher={O'Reilly},
shorttitle={Learning Web Design},
date={2018},
pagetotal={790},
keywords={Design, Handbooks, manuals, etc, Authoring programs, Cascading style sheets, Computer graphics, HTML (Document markup language), JavaScript (Computer program language), Web sites},
author={Niederst Robbins, Jennifer},
langid={english},
}

@book{2012DegrootProbability,
publisher={Addison-Wesley},
location={Boston},
edition={4th ed},
note={OCLC: ocn502674206},
keywords={Probabilities, Mathematical statistics, Textbooks},
langid={english},
author={DeGroot, Morris H. and Schervish, Mark J.},
isbn={978-0-321-50046-5},
groups={tesse:5, Probability},
pagetotal={893},
title={Probability and statistics},
date={2012},
}

@book{2018BlennowMathematical,
isbn={978-1-138-05690-9},
langid={english},
groups={tesse:5},
author={Blennow, Mattias},
date={2018},
title={Mathematical Methods for Physics and Engineering},
}

@book{2018ShoresApplied,
location={Cham},
date={2018},
author={Shores, Thomas S.},
series={Undergraduate Texts in Mathematics},
urldate={2019-04-01},
isbn={978-3-319-74747-7},
publisher={Springer International Publishing},
title={Applied Linear Algebra and Matrix Analysis},
langid={english},
groups={tesse:5, Linear Algebra},
doi={10.1007/978-3-319-74748-4},
}

@book{2012GreeneEconometric,
edition={7th ed},
isbn={978-0-13-139538-1},
publisher={Prentice Hall},
langid={english},
keywords={Econometrics},
pagetotal={1188},
note={OCLC: ocn692292382},
author={Greene, William H.},
title={Econometric analysis},
location={Boston},
date={2012},
groups={tesse:5},
}

@book{2018ImanishiSynthesis,
shorttitle={Synthesis of Biocomposite Materials},
doi={10.1201/9781351077064},
editor={Imanishi, Yukio},
date={2018-01-10},
url={https://www.taylorfrancis.com/books/9781351085519},
author={Imanishi, Yukio},
title={Synthesis of Biocomposite Materials: Chemical and Biological Modifications of Natural Polymers},
urldate={2019-04-01},
langid={english},
publisher={CRC Press},
isbn={978-1-351-07706-4},
edition={1},
}

@book{2003Buss3d,
date={2003-05-19},
author={Buss, Samuel R.},
edition={1 edition},
isbn={978-0-521-82103-2},
pagetotal={396},
location={New York},
title={3D Computer Graphics: A Mathematical Introduction with OpenGL},
abstract={This introduction to 3D computer graphics emphasizes fundamentals and the mathematics underlying computer graphics, while also covering programming techniques using OpenGL, a platform-independent graphics programming environment. The minimal prerequisites make it suitable for self-study or for use as an advanced undergraduate or introductory graduate text as the author leads step-by-step from the basics of transformations to advanced topics such as animations and kinematics. Accompanying software, including source code for a ray tracing software package, is available freely from the book's web site.},
publisher={Cambridge University Press},
shorttitle={3D Computer Graphics},
}

@book{2016GeorgeMastering,
title={Mastering Django: Core},
shorttitle={Mastering Django},
date={2016-12-06},
isbn={978-1-78728-114-1},
groups={tesse:5},
author={George, Nigel},
publisher={Packt Publishing},
pagetotal={694},
}

@book{2014ChaconPro,
date={2014-11-09},
groups={tesse:5},
edition={2nd ed. edition},
pagetotal={456},
author={Chacon, Scott and Straub, Ben},
title={Pro Git},
isbn={978-1-4842-0077-3},
publisher={Apress},
abstract={Pro Git (Second Edition) is your fully-updated guide to Git and its usage in the modern world. Git has come a long way since it was first developed by Linus Torvalds for Linux kernel development. It has taken the open source world by storm since its inception in 2005, and this book teaches you how to use it like a pro. Effective and well-implemented version control is a necessity for successful web projects, whether large or small. With this book you’ll learn how to master the world of distributed version workflow, use the distributed features of Git to the full, and extend Git to meet your every need. Written by Git pros Scott Chacon and Ben Straub, Pro Git (Second Edition) builds on the hugely successful first edition, and is now fully updated for Git version 2.0, as well as including an indispensable chapter on GitHub. It’s the best book for all your Git needs.},
location={New York, NY},
}

@book{2013PavlinaAstral,
publisher={CreateSpace Independent Publishing Platform},
abstract={Learn to Master Astral Travel Would you like to walk through walls, fly around the world, reconnect with deceased loved ones, and explore time and space? Have you had a terrifying out of body encounter that left you fearful of ever exploring astral projection? Would you like to know how to travel the astral realms safely and protect yourself from unwanted projections? Whether you are a novice or an experienced astral traveler, the Astral Projection Guidebook will teach you how to master astral projection safely and effectively. In this guide, you’ll learn how to: • Prepare yourself for a positive astral journey • Achieve separation from your body • Explore the astral realms – alone or with friends • Enjoy mind-blowing astral sex • Protect yourself from dark entities • Train to become an astral warrior Get ready to have fun exploring the astral realms!},
edition={1 edition},
isbn={978-1-4912-4697-9},
pagetotal={156},
groups={tesse:5},
date={2013-10-18},
title={The Astral Projection Guidebook: Mastering the Art of Astral Travel},
author={Pavlina, Erin},
shorttitle={The Astral Projection Guidebook},
}

@book{2012BruceAstral,
title={Astral Dynamics: A NEW Approach to Out-of-Body Experience},
author={Bruce, Robert},
abstract={This is the fabulous FIRST EDITION of Astral DynamicsThe first edition of Astral Dynamics (the book you are now reading about) contains a great deal of content that was dropped from the 2009 second edition to save space. These are essentially two quite different books. Most people want both these amazing books, so we convinced Robert Bruce to produce an eBook version of the original First Edition of Astral Dynamics, with full color illustrations and other improvements. This First Edition went out of print in 2009. THE FIRST EDITION OF ASTRAL DYNAMICS - In one fascinating volume, Robert Bruce gathered together a personal narrative, a "how-to", a troubleshooting guide, and a theoretical perspective on the nonphysical structure underlying the strange and multidimensional life we all lead. Whether you are a skeptic, a veteran astral projector, a newbie, or an armchair traveler, there's treasure here. It's in a class by itself. What he says here rings true. In fact, it does a lot more than ring true. It opens doors. Astral Dynamics provides the intelligent and motivated reader with everything needed to put theory into practice. The book's six parts may be read each on their own, but they have been placed to build nicely one upon the other. Part One, "Elements of Projection", presents Robert's theory of what actually goes on when the projectable astral double leaves the physical body. This in itself, right out of the gate, is fascinating terrain. His theory of the consequences of the mind-split that results from projection is itself worth the price of the book. Part Two, "NEW Energy Ways", presents his stunningly practical method of raising energy and awareness by using touch, which he calls Tactile Imaging. I have tried this method on half a dozen people, selected more or less at random, each of whom obtained the desired awareness within seconds! And then, as if the new method of visualization weren't enough, Robert proceeds to describe the nature and anatomy of our energy bodies, a description firmly rooted in his own personal explorations. Part Three, "Core Skills", builds on this foundation, demonstrating how to succeed at the three tasks that are essential to success in astral projection: deep physical relaxation, taming the mind, and attaining the trance state. Part Four, "Projection Exit and Technique", tells you what you need to know to stop reading about astral projection and actually do it. Part Five, "The Akashic Connection", proceeds into the realm of the theoretical, not for the sake of getting lost in theory, but in order to make sense of things seen and heard. Particularly interesting is Robert's description of the nature and meaning of what he calls the Akashic Pulse. As to his description and analysis of the astral planes, the silver cord, the etheric body, and the Akashic record — I doubt that these have been equaled anywhere in the subject's extensive modern literature. I am confident that they have not been excelled. Part Six, "Strange Astral Phenomena", takes on a few conundrums that are worth exploring. Projection into higher realms; reality fluctuations; astral noise; what Robert calls astral wildlife, and astral combat... he covers the turf as it hasn't been covered to date. And he does it so casually, in so unpretentious a manner, his mood ranging from deepest awe to casually joking, with all the range between. If you have any interest at all in the subject of astral projection, lucid dreaming — higher consciousness in general — you're going to love this book. I predict that it will become a classic, read and valued for many years to come. Frank DeMarco - Chairman, Hampton Roads Publishing Company, Inc., VA.About the Author:Robert Bruce is the author of six groundbreaking books exploring such mysteries as the human energy body, the out-of-body experience, Kundalini, mind's-eye vision, spiritual and psychic development, metaphysics, clairvoyance, and psychi},
edition={Edição: 1.1},
groups={tesse:5},
date={2012},
shorttitle={Astral Dynamics},
pagetotal={517},
publisher={Magic Light Press},
}

@book{1999KostekSoft,
author={Kostek, Bozena},
series={Studies in Fuzziness and Soft Computing},
date={1999},
url={https://www.springer.com/br/book/9783790811902},
urldate={2019-04-04},
publisher={Physica-Verlag Heidelberg},
abstract={Applications of some selected soft computing methods to acoustics and sound engineering are presented in this book. The aim of this research study is the implementation of soft computing methods to musical signal analysis and to the recognition of musical sounds and phrases. Accordingly, some methods based on such learning algorithms as neural networks, rough sets and fuzzy-logic were conceived, implemented and tested. Additionally, the above-mentioned methods were applied to the analysis and verification of subjective testing results. The last problem discussed within the framework of this book was the problem of fuzzy control of the classical pipe organ instrument. The obtained results show that computational intelligence and soft computing may be used for solving some vital problems in both musical and architectural acoustics.},
groups={tesse:5},
langid={english},
isbn={978-3-7908-1190-2},
shorttitle={Soft Computing in Acoustics},
title={Soft Computing in Acoustics: Applications of Neural Networks, Fuzzy Logic and Rough Sets to Musical Acoustics},
}

@book{2017DangetiStatistics,
abstract={Key FeaturesLearn about the statistics behind powerful predictive models with p-value, ANOVA, and F- statistics.Implement statistical computations programmatically for supervised and unsupervised learning through K-means clustering.Master the statistical aspect of Machine Learning with the help of this example-rich guide to R and Python.Book DescriptionComplex statistics in Machine Learning worry a lot of developers. Knowing statistics helps you build strong Machine Learning models that are optimized for a given problem statement. This book will teach you all it takes to perform complex statistical computations required for Machine Learning. You will gain information on statistics behind supervised learning, unsupervised learning, reinforcement learning, and more. Understand the real-world examples that discuss the statistical side of Machine Learning and familiarize yourself with it. You will also design programs for performing tasks such as model, parameter fitting, regression, classification, density collection, and more.By the end of the book, you will have mastered the required statistics for Machine Learning and will be able to apply your new skills to any sort of industry problem.What you will learnUnderstand the Statistical and Machine Learning fundamentals necessary to build modelsUnderstand the major differences and parallels between the statistical way and the Machine Learning way to solve problemsLearn how to prepare data and feed models by using the appropriate Machine Learning algorithms from the more-than-adequate R and Python packagesAnalyze the results and tune the model appropriately to your own predictive goalsUnderstand the concepts of required statistics for Machine LearningIntroduce yourself to necessary fundamentals required for building supervised ///////\\& unsupervised deep learning modelsLearn reinforcement learning and its application in the field of artificial intelligence domainAbout the AuthorPratap Dangeti develops machine learning and deep learning solutions for structured, image, and text data at TCS, analytics and insights, innovation lab in Bangalore. He has acquired a lot of experience in both analytics and data science. He received his master's degree from IIT Bombay in its industrial engineering and operations research program. He is an artificial intelligence enthusiast. When not working, he likes to read about next-gen technologies and innovative methodologies.Table of ContentsJourney from Statistics to Machine LearningParallelism of Statistics and Machine LearningLogistic Regression vs. Random ForestTree-Based Machine Learning modelsK-Nearest Neighbors ///////\\& Naive BayesSupport Vector Machines ///////\\& Neural NetworksRecommendation EnginesUnsupervised LearningReinforcement Learning},
title={Statistics for Machine Learning: Techniques for exploring supervised, unsupervised, and reinforcement learning models with Python and R},
author={Dangeti, Pratap},
pagetotal={442},
shorttitle={Statistics for Machine Learning},
date={2017-07-21},
publisher={Packt Publishing},
isbn={978-1-78829-575-8},
}

@book{2011LoyMusimathics,
shorttitle={Musimathics},
edition={Reprint edition},
pagetotal={504},
location={Cambridge, Mass.},
title={Musimathics: The Mathematical Foundations of Music I},
author={Loy, Gareth},
isbn={978-0-262-51655-6},
publisher={The MIT Press},
date={2011-08-19},
groups={tesse:5},
volume={1},
abstract={A commonsense, self-contained introduction to the mathematics and physics of music; essential reading for musicians, music engineers, and anyone interested in the intersection of art and science.“Mathematics can be as effortless as humming a tune, if you know the tune,” writes Gareth Loy. In Musimathics, Loy teaches us the tune, providing a friendly and spirited tour of the mathematics of music―a commonsense, self-contained introduction for the nonspecialist reader. It is designed for musicians who find their art increasingly mediated by technology, and for anyone who is interested in the intersection of art and science.In Volume 1, Loy presents the materials of music (notes, intervals, and scales); the physical properties of music (frequency, amplitude, duration, and timbre); the perception of music and sound (how we hear); and music composition. Calling himself “a composer seduced into mathematics,” Loy provides answers to foundational questions about the mathematics of music accessibly yet rigorously. The examples given are all practical problems in music and audio.Additional material can be found at http://www.musimathics.com.},
}

@book{2011LoyMusimathicsa,
publisher={MIT Press},
date={2011},
volume={2},
author={Loy, Gareth and Chowning, John M.},
title={Musimathics: The Mathematical Foundations of Music II},
groups={tesse:5},
isbn={978-0-262-51656-3 978-0-262-12285-6},
shorttitle={Musimathics},
edition={1st MIT Press pbk. ed},
pagetotal={562},
note={OCLC: 935185987},
location={Cambridge, Mass.},
}

@book{2017BarrosFirst,
urldate={2019-04-04},
date={2017},
publisher={Springer Berlin Heidelberg},
author={de Barros, Laécio Carvalho and Bassanezi, Rodney Carlos and Lodwick, Weldon Alexander},
title={A First Course in Fuzzy Logic, Fuzzy Dynamical Systems, and Biomathematics},
isbn={978-3-662-53322-2},
langid={english},
volume={347},
groups={tesse:5},
location={Berlin, Heidelberg},
doi={10.1007/978-3-662-53324-6},
series={Studies in Fuzziness and Soft Computing},
}

@book{2016ArgueellesMendezPractical,
isbn={978-3-319-23185-3 978-3-319-23186-0},
groups={tesse:5},
series={Studies in Fuzziness and Soft Computing},
volume={327},
location={Cham},
doi={10.1007/978-3-319-23186-0},
author={Argüelles Mendez, Luis},
date={2016},
publisher={Springer International Publishing},
title={A Practical Introduction to Fuzzy Logic using LISP},
langid={english},
urldate={2019-04-04},
}

@collection{2015TamirFifty,
title={Fifty years of fuzzy logic and its applications},
isbn={978-3-319-19682-4 978-3-319-19683-1},
note={OCLC: 908374518},
langid={english},
groups={tesse:5},
editor={Tamir, Dan E. and Rishe, Naphtali D. and Kandel, Abraham},
number={326},
publisher={Springer},
series={Studies in fuzziness and soft computing},
date={2015},
pagetotal={684},
location={Cham},
}

@book{2008RossFuzzy,
publisher={Wiley},
abstract={Fuzzy logic refers to a large subject dealing with a set of methods to characterize and quantify uncertainty in engineering systems that arise from ambiguity, imprecision, fuzziness, and lack of knowledge. Fuzzy logic is a reasoning system based on a foundation of fuzzy set theory, itself an extension of classical set theory, where set membership can be partial as opposed to all or none, as in the binary features of classical logic. Fuzzy logic is a relatively new discipline in which major advances have been made over the last decade or so with regard to theory and applications. Following on from the successful first edition, this fully updated new edition is therefore very timely and much anticipated. Concentration on the topics of fuzzy logic combined with an abundance of worked examples, chapter problems and commercial case studies is designed to help motivate a mainstream engineering audience, and the book is further strengthened by the inclusion of an online solutions manual as well as dedicated software codes. Senior undergraduate and postgraduate students in most engineering disciplines, academics and practicing engineers, plus some working in economics, control theory, operational research etc, will all find this a valuable addition to their bookshelves.},
edition={2 edition},
date={2008-03-11},
author={Ross, Timothy J.},
pagetotal={650},
title={Fuzzy Logic with Engineering Applications},
groups={tesse:5},
}

@book{2002MoravcsikMusical,
isbn={978-0-306-46710-3},
note={OCLC: 248884612},
location={New York, NY},
title={Musical sound: an introduction to the physics of music},
pagetotal={316},
publisher={Kluwer Acad./Plenum},
date={2002},
shorttitle={Musical sound},
langid={english},
author={Moravcsik, Michael J. and Rosenbluth, Darrel},
}

@article{1983McintyreOscillations,
author={McIntyre, M. E. and Schumacher, R. T. and Woodhouse, J.},
pages={1325--1345},
journaltitle={The Journal of the Acoustical Society of America},
doi={10.1121/1.390157},
langid={english},
date={1983-11},
issn={0001-4966},
groups={tesse:5},
volume={74},
urldate={2019-04-04},
number={5},
title={On the oscillations of musical instruments},
}

@article{1998KarjalainenPlucked,
shorttitle={Plucked-String Models},
langid={english},
number={3},
url={https://www.jstor.org/stable/3681155?origin=crossref},
volume={22},
journaltitle={Computer Music Journal},
author={Karjalainen, Matti and Valimaki, Vesa and Tolonen, Tero},
doi={10.2307/3681155},
issn={0148-9267},
pages={17},
date={1998},
urldate={2019-04-04},
title={Plucked-String Models: From the Karplus-Strong Algorithm to Digital Waveguides and beyond},
}

@book{2011VeselicDamped,
groups={tesse:5, Acoustics},
title={Damped oscillations of linear systems: a mathematical introduction},
isbn={978-3-642-21334-2},
location={Berlin ; London ; New York},
number={2023},
author={Veselić, Krešimir},
pagetotal={209},
publisher={Springer},
shorttitle={Damped oscillations of linear systems},
date={2011},
series={Lecture notes in mathematics},
note={OCLC: ocn747814290},
abstract={"The theory of linear damped oscillations was originally developed more than hundred years ago and is still of vital research interest to engineers, mathematicians and physicists alike. This theory plays a central role in explaining the stability of mechanical structures in civil engineering, but it also has applications in other fields such as electrical network systems and quantum mechanics. This volume gives an introduction to linear finite dimensional damped systems as they are viewed by an applied mathematician. After a short overview of the physical principles leading to the linear systems model, a largely self-contained mathematical theory for this model is presented. this includes the geometry of the underlying indefinite eigenvalue problem. Particular attention is paid to the sensitivity issues which influence numerical computations. finally, several recent research developments are included, e.g. Lyapunov stability and the perturbation of the time evolution."--P. [4] of cover},
keywords={Textbooks, Harmonic oscillators, Linear systems},
langid={english},
}

@article{Waves,
pages={394},
groups={tesse:2, Acoustics},
langid={english},
title={Waves and Oscillations, Second Edition},
}

@book{2009CorduneanuAlmost,
location={New York},
keywords={Almost periodic functions, Differential equations, Oscillation theory, Oscillations},
date={2009},
note={OCLC: ocn248978163},
pagetotal={308},
groups={tesse:5},
isbn={978-0-387-09818-0},
author={Corduneanu, C.},
title={Almost periodic oscillations and waves},
publisher={Springer},
langid={english},
}

@book{2001AnfilovPhysics,
abstract={From the primitive reed pipe to modern music "written" by computers is quite a journey. Here, in informal text and about a score of plates, is a story that takes the teenage layman on this interesting trip. The younger reader, like a good musicologist, follows the steps in the evolution of the most important instruments that make up today's symphony orchestra, and the development of music itself (scales, modes, keys, and temperaments). Physics and music is also a source, although, of necessity a modest one, of information about the music research that has been underway in the Soviet Union, especially in the scientific manufacture of the violin, and in electrophonic and synthetic music. This why the foreign reader might think of a degree of "bias" on the part of the author. Yet, it gives him an insight into what is going on in a country that has given the world quite a number of great composers.},
publisher={University Press of the Pacific},
groups={tesse:5},
pagetotal={272},
title={Physics and Music},
translator={Kuznetsov, Boris},
author={Anfilov, Gleb},
location={Honolulu, Hawaii},
date={2001-07-01},
isbn={978-0-89875-419-3},
}

@book{2010FletcherPhysics,
location={New York, NY},
title={The physics of musical instruments},
groups={tesse:5},
edition={2. ed., [Nachdr.]},
author={Fletcher, Neville H. and Rossing, Thomas D.},
isbn={978-1-4419-3120-7 978-0-387-21603-4},
note={OCLC: 780101082},
date={2010},
pagetotal={756},
publisher={Springer},
langid={english},
}

@thesis{2004DavidssonStructure,
langid={english},
shorttitle={Structure-acoustic analysis},
type={phdthesis},
institution={Univ.},
date={2004},
title={Structure-acoustic analysis: finite element modelling and reduction methods},
location={Lund},
note={OCLC: 186476406},
author={Davidsson, Peter},
}

@article{1996HainesDetermination,
langid={english},
volume={30},
title={Determination of Young's modulus for spruce, fir and isotropic materials},
author={Haines, DanielW. and Leban, Jean-Michel and Herb, Christian},
number={4},
date={1996-08},
doi={10.1007/BF00229348},
urldate={2019-04-05},
abstract={Dynamic methods provide rapid and accurate means to determine Young's modulus, i.e. the modulus of elasticity, of wood. For dry, clear specimens of picba commun (Norway spruce, picea excelsa) and sapin pictin (silver fir, abies amabilis) we present a comparison of results from tests by a resonance flexure method with results obtained from four-point static flexure tests. For a wide range of specimen size the resonance flexure method provides a simpler, more rapidly performed alternative to the classical static flexure method, giving Young's modulus values which are for the spruce and fir specimens of this study, nearly identical to those calculated from the static flexure tests. Results are also presented which show that a resonance longitudinal method yields higher values of Young's modulus and an ultrasonic method yields still higher values. We provide also a comparison of the four test methods applied to isotropic materials.},
journaltitle={Wood Science and Technology},
issn={0043-7719, 1432-5225},
subtitle={by the resonance flexure method with comparisons to static flexure and other dynamic methods},
titleaddon={by the resonance flexure method with comparisons to static flexure and other dynamic methods},
groups={tesse:5},
}

@article{1997MarmarasErgonomic,
urldate={2019-04-05},
author={Marmaras, Nikos and Zarboutis, Nikos},
pages={59--67},
doi={10.1016/s0003-6870(96)00032-4},
groups={tesse:5},
publisher={Elsevier BV},
langid={english},
number={1},
title={Ergonomic redesign of the electric guitar},
date={1997-02},
volume={28},
url={http://linkinghub.elsevier.com/retrieve/pii/S0003687096000324},
journaltitle={Applied Ergonomics},
month={February},
abstract={The present study deals with the redesign of the electric guitar, considering ergonomic criteria. Diffkulties met by novice musicians, neuro-muscular fatigue caused by guitar playing and occupational diseases occurring to professional guitarists, justify this study. Characteristics of existing electric guitar models which add to musician’s fatigue or difficulty, were identified. A number of ergonomic requirements were then derived. The redesign process tried to satisfy these requirements, considering at the same time musical requirements and technical constraints. A comparative evaluation of the designed ergonomic guitar with three existing electric guitar models, showed1 that although the designed electric guitar preserves the main features of the instrument, it achieves better user fit. Copyright @ 1996 Elsevier Science Ltd.},
journal={Applied Ergonomics},
year={1997},
}

@article{2012TorresExperimental,
journaltitle={Rev. Mex. Fis. E},
title={Experimental and simulated exploration of structural deﬂections and acoustic waves of guitar top plates},
date={2012},
author={Torres, J Alejandro and Villarreal, J Luis and Ramırez, R},
pages={6},
langid={english},
}

@article{PedgleyMaterials,
pages={20},
groups={tesse:2},
langid={english},
author={Pedgley, Owain and Norman, Eddie and Armstrong, Rob},
title={MATERIALS-INSPIRED INNOVATION FOR ACOUSTIC GUITAR DESIGN},
}

@article{2014PaivaModal,
author={Paiva, G.},
pages={7},
groups={tesse:2, Acoustics},
langid={english},
date={2014},
title={Modal Analysis of a Brazilian Guitar Body},
}

@article{2005BecacheNumerical,
volume={83},
abstract={The purpose of this study is to present a time-domain numerical modeling of the guitar. The model involves the transverse displacement of the string excited by a force pulse, the ﬂexural motion of the soundboard and the sound radiation in the air. We use a speciﬁc spectral method for solving the Kirchhoﬀ–LoveÕs dynamic plate model for orthotropic material, a ﬁctitious domain method for solving the ﬂuid–structure interaction and a conservative scheme for the time discretization. One of the originality of the proposed scheme is a stable coupling method between a continuous time resolution and a discrete one.},
groups={tesse:5},
journaltitle={Computers //\\\& Structures},
langid={english},
title={Numerical simulation of a guitar},
date={2005-01},
number={2},
url={https://linkinghub.elsevier.com/retrieve/pii/S0045794904002974},
urldate={2019-04-05},
author={Bécache, Eliane and Chaigne, Antoine and Derveaux, Gregoire and Joly, Patrick},
doi={10.1016/j.compstruc.2004.04.018},
issn={0045-7949},
pages={107--126},
}

@article{2007ShlychkovNumerical,
journaltitle={Mechanics of Composite Materials},
doi={10.1007/s11029-007-0026-y},
langid={english},
pages={269--276},
author={Shlychkov, S. V.},
issn={0191-5665, 1573-8922},
date={2007-05},
number={3},
urldate={2019-04-05},
volume={43},
title={Numerical-experimental investigation of resonance characteristics of a sounding board},
}

@article{2010HsiaoProduct,
url={https://linkinghub.elsevier.com/retrieve/pii/S0169814110000107},
volume={40},
issn={0169-8141},
langid={english},
number={3},
date={2010-05},
doi={10.1016/j.ergon.2010.01.009},
journaltitle={International Journal of Industrial Ergonomics},
title={Product-form design model based on genetic algorithms},
urldate={2019-04-05},
abstract={Industrial design attempts to enhance quality of life by designing products that meet consumer requirements. Combining concepts from various ﬁelds, including design, computer technology, aesthetics, and economics, industrial designers seek to improve quality of life by designing products that meet consumer needs. Industrial designers focus on customers' perceptions of products and their preferences for certain shapes, textures, colors, styles, linguistic variables, prices, and functions. Because new products are continuously being released, manufacturers must continually design products to satisfy customer needs to avoid displacement by market competitors. When planning strategies for marketing products to various users and consumers, managers must often consider multiple combinations of product shapes and must design products that cater to consumer tastes to minimize the risk of their products being rejected by the market. Companies with highly-skilled designers have more ideas, better and more competitive products, and shorter production times than companies with weak designers. This study analyzed product styles by applying genetic algorithms and Kansei Engineering Type II (AHP and Quantiﬁcation Theory Type I). This research transforms the psychological conceptions of consumers into linguistic variables. A MATLAB program was constructed to enable designers to simulate consumer logic. The cognitive dissonance between virtual and real models was minimized by using a 3D CAD model, and the virtual model of optimum solutions in this study employed a rapid prototyping machine to generate real models efﬁciently. Future genetic algorithm models applying different decision theories may achieve even faster and more accurate results.},
pages={237--246},
groups={tesse:5},
author={Hsiao, Shih-Wen and Chiu, Fu-Yuan and Lu, Shu-Hong},
}

@article{2014ShepherdEffects,
journaltitle={The Journal of the Acoustical Society of America},
abstract={A ﬁnite element model of a bare top plate with braces and a bridge plate was created using orthotropic material properties. The natural variation of the wood properties including dependence on moisture content was also determined. The simulated modes were then compared to experimentally obtained modes from top plate prototypes. Uncertainty analysis was also performed to determine the statistical bound of natural variability between wood samples. The natural frequencies of the model fall within the computed error bound. These results reinforce the importance of obtaining accurate material properties for acoustic guitar modeling.},
number={5},
pages={EL357--EL361},
title={The effects of wood variability on the free vibration of an acoustic guitar top plate},
doi={10.1121/1.4898740},
issn={0001-4966},
author={Shepherd, Micah R. and Hambric, Stephen A. and Wess, Dennis B.},
urldate={2019-04-05},
volume={136},
langid={english},
date={2014-11},
}

@inproceedings{2008ZoranReacoustic,
doi={10.1145/1400885.1400928},
abstract={We present a new approach for designing guitars making use of the digital environment. We aim to preserve the physical connection between a user and the instrument, while offering innovative sound design. The reAcoustic eGuitar is a digitally fabricated instrument to design sounds. A user shapes it by modifying six separate acoustic chambers. We contextualize the need for such an artifact within the evolution of the guitar.},
author={Zoran, Amit and Maes, Pattie},
eventtitle={ACM SIGGRAPH 2008 posters},
langid={english},
urldate={2019-04-05},
isbn={978-1-60558-466-9},
url={http://portal.acm.org/citation.cfm?doid=1400885.1400928},
title={The reAcoustic eGuitar},
date={2008},
booktitle={ACM SIGGRAPH 2008 posters on - SIGGRAPH '08},
pages={1},
groups={tesse:5},
location={Los Angeles, California},
publisher={ACM Press},
}

@article{2003DerveauxTime,
pages={3368--3383},
langid={english},
shorttitle={Time-domain simulation of a guitar},
author={Derveaux, Grégoire and Chaigne, Antoine and Joly, Patrick and Bécache, Eliane},
urldate={2019-04-05},
volume={114},
number={6},
issn={0001-4966},
date={2003-12},
journaltitle={The Journal of the Acoustical Society of America},
title={Time-domain simulation of a guitar: Model and method},
doi={10.1121/1.1629302},
}

@article{2012Gorrostieta-hurtadoVibration,
doi={10.5897/IJPS11.1603},
abstract={One of the main problems encountered in developing and building an acoustic guitar is to establish a formal methodology that allows us to observe the impact of some parameters involved in the process. This paper propose a modal analysis in the different stages of construction, and it is performed by -a finite element analysis of the soundboard of the instrument according to the relationship that that have the- frequencies and vibration modes, here, a set of certain parameters in the design are used to improve sound performance.},
journaltitle={International Journal of the Physical Sciences},
urldate={2019-04-05},
langid={english},
author={Gorrostieta-Hurtado, Efren},
title={Vibration analysis in the design and construction of an acoustic guitar},
issn={1992-1950},
number={13},
url={http://www.academicjournals.org/IJPS/abstracts/abstracts/abstract2012/23Mar/Gorrostieta-Hurtado///////\\%20et///////\\%20al.htm},
volume={7},
groups={tesse:5},
date={2012-03-23},
}

@article{2011ChouGestalt–minimalism,
pages={607--616},
title={A Gestalt–Minimalism-based decision-making model for evaluating product form design},
url={https://linkinghub.elsevier.com/retrieve/pii/S0169814111000886},
urldate={2019-04-05},
volume={41},
doi={10.1016/j.ergon.2011.07.006},
langid={english},
issn={0169-8141},
abstract={This paper presents a novel methodology for dealing with decision-making problems in product design ﬁelds. The purpose of this study is to evaluate product form design in terms of the perspectives of Gestalt psychology and Minimalist principles. Unlike traditional AHP methods, the proposed decision-making model uses distinct mathematical tools to establish priorities for the criteria and synthesize the evaluation results. A case study was conducted to illustrate the practicability of this proposed model. It has shown a credible result. In addition to product form design, this model can be applied to related design ﬁelds, such as plane design and other visual design.},
date={2011-11},
author={Chou, Jyh-Rong},
journaltitle={International Journal of Industrial Ergonomics},
number={6},
}

@article{2002HsiaoNeural,
author={Hsiao, Shih-Wen and Huang, H. C.},
date={2002-01},
number={1},
langid={english},
groups={tesse:5},
title={A neural network based approach for product form design},
pages={67--84},
urldate={2019-04-05},
issn={0142-694X},
journaltitle={Design Studies},
doi={10.1016/S0142-694X(01)00015-1},
volume={23},
abstract={A neural network based approach for product design is addressed in this article. Computer modeling, fuzzy set theory and semantic difference method are applied to set up an experiment. The experimental results are analyzed by applied back-propagation neural network, which establish the relationships between product–form parameters and adjective image words. A database for the connections among the design elements, product images and shape generation rules was constructed. A computer-aided system for product–form design was then developed based on this database. With the aid of this design system, a designer can generate 3D models of any product with different images by providing basic design elements and shape generation rules. Simultaneously, a rendered 3D model of the designed product and its images are also presented by this system. Therefore, changing the conﬁguration parameter(s) until the product shape is acceptable can modify the image of a product. In this manner, the designed product can ﬁt more closely to the consumers’ desire. Chair design is taken as a case study; but this method can be used to develop other products. kc 2001 Elsevier Science Ltd. All rights reserved.},
url={http://linkinghub.elsevier.com/retrieve/pii/S0142694X01000151},
}

@article{2012ZoranPlatform,
shorttitle={A platform for manipulation and examination of the acoustic guitar},
pages={338--347},
title={A platform for manipulation and examination of the acoustic guitar: The Chameleon Guitar},
volume={73},
doi={10.1016/j.apacoust.2011.10.004},
author={Zoran, Amit and Welch, Stephen and Hunt, William D.},
number={4},
url={https://linkinghub.elsevier.com/retrieve/pii/S0003682X11002738},
issn={0003-682X},
urldate={2019-04-05},
date={2012-04},
journaltitle={Applied Acoustics},
abstract={A platform for manipulation and examination the acoustic guitar is presented, based on a novel guitar design – the Chameleon Guitar – featuring a replaceable acoustic resonator functioning as the soundboard of the instrument. The goal of the design process is to create a tone as sonically close to that of a traditional guitar as possible, while maintaining an easily replaceable soundboard. An iterative, data driven approach was used, each design step coming under examination from one or more measurement tools: ﬁnite-element method, acoustic impulse testing, and laser vibrometry. Ideal resonator geometry, bridge location, and piezoelectric sensor positions were determined. The ﬁnished instrument was then examined with laser vibrometry to conﬁrm earlier results, evaluate the behavior and chosen sensor positions for various tonewoods, and examine the acoustic effects of adding sensors and wax ﬁnish. The conclusions drawn are diverse and point to the signiﬁcance of attention to detail in each step of instrument construction. For example, when changing instrument material from one softwood to another, ideal locations for piezoelectric sensors are subject to change. We conclude that detailed acoustic analysis can signiﬁcantly aid in the construction of new instruments by quantifying the impact of instrument geometry and material properties.},
langid={english},
}

@article{odonnell_study_2012,
pages={4},
author={O’Donnell, J and McRobbie, G},
date={2012},
abstract={This study will research a modern design of acoustic guitar by analysis of the vibrational modes. The guitar that will undergo testing has been provided by Emerald Guitars and is solely constructed using Carbon Fiber Reinforced Plastic (CFRP). With the use of COMSOL Multiphysics© the soundboard of the guitar will be simulated and analysis will be carried out to determine the first 10 eigenfrequencies and the modal shapes these create. This paper will detail the preliminary results obtained using the physical data collected through experimental testing in a previous study. The paper will demonstrate an application of the finite element method in the field of musical acoustics.},
langid={english},
title={A Study into the Acoustic and Vibrational Effects of Carbon Fiber Reinforced Plastic as a Sole Manufacturing Material for Acoustic Guitars},
}

@article{2015CaucaAcoustic,
url={http://aprendeenlinea.udea.edu.co/revistas/index.php/ingenieria/article/view/21021},
journaltitle={Revista Facultad de Ingeniería Universidad de Antioquia},
date={2015-12},
doi={10.17533/udea.redin.n76a04},
title={Acoustic and mechanic characterization of materials used in manufacturing the soundboard of the spanish guitar: influence in the sonority},
issn={0120-6230},
number={76},
author={del Cauca, Universidad and Idrobo-Ávila, Ennio Hugo and Vargas-Cañas, Rubiel and del Cauca, Universidad},
langid={english},
urldate={2019-04-05},
shorttitle={Acoustic and mechanic characterization of materials used in manufacturing the soundboard of the spanish guitar},
}

@misc{2013GualandriAnalysis,
date={2013},
url={https://courses.physics.illinois.edu/phys406/sp2017/Student///////\\_Projects/Spring13/Dan///////\\_Gualandri///////\\_P406///////\\_Final///////\\_Project///////\\_Report///////\\_Sp13.pdf},
pages={12},
author={Gualandri, Daniel},
groups={tesse:5},
title={Analysis of an Acoustic Guitar},
langid={english},
}

@article{2005HsiaoApplying,
title={Applying a hybrid approach based on fuzzy neural network and genetic algorithm to product form design},
pages={411--428},
number={5},
abstract={When generating new design concepts, most industrial designers tend to draw upon stereotypical images and their own personal design experiences. The evaluation of each individual design candidate in terms of its ability to meet the demands of the marketplace is a crucial step within the conceptual design stage. Consequently, this paper proposes a method which enables an automatic product form search or product image evaluation by means of fuzzy neural network and genetic algorithm. Initially, a feature-based hierarchical computer-aided design (CAD) model is constructed, in which the related form parameters are thoroughly deﬁned in applicable domains to facilitate the automatic generation of new product forms. A fuzzy neural network algorithm is then applied to establish the relationships between the input form parameters and a series of adjectival image words. In a reverse process, genetic algorithm is employed to search for a near-optimal design which satisﬁes the designer’s required product image by using the trained neural network as a ﬁtness function. The proposed method provides an automatic design system, which gives designers the ability to rapidly obtain a product form and its corresponding image, or to search for the ideal form which ﬁts a required image in a shorter lead-time. An electronic door lock design is chosen as the subject of the current investigation. However, the proposed method is equally applicable to the design of other products.},
url={https://linkinghub.elsevier.com/retrieve/pii/S0169814104002070},
langid={english},
issn={0169-8141},
author={Hsiao, Shih-Wen and Tsai, Hung-Cheng},
date={2005-05},
urldate={2019-04-05},
journaltitle={International Journal of Industrial Ergonomics},
volume={35},
doi={10.1016/j.ergon.2004.10.007},
}

@article{2008HsiaoApplying,
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S016981410800036X},
groups={tesse:5},
issn={0169-8141},
pages={910--920},
doi={10.1016/j.ergon.2008.02.009},
author={Hsiao, Shih-Wen and Chiu, Fu-Yuan and Chen, Chong Shian},
journaltitle={International Journal of Industrial Ergonomics},
date={2008-11},
number={11},
title={Applying aesthetics measurement to product design},
urldate={2019-04-05},
abstract={In the highly competitive market, varying product color to change its image is one of the best solutions to improve the product competitiveness. In this paper, the relationships among the product image, color area, and aesthetic measurement of the product are studied. The pixels of an area of color are used to obtain the proportionate relationship between different colored areas in a given solid visual angle. Based on the relationship among the Hue, Value, Chroma and colored area proposed by Munsell, the other factors are integrated to set up one formula for evaluating the aesthetic degree of color matching. The aesthetics measurement is considered to be inﬂuenced by the color environments, color areas, component colors and display angles of the product. The color planning for developing a cell phone was performed based on this model. The experimental results veriﬁed this model can be used for color planning in product design.},
volume={38},
}

@book{1999LavilleConstrucao,
author={Laville, Christian and Dionne, Jean},
date={1999},
location={Belo Horizonte; Porto Alegre (RS)},
publisher={Ed. da UFMG : ARTMED},
langid={portuguese},
title={A construção do saber: manual de metodologia da pesquisa em ciências humanas},
groups={tesse:5},
isbn={978-85-7307-489-5},
shorttitle={A construção do saber},
note={OCLC: 817592435},
}

@article{2006AssisMetodologia,
date={2006},
pages={48},
title={METODOLOGIA DO TRABALHO CIENTÍFICO},
langid={portuguese},
groups={tesse:2},
author={Assis, Maria Cristina De},
}

@book{2012AbreuArte,
langid={portuguese},
title={A Arte de Argumentar},
groups={tesse:5},
date={2012},
author={Abreu, Antônio Suárez},
isbn={978-8585851811},
}

@book{2009GilComo,
date={2009},
langid={portuguese},
location={São Paulo},
isbn={978-85-224-3169-4},
author={Gil, Antonio Carlos},
note={OCLC: 422878585},
groups={tesse:5},
publisher={Atlas},
title={Como elaborar projetos de pesquisa},
}

@article{ZanonO,
groups={tesse:2},
langid={portuguese},
author={Zanon, Fábio},
title={O violão no Brasil depois de Villa-Lobos},
pages={8},
}

@article{2002HeijinkComplexity,
abstract={The authors performed a behavioral study of the complexity of left-hand finger movements in classical guitar playing. Six professional guitarists played movement sequences in a fixed tempo. Left-hand finger movements were recorded in 3 dimensions, and the guitar sound was recorded synchronously. Assuming that performers prefer to avoid extreme joint angles when moving, the authors hypothesized 3 complexity factors. The results showed differential effects of the complexity factors on the performance measures and on participants’ judgments of complexity. The results demonstrated that keeping the joints in the middle of their range is an important principle in guitar playing, and players exploit the available tolerance in timing and placement of the left-hand fingers to control the acoustic output variability.},
date={2002-12},
title={On the Complexity of Classical Guitar Playing: Functional Adaptations to Task Constraints},
number={4},
journaltitle={Journal of Motor Behavior},
pages={339--351},
urldate={2019-04-05},
doi={10.1080/00222890209601952},
groups={tesse:5},
issn={0022-2895, 1940-1027},
langid={english},
author={Heijink, Hank and Meulenbroek, Ruud G. J.},
shorttitle={On the Complexity of Classical Guitar Playing},
volume={34},
}

@article{balleste_organizacao_2009,
author={Ballesté, Adriana Olinto},
abstract={On the musicology, in particular in the organology, terminological problems often occur. A typical example can be observed in the plunked musical instruments universe in Brazil. This paper demonstrates the importance of the application of knowledge organization methodologies in the musical domain, especially in the plunked musical instruments domain.},
title={ORGANIZAÇÃO DO CONHECIMENTO E SUA APLICAÇÃO AO UNIVERSO DE CORDAS DEDILHADAS NO RIO DE JANEIRO OITOCENTISTA},
pages={11},
date={2009},
langid={portuguese},
}

@article{2011Zoran3d,
journaltitle={Journal of New Music Research},
doi={10.1080/09298215.2011.621541},
date={2011-12},
author={Zoran, Amit},
number={4},
title={The 3D Printed Flute: Digital Fabrication and Design of Musical Instruments},
volume={40},
groups={tesse:5},
langid={english},
pages={379--387},
urldate={2019-04-05},
issn={0929-8215, 1744-5027},
shorttitle={The 3D Printed Flute},
abstract={This paper considers the controversy of modern acoustic instruments, which may have come to an evolutionary impasse, due to its high standardization that makes it diﬃcult to explore design modiﬁcations. A new approach for the design and fabrication of an acoustic instrument is presented, using digital fabrication technologies, and speciﬁcally 3D printing, which has the potential to inﬂuence new designs, and to lead to new acoustics and ergonomic innovations. This paper describes the key concepts of this approach, presenting the development process of such a 3D printed instrument—a prototype of a 3D printed concert ﬂute, some other 3D printed elements, and a conceptual example of an innovative trumpet—discussing the potential of the new technology in fabricating and designing of musical instruments.},
}

@article{2012RichardsonThree,
langid={english},
date={2012},
author={Richardson, Bernard and Johnson, Helen and Joslin, Alexandra and Perry, Ian},
groups={tesse:2, Acoustics},
title={The three-mass model for the classical guitar revisited},
pages={7},
}

@article{2013HeemskerkEye,
title={An eye-witness report on how the CD came about},
urldate={2019-04-05},
abstract={The economics of French instrumentmaking is mostly made up of very small handicraft enterprises. But craftsmen do not always have the means to embark, by themselves, on an innovation strategy. The idea is to try to respond to current challenges in instrument making; for example, the reduction in costs of design times or the adaptation to customer needs. All of this necessitates the development of low cost tools for characterising and prototyping of instruments dedicated to their use in workshops. Examples of collaborative approach between instrument makers and research laboratories are presented. Mainly, the PAFI project, for “Instrument-making Aid Platform” aims to develop characterisation tools for all the instrument families. The initiative’s originality lies in the fact that “pilot craftsmen” are associated with every stage of development. The process involves a research program and the support of craftsmen for developing the hardware and software. Bearing in mind the international economic context, these experiences may act as a basis for broadening and pursuing this initiative on an international scale. The open and progressive nature of the work means that we can consider such a prospect with a view to maintaining the smallscale production of high-quality instruments.},
pages={21--24},
volume={44},
groups={tesse:5},
doi={10.1051/epn/2013601},
date={2013-11},
author={Heemskerk, Jacques},
journaltitle={Europhysics News},
langid={english},
issn={0531-7479, 1432-1092},
number={6},
}

@article{2002ElejabarrietaCoupled,
pages={2283},
volume={111},
groups={tesse:5},
date={2002},
langid={english},
doi={10.1121/1.1470163},
issn={0001-4966},
author={Elejabarrieta, M. J. and Ezcurra, A. and Santamarı́a, C.},
urldate={2019-04-05},
journaltitle={The Journal of the Acoustical Society of America},
title={Coupled modes of the resonance box of the guitar},
number={5},
}

@article{probert_design_nodate,
author={Probert, Stephen M},
title={Design, Manufacture and Analysis of a Carbon Fiber Epoxy Composite Acoustic Guitar},
langid={english},
pages={193},
}

@article{GreenMechanical,
langid={english},
author={Green, David W. and Winandy, Jerrold E. and Kretschmann, David E.},
title={Mechanical Properties of Wood},
pages={46},
groups={tesse:2},
}

@article{2012ZillDifferential,
pages={673},
groups={tesse:2},
author={Zill, Dennis G.},
langid={english},
date={2012},
title={Differential Equations with Boundary-Value Problems},
}

@book{2005SerwayModern,
groups={tesse:5},
keywords={Textbooks, Physics},
langid={english},
author={Serway, Raymond A. and Moses, Clement J. and Moyer, Curt A.},
date={2005},
title={Modern physics},
edition={3rd ed},
pagetotal={1},
publisher={Thomson Brooks/Cole},
location={Belmont, CA},
isbn={978-0-534-49339-4 978-0-534-40624-0},
}

@book{2018ShabanaComputational,
isbn={978-1-119-29323-1},
pagetotal={1},
groups={tesse:5},
publisher={Wiley},
keywords={Continuum mechanics, Engineering mathematics},
edition={Third edition},
langid={english},
author={Shabana, Ahmed A.},
date={2018},
location={Hoboken, NJ, USA},
title={Computational continuum mechanics},
}

@book{1981GurtinIntroduction,
date={1981-12-12},
author={Gurtin, Morton E.},
title={An Introduction to Continuum Mechanics},
pagetotal={265},
edition={1 edition},
location={New York},
isbn={978-0-12-309750-7},
url={https://www.ebook.de/de/product/3659321/morton///////\\_e///////\\_gurtin///////\\_an///////\\_introduction///////\\_to///////\\_continuum///////\\_mechanics.html},
abstract={This book presents an introduction to the classical theories of continuum mechanics; in particular, to the theories of ideal, compressible, and viscous fluids, and to the linear and nonlinear theories of elasticity. These theories are important, not only because they are applicable to a majority of the problems in continuum mechanics arising in practice, but because they form a solid base upon which one can readily construct more complex theories of material behavior. Further, although attention is limited to the classical theories, the treatment is modern with a major emphasis on foundations and structure},
ean={9780123097507},
groups={tesse:5},
year={1981},
publisher={Academic Press},
}

@book{2016HastieElements,
isbn={978-0-387-84857-0},
location={New York, NY},
edition={2nd edition},
pagetotal={745},
abstract={This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression ///////\\& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
shorttitle={The Elements of Statistical Learning},
author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
title={The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
date={2016},
publisher={Springer},
}

@article{2016BasTraining,
volume={6},
abstract={In recent years, artiﬁcial neural networks have been commonly used for time series forecasting by researchers from various ﬁelds. There are some types of artiﬁcial neural networks and feed forward artiﬁcial neural networks model is one of them. Although feed forward artiﬁcial neural networks gives successful forecasting results they have a basic problem. This problem is architecture selection problem. In order to eliminate this problem, Yadav et al. (2007) proposed multiplicative neuron model artiﬁcial neural network. In this study, differential evolution algorithm is proposed for the training of multiplicative neuron model for forecasting. The proposed method is applied to two well-known different real world time series data.},
date={2016-01-01},
number={1},
issn={2083-2567},
journaltitle={Journal of Artificial Intelligence and Soft Computing Research},
pages={5--11},
title={The Training Of Multiplicative Neuron Model Based Artificial Neural Networks With Differential Evolution Algorithm For Forecasting},
url={http://content.sciendo.com/view/journals/jaiscr/6/1/article-p5.xml},
urldate={2019-04-05},
author={Bas, Eren},
langid={english},
doi={10.1515/jaiscr-2016-0001},
}

@article{2016TuMapping,
title={Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data},
journaltitle={arXiv:1603.05594 [cs, stat]},
urldate={2019-04-05},
abstract={This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The ﬁrst one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of trafﬁc in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.},
eprint={1603.05594},
eprinttype={arxiv},
url={http://arxiv.org/abs/1603.05594},
author={Tu, Enmei and Kasabov, Nikola and Yang, Jie},
date={2016-03-17},
langid={english},
}

@inproceedings{2015LiFacial,
publisher={IEEE},
urldate={2019-04-05},
isbn={978-1-4799-8633-0},
langid={english},
url={http://ieeexplore.ieee.org/document/7294547/},
groups={tesse:5},
eventtitle={2015 IEEE International Conference on Imaging Systems and Techniques (IST)},
doi={10.1109/IST.2015.7294547},
pages={1--6},
booktitle={2015 IEEE International Conference on Imaging Systems and Techniques (IST)},
title={Facial expression recognition using deep neural networks},
date={2015-09},
abstract={We develop a technique using deep neural network for human facial expression recognition. Images of human faces are preprocessed with photometric normalization and histogram manipulation to remove illumination variance. Facial features are then extracted by convolving each preprocessed image with 40 Gabor filters. Kernel PCA is applied to features before feeding them into the deep neural network that consists of 1 input layer, 2 hidden layers and a softmax classifier. The deep network is trained using greedy layer-wise strategy. We use the Extended CohnKanade Dataset for training and testing. Recognition tests are performed on six basic expressions (i.e. surprise, fear, disgust, anger, happiness, sadness). To test the robustness of the classification system further, and for benchmark comparison, we add a seventh emotion, namely “contempt”, for additional recognition tests. We construct confusion matrix to evaluate the performance of the deep network. It is demonstrated that the network generalizes to new images fairly successfully with an average recognition rate of 96.8 /\\% for six emotions and 91.7 /\\% for seven emotions. In comparison with shallower neural networks and SVM methods, the proposed deep network method can provide better recognition performance.},
author={Li, Junnan and Lam, Edmund Y.},
location={Macau, China},
}

@article{2015BalleDensity,
author={Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
title={Density Modeling of Images using a Generalized Normalization Transformation},
eprint={1511.06281},
urldate={2019-04-05},
date={2015-11-19},
abstract={We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectiﬁed and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a signiﬁcantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efﬁciently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
eprinttype={arxiv},
url={http://arxiv.org/abs/1511.06281},
groups={tesse:5},
journaltitle={arXiv:1511.06281 [cs]},
langid={english},
}

@article{2017HuangLearning,
journaltitle={IEEE Transactions on Neural Networks and Learning Systems},
volume={28},
pages={1082--1094},
groups={tesse:5},
title={Learning Kernel Extended Dictionary for Face Recognition},
doi={10.1109/TNNLS.2016.2522431},
url={http://ieeexplore.ieee.org/document/7407377/},
date={2017-05},
number={5},
abstract={A sparse representation classiﬁer (SRC) and a kernel discriminant analysis (KDA) are two successful methods for face recognition. An SRC is good at dealing with occlusion, while a KDA does well in suppressing intraclass variations. In this paper, we propose kernel extended dictionary (KED) for face recognition, which provides an efﬁcient way for combining KDA and SRC. We ﬁrst learn several kernel principal components of occlusion variations as an occlusion model, which can represent the possible occlusion variations efﬁciently. Then, the occlusion model is projected by KDA to get the KED, which can be computed via the same kernel trick as new testing samples. Finally, we use structured SRC for classiﬁcation, which is fast as only a small number of atoms are appended to the basic dictionary, and the feature dimension is low. We also extend KED to multikernel space to fuse different types of features at kernel level. Experiments are done on several large-scale data sets, demonstrating that not only does KED get impressive results for nonoccluded samples, but it also handles the occlusion well without overﬁtting, even with a single gallery sample per subject.},
author={Huang, Ke-Kun and Dai, Dao-Qing and Ren, Chuan-Xian and Lai, Zhao-Rong},
urldate={2019-04-05},
issn={2162-237X, 2162-2388},
langid={english},
}

@article{1999JiangImage,
title={Image compression with neural networks – A survey},
author={Jiang, J.},
journaltitle={Signal Processing: Image Communication},
date={1999-07},
doi={10.1016/S0923-5965(98)00041-1},
issn={0923-5965},
pages={737--760},
number={9},
url={http://linkinghub.elsevier.com/retrieve/pii/S0923596598000411},
volume={14},
urldate={2019-04-05},
abstract={Apart from the existing technology on image compression represented by series of JPEG, MPEG and H.26x standards, new technology such as neural networks and genetic algorithms are being developed to explore the future of image coding. Successful applications of neural networks to vector quantization have now become well established, and other aspects of neural network involvement in this area are stepping up to play signi"cant roles in assisting with those traditional technologies. This paper presents an extensive survey on the development of neural networks for image compression which covers three categories: direct image compression by neural networks; neural network implementation of existing techniques, and neural network based technology which provide improvement over traditional algorithms. 1999 Elsevier Science B.V. All rights reserved.},
langid={english},
groups={tesse:5},
}

@article{2015WangImage,
journaltitle={TELKOMNIKA (Telecommunication Computing Electronics and Control)},
number={1},
langid={english},
author={Wang, Bo and Gao, Yubin},
date={2015-03-01},
doi={10.12928/telkomnika.v13i1.1270},
issn={2302-9293, 1693-6930},
title={An Image Compression Scheme Based on Fuzzy Neural Network},
pages={137},
url={http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
urldate={2019-04-05},
abstract={Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, selfadaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
groups={tesse:5},
volume={13},
}

@article{2016TodericiFull,
url={http://arxiv.org/abs/1608.05148},
eprinttype={arxiv},
abstract={This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study “one-shot” versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3 /\\%–8.8 /\\% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the ﬁrst neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
journaltitle={arXiv:1608.05148 [cs]},
title={Full Resolution Image Compression with Recurrent Neural Networks},
groups={tesse:5},
author={Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
urldate={2019-04-05},
date={2016-08-17},
langid={english},
eprint={1608.05148},
}

@article{2015TodericiVariable,
urldate={2019-04-05},
url={http://arxiv.org/abs/1511.06085},
eprint={1511.06085},
journaltitle={arXiv:1511.06085 [cs]},
eprinttype={arxiv},
abstract={A large fraction of Internet trafﬁc is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will signiﬁcantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efﬁcient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32×32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10 /\\% or more.},
groups={tesse:5},
langid={english},
author={Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
title={Variable Rate Image Compression with Recurrent Neural Networks},
date={2015-11-19},
}

@article{2017SanturkarGenerative,
eprinttype={arxiv},
journaltitle={arXiv:1703.01467 [cs]},
title={Generative Compression},
langid={english},
date={2017-03-04},
abstract={Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length coding schemes.},
url={http://arxiv.org/abs/1703.01467},
urldate={2019-04-05},
eprint={1703.01467},
groups={tesse:5},
author={Santurkar, Shibani and Budden, David and Shavit, Nir},
}

@article{2017JohnstonImproved,
title={Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
urldate={2019-04-05},
eprint={1703.10114},
langid={english},
date={2017-03-29},
journaltitle={arXiv:1703.10114 [cs]},
author={Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
eprinttype={arxiv},
url={http://arxiv.org/abs/1703.10114},
abstract={We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network’s hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efﬁciently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
}

@article{2017TheisLossy,
eprinttype={arxiv},
url={http://arxiv.org/abs/1703.00395},
date={2017-03-01},
eprint={1703.00395},
urldate={2019-04-05},
journaltitle={arXiv:1703.00395 [cs, stat]},
langid={english},
groups={tesse:5},
author={Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
title={Lossy Image Compression with Compressive Autoencoders},
abstract={We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more ﬂexible than existing codecs. Autoencoders have the potential to address this need, but are difﬁcult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufﬁcient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efﬁcient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
}

@article{1994FritzkeGrowing,
date={1994-01},
author={Fritzke, Bernd},
journaltitle={Neural Networks},
doi={10.1016/0893-6080(94)90091-4},
number={9},
url={https://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
urldate={2019-04-05},
issn={0893-6080},
title={Growing cell structures—A self-organizing network for unsupervised and supervised learning},
volume={7},
langid={english},
pages={1441--1460},
}

@inproceedings{2012WuStorage,
author={Wu, Yue and Hu, Jianqing and Wu, Wei and Zhou, Yong and Du, K. L.},
pages={330--336},
abstract={The Hopfield model is a well-known dynamic associative-memory model. In this paper, we investigate various aspects of the Hopfield model for associative memory. We conduct a systematic simulation investigation of several storage algorithms for Hopfield networks, and conclude that the perceptron learning based storage algorithms can achieve much better storage capacity than the Hebbian learning based algorithms.},
location={Zhangjiajie, Hunan, China},
booktitle={2012 Fifth International Conference on Intelligent Computation Technology and Automation},
title={Storage Capacity of the Hopfield Network Associative Memory},
groups={tesse:5},
url={http://ieeexplore.ieee.org/document/6150208/},
isbn={978-1-4673-0470-2 978-0-7695-4637-7},
eventtitle={2012 Fifth International Conference on Intelligent Computation Technology and Automation (ICICTA)},
date={2012-01},
publisher={IEEE},
urldate={2019-04-05},
langid={english},
doi={10.1109/ICICTA.2012.89},
}

@article{2013CluneEvolutionary,
urldate={2019-04-05},
volume={280},
langid={english},
date={2013-01-30},
groups={tesse:5},
author={Clune, J. and Mouret, J.-B. and Lipson, H.},
doi={10.1098/rspb.2012.2863},
issn={0962-8452, 1471-2954},
number={1755},
journaltitle={Proceedings of the Royal Society B: Biological Sciences},
pages={20122863--20122863},
title={The evolutionary origins of modularity},
}

@article{2014KimConvolutional,
author={Kim, Yoon},
date={2014-08-25},
eprint={1408.5882},
eprinttype={arxiv},
journaltitle={arXiv:1408.5882 [cs]},
langid={english},
title={Convolutional Neural Networks for Sentence Classification},
url={http://arxiv.org/abs/1408.5882},
urldate={2019-04-05},
abstract={We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classiﬁcation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speciﬁc vectors through ﬁne-tuning offers further gains in performance. We additionally propose a simple modiﬁcation to the architecture to allow for the use of both task-speciﬁc and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation.},
groups={tesse:5},
}

@article{2015TangJoint,
langid={english},
title={A Joint Segmentation and Classification Framework for Sentence Level Sentiment Classification},
journaltitle={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
date={2015-11},
doi={10.1109/TASLP.2015.2449071},
volume={23},
number={11},
url={http://ieeexplore.ieee.org/document/7138591/},
author={Tang, Duyu and Qin, Bing and Wei, Furu and Dong, Li and Liu, Ting and Zhou, Ming},
urldate={2019-04-05},
abstract={In this paper, we propose a joint segmentation and classiﬁcation framework for sentence-level sentiment classiﬁcation. It is widely recognized that phrasal information is crucial for sentiment classiﬁcation. However, existing sentiment classiﬁcation algorithms typically split a sentence as a word sequence, which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains, such as “not bad,” “bad” and “a great deal of,” “great”. We address this issue by developing a joint framework for sentence-level sentiment classiﬁcation. It simultaneously generates useful segmentations and predicts sentence-level polarity based on the segmentation results. Speciﬁcally, we develop a candidate generation model to produce segmentation candidates of a sentence; a segmentation ranking model to score the usefulness of a segmentation candidate for sentiment classiﬁcation; and a classiﬁcation model for predicting the sentiment polarity of a segmentation. We train the joint framework directly from sentences annotated with only sentiment polarity, without using any syntactic or sentiment annotations in segmentation level. We conduct experiments for sentiment classiﬁcation on two benchmark datasets: a tweet dataset and a review dataset. Experimental results show that: 1) our method performs comparably with state-of-the-art methods on both datasets; 2) joint modeling segmentation and classiﬁcation outperforms pipelined baseline methods in various experimental settings.},
issn={2329-9290},
groups={tesse:5},
pages={1750--1761},
}

@article{2017ChenImproving,
author={Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
issn={0957-4174},
pages={221--230},
volume={72},
langid={english},
urldate={2019-04-05},
title={Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN},
date={2017-04},
url={https://linkinghub.elsevier.com/retrieve/pii/S0957417416305929},
journaltitle={Expert Systems with Applications},
doi={10.1016/j.eswa.2016.10.065},
abstract={Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classiﬁcation research focuses on one-technique-ﬁts-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which ﬁrst classiﬁes sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we ﬁnd that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to ﬁrst apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classiﬁcation. Our approach has been evaluated on four sentiment classiﬁcation datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classiﬁcation can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
}

@inproceedings{2016WangSam,
urldate={2019-04-05},
abstract={Convolutional neural networks play an increasingly important role in computer vision tasks, especially in the ﬁeld of visual object recognition. Many prominent models, such as Inception, Maxout, ResNet, and NIN, have been proposed to signiﬁcantly improve recognition performance. Inspired from those models, we propose a novel module called self-adaptive module (SAM). SAM consists of four passes and one selector. Speciﬁcally, the four passes include two direct passes with different receptive ﬁelds and depths, one residual pass, and one Maxout pass. Actually, the residual pass is used to speed up convergence, while we take advantage of the Maxout pass to enhance approximate capabilities of SAM. The selector is further designed to help choose reasonable output. Basically, SAM is intended to simplify design of any new deep learning architecture, since it no longer requires consideration of how to select receptive ﬁelds and depths. Our SAM is tested on the visual object recognition datasets including CIFAR-10, CIFAR100, MNIST, and SVHN. The experimental results demonstrate that the SAM-Net has superior recognition performances on the four benchmarks, which achieve test errors of 5.76///////\\%, 28.56///////\\%, 0.31///////\\%, and 1.98///////\\%, respectively.},
doi={10.1109/IJCNN.2016.7727308},
langid={english},
pages={1008--1014},
publisher={IEEE},
booktitle={2016 International Joint Conference on Neural Networks (IJCNN)},
author={Wang, Zhenyang and Deng, Zhidong and Wang, Shiyao},
title={SAM: A rethinking of prominent convolutional neural network architectures for visual object recognition},
date={2016-07},
isbn={978-1-5090-0620-5},
eventtitle={2016 International Joint Conference on Neural Networks (IJCNN)},
url={http://ieeexplore.ieee.org/document/7727308/},
location={Vancouver, BC, Canada},
shorttitle={SAM},
}

@article{2017AraqueEnhancing,
author={Araque, Oscar and Corcuera-Platas, Ignacio and Sánchez-Rada, J. Fernando and Iglesias, Carlos A.},
title={Enhancing deep learning sentiment analysis with ensemble techniques in social applications},
pages={236--246},
url={https://linkinghub.elsevier.com/retrieve/pii/S0957417417300751},
doi={10.1016/j.eswa.2017.02.002},
volume={77},
langid={english},
urldate={2019-04-05},
date={2017-07},
issn={0957-4174},
journaltitle={Expert Systems with Applications},
}

@article{1982HopfieldNeural,
issn={0027-8424, 1091-6490},
journaltitle={Proceedings of the National Academy of Sciences},
author={Hopfield, J. J.},
pages={2554--2558},
volume={79},
urldate={2019-04-05},
number={8},
doi={10.1073/pnas.79.8.2554},
abstract={Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcontent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices.},
langid={english},
title={Neural networks and physical systems with emergent collective computational abilities.},
date={1982-04-01},
}

@inproceedings{2004HuangExtreme,
abstract={It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradientbased learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for singlehidden layer feedforward neural networks (SLFNs) which randomly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on realworld benchmarking function approximation and classiﬁcation problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.},
author={Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
booktitle={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
url={http://ieeexplore.ieee.org/document/1380068/},
doi={10.1109/IJCNN.2004.1380068},
title={Extreme learning machine: a new learning scheme of feedforward neural networks},
eventtitle={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
pages={985--990},
volume={2},
isbn={978-0-7803-8359-3},
date={2004},
langid={english},
shorttitle={Extreme learning machine},
location={Budapest, Hungary},
urldate={2019-04-05},
publisher={IEEE},
}

@article{2014GravesNeural,
abstract={We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efﬁciently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
eprinttype={arxiv},
author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
eprint={1410.5401},
title={Neural Turing Machines},
url={http://arxiv.org/abs/1410.5401},
urldate={2019-04-05},
groups={tesse:5},
langid={english},
date={2014-10-20},
journaltitle={arXiv:1410.5401 [cs]},
}

@inproceedings{1992BoserTraining,
groups={tesse:5},
author={Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle={Proceedings of the fifth annual workshop on Computational learning theory},
date={1992},
title={A training algorithm for optimal margin classifiers},
publisher={ACM},
pages={144--152},
}

@report{1985RumelhartLearning,
title={Learning internal representations by error propagation},
author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
date={1985},
institution={California Univ San Diego La Jolla Inst for Cognitive Science},
type={resreport},
groups={tesse:5},
year={1985},
}

@article{2017DengDeep,
urldate={2019-04-05},
journaltitle={IEEE Transactions on Neural Networks and Learning Systems},
url={http://ieeexplore.ieee.org/document/7414528/},
title={On Deep Learning for Trust-Aware Recommendations in Social Networks},
number={5},
langid={english},
doi={10.1109/TNNLS.2016.2514368},
author={Deng, Shuiguang and Huang, Longtao and Xu, Guandong and Wu, Xindong and Wu, Zhaohui},
volume={28},
abstract={With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major beneﬁt of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user’s trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users’ interests and their trusted friends’ interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.},
date={2017-05},
issn={2162-237X, 2162-2388},
pages={1164--1177},
}

@misc{2012HintonLecture,
title={Lecture 6a Overview of mini-batch gradient descent. Coursera.},
author={Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
date={2012},
}

@article{2015BennasarFeature,
author={Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
title={Feature selection using Joint Mutual Information Maximisation},
urldate={2019-04-05},
journaltitle={Expert Systems with Applications},
pages={8520--8532},
volume={42},
doi={10.1016/j.eswa.2015.07.007},
issn={0957-4174},
abstract={Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efﬁciency, scalability in terms of the dataset dimensionality, and independence from the classiﬁer. Common drawbacks of this approach are the lack of information about the interaction between the features and the classiﬁer, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature signiﬁcance.},
langid={english},
date={2015-12},
groups={tesse:5},
url={https://linkinghub.elsevier.com/retrieve/pii/S0957417415004674},
number={22},
}

@article{2015LiaoImportance,
urldate={2019-04-05},
date={2015-08-03},
abstract={Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets (e.g., CIFAR-10, CIFAR100, MNIST, and SVHN). The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classiﬁcation of similar input examples. During the training process, these subnetworks avoid overﬁtting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a signiﬁcant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classiﬁcation results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
journaltitle={arXiv:1508.00330 [cs]},
langid={english},
eprint={1508.00330},
title={On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units},
url={http://arxiv.org/abs/1508.00330},
eprinttype={arxiv},
author={Liao, Zhibin and Carneiro, Gustavo},
}

@article{2015HuangWhat,
abstract={The emergent machine learning technique—extreme learning machines (ELMs)—has become a hot area of research over the past years, which is attributed to the growing research activities and signiﬁcant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that (1) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes/neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes/ neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc.). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; (2) ELM is proposed for both generalized single-hidden-layer feedforward network and multi-hidden-layer feedforward networks (including biological neural networks); (3) homogeneous architecture-based ELM is proposed for feature learning, clustering, regression and (binary/multi-class) classiﬁcation. (4) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi-hidden-layer feedforward networks either.},
issn={1866-9956, 1866-9964},
number={3},
shorttitle={What are Extreme Learning Machines?},
langid={english},
title={What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt’s Dream and John von Neumann’s Puzzle},
doi={10.1007/s12559-015-9333-0},
journaltitle={Cognitive Computation},
pages={263--278},
urldate={2019-04-05},
author={Huang, Guang-Bin},
volume={7},
date={2015-06},
}

@article{2016TangExtreme,
number={4},
title={Extreme Learning Machine for Multilayer Perceptron},
issn={2162-237X, 2162-2388},
author={Tang, Jiexiong and Deng, Chenwei and Huang, Guang-Bin},
pages={809--821},
journaltitle={IEEE Transactions on Neural Networks and Learning Systems},
url={http://ieeexplore.ieee.org/document/7103337/},
abstract={Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classiﬁcation and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before ﬁnal decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are ﬁxed without ﬁne-tuning. Therefore, it has much better learning efﬁciency than the DL. Extensive experiments on various widely used classiﬁcation data sets show that the proposed algorithm achieves better and faster convergence than the existing stateof-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further conﬁrm the generality and capability of the proposed learning scheme.},
doi={10.1109/TNNLS.2015.2424995},
date={2016-04},
urldate={2019-04-05},
volume={27},
langid={english},
groups={tesse:5},
}

@article{2017SongRobust,
journaltitle={IEEE Transactions on Neural Networks and Learning Systems},
volume={28},
doi={10.1109/TNNLS.2016.2518223},
pages={1068--1081},
groups={tesse:5},
author={Song, Qing and Zhao, Xu and Fan, Haijin and Wang, Danwei},
langid={english},
url={http://ieeexplore.ieee.org/document/7407373/},
urldate={2019-04-05},
title={Robust Recurrent Kernel Online Learning},
number={5},
issn={2162-237X, 2162-2388},
abstract={We propose a robust recurrent kernel online learning (RRKOL) algorithm based on the celebrated real-time recurrent learning approach that exploits the kernel trick in a recurrent online training manner. The novel RRKOL algorithm guarantees weight convergence with regularized risk management through the use of adaptive recurrent hyperparameters for superior generalization performance. Based on a new concept of the structure update error with a variable parameter length, we are the ﬁrst one to propose the detailed structure update error, such that the weight convergence and robust stability proof can be integrated with a kernel sparsiﬁcation scheme based on a solid theoretical ground. The RRKOL algorithm automatically weighs the regularized term in the recurrent loss function, such that we not only minimize the estimation error but also improve the generalization performance through sparsiﬁcation with simulation support.},
date={2017-05},
}

@article{2017OjhaMetaheuristic,
eprinttype={arxiv},
abstract={Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such diﬀerent viewpoints mainly to improve the FNN’s generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimize the FNNs. Its success is evident from the FNN’s application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.},
date={2017-04},
author={Ojha, Varun Kumar and Abraham, Ajith and Snášel, Václav},
volume={60},
issn={0952-1976},
langid={english},
journaltitle={Engineering Applications of Artificial Intelligence},
eprint={1705.05584},
doi={10.1016/j.engappai.2017.01.013},
pages={97--116},
urldate={2019-04-05},
title={Metaheuristic Design of Feedforward Neural Networks: A Review of Two Decades of Research},
url={http://arxiv.org/abs/1705.05584},
shorttitle={Metaheuristic Design of Feedforward Neural Networks},
}

@article{2005PalmesMutation,
doi={10.1109/TNN.2005.844858},
journaltitle={IEEE Transactions on Neural Networks},
langid={english},
groups={tesse:5},
pages={587--600},
number={3},
title={Mutation-Based Genetic Neural Network},
author={Palmes, P. P. and Hayasaka, T. and Usui, S.},
date={2005-05},
url={http://ieeexplore.ieee.org/document/1427764/},
urldate={2019-04-05},
issn={1045-9227},
volume={16},
abstract={Evolving gradient-learning artiﬁcial neural networks (ANNs) using an evolutionary algorithm (EA) is a popular approach to address the local optima and design problems of ANN. The typical approach is to combine the strength of backpropagation (BP) in weight learning and EA’s capability of searching the architecture space. However, the BP’s “gradient descent” approach requires a highly computer-intensive operation that relatively restricts the search coverage of EA by compelling it to use a small population size. To address this problem, we utilized mutation-based genetic neural network (MGNN) to replace BP by using the mutation strategy of local adaptation of evolutionary programming (EP) to effect weight learning. The MGNN’s mutation enables the network to dynamically evolve its structure and adapt its weights at the same time. Moreover, MGNN’s EP-based encoding scheme allows for a ﬂexible and less restricted formulation of the ﬁtness function and makes ﬁtness computation fast and efﬁcient. This makes it feasible to use larger population sizes and allows MGNN to have a relatively wide search coverage of the architecture space. MGNN implements a stopping criterion where overﬁtness occurrences are monitored through “sliding-windows” to avoid premature learning and overlearning. Statistical analysis of its performance to some well-known classiﬁcation problems demonstrate its good generalization capability. It also reveals that locally adapting or scheduling the strategy parameters embedded in each individual network may provide a proper balance between the local and global searching capabilities of MGNN.},
}

@article{2008FloreanoNeuroevolution,
volume={1},
abstract={Artiﬁcial neural networks (ANNs) are applied to many real-world problems, ranging from pattern classiﬁcation to robot control. In order to design a neural network for a particular task, the choice of an architecture (including the choice of a neuron model), and the choice of a learning algorithm have to be addressed. Evolutionary search methods can provide an automatic solution to these problems. New insights in both neuroscience and evolutionary biology have led to the development of increasingly powerful neuroevolution techniques over the last decade. This paper gives an overview of the most prominent methods for evolving ANNs with a special focus on recent advances in the synthesis of learning architectures.},
groups={tesse:5},
pages={47--62},
langid={english},
shorttitle={Neuroevolution},
date={2008-03},
doi={10.1007/s12065-007-0002-4},
urldate={2019-04-05},
author={Floreano, Dario and Dürr, Peter and Mattiussi, Claudio},
issn={1864-5909, 1864-5917},
number={1},
title={Neuroevolution: from architectures to learning},
journaltitle={Evolutionary Intelligence},
}

@article{2015DengExtreme,
author={Deng, ChenWei and Huang, GuangBin and Xu, Jia and Tang, JieXiong},
title={Extreme learning machines: new trends and applications},
issn={1674-733X, 1869-1919},
number={2},
volume={58},
pages={1--16},
langid={english},
abstract={Extreme learning machine (ELM), as a new learning framework, draws increasing attractions in the areas of large-scale computing, high-speed signal processing, artiﬁcial intelligence, and so on. ELM aims to break the barriers between the conventional artiﬁcial learning techniques and biological learning mechanism and represents a suite of machine learning techniques in which hidden neurons need not to be tuned. ELM theories and algorithms argue that “random hidden neurons” capture the essence of some brain learning mechanisms as well as the intuitive sense that the eﬃciency of brain learning need not rely on computing power of neurons. Thus, compared with traditional neural networks and support vector machine, ELM oﬀers signiﬁcant advantages such as fast learning speed, ease of implementation, and minimal human intervention. Due to its remarkable generalization performance and implementation eﬃciency, ELM has been applied in various applications. In this paper, we ﬁrst provide an overview of newly derived ELM theories and approaches. On the other hand, with the ongoing development of multilayer feature representation, some new trends on ELM-based hierarchical learning are discussed. Moreover, we also present several interesting ELM applications to showcase the practical advances on this subject.},
doi={10.1007/s11432-014-5269-3},
journaltitle={Science China Information Sciences},
urldate={2019-04-05},
shorttitle={Extreme learning machines},
groups={tesse:5},
date={2015-02},
}

@article{2015SatoApac,
title={APAC: Augmented PAttern Classification with Neural Networks},
groups={tesse:5},
abstract={Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classiﬁcation problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classiﬁers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classiﬁcation, which is a way of classiﬁcation using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classiﬁers. Even our multilayer perceptron model beats some of the convolutional models with recentlyinvented stochastic regularization techniques on the CIFAR10 dataset.},
shorttitle={APAC},
langid={english},
url={http://arxiv.org/abs/1505.03229},
date={2015-05-12},
eprinttype={arxiv},
journaltitle={arXiv:1505.03229 [cs]},
author={Sato, Ikuro and Nishimura, Hiroki and Yokoi, Kensuke},
eprint={1505.03229},
urldate={2019-04-05},
}

@article{2006LudermirOptimization,
groups={tesse:5},
title={An Optimization Methodology for Neural Network Weights and Architectures},
url={http://ieeexplore.ieee.org/document/4012033/},
langid={english},
journaltitle={IEEE Transactions on Neural Networks},
volume={17},
number={6},
doi={10.1109/TNN.2006.881047},
abstract={This paper introduces a methodology for neural network global optimization. The aim is the simultaneous optimization of multilayer perceptron (MLP) network weights and architectures, in order to generate topologies with few connections and high classiﬁcation performance for any data sets. The approach combines the advantages of simulated annealing, tabu search and the backpropagation training algorithm in order to generate an automatic process for producing networks with high classiﬁcation performance and low complexity. Experimental results obtained with four classiﬁcation problems and one prediction problem has shown to be better than those obtained by the most commonly used optimization techniques.},
urldate={2019-04-05},
issn={1045-9227, 1941-0093},
author={Ludermir, T. B. and Yamazaki, A. and Zanchettin, C.},
pages={1452--1459},
date={2006-11},
}

@article{2006TsaiTuning,
date={2006-01},
pages={69--80},
journaltitle={IEEE Transactions on Neural Networks},
issn={1045-9227},
abstract={In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.},
langid={english},
number={1},
url={http://ieeexplore.ieee.org/document/1593693/},
doi={10.1109/TNN.2005.860885},
urldate={2019-04-05},
volume={17},
title={Tuning the Structure and Parameters of a Neural Network by Using Hybrid Taguchi-Genetic Algorithm},
author={Tsai, J.-T. and Chou, J.-H. and Liu, T.-K.},
}

@article{2015SalamaLearning,
date={2015-12},
number={4},
author={Salama, Khalid M. and Abdelbar, Ashraf M.},
doi={10.1007/s11721-015-0112-z},
urldate={2019-04-05},
volume={9},
groups={tesse:5},
journaltitle={Swarm Intelligence},
title={Learning neural network structures with ant colony algorithms},
issn={1935-3812, 1935-3820},
abstract={Ant colony optimization (ACO) has been successfully applied to classiﬁcation, where the aim is to build a model that captures the relationships between the input attributes and the target class in a given domain’s dataset. The constructed classiﬁcation model can then be used to predict the unknown class of a new pattern. While artiﬁcial neural networks are one of the most widely used models for pattern classiﬁcation, their application is commonly restricted to fully connected three-layer topologies. In this paper, we present a new algorithm, ANN-Miner, which uses ACO to learn the structure of feed-forward neural networks. We report computational results on 40 benchmark datasets for several variations of the algorithm. Performance is compared to the standard three-layer structure trained with two different weight-learning algorithms (back propagation, and the ACOR algorithm), and also to a greedy algorithm for learning NN structures. A nonparametric Friedman test is used to determine statistical signiﬁcance. In addition, we compare our proposed algorithm with NEAT, a prominent evolutionary algorithm for evolving neural networks, as well as three different well-known state-of-the-art classiﬁers, namely the C4.5 decision tree induction algorithm, the Ripper classiﬁcation rule induction algorithm, and support vector machines.},
langid={english},
pages={229--265},
}

@article{2007BenardosOptimizing,
urldate={2019-04-05},
langid={english},
title={Optimizing feedforward artificial neural network architecture},
pages={365--382},
volume={20},
groups={tesse:5},
doi={10.1016/j.engappai.2006.06.005},
url={https://linkinghub.elsevier.com/retrieve/pii/S0952197606001072},
number={3},
date={2007-04},
author={Benardos, P. G. and Vosniakos, G.-C.},
journaltitle={Engineering Applications of Artificial Intelligence},
issn={0952-1976},
}

@article{2016CortesAdanet,
eprint={1607.01097},
date={2016-07-04},
url={http://arxiv.org/abs/1607.01097},
author={Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
urldate={2019-04-05},
abstract={We present new algorithms for adaptively learning artiﬁcial neural networks. Our algorithms (ADANET) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classiﬁcation tasks extracted from the CIFAR-10 dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.},
eprinttype={arxiv},
title={AdaNet: Adaptive Structural Learning of Artificial Neural Networks},
langid={english},
shorttitle={AdaNet},
journaltitle={arXiv:1607.01097 [cs]},
}

@article{RoebelNeural,
abstract={Time series prediction is one of the major applications of neural networks. After a short introduction into the basic theoretical foundations we argue that the iterated prediction of a dynamical system may be interpreted as a model of the system dynamics. By means of RBF neural networks we describe a modeling approach and extend it to be able to model instationary systems. As a practical test for the capabilities of the method we investigate the modeling of musical and speech signals and demonstrate that the model may be used for synthesis of musical and speech signals.},
title={Neural Network Modeling of Speech and Music Signals},
groups={tesse:2},
pages={7},
langid={english},
author={Röbel, Alex},
}

@article{2013RaczynskiDynamic,
doi={10.1109/TASL.2013.2258012},
author={Raczynski, Stanislaw A. and Vincent, Emmanuel and Sagayama, Shigeki},
volume={21},
abstract={Symbolic pitch modelling is a way of incorporating knowledge about relations between pitches into the process of analysing musical information or signals. In this paper, we propose a family of probabilistic symbolic polyphonic pitch models, which account for both the “horizontal” and the “vertical” pitch structure. These models are formulated as linear or loglinear interpolations of up to ﬁve sub-models, each of which is responsible for modelling a different type of relation.},
url={https://ieeexplore.ieee.org/document/6502207/},
issn={1558-7916, 1558-7924},
number={9},
journaltitle={IEEE Transactions on Audio, Speech, and Language Processing},
title={Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling},
groups={tesse:5},
date={2013-09},
pages={1830--1840},
urldate={2019-04-05},
langid={english},
}

@article{2016MehriSamplernn,
langid={english},
title={SampleRNN: An Unconditional End-to-End Neural Audio Generation Model},
shorttitle={SampleRNN},
journaltitle={arXiv:1612.07837 [cs]},
eprint={1612.07837},
eprinttype={arxiv},
urldate={2019-04-05},
author={Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
abstract={In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which proﬁts from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
url={http://arxiv.org/abs/1612.07837},
groups={tesse:5},
date={2016-12-22},
}

@article{2017WangTacotron,
url={http://arxiv.org/abs/1703.10135},
title={Tacotron: Towards End-to-End Speech Synthesis},
journaltitle={arXiv:1703.10135 [cs]},
date={2017-03-29},
eprinttype={arxiv},
author={Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
urldate={2019-04-05},
langid={english},
abstract={A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-tosequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods.},
groups={tesse:5},
shorttitle={Tacotron},
eprint={1703.10135},
}

@book{2010GrahameWind,
title={The wind in the willows},
note={OCLC: ocn619552325},
publisher={Oxford University Press},
pagetotal={170},
edition={New ed},
langid={english},
author={Grahame, Kenneth and Hunt, Peter},
date={2010},
location={Oxford ; New York},
isbn={978-0-19-956756-0},
groups={tesse:5},
keywords={Country life, England, Fiction, Friendship, Pastoral fiction, River life, Toad, of Toad Hall},
}

@collection{2012AtkinsonManagement,
editor={Atkinson, Anthony A.},
shorttitle={Management accounting},
location={Upper Saddle River, N.J},
pagetotal={526},
publisher={Pearson},
isbn={978-0-13-702497-1},
keywords={Managerial accounting},
note={OCLC: ocn639166510},
edition={6th ed},
date={2012},
title={Management accounting: information for decision-making and strategy execution},
langid={english},
}

@book{2005BrownStrategic,
langid={english},
location={Amsterdam},
year={2005},
isbn={978-0-7506-6319-9},
date={2005},
publisher={Elsevier, Butterworth-Heinemann},
groups={tesse:5},
author={Brown, Steve},
title={Strategic operations management},
note={OCLC: 249641155},
edition={2. ed},
editor={Brown, Steve},
pagetotal={420},
}

@article{2014LongFully,
groups={tesse:5},
author={Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
abstract={Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build “fully convolutional” networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20 /\\% relative improvement to 62.2 /\\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
langid={english},
urldate={2019-04-05},
eprinttype={arxiv},
title={Fully Convolutional Networks for Semantic Segmentation},
journaltitle={arXiv:1411.4038 [cs]},
eprint={1411.4038},
url={http://arxiv.org/abs/1411.4038},
date={2014-11-14},
}

@article{2015RadfordUnsupervised,
eprinttype={arxiv},
eprint={1511.06434},
url={http://arxiv.org/abs/1511.06434},
abstract={In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
author={Radford, Alec and Metz, Luke and Chintala, Soumith},
date={2015-11-19},
title={Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
urldate={2019-04-05},
journaltitle={arXiv:1511.06434 [cs]},
langid={english},
}

@article{2017ZhuUnpaired,
eprint={1703.10593},
journaltitle={arXiv:1703.10593 [cs]},
title={Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
eprinttype={arxiv},
abstract={Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
langid={english},
author={Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
urldate={2019-04-05},
url={http://arxiv.org/abs/1703.10593},
groups={tesse:5},
date={2017-03-30},
}

@article{2019EngelGansynth,
author={Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
shorttitle={GANSynth},
journaltitle={arXiv:1902.08710 [cs, eess, stat]},
title={GANSynth: Adversarial Neural Audio Synthesis},
urldate={2019-04-05},
url={http://arxiv.org/abs/1902.08710},
abstract={Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
date={2019-02-22},
langid={english},
eprint={1902.08710},
eprinttype={arxiv},
}

@book{2018SpizhevoiOpencv,
title={OpenCV 3 Computer Vision with Python Cookbook: Leverage the power of OpenCV 3 and Python to build computer vision applications.},
urldate={2019-04-05},
date={2018},
langid={english},
abstract={OpenCV 3 is a native cross-platform library for computer vision, machine learning, and image processing. OpenCV's convenient high-level APIs hide very powerful internals designed for computational efficiency that can take advantage of multicore and GPU processing. This book will help you tackle increasingly challenging computer vision problems ...},
url={http://public.eblib.com/choice/publicfullrecord.aspx?p=5332144},
author={Spizhevoi, Aleksei and Rybnikov, Aleksandr},
isbn={978-1-78847-875-5 978-1-78847-444-3},
shorttitle={OpenCV 3 Computer Vision with Python Cookbook},
publisher={Packt Publishing},
note={OCLC: 1030817073},
location={Birmingham},
}

@book{1997LockeConduct,
abstract={This letter shows the importance Locke attached to the Conduct, and its intended place within the body of the Essay. Unfortunately he never completed the work, which was finally published only in the Posthumous Works of 1706. The big topic which Locke was only just beginning to open up was that of the 'Ethics of Belief'. Every man, he tells us, should regulate his ascent by the evidence alone - a maxim easier to state than to understand, and easier to understand that to put into practice.},
location={Bristol},
date={1997-06-01},
pagetotal={160},
edition={Facsimile edition},
author={Locke, John},
title={Of the Conduct of the Understanding},
isbn={978-1-85506-225-2},
publisher={Thoemmes Pr},
}

@book{2010LuptonThinking,
author={Lupton, Ellen},
edition={2nd rev. and expanded ed},
note={OCLC: ocn528665832},
isbn={978-1-56898-969-3},
langid={english},
date={2010},
keywords={Graphic design (Typography), Type and type-founding},
title={Thinking with type: a critical guide for designers, writers, editors, ////////\\\& students},
publisher={Princeton Architectural Press},
shorttitle={Thinking with type},
location={New York},
pagetotal={224},
}

@book{2006DevoreModern,
author={Devore, Jay},
shorttitle={A Modern Introduction to Probability and Statistics},
title={A Modern Introduction to Probability and Statistics: Understanding Why and How},
date={2006-03},
urldate={2019-04-06},
langid={english},
doi={10.1198/jasa.2006.s72},
volume={101},
}

@book{2006LawsonHow,
isbn={978-0-7506-6077-8},
date={2006},
pagetotal={321},
publisher={Elsevier/Architectural Press},
note={OCLC: 254426229},
shorttitle={How designers think},
edition={4. ed},
title={How designers think: the design process demystified},
author={Lawson, Bryan},
langid={english},
location={Amsterdam},
}

@book{2015HortonAndroid,
date={2015},
isbn={978-1-78588-326-2},
publisher={Packt Publishing},
location={Birmingham},
author={Horton, John},
title={Android programming for beginners: learn all the Java and Android skills you need to start making powerful mobile applications},
note={OCLC: 1011380341},
pagetotal={660},
edition={First published},
langid={english},
shorttitle={Android programming for beginners},
}

@book{2004MccorduckMachines,
pagetotal={565},
title={Machines who think: a personal inquiry into the history and prospects of artificial intelligence},
edition={25th anniversary update},
isbn={978-1-56881-205-2},
author={McCorduck, Pamela},
shorttitle={Machines who think},
location={Natick, Mass},
publisher={A.K. Peters},
keywords={Artificial intelligence, History},
langid={english},
date={2004},
}

@book{2010RumseyStatistics,
groups={tesse:5, Probability},
keywords={Statistics},
langid={english},
series={For dummies},
pagetotal={178},
author={Rumsey, Deborah J.},
date={2010},
publisher={Wiley Pub., Inc},
title={Statistics essentials for dummies},
isbn={978-0-470-61839-4},
location={Indianapolis, IN},
}

@book{2018LoderWeb,
publisher={Apress},
doi={10.1007/978-1-4842-2610-0},
title={Web Applications with Elm: Functional Programming for the Web},
shorttitle={Web Applications with Elm},
urldate={2019-04-06},
location={Berkeley, CA},
date={2018},
isbn={978-1-4842-2609-4 978-1-4842-2610-0},
author={Loder, Wolfgang},
langid={english},
}

@book{noauthor_portugues_nodate,
langid={portuguese},
title={PORTUGUÊS Cesgranrio},
isbn={978-85-352-5911-7},
}

@book{2011DefantClassical,
date={2011},
author={Defant, Andreas},
title={Classical summation in commutative and noncommutative Lp-spaces},
note={OCLC: ocn746846973},
pagetotal={171},
publisher={Springer},
keywords={Lp spaces},
langid={english},
number={2021},
series={Lecture notes in mathematics},
isbn={978-3-642-20437-1 978-3-642-20438-8},
location={Heidelberg ; New York},
}

@book{2017GeronHands,
author={Géron, Aurélien},
location={Sebastopol, CA},
edition={First edition},
abstract={"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks--Scikit-Learn and TensorFlow--author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started" -- Back cover},
pagetotal={551},
date={2017},
note={OCLC: ocn953432302},
title={Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems},
isbn={978-1-4919-6229-9},
shorttitle={Hands-on machine learning with Scikit-Learn and TensorFlow},
publisher={O'Reilly Media},
}

@book{2015TallaridaPocket,
title={Pocket book of integrals and mathematical formulas},
abstract={1. Elementary algebra and geometry -- 2. Determinants, matrices, and linear systems of equations -- 3. Trigonometry -- 4. Analytic geometry -- 5. Series, number facts, and theory -- 6. Differential calculus -- 7. Integral calculus -- 8. Vector analysis -- 9. Special functions -- 10. Differential equations -- 11. Statistics -- 12. Financial mathematics.},
author={Tallarida, Ronald J.},
date={2015},
groups={tesse:5, Calculus},
isbn={978-1-4987-0476-2},
note={OCLC: 946827504},
}

@book{1995BaxterProduct,
pagetotal={324},
author={Baxter, Mike},
edition={1 edition},
isbn={978-0-7487-4197-7},
shorttitle={Product Design},
title={Product Design: Practical Methods for the Systematic Development of New Products},
publisher={Routledge},
abstract={The discovery of market needs and the manufacture of a product to meet those needs are integral parts of the same process. Since most textbooks on new product development are written from either a marketing or an engineering perspective, it is important for students to encounter these two aspects of product development together in a single text.Product Design: Practical Methods for the Systematic Development of New Products covers the entire new product development process, from market research through concept design, embodiment design, design for manufacture, and product launch. Systematic and practical in its approach, the text offers both a structured management framework for product development and an extensive range of specific design methods. Chapters feature "Design Toolkits" that provide detailed guidance on systematic design methods, present examples with familiar products, and conclude with reviews of key concepts.This major text aims to turn the often haphazard and unstructured product design process into a quality-controlled, streamlined, and manageable procedure. It is ideal for students of engineering, design, and technology on their path to designing new products.},
location={London},
date={1995-01-31},
}

@article{gandhi_central_nodate,
langid={portuguese},
pages={106},
author={Gandhi, Praça Mahatma},
title={Central de Concursos / Degrau Cultural},
}

@book{2011JamesStudents,
edition={3rd ed},
shorttitle={A student's guide to Fourier transforms},
location={Cambridge ; New York},
isbn={978-0-521-17683-5},
title={A student's guide to Fourier transforms: with applications in physics and engineering},
author={James, J. F.},
date={2011},
pagetotal={146},
publisher={Cambridge University Press},
}

@book{2011TzengMultiple,
author={Tzeng, Gwo-hshiung and Huang, Jih-Jeng},
date={2011},
isbn={978-1-4398-6157-8},
pagetotal={335},
series={A Chapman ////////\\\& Hall book},
title={Multiple attribute decision making: methods and appliations},
location={Boca Raton, Fla.},
note={OCLC: 746841344},
publisher={CRC Press},
shorttitle={Multiple attribute decision making},
}

@book{martins_lingua_nodate,
author={Martins, Andréa},
langid={portuguese},
title={LÍNGUA PORTUGUESA 1.000 Exercícios Questões simuladas e questões de concursos anteriores com os gabaritos},
publisher={Central de Concursos / Degrau Cultural},
}

@book{2017HeizerOperations,
author={Heizer, Jay and Render, Barry and Munson, Chuck},
date={2017},
title={Operations management: sustainability and supply chain management},
edition={12},
keywords={Production management},
langid={english},
pagetotal={806},
publisher={Pearson},
location={Boston},
isbn={978-0-13-413042-2},
shorttitle={Operations management},
groups={tesse:5},
}

@book{2015FrazierOperations,
author={Frazier, Gaither},
isbn={978-81-315-0048-4},
location={Australia},
abstract={Printed in Asia - Carries Same Contents as of US edition - Opt Expedited Shipping for 3 to 4 day delivery - - Includes CD-ROM - Printed in COLORS},
edition={9th edition},
groups={tesse:4},
publisher={Thomson Press},
title={Operations Management},
date={2015-11-12},
}

@book{2016SlackOperations,
author={Slack, Prof Nigel and Brandon-Jones, Prof Alistair and Johnston, Prof Robert},
pagetotal={752},
edition={8},
isbn={978-1-292-09867-8},
publisher={Pearson},
abstract={Were you looking for the book with access to MyOMLab? This product is the book alone and does NOT come with access to MyOMLab. Buy Operations Management, 8th edition with MyOMLab access card (ISBN 9781292098777) if you need access to MyOMLab as well, and save money on this resource. You will also need a course ID from your instructor to access MyOMLab. Operations management is important, exciting, challenging … and everywhere you look! Important, because it enables organizations to provide services and products that we all need Exciting, because it is central to constant changes in customer preference, networks of supply and demand, and developments in technology Challenging, because solutions must be must be financially sound, resource-efficient, as well as environmentally and socially responsible And everywhere, because in our daily lives, whether at work or at home, we all experience and manage processes and operations. New features: There are now more than 110 of the popular ‘Operations in Practice’ examples throughout the book, over 40 /\\% of which are new. The importance of sustainability and Corporate Social Responsibility (CSR) has been emphasized further, and included throughout the book. We have even further strengthened the emphasis on the idea that ‘operations management’ is relevant to every type of business and all functional areas of the organization. Many new ideas in operations management have been incorporated, including the ‘three level’ approach to performance, the relationship between innovation, creativity and design, crowdsourcing, ideas management, business ecosystems, triadic relationships, office layout, telecommuting and organisational ‘ambidexterity’. However, we have retained the emphasis on the foundations of the subject. Six of the 19 cases at the end of the chapter are new (but the old ones are still available on the web site), and provide an up-to-date selection of operations issues. Operations Management focuses on the sustainable and socially responsible imperatives of operations management, using over 120 cases and illustrations of real-life operations around the world, including Apple, Médecins Sans Frontières, Amazon, Ecover, Dyson, Disneyland Paris, Google, The North Face, and many more. This is 24-carat excellence'Par Åhlström, Torsten and Ragnar Söderberg Chair of Business Administration, Stockholm School of Economics 'Operations Management is engaging and accessible, but it never dumbs-down. The book is comprehensive, but not overwhelming. Students hold on to this one; it’s a ‘keeper’.'Michael Shulver, Birmingham Business School 'This continues to be the definitive operations Management text … written by the masters of the field!'Dr Ross Ritchie, Lecturer in Operations Management, Loughborough University 'An essential text packed full of up-to-date examples that really bring the subject to life'Claire Moxham, University of Liverpool Management School 'An excellent book for those studying operations management. This book provides great illustrations to seamlessly link theory with practice' Frank Wiengarten, ESADE Business School Operations Management by Nigel Slack and Alistair Brandon-Jones is quite simply the best text on operations management. Comprehensive, engaging and insightful, I cannot recommend this book highly enough'Professor Andy Neely, Head, Institute for Manufacturing, Cambridge University Carrie Queenan, University of South Carolina Peter Race, Henley Business School, University of Reading},
location={Harlow, England London New York},
groups={tesse:4},
title={Operations Management},
date={2016-06-24},
}

@book{2013OliveiraNocoes,
langid={portuguese},
isbn={978-85-64124-17-2},
date={2013},
title={Noções de contabilidade básica para cursos técnicos},
author={Oliveira, Michelle Silva De and Moreira, Sherley Cabral},
}

@book{2017WalpoleProbability,
date={2017},
isbn={978-1-292-16141-9},
langid={english},
shorttitle={Probability ///////\\& statistics for engineers ///////\\& scientists},
title={Probability ////////\\\& statistics for engineers ////////\\\& scientists: MyStatLab update},
author={Walpole, Ronald E and Myers, Raymond H and Myers, Sharon L and Ye, Keying},
url={http://www.myilibrary.com?id=947904},
note={OCLC: 1014366070},
urldate={2019-04-07},
}

@collection{2008GowersPrinceton,
location={Princeton},
keywords={Mathematics},
editor={Gowers, Timothy and Barrow-Green, June and Leader, Imre},
isbn={978-0-691-11880-2},
date={2008},
pagetotal={1034},
groups={tesse:5},
langid={english},
title={The Princeton companion to mathematics},
year={2008},
publisher={Princeton University Press},
}

@book{2012GitmanPrinciples,
date={2012},
location={Boston (Estados Unidos},
title={Principles of managerial finance},
groups={tesse:5},
author={Gitman, Lawrence J. and Zutter, Chad J.},
isbn={978-0-13-611946-3},
note={OCLC: 991834493},
langid={english},
publisher={Prentice Hall},
}

@book{2003BilaniukProblem,
title={A Problem Course in Mathematical Logic},
author={Bilaniuk, Stefan},
abstract={This is a text for a problem-oriented course on mathematical logic and computability.},
date={2003},
langid={english},
groups={tesse:5},
}

@book{2009FlajoletAnalytic,
publisher={Cambridge University Press},
groups={tesse:5},
keywords={Combinatieleer, Combinatorial analysis},
langid={english},
title={Analytic combinatorics},
location={Cambridge ; New York},
isbn={978-0-521-89806-5},
date={2009},
author={Flajolet, Philippe and Sedgewick, Robert},
note={OCLC: ocn244767782},
pagetotal={810},
}

@book{2016GastelHow,
keywords={Technical writing},
location={Santa Barbara, California},
groups={tesse:5},
title={How to write and publish a scientific paper},
isbn={978-1-4408-4262-7 978-1-4408-4280-1},
author={Gastel, Barbara and Day, Robert A.},
pagetotal={326},
edition={Eighth edition},
langid={english},
date={2016},
publisher={Greenwood, an imprint of ABC-CLIO, LLC},
}

@book{2009DeKlerkIllustrated,
isbn={978-0-7339-8661-1},
title={Illustrated maths dictionary},
date={2009},
note={OCLC: 937425073},
langid={english},
abstract={Presenting the Illustrated Maths Dictionary 4th Edition, the revised and improved edition of Australia's best-selling mathematics dictionary by Judith De Klerk. New features in this fourth edition: new, cleaner design; computer terms with mathematical connotations; conversion tables; over fifty new entries; a bonus electronic version of the dictionary on CD-ROM. Together with the best features of previous editions: definitions written in simple language that children can understand but without losing accuracy; clear, precise and concise explanations of difficult terms; definitions that are supported by hundreds of examples and illustrations; a Useful Information section providing symbols, formulae and other supporting mathematical terms. The Illustrated Maths Dictionary 4th edition is an essential resource for primary and lower secondary students, teachers, student teachers and parents.},
location={Port Melbourne},
author={De Klerk, Judith and Green, Sally},
groups={tesse:5},
publisher={Pearson},
}

@book{2012GarrisonManagerial,
note={OCLC: ocn674938871},
pagetotal={762},
keywords={Managerial accounting},
langid={english},
edition={14th ed},
isbn={978-0-07-811100-6},
date={2012},
location={New York},
groups={tesse:5},
title={Managerial accounting},
publisher={McGraw-Hill/Irwin},
author={Garrison, Ray H. and Noreen, Eric W. and Brewer, Peter C.},
}

@book{2009ClaphamConcise,
publisher={Oxford University Press},
series={Oxford paperback reference},
groups={tesse:5},
edition={4},
title={The concise Oxford dictionary of mathematics},
note={OCLC: ocn299242316},
pagetotal={510},
author={Clapham, Christopher and Nicholson, James},
date={2009},
location={Oxford ; New York},
isbn={978-0-19-923594-0},
}

@collection{2012WieringReinforcement,
volume={12},
series={Adaptation, learning, and optimization},
publisher={Springer},
shorttitle={Reinforcement learning},
isbn={978-3-642-27644-6 978-3-642-27645-3},
pagetotal={638},
date={2012},
location={Heidelberg ; New York},
title={Reinforcement learning: state-of-the-art},
note={OCLC: ocn768170254},
editor={Wiering, Marco and Otterlo, Martijn van},
}

@article{2017RisiNeuroevolution,
keywords={neural networks, Artificial intelligence, artificial neural network training, Biological neural networks, computer game, computer games, evolutionary algorithm, Evolutionary algorithms, evolutionary computation, Evolutionary computation, Games, Genetic algorithms, learning (artificial intelligence), NE, Network topology, neural nets, neuroevolution},
groups={tesse:5},
url={http://ieeexplore.ieee.org/document/7307180/},
langid={english},
author={Risi, Sebastian and Togelius, Julian},
urldate={2019-04-07},
shorttitle={Neuroevolution in Games},
date={2017-03},
journaltitle={IEEE Transactions on Computational Intelligence and AI in Games},
issn={1943-068X, 1943-0698},
pages={25--41},
abstract={This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artiﬁcial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyze the application of NE in games along ﬁve different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the ﬁtness is determined and what type of input the network receives. The paper also highlights important open research challenges in the ﬁeld.},
title={Neuroevolution in Games: State of the Art and Open Challenges},
number={1},
volume={9},
doi={10.1109/TCIAIG.2015.2494596},
}

@article{2017FransOutline,
url={http://arxiv.org/abs/1704.08834},
urldate={2019-04-07},
journaltitle={arXiv:1704.08834 [cs]},
date={2017-04-28},
eprinttype={arxiv},
author={Frans, Kevin},
title={Outline Colorization through Tandem Adversarial Networks},
abstract={When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-deﬁned color scheme.},
groups={tesse:5},
langid={english},
eprint={1704.08834},
}

@article{2016IsolaImage,
groups={tesse:5},
journaltitle={arXiv:1611.07004 [cs]},
title={Image-to-Image Translation with Conditional Adversarial Networks},
eprinttype={arxiv},
abstract={We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
langid={english},
urldate={2019-04-07},
author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
eprint={1611.07004},
date={2016-11-21},
url={http://arxiv.org/abs/1611.07004},
}

@article{sigtia_end--end_nodate,
title={An End-to-End Neural Network for Polyphonic Music Transcription},
author={Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
abstract={We present a neural network model for polyphonic music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony or the number or type of instruments. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We investigate various neural network architectures for the acoustic models and compare their performance to two popular state-of-the-art acoustic models. We also present an efﬁcient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications. We evaluate the model’s performance on the MAPS dataset and show that the proposed model outperforms state-ofthe-art transcription systems.},
pages={14},
langid={english},
}

@article{2013ChadeganiComparison,
journaltitle={Asian Social Science},
title={A Comparison between Two Main Academic Literature Collections: Web of Science and Scopus Databases},
issn={1911-2025, 1911-2017},
shorttitle={A Comparison between Two Main Academic Literature Collections},
langid={english},
number={5},
author={Chadegani, Arezoo Aghaei and Salehi, Hadi and Yunus, Melor Md and Farhadi, Hadi and Fooladi, Masood and Farhadi, Maryam and Ebrahim, Nader Ale},
doi={10.5539/ass.v9n5p18},
urldate={2019-04-07},
date={2013-04-27},
url={http://www.ccsenet.org/journal/index.php/ass/article/view/26960},
volume={9},
abstract={Nowadays, the world’s scientific community has been publishing an enormous number of papers in different scientific fields. In such environment, it is essential to know which databases are equally efficient and objective for literature searches. It seems that two most extensive databases are Web of Science and Scopus. Besides searching the literature, these two databases used to rank journals in terms of their productivity and the total citations received to indicate the journals impact, prestige or influence. This article attempts to provide a comprehensive comparison of these databases to answer frequent questions which researchers ask, such as: How Web of Science and Scopus are different? In which aspects these two databases are similar? Or, if the researchers are forced to choose one of them, which one should they prefer? For answering these questions, these two databases will be compared based on their qualitative and quantitative characteristics.},
}

@article{2016SinghChawlaUnsung,
number={7584},
urldate={2019-04-07},
doi={10.1038/529115a},
issn={0028-0836, 1476-4687},
author={Singh Chawla, Dalmeet},
date={2016-01-04},
volume={529},
title={The unsung heroes of scientific software},
groups={tesse:5},
langid={english},
pages={115--116},
journaltitle={Nature},
}

@article{1972GarfieldCitation,
groups={tesse:5},
date={1972},
journaltitle={Essays of an Information Scientist},
title={Citation Analysis as a Tool in Journal Evaluation},
author={Garfield, Eugene},
pages={24},
langid={english},
}

@article{2018KumarIs,
date={2018},
title={Is “Impact” the “Factor” that matters…? (Part I)},
urldate={2019-04-07},
pages={95},
langid={english},
author={Kumar, Ashish},
issn={0972-124X},
doi={10.4103/jisp.jisp///////\\_195///////\\_18},
journaltitle={Journal of Indian Society of Periodontology},
shorttitle={Is “Impact” the “Factor” that matters…?},
volume={22},
number={2},
url={http://www.jisponline.com/text.asp?2018/22/2/95/230828},
}

@article{2005JacsoAs,
pages={11},
author={Jacso, Peter},
langid={english},
number={9},
date={2005},
volume={89},
title={As we may search – Comparison of major features of the Web of Science, Scopus, and Google Scholar citation-based and citation-enhanced databases},
journaltitle={CURRENT SCIENCE},
}

@article{2006EditorsImpact,
issn={1549-1676},
title={The Impact Factor Game},
pages={e291},
date={2006-06-06},
urldate={2019-04-07},
author={Editors, The PLoS Medicine},
number={6},
journaltitle={PLoS Medicine},
doi={10.1371/journal.pmed.0030291},
volume={3},
groups={tesse:5},
langid={english},
}

@article{meho_impact_nodate,
title={Impact of Data Sources on Citation Counts and Rankings of LIS Faculty: Web of Science vs. Scopus and Google Scholar},
pages={48},
langid={english},
author={Meho, Lokman I and Yang, Kiduk},
abstract={The Institute for Scientific Information’s (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours.},
}

@article{2008FalagasComparison,
pages={338--342},
langid={english},
title={Comparison of PubMed, Scopus, Web of Science, and Google Scholar: strengths and weaknesses},
volume={22},
date={2008-02},
journaltitle={The FASEB Journal},
doi={10.1096/fj.07-9492LSF},
author={Falagas, Matthew E. and Pitsouni, Eleni I. and Malietzis, George A. and Pappas, Georgios},
issn={0892-6638, 1530-6860},
urldate={2019-04-07},
shorttitle={Comparison of PubMed, Scopus, Web of Science, and Google Scholar},
number={2},
}

@article{rossner_irreproducible_nodate,
pages={2},
author={Rossner, Mike and Epps, Heather Van and Hill, Emma},
langid={english},
title={Irreproducible results: a response to Thomson Scientiﬁc},
}

@article{2009BrumbackImpact,
doi={10.1177/0883073808331366},
date={2009-03},
title={Impact Factor Wars: Episode V—The Empire Strikes Back},
issn={0883-0738, 1708-8283},
urldate={2019-04-07},
langid={english},
number={3},
author={Brumback, Roger A.},
journaltitle={Journal of Child Neurology},
volume={24},
pages={260--262},
shorttitle={Impact Factor Wars},
}

@article{2007RossnerShow,
date={2007-12-17},
volume={179},
langid={english},
journaltitle={The Journal of Cell Biology},
pages={1091--1092},
number={6},
title={Show me the data},
groups={tesse:5},
author={Rossner, Mike and Van Epps, Heather and Hill, Emma},
issn={0021-9525, 1540-8140},
doi={10.1083/jcb.200711140},
urldate={2019-04-07},
}

@inproceedings{2009AlvesPre,
url={http://www.onepetro.org/mslib/servlet/onepetropreview?id=OTC-20177-MS///////\\&soc=OTC},
date={2009-05},
urldate={2019-04-07},
groups={tesse:5},
langid={english},
publisher={The Offshore Technology Conference},
title={Pre-Salt Santos Basin—Well Construction Learning Curve Acceleration},
doi={10.4043/OTC-20177-MS},
abstract={Drilling and completing either exploratory or development wells in Pre-Salt prospects present several challenges. The wells are located in very deep waters, beyond 2,000 m WD and they are also deep wells, with more than 5,000 m TVD. Pressure and temperature is normal, but contaminants such as H2S and CO2 represent an additional difficulty.},
eventtitle={Offshore Technology Conference},
author={Alves, Ivan and Aragao, Atila and Bastos, Braulio-Luis and Falcao, Jose and Fartes, E.},
booktitle={Proceedings of Offshore Technology Conference},
}

@article{2010VaccaroProspective,
author={Vaccaro, Guilherme Luís Roehe and Pohlmann, Christopher and Lima, André Cirne and dos Santos, Manoela Silveira and de Souza, Cristina Botti and Azevedo, Debora},
date={2010-05},
title={Prospective scenarios for the biodiesel chain of a Brazilian state},
urldate={2019-04-07},
volume={14},
abstract={This paper presents a study based on the Systems Thinking and Scenario Planning (STSP) method, focusing the biodiesel production chain of Rio Grande do Sul State. The aim of the study was to identify key elements to comprehend the systemic structure of interaction among the ties of this chain. The study was held by a team of specialists over ﬁve months, including 15 meetings. Discussions were based on quantitative and qualitative data and a systemic map was constructed and reﬁned. Based on this modeling, four different prospective scenarios were comparatively analyzed in order to propose strategic actions to promote the sustainability and competitiveness of the chain. The results were then presented to two different groups of external specialists in order to validate the conclusions drawn and the proposals. Both groups agreed with the ideas presented. The paper is constructed as follows: a brief introduction focusing on contextual elements of the biodiesel production in Brazil and in Rio Grande do Sul; some background material regarding agroindustrial production chains and an overview of the biodiesel production chain of interest; a description of the method used to perform the research; main results and discussion; and conclusions. With this paper the authors also hope to contribute to the discussion regarding competitiveness and sustainability of biofuel chains in Brazil.},
doi={10.1016/j.rser.2009.12.008},
issn={1364-0321},
number={4},
pages={1263--1272},
url={https://linkinghub.elsevier.com/retrieve/pii/S1364032109002883},
journaltitle={Renewable and Sustainable Energy Reviews},
langid={english},
}

@article{gouveia_tecnologia_nodate,
pages={6},
author={Gouveia, Flávia},
langid={portuguese},
title={Tecnologia nacional para extrair petróleo e gás do pré-sal},
}

@article{2013AmerReview,
abstract={This paper reviews the scenario planning literature looking for answers for the following questions: How do qualitative and quantitative scenario methods differ and what are the advantages and disadvantages? What methods exist for quantitative scenario planning? Particularly quantitative scenario methods often lead to a large number of so-called ‘‘raw’’ scenarios that need to be further reﬁned, discussed, and verbally described. How do scenario planners select raw scenarios for further exploration and how many should they choose? How is the problem of validation addressed in scenario studies? ß 2012 Elsevier Ltd. All rights reserved.},
url={https://linkinghub.elsevier.com/retrieve/pii/S0016328712001978},
journaltitle={Futures},
author={Amer, Muhammad and Daim, Tugrul U. and Jetter, Antonie},
doi={10.1016/j.futures.2012.10.003},
issn={0016-3287},
date={2013-02},
pages={23--40},
langid={english},
title={A review of scenario planning},
groups={tesse:5},
volume={46},
urldate={2019-04-07},
}

@inproceedings{2012PizarroChallenges,
publisher={Society of Petroleum Engineers},
location={Muscat, Oman},
author={Pizarro, Jorge Oscar De Santanna and Branco, Celso Cesar M.},
title={Challenges in Implementing an EOR Project in the Pre-Salt Province in Deep Offshore Brasil},
doi={10.2118/155665-MS},
urldate={2019-04-07},
abstract={Carbonate reservoirs contain expressive part of the world oil reserves. The exploitation of these resources, however, presents several challenges mainly associated with their complex pore geometry, large scale variation in permeability and sometimes unfavorable wettability. These challenges can become more significant when we move to a deep offshore environment. As offshore projects need to be planned well in advance, due to the lack of room in the platforms for future expansion, the pioneer application of EOR methods needs to be considered from the conceptual stage of the development.},
eventtitle={SPE EOR Conference at Oil and Gas West Asia},
booktitle={SPE EOR Conference at Oil and Gas West Asia},
langid={english},
date={2012},
}

@article{2012RiccominiPre,
pages={33},
issn={2316-9036, 0103-9989},
volume={0},
number={95},
date={2012-11-30},
title={Pré-sal: geologia e exploração},
abstract={Pre-salt is the world’s greatest oil-related finding of the past 50 years. Pre-salt oil lies in reserves located below large and thick salt layers found offshore Brazil, along the coast from the states of Espírito Santo to Santa Catarina, in deep and ultra-deep waters, and situated below 3-to-4-km-thick rock layers, under the seabed. The investigations carried out so far in some pre-salt areas have shown that giant and super-giant fields are likely to exist. Such fields could place Brazil among the leading oil producers as they are thought to hold up to 16 billion barrels of recoverable oil equivalent – boe (sum of oil and natural gas), and are likely to yield from 70 to 100 billion barrels of oil equivalent. This article deals with the geological conditions and the past scenario in which pre-salt reserves were formed, the characteristics of the oil system, estimated reserves, exploration and production.},
doi={10.11606/issn.2316-9036.v0i95p33-42},
url={http://www.revistas.usp.br/revusp/article/view/52236},
urldate={2019-04-07},
langid={portuguese},
journaltitle={Revista USP},
author={Riccomini, Claudio and Sant´Anna, Lucy Gomes and Tassinari, Colombo Celso Gaeta},
shorttitle={Pré-sal},
}

@inproceedings{2012MoczydlowerDevelopment,
abstract={The Santos Basin Pre-Salt Cluster (SBPSC), Offshore Southeast Brazil, is a unique scenario, posing great development challenges. The microbial carbonate reservoir is unusual regarding its origin and petrophysical properties; the fluids have a variable CO2 content; the few analogue reservoirs around the world do not compare in terms of volumes, water depth and distance to the coast; and there are also flow assurance issues.},
author={Moczydlower, Bruno and Salomao, Marcelo Curzio and Branco, Celso Cesar M. and Romeu, Regis Kruel and Homem, Tiago Da Rosa and De Freitas, Luiz Carlos and Lima, Helena Assaf T Souza},
publisher={Society of Petroleum Engineers},
location={Mexico City, Mexico},
date={2012},
langid={english},
doi={10.2118/152860-MS},
title={Development of the Brazilian Pre-Salt Fields - When To Pay for Information and When To Pay for Flexibility},
urldate={2019-04-07},
eventtitle={SPE Latin America and Caribbean Petroleum Engineering Conference},
booktitle={SPE Latin America and Caribbean Petroleum Engineering Conference},
}

@article{2013PardoProspective,
author={Pardo, Nicolás and Moya, José Antonio},
issn={0360-5442},
pages={113--128},
title={Prospective scenarios on energy efficiency and CO2 emissions in the European Iron ////////\\\& Steel industry},
doi={10.1016/j.energy.2013.03.015},
journaltitle={Energy},
url={https://linkinghub.elsevier.com/retrieve/pii/S0360544213001928},
urldate={2019-04-07},
abstract={The Iron ///////\\& Steel industry is one of the biggest industrial CO2 emitters in the European Union. The present work analyzes the potential for the improvement of the energy efﬁciency and CO2 emission reduction for this sector up to 2030. Three scenarios are analyzed: baseline scenario (BS) representing the current evolution of this sector and two alternative scenarios (AS1 and AS2) to study the sensitivity of fuel and resource prices and CO2 emission prices. In the integrated production route all the scenarios highlight the importance of breakthrough technologies, such as Top Gas Recycling, to obtain performance improvement in energy consumption and CO2 emissions. In the secondary production route conditions under BS scenario are enough for a suitable incorporation of the best technologies in this route making insensitive the possible incentives due to higher prices in fuel and resources or CO2 emissions.},
langid={english},
date={2013-06},
volume={54},
}

@inproceedings{2015CostaFragaBrazilian,
booktitle={Offshore Technology Conference},
location={Houston, Texas, USA},
author={da Costa Fraga, Carlos Tadeu and Capeleiro Pinto, Antonio Carlos and Branco, Celso Cesar Moreira and de Sant´Anna Pizarro, Jorge Oscar and da Silva Paulo, Cezar Augusto},
date={2015},
publisher={Offshore Technology Conference},
urldate={2019-04-07},
abstract={In the early 2000s the context of the pre-salt formations was conceptual geological models of possible oil bearing reservoirs underneath a thick salt layer, many technical challenges, uncertainties and risks. Past only eight years from first discovery (2006), there are nine production systems, FPSOs, in operation, reaching an average oil rate of more than 700 thousand barrels per day and a cumulative production greater than 400 million barrels of oil, through 34 production wells. To optimize recovery, the first desulfated sea water and gas injectors were started. But these impressive numbers cannot be taken from granted: although nature has revealed prolific reservoirs, much experience, talent, planning and perseverance were necessary.},
langid={english},
eventtitle={Offshore Technology Conference},
doi={10.4043/25710-MS},
shorttitle={Brazilian Pre-Salt},
title={Brazilian Pre-Salt: An Impressive Journey from Plans and Challenges to Concrete Results},
}

@inproceedings{2016CaoData,
publisher={Society of Petroleum Engineers},
urldate={2019-04-07},
abstract={Forecasting of production in unconventional prospects has gained a lot of attention in the recent years. The key challenges in unconventional reservoirs have been the requirement to put online a) a large number of wells in a short period of time, b) well productivity significantly driven by completion characteristics and that c) the physics of fluid flow in these prospects still remain uncertain. In this paper, machine learning algorithms are used to forecast production for existing and new wells in unconventional assets using inputs like geological maps, production history, pressure data and operational constraints. One of the most popular Machine Learning methods – Artificial Neural Network (ANN) is employed for this purpose. ANN can learn from large volume of data points without assuming a predetermined model and can adapt to newer data as and when it becomes available. The workflow involves using these data sets to train and optimize the ANN model which, subsequently, is used to predict the well production performance of both existing wells using their own history and new wells by using the history of nearby wells which were drilled in analogous geological locations. The proposed technique requires users to do less data conditioning and model building and focus more on analyzing what-if scenarios and determining the well performance.},
langid={english},
author={Cao, Q. and Banerjee, R. and Gupta, S. and Li, J. and Zhou, W. and Jeyachandra, B.},
title={Data Driven Production Forecasting Using Machine Learning},
doi={10.2118/180984-MS},
eventtitle={SPE Argentina Exploration and Production of Unconventional Resources Symposium},
booktitle={SPE Argentina Exploration and Production of Unconventional Resources Symposium},
date={2016},
location={Buenos Aires, Argentina},
}

@inproceedings{2016MoraesCruzLula,
title={Lula NE Pilot Project - An Ultra-Deep Success in the Brazilian Pre-Salt},
doi={10.4043/27297-MS},
eventtitle={Offshore Technology Conference},
location={Houston, Texas, USA},
abstract={This paper presents the successful history of Lula NE Pilot Project, a challenging megaproject with an aggressive time-driven schedule, faster than industry average, that demanded new technology development in a scenario of uncertainty. The area is part of the supergiant Lula field, located in the pre-salt region of Santos Basin, Southeast Brazil, 300 km off the coast of Rio de Janeiro state, in 2000 m water depth. It is a joint venture with Petrobras as the Operator, and BG E///////\\&P Brasil and Petrogal Brasil as partners. The project was designed as a Pilot aiming to test some new concepts for the production development in the pre-salt area. In terms of subsea gathering system, an innovative concept was deployed, combining flexible flowlines lying on sea floor, with rigid steel catenary risers (SCR) supported by a buoy positioned 250 m below sea level. The drainage plan considered eight oil producers, some of them with intelligent completion, one gas / CO2 injection well and five water alternating gas (WAG) injectors (two subsea WAG manifolds were also installed). A balanced approach between data acquisition and facilities flexibility made possible to face the many reservoir and production uncertainties. Details of the development concept will be discussed, as well as the main results obtained so far, highlighting the strategies adopted in order to mitigate risks and the influence of the acquired information to the following projects in the area.},
urldate={2019-04-07},
booktitle={Offshore Technology Conference},
langid={english},
publisher={Offshore Technology Conference},
date={2016},
author={de Moraes Cruz, Rafael Oscar and Rosa, Marcelo Becher and Branco, Celso Cesar Moreira and de Sant'Anna Pizarro, Jorge Oscar and de Souza Silva, Celso Tarcisio},
}

@book{2016KowsmannGeology,
note={OCLC: 965386433},
urldate={2019-04-07},
langid={english},
title={Geology and geomorphology: regional environmental characterization of the Campos Basin, Southwest Atlantic},
author={Kowsmann, Renato O},
shorttitle={Geology and geomorphology},
date={2016},
isbn={978-85-352-8444-7},
url={http://www.sciencedirect.com/science/book/9788535284447},
}

@article{2016DiasSustainability,
shorttitle={Sustainability in the prospective scenarios methods},
issn={0016-3287},
title={Sustainability in the prospective scenarios methods: A case study of scenarios for biodiesel industry in Brazil, for 2030},
urldate={2019-04-07},
volume={82},
abstract={In order to build prospective scenarios for biodiesel industry in Brazil, with a sustainable perspective, it was necessary to develop a cross-disciplinary work to include Sachs’ dimensions of sustainability into the scenarios method. This meant linking concepts from different disciplines, without transforming it in a new discipline. In order to support the proposition for the new method, a study case is presented, the framework for the biodiesel scenarios in Brazil, by 2030. An in-depth interview was used to test the proposition of having the sustainability dimensions as driving forces. The result was the identiﬁcation of a critical uncertainty composed of various aspects related the response to climate change and environmental conservation. The scenario storylines that were developed based on the critical uncertainties showed that sustainable options for the future are possible if the mental maps are enlarged to see beyond the business as usual.},
pages={1--14},
url={https://linkinghub.elsevier.com/retrieve/pii/S0016328716300167},
date={2016-09},
doi={10.1016/j.futures.2016.06.005},
author={Dias, Maria Amelia de Paula and Vianna, João Nildo de Souza and Felby, Claus},
journaltitle={Futures},
langid={english},
}

@article{2017GomesSensibility,
pages={00--00},
urldate={2019-04-07},
doi={10.1108/JM2-01-2016-0005},
issn={1746-5664},
editor={Huang, Zhimin and Shi, Xunpeng},
journaltitle={Journal of Modelling in Management},
author={Gomes, Carlos Francisco Simões and Costa, Helder Gomes and de Barros, Alexandre P},
langid={english},
title={Sensibility analysis of MCDA using prospective in Brazilian energy sector},
date={2017-07-03},
}

@inproceedings{2018PankajApplication,
abstract={In today's data-driven economy, operators that integrate vast stores of fundamental reservoir and production data with the highperformance predictive analytics solutions can emerge as winners in the contest of maximizing estimated ultimate recovery (EUR). The scope of this study is to demonstrate a new workflow coupling earth sciences with data analytics to operationalize well completion optimization. The workflow aims to build a robust predictive model that allows users to perform sensitivity analysis on completion designs within a few hours.},
date={2018},
booktitle={Offshore Technology Conference},
eventtitle={Offshore Technology Conference},
author={Pankaj, Piyush and Geetan, Steve and MacDonald, Richard and Shukla, Priyavrat and Sharma, Abhishek and Menasria, Samir and Xue, Han and Judd, Tobias},
location={Houston, Texas, USA},
urldate={2019-04-07},
doi={10.4043/28632-MS},
publisher={Offshore Technology Conference},
title={Application of Data Science and Machine Learning for Well Completion Optimization},
langid={english},
}

@inproceedings{2018PollockMachine,
author={Pollock, Jacob and Stoecker-Sylvia, Zachary and Veedu, Vinod and Panchal, Neil and Elshahawi, Hani},
date={2018},
doi={10.4043/28633-MS},
location={Houston, Texas, USA},
urldate={2019-04-07},
booktitle={Offshore Technology Conference},
groups={tesse:5},
eventtitle={Offshore Technology Conference},
abstract={Directional drilling is a complex process involving the remote control of tool alignment and force application to a very long drill string subject to variable external forces. Controlling bit tool face orientation while ensuring adequate rate of penetration (ROP) is quite challenging, with aspects that have been described as more art than science. Improving this control helps preserve proper well trajectory and eliminate deviations that require corrective measures and add to well costs.},
title={Machine Learning for Improved Directional Drilling},
publisher={Offshore Technology Conference},
langid={english},
}

@inproceedings{2018PankajNeed,
location={Calgary, Alberta, Canada},
booktitle={SPE Canada Unconventional Resources Conference},
urldate={2019-04-07},
abstract={In today's data-driven economy, operators that integrate vast stores of fundamental reservoir and production data with the high-performance predictive analytics solutions can emerge as winners in the contest of maximizing estimated ultimate recovery (EUR). The scope of this study is to demonstrate a new workflow coupling earth sciences with data analytics to operationalize well completion optimization. The workflow aims to build a robust predictive model that allows users to perform sensitivity analysis on completion designs within a few hours.},
shorttitle={Need for Speed},
date={2018},
title={Need for Speed: Data Analytics Coupled to Reservoir Characterization Fast Tracks Well Completion Optimization},
eventtitle={SPE Canada Unconventional Resources Conference},
doi={10.2118/189790-MS},
author={Pankaj, Piyush and Geetan, Steve and MacDonald, Richard and Shukla, Priyavrat and Sharma, Abhishek and Menasria, Samir and Judd, Tobias},
publisher={Society of Petroleum Engineers},
langid={english},
}

@inproceedings{2018NoshiRole,
publisher={Society of Petroleum Engineers},
date={2018},
booktitle={SPE/AAPG Eastern Regional Meeting},
author={Noshi, Christine I. and Schubert, Jerome J.},
urldate={2019-04-07},
title={The Role of Machine Learning in Drilling Operations; A Review},
langid={english},
eventtitle={SPE/AAPG Eastern Regional Meeting},
location={Pittsburgh, Pennsylvania, USA},
doi={10.2118/191823-18ERM-MS},
abstract={Drilling problems such as stick slip vibration/hole cleaning, pipe failures, loss of circulation, BHA whirl, stuck pipe incidents, excessive torque and drag, low ROP, bit wear, formation damage and borehole instability, and the drilling of highly tortuous wells have only been tackled using physics-based models. Despite the mammoth generation of real-time metadata, there is a tremendous gap between statistical based models and empirical, mathematical, and physical-based models. Data mining techniques have made prominent contributions across a broad spectrum of industries. Its value is widely appreciated in a variety of applications, but its potential has not been fully tapped in the oil and gas industry. This paper presents a review compiling several years of Data Analytics applications in the drilling operations. This review discusses the benefits, deficiencies of the present practices, challenges, and novel applications under development to overcome industry deficiencies. This study encompasses a comprehensive compilation of data mining algorithms and industry applications from a predictive analytics standpoint using supervised and unsupervised advanced analytics algorithms to identify hidden patterns and help mitigate drilling challenges. Traditional data preparation and analysis methods are not sufficiently capable of rapid information extraction and clear visualization of big complicated data sets. Due to the petroleum industry's unfulfilled demand, Machine Learning (ML)-assisted industry workflow in the fields of drilling optimization and real time parameter analysis and mitigation is presented.},
}

@article{1980EnzerInterax—an,
abstract={In exploring alternative futures, strategic planners are frequently forced into making simplifying assumptions because of time and methodological limitations. These assumptions typically reduce the interdisciplinary perspective, limit the range of policy choices, and restrict the investigation to only the “most likely” future changes. By so doing, many sources of future sutprise are eliminated from the analysis. This article describes INTERAX, a method for generating alternative future scenarios which requires very few simplifying assumptions. To accelerate the time required for the analysis, INTERAX includes a large multidisciplinary data base, which is of immediate use in a broad range of long-range strategic issues. INTERAX is described in two articles. This article describes the method and its philosophy and presents the data base. The second part, which will appear in the next issue of Technological Forecasting and Social Change, describes bow INTERAX generates scenarios and their use in exploring a variety of strategic issues.},
author={Enzer, Selwyn},
volume={17},
issn={0040-1625},
pages={141--159},
url={http://linkinghub.elsevier.com/retrieve/pii/0040162580900499},
urldate={2019-04-07},
langid={english},
date={1980-06},
journaltitle={Technological Forecasting and Social Change},
shorttitle={INTERAX—An interactive model for studying future business environments},
title={INTERAX—An interactive model for studying future business environments: Part I},
doi={10.1016/0040-1625(80)90049-9},
number={2},
}

@article{1980EnzerInterax—ana,
journaltitle={Technological Forecasting and Social Change},
urldate={2019-04-07},
url={http://linkinghub.elsevier.com/retrieve/pii/0040162580900645},
abstract={In exploring alternative futures, strategic planners are frequently forced into making simplifying assumptions because of time and methodological limitations. These assumptions typically reduce the interdisciplinary perspective, limit the range of policy choices, and restrict the investigation to only the “most likely” future changes. By so doing, many sources of future surprise am eliminated from the analysis. This article describes INTERAX, a method for generating alternative future scenarios which requires very few simplifying assumptions. To accelerate the time required for the analysis, INTERAX includes a large multidisciplinary data base, which is of immediate use in a broad range of long-range strategic issues. INTERAX is described in two parts. The previous issue of Technological Forecusring md Socinl Chnnge contained a description of the method and its philosophy and presented the data base. This article describes how INTERAX generates scenarios and uses them in exploring a variety of strategic issues.},
langid={english},
shorttitle={INTERAX—An interactive model for studying future business environments},
volume={17},
author={Enzer, Selwyn and Director, o̊Associate},
issn={0040-1625},
title={INTERAX—An interactive model for studying future business environments: Part II},
date={1980-07},
number={3},
doi={10.1016/0040-1625(80)90064-5},
pages={211--242},
}

@article{corsano_drilling_nodate,
langid={english},
abstract={The Drilling Performance Curve (DPC) ia a simple yet powerful tool to assess the drilling performance in any given area where a c,;nsecutive series of similar wells have been drilled. All the information that is needed to perform the analysis is the sequence numbers of the well and the time it takes to reach a given depth.},
title={The Drilling Performance Curve: A Yardstick for Judging Drilling Performance},
author={Corsano, Alfred},
pages={12},
}

@article{2005MietznerAdvantages,
doi={10.1504/IJTIP.2005.006516},
journaltitle={International Journal of Technology Intelligence and Planning},
date={2005},
pages={220},
url={http://www.inderscience.com/link.php?id=6516},
urldate={2019-04-07},
abstract={Scenarios, as a prime technique of future studies, have long been used by government planners, corporate managers and military analysts as powerful tools to aid in decision making in the face of uncertainty. The idea behind them is to establish thinking about possible futures which can minimise surprises and broaden the span of managers’ thinking about different possibilities. Today the question of what scenarios are is unclear except with regard to one point - they have become extremely popular. This paper attempts to shed light on differences in scenario approaches. It will describe the origin of scenarios and the development of different understandings and purposes for managers. Categories are developed to compare the different ways scenarios are performed. Finally, the advantages and disadvantages of scenario approaches are analysed.},
volume={1},
langid={english},
groups={tesse:5},
issn={1740-2832, 1740-2840},
number={2},
title={Advantages and disadvantages of scenario approaches for strategic foresight},
author={Mietzner, Dana and Reger, Guido},
}

@article{lima_os_nodate,
title={OS DESAFIOS, OS IMPACTOS E A GESTÃO DA EXPLORAÇÃO DO PRÉ-SAL},
author={Lima, Paulo César Ribeiro},
pages={25},
langid={portuguese},
}

@inproceedings{2009BeltraoChallenges,
author={Beltrao, Ricardo L. and Sombra, Cristiano and Lage, Antonio and Fagundes Netto, Jose and Henriques, Carlos},
title={Challenges and New Technologies for the Development of the Pre-Salt Cluster, Santos Basin, Brazil},
doi={10.4043/OTC-19880-MS},
publisher={The Offshore Technology Conference},
url={http://www.onepetro.org/mslib/servlet/onepetropreview?id=OTC-19880-MS///////\\&soc=OTC},
date={2009-05},
booktitle={Proceedings of Offshore Technology Conference},
eventtitle={Offshore Technology Conference},
abstract={Pre-salt carbonate reservoirs from Santos Basin represent a great opportunity and probably the most important recent oil discovery. Tupi area (estimated to have recoverable volume of 5 to 8 bboe), which is the most known amongst several other leads in the cluster, is going to be a great insight for the production project.},
langid={english},
urldate={2019-04-07},
}

@article{2002BerkhoutForesight,
journaltitle={Greener Management International},
pages={37--53},
date={2002},
author={Berkhout, Frans and Hertin, Julia},
shorttitle={Foresight futures scenarios},
title={Foresight futures scenarios: developing and applying a participative strategic planning tool},
}

@article{1995SchoemakerScenario,
volume={36},
number={2},
date={1995},
pages={25--50},
journaltitle={Sloan management review},
shorttitle={Scenario planning},
title={Scenario planning: a tool for strategic thinking},
author={Schoemaker, Paul JH},
}

@collection{2010ChenPlastics,
publisher={Springer},
location={Heidelberg ; New York},
note={OCLC: ocn428029360},
editor={Chen, Guo-Qiang},
pagetotal={450},
series={Microbiology monographs ; v. 14},
langid={english},
shorttitle={Plastics from bacteria},
isbn={978-3-642-03286-8},
title={Plastics from bacteria: natural functions and applications},
date={2010},
keywords={Poly-beta-hydroxyalkanoates},
}

@article{1993SwansonStarch,
doi={10.1007/BF01418208},
number={2},
pages={155--166},
urldate={2019-04-07},
volume={1},
langid={english},
shorttitle={Starch-plastic materials?},
author={Swanson, C. L. and Shogren, R. L. and Fanta, G. F. and Imam, S. H.},
title={Starch-plastic materials-Preparation, physical properties, and biodegradability (a review of recent USDA research)},
date={1993-04},
issn={1064-7564, 1572-8900},
journaltitle={Journal of Environmental Polymer Degradation},
}

@article{1997VanSoestInfluence,
journaltitle={Journal of Applied Polymer Science},
author={Van Soest, J. J. G. and Knooren, N.},
date={1997-05-16},
pages={1411--1422},
title={Influence of glycerol and water content on the structure and properties of extruded starch plastic sheets during aging},
number={7},
urldate={2019-04-07},
doi={10.1002/(SICI)1097-4628(19970516)64:7<1411::AID-APP21>3.0.CO;2-Y},
volume={64},
langid={english},
issn={0021-8995, 1097-4628},
}

@article{1998LuntLarge,
urldate={2019-04-07},
date={1998-01},
issn={0141-3910},
url={http://linkinghub.elsevier.com/retrieve/pii/S0141391097001481},
number={1},
author={Lunt, James},
title={Large-scale production, properties and commercial applications of polylactic acid polymers},
pages={145--152},
volume={59},
doi={10.1016/S0141-3910(97)00148-1},
langid={english},
journaltitle={Polymer Degradation and Stability},
}

@article{2000DrumrightPolylactic,
journaltitle={Advanced Materials},
date={2000-12},
doi={10.1002/1521-4095(200012)12:23<1841::AID-ADMA1841>3.0.CO;2-E},
number={23},
volume={12},
groups={tesse:5},
title={Polylactic Acid Technology},
langid={english},
urldate={2019-04-07},
author={Drumright, R. E. and Gruber, P. R. and Henton, D. E.},
pages={1841--1846},
issn={0935-9648, 1521-4095},
}

@article{2003LuengoBioplastics,
number={3},
url={https://linkinghub.elsevier.com/retrieve/pii/S1369527403000407},
journaltitle={Current Opinion in Microbiology},
doi={10.1016/S1369-5274(03)00040-7},
groups={tesse:5},
langid={english},
author={Luengo, José M. and Garcı́a, Belén and Sandoval, Angel and Naharro, Germán and Olivera, Elı́as R.},
volume={6},
urldate={2019-04-07},
date={2003-06},
pages={251--260},
issn={1369-5274},
title={Bioplastics from microorganisms},
}

@article{2004DomenekBiodegradability,
journaltitle={Chemosphere},
issn={0045-6535},
urldate={2019-04-07},
author={Domenek, Sandra and Feuilloley, Pierre and Gratraud, Jean and Morel, Marie-Hélène and Guilbert, Stéphane},
title={Biodegradability of wheat gluten based bioplastics},
doi={10.1016/S0045-6535(03)00760-4},
abstract={A large variety of wheat gluten based bioplastics, which were plasticized with glycerol, were subjected to biodegradation. The materials covered the total range available for the biochemical control parameter Fi, which expresses the percentage of aggregated proteins. This quantity can be related to the density of covalent crosslinks in the wheat gluten network, which are induced by technological treatments. The biodegradability tests were performed in liquid medium (modiﬁed Sturm test) and in farmland soil. All gluten materials were fully degraded after 36 days in aerobic fermentation and within 50 days in farmland soil. No signiﬁcant diﬀerences were observed between the samples. The mineralization half-life time of 3.8 days in the modiﬁed Sturm test situated gluten materials among fast degrading polymers. The tests of microbial inhibition experiments revealed no toxic eﬀects of the modiﬁed gluten or of its metabolites. Thus, the protein bulk of wheat gluten materials is non-toxic and fully biodegradable, whatever the technological process applied.},
volume={54},
url={https://linkinghub.elsevier.com/retrieve/pii/S0045653503007604},
pages={551--559},
date={2004-01},
groups={tesse:5},
number={4},
langid={english},
}

@article{2008JacquelIsolation,
abstract={The isolation and the puriﬁcation of bacterial polyhydroxyalkanoates are the key step of the process proﬁtability in the fermentation system. That is why many scientists have studied this ﬁeld for the production of this biodegradable polymer. The ideal method should lead to a high purity and recovery level at a low production cost. This paper reviews four isolation methods, i.e. solvent extraction of halosolvent and nonhalosolvent, digestion of non-polyhydroxyalkanoate cell material involving surfactants, sodium hypochlorite or enzyme, mechanical cell disruption methods like using bead mills and high pressure homogenization, and new methods like spontaneous liberation of poly(3-hydroxybutyrate), dissolved air ﬂotation, air classiﬁcation, or by using supercritical CO2. The pretreatment of cell disruption and the puriﬁcation methods and analytical methods of polyhydroxyalkanoates are also presented.},
groups={tesse:5},
langid={english},
journaltitle={Biochemical Engineering Journal},
doi={10.1016/j.bej.2007.11.029},
pages={15--27},
volume={39},
date={2008-04},
title={Isolation and purification of bacterial poly(3-hydroxyalkanoates)},
author={Jacquel, Nicolas and Lo, Chi-Wei and Wei, Yu-Hong and Wu, Ho-Shing and Wang, Shaw S.},
urldate={2019-04-07},
issn={1369-703X},
number={1},
url={https://linkinghub.elsevier.com/retrieve/pii/S1369703X07004433},
}

@article{2009SatyanarayanaBiodegradable,
date={2009-09},
issn={0079-6700},
volume={34},
url={https://linkinghub.elsevier.com/retrieve/pii/S0079670008001214},
langid={english},
title={Biodegradable composites based on lignocellulosic fibers—An overview},
groups={tesse:5},
pages={982--1021},
journaltitle={Progress in Polymer Science},
urldate={2019-04-07},
number={9},
author={Satyanarayana, Kestur G. and Arizaga, Gregorio G. C. and Wypych, Fernando},
doi={10.1016/j.progpolymsci.2008.12.002},
abstract={The development of commercially viable “green products” based on natural resources for both matrices and reinforcements for a wide range of applications is on the rise. This effort includes new pathways to produce natural polymers with better mechanical properties and thermal stability using nanotechnology and use of natural polymers to make biodegradable plastics and their composites with lignocellulosic ﬁbers. This paper presents an overview of the developments made in the area of biodegradable composites, in terms of market, processing methods, matrix–reinforcement systems, morphology, properties and product development. Some critical issues and suggestions for future work are discussed, underscoring the roles of materials scientists and textile engineers for the future of these new “green” materials through value addition to enhance their use.},
}

@article{chee_bacterially_2010,
pages={10},
langid={english},
date={2010},
author={Chee, Jiun-Yee and Yoga, Sugama-Salim and Lau, Nyok-Sean and Ling, Siew-Chen and Abed, Raeid M M and Sudesh, Kumar},
title={Bacterially Produced Polyhydroxyalkanoate (PHA): Converting Renewable Resources into Bioplastics},
}

@article{2010KeshavarzPolyhydroxyalkanoates,
journaltitle={Current Opinion in Microbiology},
shorttitle={Polyhydroxyalkanoates},
issn={1369-5274},
author={Keshavarz, Tajalli and Roy, Ipsita},
number={3},
title={Polyhydroxyalkanoates: bioplastics with a green agenda},
pages={321--326},
doi={10.1016/j.mib.2010.02.006},
langid={english},
volume={13},
date={2010-06},
url={https://linkinghub.elsevier.com/retrieve/pii/S1369527410000275},
urldate={2019-04-07},
}

@article{2010AkaraonyeProduction,
journaltitle={Journal of Chemical Technology ////////\\\& Biotechnology},
date={2010-04-23},
volume={85},
langid={english},
abstract={Polyhydroxyalkanoates (PHAs) have recently been the focus of attention as a biodegradable and biocompatible substitute for conventional non degradable plastics. The cost of large-scale production of these polymers has inhibited its widespread use. Thus, economical, large-scale production of PHAs is currently being studied intensively. Various bacterial strains, either wild-type or recombinant have been utilized with a wide spectrum of utilizable carbon sources. New fermentation strategies have been developed for the efﬁcient production of PHAs at high concentration and productivity. With the current advances, PHAs can now be produced to a concentration of 80 g L−1 with productivities greater than 4 g PHA L−1 h−1. These advances will further lower the production cost of PHAs and allow this family of polymers to become a leading biodegradable polymer in the near future. This review describes the properties of PHAs, their uses, the various attempts towards the production of PHAs, focusing on the utilization of cheap substrates and the development of different fermentation strategies for the production of these polymers, an essential step forward towards their widespread use.},
author={Akaraonye, Everest and Keshavarz, Tajalli and Roy, Ipsita},
pages={732--743},
shorttitle={Production of polyhydroxyalkanoates},
title={Production of polyhydroxyalkanoates: the future green materials of choice},
urldate={2019-04-07},
number={6},
doi={10.1002/jctb.2392},
issn={02682575, 10974660},
}

@incollection{2012KollerWhey,
booktitle={Polyester},
urldate={2019-04-07},
editor={Saleh, Hosam El-Din},
publisher={InTech},
doi={10.5772/48737},
title={Whey Lactose as a Raw Material for Microbial Production of Biodegradable Polyesters},
url={http://www.intechopen.com/books/polyester/whey-lactose-as-a-raw-material-for-microbial-production-of-biodegradable-polyesters},
date={2012-09-26},
langid={english},
author={Koller, Martin and Salerno, Anna and Muhr, Alexander and Reiterer, Angelika and Chiellini, Emo and Casella, Sergio and Horvat, Predrag and Braunegg, Gerhart},
isbn={978-953-51-0770-5},
}

@article{2016Mozejko-ciesielskaBacterial,
pages={271--282},
doi={10.1016/j.micres.2016.07.010},
shorttitle={Bacterial polyhydroxyalkanoates},
title={Bacterial polyhydroxyalkanoates: Still fabulous?},
issn={0944-5013},
author={Możejko-Ciesielska, Justyna and Kiewisz, Robert},
date={2016-11},
url={https://linkinghub.elsevier.com/retrieve/pii/S094450131630043X},
journaltitle={Microbiological Research},
urldate={2019-04-07},
volume={192},
abstract={Bacterial polyhydroxyalkanoates (PHA) are polyesters accumulated as carbon and energy storage materials under limited growth conditions in the presence of excess carbon sources. They have been developed as biomaterials with unique properties for the past many years being considered as a potential substitute for conventional non-degradable plastics. Due to the increasing concern towards global climate change, depleting petroleum resource and problems with an utilization of a growing number of synthetic plastics, PHAs have gained much more attention from industry and research. These environmentally friendly microbial polymers have great potential in biomedical, agricultural, and industrial applications. However, their production on a large scale is still limited. This paper describes the backgrounds of PHAs and discussed the current state of knowledge on the polyhydroxyalkanoates. Ability of bacteria to convert different carbon sources to PHAs, the opportunities and challenges of their introduction to global market as valuable renewable products have been also discussed.},
langid={english},
}

@article{2018RazaPolyhydroxyalkanoates,
author={Raza, Zulfiqar Ali and Abid, Sharjeel and Banat, Ibrahim M.},
date={2018-01},
volume={126},
abstract={Polyhydroxyalkanoates (PHAs) are biopolyesters, stored within cells as energy storage materials by various microorganisms. Due to their biocompatibility and biodegradability, PHAs have a wide range of applications in various industries such as biomedical sector including tissue engineering, bio-implant patches, drug delivery, surgery and wound dressing. PHAs are green plastics and they have positive social and environmental impact when compared with conventional plastics in terms of production and recycling. Moreover, PHAs do not possess acute and chronic health eﬀects when used in vivo. These bioplastics represent a renewable and sustainable resource to reduce landﬁll requirements without being persistence or causing pollution. A wide range of carbon sources, bacterial strains, fermentation conditions and recovery methods have been purposed by various researchers for better yield and economical perspectives. Recent advancements in synthetic biology and genetic engineering has led to the production of PHAs from non-PHAs producing strains with no toxins. Progression in recovery techniques has improved the extraction eﬃcacy from biomass with high purity. This review outlines production and characteristics of PHAs, developments in their production, and applications in various industries including nanotechnology.},
issn={0964-8305},
shorttitle={Polyhydroxyalkanoates},
langid={english},
pages={45--56},
url={https://linkinghub.elsevier.com/retrieve/pii/S0964830517300276},
journaltitle={International Biodeterioration ////////\\\& Biodegradation},
urldate={2019-04-07},
doi={10.1016/j.ibiod.2017.10.001},
title={Polyhydroxyalkanoates: Characteristics, production, recent developments and applications},
}

@misc{2017RosenheimFrequently,
title={Frequently asked questions on bioplastics},
groups={tesse:5},
langid={english},
date={2017},
author={Rosenheim, HypoVereinsbank and De, Iban and Hyvedemm, Swift},
}

@misc{2011BajpaiDurable,
title={Durable Bioplastics},
date={2011},
author={Bajpai, Pratima},
year={2011},
editor={IntertechPira},
langid={english},
groups={tesse:5},
pages={74},
}

@article{2014AgustinBioplastic,
pages={2205--2213},
langid={english},
volume={33},
issn={0731-6844, 1530-7964},
date={2014-12},
title={Bioplastic based on starch and cellulose nanocrystals from rice straw},
doi={10.1177/0731684414558325},
author={Agustin, Melissa B. and Ahmmad, Bashir and Alonzo, Shanna Marie M. and Patriana, Famille M.},
groups={tesse:5},
number={24},
journaltitle={Journal of Reinforced Plastics and Composites},
urldate={2019-04-07},
abstract={Bioplastic based on starch as the matrix and cellulose nanocrystals from rice straw as reinforcing filler were prepared in this study. The isolation of cellulose nanocrystal (CNC) followed a series of steps: delignification, sulfuric acid hydrolysis, and sonication. The process afforded short, rod-like CNCs with particle diameter ranging from 10 to 12 nm and crystallinity index of 76.1 /\\%. Fourier transform infrared analysis of the CNCs also confirmed absorption patterns typical of cellulose and the removal of silica. Bioplastic with different starch-to-CNC ratios were prepared by solution casting and evaporation method. Scanning electron micrographs of the films showed uniform dispersion of CNC in the starch matrix. Mechanical tests revealed that both tensile strength and modulus significantly increased with increasing CNC load while percent elongation decreased. The moisture uptake of the films reinforced with CNC also decreased an indication of improvement in water resistance. However, the thermal stability of the films decreased by the addition of CNC.},
}

@article{2015LiewEffect,
url={https://linkinghub.elsevier.com/retrieve/pii/S1018363913000408},
journaltitle={Journal of King Saud University - Engineering Sciences},
author={Liew, Kang Chiang and Khor, Lian Kim},
doi={10.1016/j.jksues.2013.08.001},
langid={english},
issn={1018-3639},
volume={27},
number={2},
pages={137--141},
urldate={2019-04-07},
title={Effect of different ratios of bioplastic to newspaper pulp fibres on the weight loss of bioplastic pot},
abstract={Nowadays, industry is searching for an alternative to reduce the usage of petroleumbased non-degradable conventional seedling plant pots. In this study, three different types of bioplastic pots incorporated with newspaper pulp ﬁbres (the ratio of B75///////\\%:N25///////\\%, B50///////\\%:N50///////\\% and B25///////\\%:N75///////\\% denotes percentage of bioplastic to percentage of newspaper pulp ﬁbres) were produced while B0///////\\%:N100///////\\% acted as the control. All cylinder square shape moulded bioplastic pots with 100 mm height and 2 mm thickness were planted with Leucaena leucocephala seedlings for 60 days in two ground levels (below ground and above ground). Weight loss for bioplastic pots was evaluated. Results showed that bioplastic pots tested below ground had a higher percentage of weight loss than those planted above ground. For percentage of weight loss of bioplastic pots, most bioplastic pots that were tested in both ground levels only showed a signiﬁcant difference at p 6 0.05 after 30 days. Bioplastic pots B75///////\\%:N25///////\\% that were tested below ground have the highest percentage of weight loss with 77.93///////\\%. As conclusion, B50///////\\%:N50///////\\% is the most suitable ratio for the production of bioplastic pot.},
date={2015-07},
}

@collection{2005MohantyNatural,
location={Boca Raton, FL},
groups={tesse:5},
langid={english},
pagetotal={875},
keywords={Biopolymers, Fibers, Plant fibers, Polymeric composites},
title={Natural fibers, biopolymers, and biocomposites},
publisher={Taylor //\\\& Francis},
editor={Mohanty, Amar K. and Misra, Manjusri and Drzal, Lawrence T.},
date={2005},
isbn={978-0-8493-1741-5},
}

@misc{2017EuropePlastics,
year={2017},
date={2017},
groups={tesse:5},
publisher={PlasticsEurope},
author={Europe, Plastics},
title={Plastics – the Facts 2017},
}

@unpublished{2018BioplasticsBioplastics,
author={Bioplastics, European},
date={2018},
groups={tesse:5},
title={Bioplastics Fact Sheet},
year={2018},
publisher={European Bioplastics},
}

@article{mohanty_sustainable_2002,
title={Sustainable Bio-Composites from Renewable Resources: Opportunities and Challenges in the Green Materials World},
pages={8},
date={2002},
langid={english},
author={Mohanty, A K and Misra, M and Drzal, L T},
}

@article{2008SudeshSustainability,
issn={18630650, 18630669},
volume={36},
journaltitle={CLEAN - Soil, Air, Water},
urldate={2019-04-07},
groups={tesse:5},
langid={english},
title={Sustainability of Biobased and Biodegradable Plastics},
date={2008-06},
doi={10.1002/clen.200700183},
pages={433--442},
number={5},
author={Sudesh, Kumar and Iwata, Tadahisa},
}

@article{2009ThompsonPlastics,
langid={english},
date={2009-07-27},
volume={364},
number={1526},
doi={10.1098/rstb.2009.0053},
urldate={2019-04-07},
pages={2153--2166},
journaltitle={Philosophical Transactions of the Royal Society B: Biological Sciences},
author={Thompson, Richard C. and Moore, Charles J. and vom Saal, Frederick S. and Swan, Shanna H.},
issn={0962-8436, 1471-2970},
shorttitle={Plastics, the environment and human health},
title={Plastics, the environment and human health: current consensus and future trends},
}

@article{2010TaboneSustainability,
date={2010-11},
journaltitle={Environmental Science ////////\\\& Technology},
langid={english},
title={Sustainability Metrics: Life Cycle Assessment and Green Design in Polymers},
number={21},
urldate={2019-04-07},
author={Tabone, Michaelangelo D. and Cregg, James J. and Beckman, Eric J. and Landis, Amy E.},
doi={10.1021/es101640n},
issn={0013-936X, 1520-5851},
shorttitle={Sustainability Metrics},
pages={8264--8269},
volume={44},
}

@article{2012KaranaCharacterization,
doi={10.1016/j.jclepro.2012.07.034},
url={https://linkinghub.elsevier.com/retrieve/pii/S0959652612003721},
volume={37},
journaltitle={Journal of Cleaner Production},
abstract={Over the past decade, the deployment of sustainable product design has led to a dramatic increase in the use of bio-plastics as an environmentally sensitive substitute to regular petroleum-based ones. Published literature has explored the environmental performance and their suitability as an alternative for regular plastics. However, the reception of these materials by users, who come into contact with these materials embodied in consumer products, has not been researched and published. Even though the principle of using such materials with improved environmental credentials is sound, it is down to the users’ appreciation of those materials that ultimately determine their commercial success. A signiﬁcant challenge faced by material developers and product designers is to facilitate the appraisal of bio-plastics as a natural alternative to regular plastics, whilst at the same time meeting users’ perceptions of quality.},
langid={english},
urldate={2019-04-07},
author={Karana, Elvin},
date={2012-12},
title={Characterization of ‘natural’ and ‘high-quality’ materials to improve perception of bio-plastics},
pages={316--325},
issn={0959-6526},
}

@article{2013PhilpBioplastics,
urldate={2019-04-07},
groups={tesse:5},
doi={10.1016/j.nbt.2012.11.021},
journaltitle={New Biotechnology},
langid={english},
title={Bioplastics science from a policy vantage point},
number={6},
url={https://linkinghub.elsevier.com/retrieve/pii/S1871678412008783},
issn={1871-6784},
date={2013-09},
pages={635--646},
volume={30},
author={Philp, Jim C. and Bartsev, Alexandre and Ritchie, Rachael J. and Baucher, Marie-Ange and Guy, K.},
}

@article{2014VanCauwenbergheMicroplastics,
date={2014-10},
urldate={2019-04-07},
abstract={Microplastics are present throughout the marine environment and ingestion of these plastic particles (<1 mm) has been demonstrated in a laboratory setting for a wide array of marine organisms. Here, we investigate the presence of microplastics in two species of commercially grown bivalves: Mytilus edulis and Crassostrea gigas. Microplastics were recovered from the soft tissues of both species. At time of human consumption, M. edulis contains on average 0.36 ± 0.07 particles gÀ1 (wet weight), while a plastic load of 0.47 ± 0.16 particles gÀ1 ww was detected in C. gigas. As a result, the annual dietary exposure for European shellﬁsh consumers can amount to 11,000 microplastics per year. The presence of marine microplastics in seafood could pose a threat to food safety, however, due to the complexity of estimating microplastic toxicity, estimations of the potential risks for human health posed by microplastics in food stuffs is not (yet) possible.},
groups={tesse:5},
volume={193},
langid={english},
author={Van Cauwenberghe, Lisbeth and Janssen, Colin R.},
title={Microplastics in bivalves cultured for human consumption},
issn={0269-7491},
pages={65--70},
journaltitle={Environmental Pollution},
url={https://linkinghub.elsevier.com/retrieve/pii/S0269749114002425},
doi={10.1016/j.envpol.2014.06.010},
}

@article{2014LawMicroplastics,
groups={tesse:5},
title={Microplastics in the seas},
urldate={2019-04-07},
author={Law, K. L. and Thompson, R. C.},
doi={10.1126/science.1254065},
number={6193},
pages={144--145},
issn={0036-8075, 1095-9203},
volume={345},
date={2014-07-11},
langid={english},
journaltitle={Science},
}

@article{2015LiMicroplastics,
volume={207},
title={Microplastics in commercial bivalves from China},
pages={190--195},
url={https://linkinghub.elsevier.com/retrieve/pii/S0269749115300658},
journaltitle={Environmental Pollution},
issn={0269-7491},
date={2015-12},
doi={10.1016/j.envpol.2015.09.018},
abstract={We investigated microplastic pollution in 9 commercial bivalves from a ﬁshery market in China. Multiple types of microplastics, including ﬁbers, fragments and pellets, occurred in the tissue of all bivalves. The number of total microplastics varied from 2.1 to 10.5 items/g and from 4.3 to 57.2 items/individual for bivalves. Scapharca subcrenata contained on average 10.5 items/g and exhibited the highest levels of microplastics by weight. Fibers were the most common microplastics and consisted of more than half of the total microplastics in each of the 8 species. In Alectryonella plicatula, pellets accounted for 60 /\\% of the total microplastics. The most common size class was less than 250 mm and accounted for 33e84 /\\% of the total microplastics calculated by species. Our results suggest that microplastic pollution was widespread and exhibited a relatively high level in commercial bivalves from China. More intensive investigations on microplastics should be conducted in seafood.},
groups={tesse:5},
author={Li, Jiana and Yang, Dongqi and Li, Lan and Jabeen, Khalida and Shi, Huahong},
langid={english},
urldate={2019-04-07},
}

@article{2016Costananoplastics,
issn={0048-9697},
url={https://linkinghub.elsevier.com/retrieve/pii/S0048969716309731},
abstract={There has been a considerable increase on research of the ecological consequences of microplastics released into the environment, but only a handful of works have focused on the nano-sized particles of polymer-based materials. Though their presence has been difﬁcult to adequately ascertain, due to the inherent technical difﬁculties for isolating and quantifying them, there is an overall consensus that these are not only present in the environment –either directly released or as the result of weathering of larger fragments – but that they also pose a signiﬁcant threat to the environment and human health, as well. The reduced size of these particulates (b 1 μm) makes them susceptible of ingestion by organisms that are at the base of the food-chain. Moreover, the characteristic high surface area-to-volume ratio of nanoparticles may add to their potential hazardous effects, as other contaminants, such as persistent organic pollutants, could be adsorbed and undergo bioaccumulation and bioampliﬁcation phenomena.},
date={2016-10},
volume={566-567},
langid={english},
title={(Nano)plastics in the environment – Sources, fates and effects},
urldate={2019-04-07},
journaltitle={Science of The Total Environment},
pages={15--26},
author={da Costa, João Pinto and Santos, Patrícia S. M. and Duarte, Armando C. and Rocha-Santos, Teresa},
doi={10.1016/j.scitotenv.2016.05.041},
groups={tesse:5},
}

@article{2016BrockhausCrossroads,
doi={10.1016/j.jclepro.2016.04.003},
issn={0959-6526},
date={2016-07},
pages={84--95},
title={A crossroads for bioplastics: exploring product developers' challenges to move beyond petroleum-based plastics},
url={https://linkinghub.elsevier.com/retrieve/pii/S0959652616302414},
volume={127},
abstract={Bioplastics play an increasingly important role for consumer products. These new materials might increase product sustainability but they are currently confined to niche markets. While research has gained important insight into the technical challenges, few studies to date explore the behavioral aspects for product developers as they move to employ bioplastics in their development efforts. This manuscript reports the findings of a grounded inductive study based on interview data with 32 product developers in the consumer goods industry. The Theory of Planned Behavior is employed to guide the research and provide a theoretical background to derive implications. The study finds that behavioral challenges impede the increased use of bioplastics. Product developers experience a lack of perceived behavioral control and struggle with doubts about the environmental benefits and incurring trade-offs of bioplastics with respect to the Triple Bottom Line. While product developers are intrinsically motivated to make more use of bioplastics, they often refrain from bringing products to the mass market due to uncertainties of customer receptiveness and fears of greenwashing allegations. Implications for industry and research are detailed.},
langid={english},
journaltitle={Journal of Cleaner Production},
author={Brockhaus, Sebastian and Petersen, Moritz and Kersten, Wolfgang},
urldate={2019-04-07},
shorttitle={A crossroads for bioplastics},
}

@article{2016SijtsemaConsumer,
journaltitle={NJAS - Wageningen Journal of Life Sciences},
title={Consumer perception of bio-based products—An exploratory study in 5 European countries},
date={2016-06},
url={https://linkinghub.elsevier.com/retrieve/pii/S1573521416300070},
issn={1573-5214},
urldate={2019-04-07},
doi={10.1016/j.njas.2016.03.007},
pages={61--69},
abstract={This study explores people’s perceptions (i.e., positive and negative associations, mixed feelings) regarding the concept of ‘bio-based’ in general and speciﬁc bio-based products. This exploratory study is one of the ﬁrst consumer studies in the ﬁeld of bio-based research. Three focus group discussions were organized in the Czech Republic, Denmark, Germany, Italy, and The Netherlands (with 89 participants in total) in which projective techniques were applied.},
langid={english},
author={Sijtsema, Siet J. and Onwezen, Marleen C. and Reinders, Machiel J. and Dagevos, Hans and Partanen, Asta and Meeusen, Marieke},
volume={77},
}

@article{2017CecchiniBioplastics,
doi={10.1080/14606925.2017.1352684},
title={Bioplastics made from upcycled food waste. Prospects for their use in the field of design},
volume={20},
abstract={The negative effects on the environment of the intensive use of synthetic, oil-derived plastics to make products have given renewed impetus to the search for biopolymers derived from vegetable, animal or microbial matter that could prove to be a sound alternative in a number of applications. However, the real challenge is to create new materials from food waste and not from specially grown crops, whose production in any case comes at an environmental cost.},
issue={sup1},
urldate={2019-04-07},
langid={english},
journaltitle={The Design Journal},
date={2017-07-28},
author={Cecchini, Cecilia},
issn={1460-6925, 1756-3062},
pages={S1596--S1610},
}

@article{2018LebretonEvidence,
url={http://www.nature.com/articles/s41598-018-22939-w},
title={Evidence that the Great Pacific Garbage Patch is rapidly accumulating plastic},
groups={tesse:5},
number={1},
langid={english},
urldate={2019-04-07},
author={Lebreton, L. and Slat, B. and Ferrari, F. and Sainte-Rose, B. and Aitken, J. and Marthouse, R. and Hajbane, S. and Cunsolo, S. and Schwarz, A. and Levivier, A. and Noble, K. and Debeljak, P. and Maral, H. and Schoeneich-Argent, R. and Brambini, R. and Reisser, J.},
volume={8},
journaltitle={Scientific Reports},
date={2018-12},
issn={2045-2322},
doi={10.1038/s41598-018-22939-w},
}

@article{2015RognoliDiy,
urldate={2019-04-07},
date={2015-12},
groups={tesse:5},
author={Rognoli, Valentina and Bianchini, Massimo and Maffei, Stefano and Karana, Elvin},
pages={692--702},
volume={86},
journaltitle={Materials //\\\& Design},
doi={10.1016/j.matdes.2015.07.020},
title={DIY materials},
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S0264127515300964},
abstract={The democratization of personal fabrication technologies in parallel to the rising desire of individuals for personalizing their products offers great opportunities to experiment with advanced, distributed and shared production processes as well as design new materials. In this article, we introduce the notion of Do-It-Yourself (DIY) Materials, which are created through individual or collective self-production practices, often by techniques and processes of the designer‟s own invention. They can be totally new materials, modified, or further developed versions of existing materials. In order to provide an operational vocabulary to discuss DIY materials, we have collected 27 DIY material cases developed in the last five years. We group the collected cases under two main categories: (1) DIY new materials: which focus on creative material ingredients (e.g. a material made of dried, blended waste citrus peel combined with natural binders); and (2) DIY new identities for conventional materials: which focus on new production techniques, giving new expressions to existing materials (i.e. they do not necessarily contain new ingredients, such as 3D printed metal). Grounded on the commonalities of collected cases, we discuss the design opportunities, including new aesthetic impressions offered through DIY material design practices.},
issn={0264-1275},
}

@article{2015KaranaMethod,
volume={9},
date={2015},
journaltitle={International Journal Of Design},
title={A Method to Design for Material Experiences},
groups={tesse:5},
langid={english},
author={Karana, Elvin and Barati, Bahareh and Rognoli, Valentina},
number={2},
pages={20},
}

@article{2016RognoliMaterial,
volume={4},
groups={tesse:2},
title={The material experiences as DIY-Materials:},
date={2016},
author={Rognoli, Valentina and Garcia, Camilo Ayala and Parisi, Stefano},
langid={english},
pages={9},
abstract={This paper presents the growing phenomena of self-produced materials, namely ‘Do-It-Yourself (DIY) Materials’, and illustrates the pattern of conditions and circumstances which led to the emergence of this approach to materials. The DIY Materials phenomena is derived from a broader concept defined as ‘emerging materials experiences’ and through their definition we think in new and meaningful interaction between users and these new materials qualities derived by a self-production process. Through the presentation of a case study, NeWool, a bio composite made of starch and wool fibres, we outline a design practice based on direct experimentation, tinkering and envisioning for designer.},
}

@article{2017KaranaGrowing,
pages={16},
author={Karana, Elvin},
abstract={The possibility to fabricate materials from living organisms offers appealing advantages for product design, such as higher sustainability and an interesting novel aesthetics. Several designers are now ‘growing’ their own materials. Despite the large interest shown, this emerging material practice is still scarcely understood in design literature. The aim of this paper is to shed light on what it means to design with growing organisms as collaborators, identifying the defining traits of this novel, designerly way of ‘doing materials’. To do so, we first compare this specific approach to the approaches of others working in the intersections of biology and design. In this way, we outline the boundaries of Growing Design, defining its unique characteristics. We then provide detailed descriptions of three classes of Growing Materials: fungal, bacterial and algal materials. For each class, we bring two examples of designers utilizing these materials for industrial design purposes. This helps to further explain what truly distinguishes Growing Materials from other conventional materials and to understand the challenges in working with them. Finally, this discussion enables us to set out a research agenda for Growing Design, supporting the development of these materials for industrial production.},
groups={tesse:2},
title={Growing materials for product design},
date={2017},
langid={english},
}

@article{2018CamereFabricating,
urldate={2019-04-07},
url={https://linkinghub.elsevier.com/retrieve/pii/S0959652618307388},
abstract={Biotechnology offers exciting opportunities for novel and more sustainable alternatives for the design and manufacturing of products. One of the most promising approaches is the fabrication of materials from living organisms, such as fungi and bacteria. An increasing number of designers are engaging in this "Growing Design" practice, exploring the unique potentials of the grown materials for product design. In Growing Design, designers operate in interdisciplinary contexts, engaging in early stage material developments. Despite the widespread interest towards Growing Design, no systematic study has been conducted so far to understand how this practice unfolds and its contribution to the progression towards cleaner production. To this end, eight recognized professionals in the field were interviewed. The results illustrate how the conception of materials in design evolves when designers co-perform with biological organisms. This alters how the design process unfolds and the mindset adopted in design practice, shaping a novel, systemic vision on production and consumption practices. The paper further discusses the need for developing new sensibilities to face complex interdisciplinary problems in Growing Design and highlights the role designers can take in developing new materials for sustainable production.},
shorttitle={Fabricating materials from living organisms},
author={Camere, Serena and Karana, Elvin},
langid={english},
title={Fabricating materials from living organisms: An emerging design practice},
doi={10.1016/j.jclepro.2018.03.081},
date={2018-06},
journaltitle={Journal of Cleaner Production},
issn={0959-6526},
pages={570--584},
volume={186},
}

@article{2017BridgensDesign,
urldate={2019-04-07},
author={Bridgens, Ben and Lilley, Debra},
date={2017-07-28},
journaltitle={The Design Journal},
issue={sup1},
pages={S160--S171},
doi={10.1080/14606925.2017.1352715},
issn={1460-6925, 1756-3062},
langid={english},
volume={20},
title={Design for Next… Year. The Challenge of Designing for Material Change},
abstract={From the moment of purchase, pristine objects are subjected to an array of stimuli including wear, impact, heat, light, water and air which alter their tactile and aesthetic properties. Material change is often regarded as ‘damage’ or ‘degradation’, but has potential to be used as a tool to engender emotional engagement to an object. We present a framework for designers to better understand how materials change with use, and in turn how people respond to materials as they change. Key challenges are identified which must be overcome to use this framework in design practice – people’s physical interaction with objects is poorly understood, it is difficult to simulate material change, materials resources for designers do not provide information about material change, and people’s responses to aged materials depend on a complex web of interacting factors.},
}

@article{2008JohnBiofibres,
groups={tesse:5},
author={John, M. and Thomas, S.},
url={https://linkinghub.elsevier.com/retrieve/pii/S0144861707002974},
pages={343--364},
number={3},
volume={71},
langid={english},
title={Biofibres and biocomposites},
journaltitle={Carbohydrate Polymers},
urldate={2019-04-07},
doi={10.1016/j.carbpol.2007.05.040},
abstract={This review deals with a recent study of the literature on the various aspects of cellulosic ﬁbres and biocomposites. Cellulosic ﬁbre reinforced polymeric composites are ﬁnding applications in many ﬁelds ranging from construction industry to automotive industry. The pros and cons of using these ﬁbres are enumerated in this review. The classiﬁcation of composites into green composites, hybrid biocomposites and textile biocomposites are discussed. New developments dealing with cellulose based nanocomposites and electrospinning of nanoﬁbres have also been presented. Recent studies pertaining to the above topics have also been cited. Finally, the applications of cellulosic ﬁbre reinforced polymeric composites have been highlighted.},
date={2008-02-08},
issn={0144-8617},
}

@article{2014ThakurReview,
author={Thakur, Vijay Kumar and Thakur, Manju Kumari and Gupta, Raju Kumar},
doi={10.1080/1023666X.2014.880016},
title={Review: Raw Natural Fiber–Based Polymer Composites},
pages={256--271},
volume={19},
issn={1023-666X, 1563-5341},
shorttitle={Review},
journaltitle={International Journal of Polymer Analysis and Characterization},
urldate={2019-04-07},
langid={english},
number={3},
date={2014-04-03},
}

@article{2016PickeringReview,
pages={98--112},
abstract={Recently, there has been a rapid growth in research and innovation in the natural ﬁbre composite (NFC) area. Interest is warranted due to the advantages of these materials compared to others, such as synthetic ﬁbre composites, including low environmental impact and low cost and support their potential across a wide range of applications. Much effort has gone into increasing their mechanical performance to extend the capabilities and applications of this group of materials. This review aims to provide an overview of the factors that affect the mechanical performance of NFCs and details achievements made with them.},
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S1359835X15003115},
journaltitle={Composites Part A: Applied Science and Manufacturing},
date={2016-04},
doi={10.1016/j.compositesa.2015.08.038},
urldate={2019-04-07},
volume={83},
author={Pickering, K.L. and Efendy, M.G. Aruan and Le, T.M.},
title={A review of recent developments in natural fibre composites and their mechanical performance},
issn={1359-835X},
}

@article{2016OmraniState,
urldate={2019-04-07},
issn={2215-0986},
langid={english},
author={Omrani, Emad and Menezes, Pradeep L. and Rohatgi, Pradeep K.},
pages={717--736},
url={https://linkinghub.elsevier.com/retrieve/pii/S221509861500172X},
title={State of the art on tribological behavior of polymer matrix composites reinforced with natural fibers in the green materials world},
volume={19},
date={2016-06},
doi={10.1016/j.jestch.2015.10.007},
journaltitle={Engineering Science and Technology, an International Journal},
abstract={Natural ﬁber reinforced polymer composites have emerged as a potential environmentally friendly and cost-effective alternative to synthetic ﬁber reinforced composites. Therefore, in the past decade, a number of major industries, such as the automotive, construction and packaging industries, have shown a considerable interest in the progress of new natural ﬁber reinforced composite materials. The availability of natural ﬁbers and the ease of manufacturing have tempted researchers to study their feasibility of their application as reinforcement and the extent to which they satisfy the required speciﬁcations in tribological applications. However, less information concerning the tribological performance of natural ﬁber reinforced composite material is available in the literature. Hence, the aim of this bibliographic review is to demonstrate the tribological behavior of natural ﬁber reinforced composites and ﬁnd a knowledge about their usability for various applications that tribology plays a dominant role. This review presents the reported work on natural ﬁber reinforced composites with special reference to the type of ﬁbers, matrix polymers, treatment of ﬁbers and test parameters. The results show that composites reinforced with natural ﬁbers have an improvement in tribological properties and their properties are comparable with conventional ﬁbers. In addition, ﬁber treatment and ﬁber orientation are two important factors can affect tribological properties where treated ﬁbers and normal oriented ﬁbers exhibit better friction and wear behavior. This review is trying to evaluate the effect of test parameter including normal load and sliding speed on tribological properties, and the results vary based on type of reinforcement. Generally, due to their positive economic and environmental aspects, as well as their good tribological properties, natural composites are showing a good potential for employing in several applications.},
number={2},
}

@article{2018SanjayCharacterization,
date={2018-01},
journaltitle={Journal of Cleaner Production},
doi={10.1016/j.jclepro.2017.10.101},
urldate={2019-04-07},
title={Characterization and properties of natural fiber polymer composites: A comprehensive review},
author={Sanjay, M.R. and Madhu, P. and Jawaid, Mohammad and Senthamaraikannan, P. and Senthil, S. and Pradeep, S.},
volume={172},
abstract={The world is in need of more eco-friendly material, therefore researchers around the globe focus on developing new materials that would improve the environmental quality of products. This need for new green materials has led to the utilization of composites made from raw natural fibers and polymer matrices, and this has become one of the most widely investigated research topics in recent times. Natural fiber composites are an alternative for replacing environmentally harmful synthetic materials and help control pollution problems. In addition, they are low cost, have better mechanical properties and require low production energy consumption. Also, using such materials in construction works, it is possible to improve the sustainability by eliminating construction wastes. Keeping in view all the benefits of natural fiber reinforced polymer composites, this paper first discusses various fabrication techniques employed for the production of these composites and then presents a detailed review of the research devoted to the analysis of their structure and properties by a variety of characterization techniques.},
langid={english},
shorttitle={Characterization and properties of natural fiber polymer composites},
pages={566--581},
issn={0959-6526},
url={https://linkinghub.elsevier.com/retrieve/pii/S0959652617323946},
}

@article{2018ElanchezhianReview,
date={2018},
urldate={2019-04-07},
groups={tesse:5},
number={1},
url={https://linkinghub.elsevier.com/retrieve/pii/S2214785317325440},
langid={english},
pages={1785--1790},
journaltitle={Materials Today: Proceedings},
issn={2214-7853},
title={Review on mechanical properties of natural fiber composites.},
volume={5},
author={Elanchezhian, C. and Ramnath, B. Vijaya and Ramakrishnan, G. and Rajendrakumar, M. and Naveenkumar, V. and Saravanakumar, M. K.},
doi={10.1016/j.matpr.2017.11.276},
abstract={The present experimental study aims at learning the mechanical behavior of natural fiber composites. Natural fibers have an attracting the interest to engineers, researchers, professionals and scientists all over the world as an alternative reinforcement, because of its superior properties such as high specific strength, low weight, low cost, fairly good mechanical properties, nonabrasive, eco-friendly and bio-degradable characteristics. A brief review has been carried out to make use of natural fibers (such as abaca, jute, sisal, banana, cotton, coir, hemp, etc) abundantly available in India. This paper presents a review on the mechanical properties of Abaca, Jute, Sisal.},
}

@article{2007NeillHow,
doi={10.1172/JCI34288},
pages={3599--3602},
date={2007-12-03},
title={How to write a scientific masterpiece},
volume={117},
url={http://www.jci.org/articles/view/34288},
urldate={2019-04-07},
author={Neill, Ushma S.},
journaltitle={Journal of Clinical Investigation},
abstract={The abstract is your hook, the most important information readers use when deciding whether to abandon your article or read it more in depth. It pays to make it easy to understand and broadly appealing; informative but not too detailed. What is the formula for such a masterpiece? Here is one that we try to follow as best we can: first, start with a sentence or two that frames the work. Introduce the disease and system you are studying and indicate what was previously unknown — framing why we are here now. Move on to the major finding, then spend a few sentences detailing the steps and mechanisms uncovered. Make sure to indicate the species studied, especially when this changes with different experiments. End the abstract with a sentence or two indicating the implications of the work without inflating the relevance.},
issn={0021-9738},
number={12},
groups={tesse:5, Writing},
langid={english},
}

@article{2011AndradeHow,
groups={tesse:5},
abstract={Abstracts of scientific papers are sometimes poorly written, often lack important information, and occasionally convey a biased picture. This paper provides detailed suggestions, with examples, for writing the background, methods, results, and conclusions sections of a good abstract. The primary target of this paper is the young researcher; however, authors with all levels of experience may find useful ideas in the paper.},
issn={0019-5545},
number={2},
date={2011},
pages={172},
urldate={2019-04-07},
journaltitle={Indian Journal of Psychiatry},
author={Andrade, Chittaranjan},
doi={10.4103/0019-5545.82558},
url={http://www.indianjpsychiatry.org/text.asp?2011/53/2/172/82558},
volume={53},
langid={english},
title={How to write a good abstract for a scientific paper or conference presentation},
}

@article{2014DerntlBasics,
urldate={2019-04-07},
volume={6},
langid={english},
issn={1753-5255, 1753-5263},
pages={105},
abstract={Publishing research results is an integral part of a researcher’s professional life. However, writing is not every researcher’s favourite activity, and getting a paper published can be a very tedious and time-consuming process. Fortunately, many of the obstacles along the writing and publishing path can be avoided by following some simple guidelines and practices. This paper presents a synthesis of guidelines found in literature about structuring and writing scientific papers. The paper outlines the process of publishing research papers in journals and conference proceedings, aiming to provide early-stage researchers with a handy introduction to essential issues. The paper takes an interdisciplinary stance by giving examples from technology-enhanced learning research and borrowing from literature in social, natural and computing sciences.},
groups={tesse:5},
doi={10.1504/IJTEL.2014.066856},
journaltitle={International Journal of Technology Enhanced Learning},
number={2},
author={Derntl, Michael},
date={2014},
title={Basics of research paper writing and publishing},
url={http://www.inderscience.com/link.php?id=66856},
}

@article{hanauer_one_nodate,
langid={english},
author={Hanauer, Sue},
title={One picture is worth a thousand words.},
pages={4},
}

@misc{2018ElsevierElsevier,
title={Elsevier How to publish in scholarly journals},
groups={tesse:5},
author={Elsevier},
date={2018},
}

@book{2013TurabianManual,
location={Chicago},
publisher={University of Chicago Press},
shorttitle={A Manual for Writers of Research Papers, Theses, and Dissertations, Eighth Edition},
title={A Manual for Writers of Research Papers, Theses, and Dissertations},
abstract={A little more than seventy-five years ago, Kate L. Turabian drafted a set of guidelines to help students understand how to write, cite, and formally submit research writing. Seven editions and more than nine million copies later, the name Turabian has become synonymous with best practices in research writing and style. Her Manual for Writers continues to be the gold standard for generations of college and graduate students in virtually all academic disciplines. Now in its eighth edition, A Manual for Writers of Research Papers, Theses, and Dissertations has been fully revised to meet the needs of today’s writers and researchers.The Manual retains its familiar three-part structure, beginning with an overview of the steps in the research and writing process, including formulating questions, reading critically, building arguments, and revising drafts. Part II provides an overview of citation practices with detailed information on the two main scholarly citation styles (notes-bibliography and author-date), an array of source types with contemporary examples, and detailed guidance on citing online resources.The final section treats all matters of editorial style, with advice on punctuation, capitalization, spelling, abbreviations, table formatting, and the use of quotations. Style and citation recommendations have been revised throughout to reflect the sixteenth edition of The Chicago Manual of Style. With an appendix on paper format and submission that has been vetted by dissertation officials from across the country and a bibliography with the most up-to-date listing of critical resources available, A Manual for Writers remains the essential resource for students and their teachers.},
groups={tesse:5},
pagetotal={464},
edition={8},
editor={Booth, Wayne C. and Colomb, Gregory G. and Williams, Joseph M. and Staff, University of Chicago Press},
date={2013-03-28},
author={Turabian, Kate L.},
isbn={978-0-226-81638-8},
}

@book{2013O′learyEssential,
isbn={978-1-4462-5897-2},
abstract={The Essential Guide to Doing Your Research Project 2e is the ultimate companion to successfully completing your research project. Warm and pragmatic, it gives you the skills and the confidence needed to succeed no matter what happens along the way. The book guides you through every step of your research project, from getting started to analysing data and writing up. Each stage is clearly set out, highlighting best practice and providing practical tips and down-to-earth advice for actually doing research. Key features include: Fully developed companion website including podcasts, worksheets, examples of real projects and links to journal articles Chapter summaries Boxed definitions of key terms Full glossary Suggestions for further reading Bursting with real world examples and multidisciplinary case studies, this book addresses the key questions posed by anyone hoping to complete a research project. It is the must-have textbook every student needs. Available with Perusall―an eBook that makes it easier to prepare for class Perusall is an award-winning eBook platform featuring social annotation tools that allow students and instructors to collaboratively mark up and discuss their SAGE textbook. Backed by research and supported by technological innovations developed at Harvard University, this process of learning through collaborative annotation keeps your students engaged and makes teaching easier and more effective. Learn more.},
author={O′Leary, Zina},
location={Los Angeles},
groups={tesse:5},
pagetotal={384},
date={2013-12-20},
title={The Essential Guide to Doing Your Research Project},
publisher={SAGE Publications Ltd},
edition={Second edition},
}

@article{2015SanyangEffect,
journaltitle={Polymers},
doi={10.3390/polym7061106},
url={http://www.mdpi.com/2073-4360/7/6/1106},
issn={2073-4360},
number={6},
title={Effect of Plasticizer Type and Concentration on Tensile, Thermal and Barrier Properties of Biodegradable Films Based on Sugar Palm (Arenga pinnata) Starch},
urldate={2019-04-07},
date={2015-06-18},
langid={english},
author={Sanyang, Muhammed and Sapuan, Salit and Jawaid, Mohammad and Ishak, Mohamad and Sahari, Japar},
volume={7},
pages={1106--1124},
}

@article{2016MendesBiodegradable,
date={2016-02},
issn={0144-8617},
volume={137},
langid={english},
author={Mendes, J.F. and Paschoalin, R.T and Carmona, V.B. and Sena Neto, Alfredo R and Marques, A.C.P. and Marconcini, J.M. and Mattoso, L.H.C. and Medeiros, E.S. and Oliveira, J.E.},
pages={452--458},
urldate={2019-04-07},
url={https://linkinghub.elsevier.com/retrieve/pii/S0144861715010735},
title={Biodegradable polymer blends based on corn starch and thermoplastic chitosan processed by extrusion},
abstract={Blends of thermoplastic cornstarch (TPS) and chitosan (TPC) were obtained by melt extrusion. The effect of TPC incorporation in TPS matrix and polymer interaction on morphology and thermal and mechanical properties were investigated. Possible interactions between the starch molecules and thermoplastic chitosan were assessed by XRD and FTIR techniques. Scanning Electron Microscopy (SEM) analyses showed a homogeneous fracture surface without the presence of starch granules or chitosan aggregates. Although the incorporation of thermoplastic chitosan caused a decrease in both tensile strength and stiffness, ﬁlms with better extensibility and thermal stability were produced.},
journaltitle={Carbohydrate Polymers},
doi={10.1016/j.carbpol.2015.10.093},
}

@article{2018WahyuningtiyasProperties,
groups={tesse:5},
number={1},
volume={2},
doi={10.17977/um016v2i12018p020},
date={2018-06-15},
issn={25800817, 25802402},
journaltitle={Journal of Mechanical Engineering Science and Technology},
title={Properties of Cassava Starch based Bioplastic Reinforced by Nanoclay},
pages={20--26},
abstract={Synthetic Synthetic plastic is chemical materials which cause severe environmental problems. Incinerating plastic waste leads to release of hazardous gases, which is not good for humans. Bioplastic can help reduce the dependence on fossil fuels and petroleum, that bioplastic can solve the problem of synthetic plastic use. This research aims to define the properties of the cassava starch-based bioplastic reinforced by nanoclay. Methods were experimental with bioplastic component of cassava starch, glycerol as plasticizer and nanoclay as reinforcement. The bioplastic was analyzed using XRD, tensile test, moisture absorption, biodegradability, and compared with another bioplastic. The results show that the addition of nanoclay into bioplastic results increasing the tensile strength of bioplastic also increases from 5.2 MPa to 6.3 MPa. This research revealed that complete degradation of nanoclay reinforced bioplastic could be achieved on the 6th day.},
author={Wahyuningtiyas, Nanang Eko and Suryanto, Heru},
url={http://journal2.um.ac.id/index.php/jmest/article/view/5066},
urldate={2019-04-07},
langid={english},
}

@article{2006HuangHigh,
urldate={2019-04-07},
volume={63},
abstract={Biodegradable nanocomposites have been successfully fabricated from the thermoplastic cornstarch (TPCS) and activated-montmorillonite (MMT) by melt-intercalation. TPCS was plasticized with novel plasticizers urea and formamide, and the activated-montmorillonites were obtained using citric acid as the activated solvent. Compared with urea and formamide-plasticized thermoplastic cornstarch (UFTPCS), the mechanical properties of nanocomposites were very good. The thermal analysis was investigated by Differential Scanning Calorimetry (DSC). The effect of water content on the mechanical properties of nanocomposites was studied. Dynamic Mechanical Thermal Analysis (DMTA) was also carried out. The structure and morphology of biodegradable nanocomposites were characterized by wide-angle X-ray diffraction (WAXD), scanning electron microscope (SEM) and transmission electron microscope (TEM). It was revealed that UFTPCS were intercalated into the layers of MMT successfully, and layers of MMT were fully exfoliated and so formed the exfoliated nanocomposites with MMT. This manufacturing process is simple and environmentally friendly.},
langid={english},
number={3},
date={2006-03-03},
pages={393--399},
doi={10.1016/j.carbpol.2005.09.006},
journaltitle={Carbohydrate Polymers},
author={Huang, M and Yu, J and Ma, X},
title={High mechanical performance MMT-urea and formamide-plasticized thermoplastic cornstarch biodegradable nanocomposites},
url={https://linkinghub.elsevier.com/retrieve/pii/S0144861705004406},
issn={0144-8617},
}

@article{2011VieiraNatural,
urldate={2019-04-07},
number={3},
pages={254--263},
author={Vieira, Melissa Gurgel Adeodato and da Silva, Mariana Altenhofen and dos Santos, Lucielen Oliveira and Beppu, Marisa Masumi},
doi={10.1016/j.eurpolymj.2010.12.011},
volume={47},
abstract={In recent years, much attention has been focused on research to replace petroleum-based commodity plastics, in a cost-effective manner, with biodegradable materials offering competitive mechanical properties. Biopolymers have been considered as the most promising materials for this purpose. However, they generally present poor mechanical properties regarding processability and end-use application, since the fragility and brittleness exhibited during thermoformation can limit their potential for application. In order to overcome this problem, plasticizers are added to provide the necessary workability to biopolymers. This class of products became more visible when biodegradable additives and plasticizers also became the focus of material scientists. The use of natural and/or biodegradable plasticizers, with low toxicity and good compatibility with several plastics, resins, rubber and elastomers in substitution of conventional plasticizers, such as phthalates and other synthetic conventional plasticizers attracted the market along with the increasing worldwide trend towards use of biopolymers. Here we discuss the main results and developments in natural plasticizer/synthetic and biopolymer-based ﬁlms during the last decades.},
issn={0014-3057},
title={Natural-based plasticizers and biopolymer films: A review},
url={https://linkinghub.elsevier.com/retrieve/pii/S0014305710004763},
journaltitle={European Polymer Journal},
langid={english},
date={2011-03},
shorttitle={Natural-based plasticizers and biopolymer films},
}

@article{rahmatiah_al_faruqy_properties_2016,
title={Properties of Bioplastic Sheets Made from Different Types of Starch Incorporated With Recycled Newspaper Pulp},
author={Rahmatiah Al Faruqy, M. Sujuthi and Liew, Kang Chang},
date={2016},
}

@article{2011YunosEffect,
author={Yunos, M. Z. B. and Rahman, WAWA},
title={Effect of glycerol on performance rice straw/starch based polymer},
number={13},
volume={11},
journaltitle={J. Appl. Sci},
pages={2456--2459},
date={2011},
doi={10.3923/jas.2011.2456.2459},
}

@incollection{2004VilpouxStarch,
date={2004},
langid={english},
pages={33},
booktitle={Technology, use and potentialities of Latin American starchy tubers},
author={Vilpoux, Olivier and Averous, Luc},
groups={tesse:5},
title={Starch-based plastics},
}

@thesis{2014NazriProduction,
institution={UNIVERSITI MALAYSIA PAHANG},
langid={english},
title={Production Of Bio Resin From Palm Oil},
groups={tesse:5},
date={2014},
type={candthesis},
author={Nazri, Nur Syazana Bt},
year={2014},
}

@article{2017XiongMicrosoft,
journaltitle={arXiv:1708.06073 [cs]},
author={Xiong, W. and Wu, L. and Alleva, F. and Droppo, J. and Huang, X. and Stolcke, A.},
abstract={We describe the 2017 version of Microsoft’s conversational speech recognition system, in which we update our 2016 system with recent developments in neural-network-based acoustic and language modeling to further advance the state of the art on the Switchboard speech recognition task. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a twostage approach, whereby subsets of acoustic models are ﬁrst combined at the senone/frame level, followed by a word-level voting via confusion networks. We also added a confusion network rescoring step after system combination. The resulting system yields a 5.1 /\\% word error rate on the 2000 Switchboard evaluation set.},
url={http://arxiv.org/abs/1708.06073},
groups={tesse:5},
date={2017-08-20},
eprint={1708.06073},
langid={english},
urldate={2019-04-07},
eprinttype={arxiv},
title={The Microsoft 2017 Conversational Speech Recognition System},
}

@inproceedings{2017LiAcoustic,
date={2017-08-20},
abstract={This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-LSTMs to model frequency variations. On the system level, improvements include adapting the model using Google Home speciﬁc data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of WER of 8-28 /\\% relative compared to the current production system.},
groups={tesse:5},
booktitle={Interspeech 2017},
langid={english},
urldate={2019-04-07},
url={http://www.isca-speech.org/archive/Interspeech///////\\_2017/abstracts/0234.html},
eventtitle={Interspeech 2017},
pages={399--403},
author={Li, Bo and Sainath, Tara N. and Narayanan, Arun and Caroselli, Joe and Bacchiani, Michiel and Misra, Ananya and Shafran, Izhak and Sak, Haşim and Pundak, Golan and Chin, Kean and Sim, Khe Chai and Weiss, Ron J. and Wilson, Kevin W. and Variani, Ehsan and Kim, Chanwoo and Siohan, Olivier and Weintraub, Mitchel and McDermott, Erik and Rose, Richard and Shannon, Matt},
doi={10.21437/Interspeech.2017-234},
publisher={ISCA},
title={Acoustic Modeling for Google Home},
}

@inproceedings{2017DhamdhereAnalyza,
urldate={2019-04-07},
langid={english},
url={http://dl.acm.org/citation.cfm?doid=3025171.3025227},
pages={493--504},
booktitle={Proceedings of the 22nd International Conference on Intelligent User Interfaces - IUI '17},
location={Limassol, Cyprus},
publisher={ACM Press},
eventtitle={the 22nd International Conference},
abstract={We describe Analyza, a system that helps lay users explore data. Analyza has been used within two large real world systems. The ﬁrst is a question-and-answer feature in a spreadsheet product. The second provides convenient access to a revenue/inventory database for a large sales force. Both user bases consist of users who do not necessarily have coding skills, demonstrating Analyza’s ability to democratize access to data.},
shorttitle={Analyza},
date={2017},
doi={10.1145/3025171.3025227},
author={Dhamdhere, Kedar and McCurley, Kevin S. and Nahmias, Ralfi and Sundararajan, Mukund and Yan, Qiqi},
title={Analyza: Exploring Data with Conversation},
isbn={978-1-4503-4348-0},
}

@article{2017SeeGet,
title={Get To The Point: Summarization with Pointer-Generator Networks},
urldate={2019-04-07},
url={http://arxiv.org/abs/1704.04368},
author={See, Abigail and Liu, Peter J. and Manning, Christopher D.},
date={2017-04-14},
eprint={1704.04368},
journaltitle={arXiv:1704.04368 [cs]},
eprinttype={arxiv},
abstract={Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
langid={english},
shorttitle={Get To The Point},
}

@article{2017YuLearning,
title={Learning to Skim Text},
abstract={Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classiﬁcation to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difﬁcult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classiﬁcation and automatic Q /\\&A, our proposed model, a modiﬁed LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.},
author={Yu, Adams Wei and Lee, Hongrae and Le, Quoc V.},
eprint={1704.06877},
journaltitle={arXiv:1704.06877 [cs]},
eprinttype={arxiv},
urldate={2019-04-07},
url={http://arxiv.org/abs/1704.06877},
groups={tesse:5},
langid={english},
date={2017-04-22},
}

@article{2016NallapatiAbstractive,
url={http://arxiv.org/abs/1602.06023},
urldate={2019-04-07},
langid={english},
author={Nallapati, Ramesh and Zhou, Bowen and santos, Cicero Nogueira dos and Gulcehre, Caglar and Xiang, Bing},
date={2016-02-18},
eprinttype={arxiv},
groups={tesse:5},
abstract={In this work, we model abstractive text summarization using Attentional EncoderDecoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-toword structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
title={Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond},
eprint={1602.06023},
journaltitle={arXiv:1602.06023 [cs]},
}

@article{2016MiyatoAdversarial,
langid={english},
eprinttype={arxiv},
urldate={2019-04-07},
groups={tesse:5},
date={2016-05-25},
author={Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
eprint={1605.07725},
abstract={Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overﬁtting.},
url={http://arxiv.org/abs/1605.07725},
journaltitle={arXiv:1605.07725 [cs, stat]},
title={Adversarial Training Methods for Semi-Supervised Text Classification},
}

@article{lu_best_2017,
date={2017},
author={Lu, Jiasen and Kannan, Anitha and Yang, Jianwei and Parikh, Devi and Batra, Dhruv},
langid={english},
pages={11},
title={Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model},
}

@online{noauthor_deep_2017,
date={2017},
langid={american},
abstract={Apple Machine Learning Journal publishes posts written by Apple engineers about their work using machine learning technologies to help build innovative products for millions of people around the world.},
shorttitle={Deep Learning for Siri’s Voice},
title={Deep Learning for Siri’s Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis - Apple},
url={https://machinelearning.apple.com/2017/08/06/siri-voices.html},
titleaddon={Apple Machine Learning Journal},
urldate={2019-04-07},
}

@online{2017Improving,
titleaddon={Apple Machine Learning Journal},
abstract={Apple Machine Learning Journal publishes posts written by Apple engineers about their work using machine learning technologies to help build innovative products for millions of people around the world.},
urldate={2019-04-07},
groups={tesse:2},
url={https://machinelearning.apple.com/2017/07/07/GAN.html},
title={Improving the Realism of Synthetic Images - Apple},
date={2017},
langid={american},
}

@article{2014HussainAffective,
langid={english},
url={https://linkinghub.elsevier.com/retrieve/pii/S0893608014001762},
author={Hussain, Amir and Cambria, Erik and Schuller, Björn and Howard, Newton},
groups={tesse:5},
doi={10.1016/j.neunet.2014.07.010},
date={2014-10},
journaltitle={Neural Networks},
title={Affective neural networks and cognitive learning systems for big data analysis},
issn={0893-6080},
pages={1--3},
urldate={2019-04-07},
volume={58},
}

@inproceedings{raza_disjunctive_2018,
date={2018},
title={Disjunctive Program Synthesis: a Robust Approach to Programming by Example},
pages={10},
eventtitle={AAAI 2018},
langid={english},
abstract={Programming by example (PBE) systems allow end users to easily create programs by providing a few input-output examples to specify their intended task. The system attempts to generate a program in a domain speciﬁc language (DSL) that satisﬁes the given examples. However, a key challenge faced by existing PBE techniques is to ensure the robustness of the programs that are synthesized from a small number of examples, as these programs often fail when applied to new inputs. This is because there can be many possible programs satisfying a small number of examples, and the PBE system has to somehow rank between these candidates and choose the correct one without any further information from the user. In this work we present a different approach to PBE in which the system avoids making a ranking decision at the synthesis stage, by instead synthesizing a disjunctive program that includes the many possible top-ranked programs as possible alternatives and selects between these different choices upon execution on a new input. This delayed choice brings the important beneﬁt of comparing the possible outputs produced by the different disjuncts on a given input at execution time. We present a generic framework for synthesizing such disjunctive programs in arbitrary DSLs, and describe two concrete implementations of disjunctive synthesis in the practical domains of data extraction from plain text and HTML documents. We present an evaluation showing the signiﬁcant increase in robustness achieved with our disjunctive approach, as illustrated by an increase from 59///////\\% to 93///////\\% of tasks for which correct programs can be learnt from a single example.},
author={Raza, Mohammad and Gulwani, Sumit},
}

@inproceedings{wang_show_2018,
pages={8},
date={2018},
eventtitle={AAAI 2018},
abstract={Impressive image captioning results (i.e., an objective description for an image) are achieved with plenty of training pairs. In this paper, we take one step further to investigate the creation of narrative paragraph for a photo stream. This task is even more challenging due to the difﬁculty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. The difﬁculty can even be exacerbated by the limited training data, so that existing approaches almost focus on searchbased solutions. To deal with these challenges, we propose a sequence-to-sequence modeling approach with reinforcement learning and adversarial training. First, to model the ordered photo stream, we propose a hierarchical recurrent neural network as story generator, which is optimized by reinforcement learning with rewards. Second, to generate relevant and story-style paragraphs, we design the rewards with two critic networks, including a multi-modal and a languagestyle discriminator. Third, we further consider the story generator and reward critics as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories, whereas the critics aim at distinguishing them and further improving the generator by policy gradient. Experiments on three widely-used datasets show the effectiveness, against state-of-the-art methods with relative increase of 20.2///////\\% by METEOR. We also show the subjective preference for the proposed approach over the baselines through a user study with 30 human subjects.},
langid={english},
title={Show, Reward and Tell: Automatic Generation of Narrative Paragraph from Photo Stream by Adversarial Training},
author={Wang, Jing and Fu, Jianlong and Tang, Jinhui and Li, Zechao and Mei, Tao},
}

@article{2014GraefeMemory,
issn={2150-8097},
journaltitle={Proceedings of the VLDB Endowment},
urldate={2019-04-07},
volume={8},
title={In-memory performance for big data},
langid={english},
groups={tesse:5},
author={Graefe, Goetz and Volos, Haris and Kimura, Hideaki and Kuno, Harumi and Tucek, Joseph and Lillibridge, Mark and Veitch, Alistair},
doi={10.14778/2735461.2735465},
url={http://dl.acm.org/citation.cfm?doid=2735461.2735465},
number={1},
date={2014-09-01},
pages={37--48},
}

@article{2014GiannakisSignal,
title={Signal Processing for Big Data [From the Guest Editors]},
date={2014-09},
pages={15--16},
urldate={2019-04-07},
journaltitle={IEEE Signal Processing Magazine},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6879633},
doi={10.1109/MSP.2014.2330054},
volume={31},
author={Giannakis, Georgios B. and Bach, Francis and Cendrillon, Raphael and Mahoney, Michael and Neville, Jennifer},
number={5},
groups={tesse:5},
langid={english},
issn={1053-5888},
}

@book{2011WittenData,
title={Data Mining: Practical Machine Learning Tools and Techniques},
edition={3 edition},
date={2011-01-20},
author={Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
publisher={Morgan Kaufmann},
abstract={Data Mining: Practical Machine Learning Tools and Techniques, Third Edition, offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. The book is targeted at information systems practitioners, programmers, consultants, developers, information technology managers, specification writers, data analysts, data modelers, database R///////\\&D professionals, data warehouse engineers, data mining professionals. The book will also be useful for professors and students of upper-level undergraduate and graduate-level data mining and machine learning courses who want to incorporate data mining as part of their data management knowledge base and expertise.Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projectsOffers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methodsIncludes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks―in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization},
shorttitle={Data Mining},
pagetotal={664},
isbn={978-0-12-374856-0},
location={Burlington, MA},
}

@article{2011ArnoldNefarious,
title={Nefarious numbers},
date={2011},
journaltitle={Notices of the AMS},
pages={434--437},
volume={58},
groups={tesse:5},
number={3},
author={Arnold, Douglas N. and Fowler, Kristine K.},
}

@article{2018OliveiraProspective,
issn={0016-3287},
author={Oliveira, Altina Silva and de Barros, Marta Duarte and de Carvalho Pereira, Fernanda and Gomes, Carlos Francisco Simões and da Costa, Helder Gomes},
pages={20--33},
journaltitle={Futures},
url={https://linkinghub.elsevier.com/retrieve/pii/S0016328716302476},
urldate={2019-04-07},
abstract={The use of prospective scenarios has been discussed by companies in diﬀerent sectors. As such, this work seeks to study the literature on the prospection of scenarios, permitting diﬀerent types of analyses and applications. A bibliometric study was performed on the Scopus database, accessed from the CAPES portal in April 2015 and updated in June 2017 in order to identify how the articles about the term prospective scenarios are presented in the literature. 87 articles on the subject were found indexed on Scopus, of which only 17 were from Brazil. It is expected, therefore, that this work will contribute to the construction of an overview of the existing literature on prospective scenarios in order to stimulate the interest of more researchers for the subject.},
langid={english},
shorttitle={Prospective scenarios},
volume={100},
date={2018-06},
title={Prospective scenarios: A literature review on the Scopus database},
doi={10.1016/j.futures.2018.03.005},
}

@book{2017KaehlerLearning,
edition={1 edition},
isbn={978-1-4919-3799-0},
date={2017-01-08},
author={Kaehler, Adrian and Bradski, Gary},
title={Learning OpenCV 3: Computer Vision in C++ with the OpenCV Library},
location={Sebastopol, CA},
publisher={O'Reilly Media},
abstract={Get started in the rapidly expanding field of computer vision with this practical guide. Written by Adrian Kaehler and Gary Bradski, creator of the open source OpenCV library, this book provides a thorough introduction for developers, academics, roboticists, and hobbyists. You’ll learn what it takes to build applications that enable computers to "see" and make decisions based on that data.With over 500 functions that span many areas in vision, OpenCV is used for commercial applications such as security, medical imaging, pattern and face recognition, robotics, and factory product inspection. This book gives you a firm grounding in computer vision and OpenCV for building simple or sophisticated vision applications. Hands-on exercises in each chapter help you apply what you’ve learned.This volume covers the entire library, in its modern C++ implementation, including machine learning tools for computer vision.Learn OpenCV data types, array types, and array operationsCapture and store still and video images with HighGUITransform images to stretch, shrink, warp, remap, and repairExplore pattern recognition, including face detectionTrack objects and motion through the visual fieldReconstruct 3D images from stereo visionDiscover basic and advanced machine learning techniques in OpenCV},
shorttitle={Learning OpenCV 3},
pagetotal={1024},
}

@book{2018TazehkandiComputer,
title={Computer Vision with OpenCV 3 and Qt5: Build visually appealing, multithreaded, cross-platform computer vision applications},
pagetotal={488},
edition={Edição: 1},
shorttitle={Computer Vision with OpenCV 3 and Qt5},
author={Tazehkandi, Amin Ahmadi},
abstract={Blend the power of Qt with OpenCV to build cross-platform computer vision applications Key Features ● Start creating robust applications with the power of OpenCV and Qt combined ● Learn from scratch how to develop cross-platform computer vision applications ● Accentuate your OpenCV applications by developing them with Qt Book Description Developers have been using OpenCV library to develop computer vision applications for a long time. However, they now need a more effective tool to get the job done and in a much better and modern way. Qt is one of the major frameworks available for this task at the moment. This book will teach you to develop applications with the combination of OpenCV 3 and Qt5, and how to create cross-platform computer vision applications. We’ll begin by introducing Qt, its IDE, and its SDK. Next you’ll learn how to use the OpenCV API to integrate both tools, and see how to configure Qt to use OpenCV. You’ll go on to build a full-fledged computer vision application throughout the book. Later, you’ll create a stunning UI application using the Qt widgets technology, where you’ll display the images after they are processed in an efficient way. At the end of the book, you’ll learn how to convert OpenCV Mat to Qt QImage. You’ll also see how to efficiently process images to filter them, transform them, detect or track objects as well as analyze video. You’ll become better at developing OpenCV applications. What you will learn ● Get an introduction to Qt IDE and SDK ● Be introduced to OpenCV and see how to communicate between OpenCV and Qt ● Understand how to create UI using Qt Widgets ● Learn to develop cross-platform applications using OpenCV 3 and Qt 5 ● Explore the multithreaded application development features of Qt5 ● Improve OpenCV 3 application development using Qt5 ● Build, test, and deploy Qt and OpenCV apps, either dynamically or statically ● See Computer Vision technologies such as filtering and transformation of images, detecting and matching objects, template matching, object tracking, video and motion analysis, and much more ● Be introduced to QML and Qt Quick for iOS and Android application development Who this book is for This book is for readers interested in building computer vision applications. Intermediate knowledge of C++ programming is expected. Even though no knowledge of Qt5 and OpenCV 3 is assumed, if you’re familiar with these frameworks, you’ll benefit. Table of Contents Introduction to Qt and OpenCV Creating our first Qt and OpenCV project Creating a comprehensive Qt+OpenCV project Mat and Qimage The Graphics View Framework Image Processing in OpenCV Features and Descriptors Multi-Threading Video Analysis Debugging and Testing Static Linking and Deployment Computer Vision Apps for Android and iOS},
publisher={Packt Publishing},
date={2018-01-02},
}

@book{2018JohnsonHands,
isbn={978-1-78883-935-8},
publisher={Packt Publishing},
shorttitle={Hands-On Functional Programming in Rust},
date={2018-05-31},
author={Johnson, Andrew},
abstract={Explore the support Rust offers for creating functional applications in Rust. Learn about various design patterns, implementing concurrency, metaprogramming, and so on in the processKey FeaturesLearn generics, organization, and design patterns in functional programmingModularize your applications and make them highly reusable and testable using functional design patternsGet familiar with complex concepts such as metaprogramming, concurrency, and immutabilityBook DescriptionFunctional programming allows developers to divide programs into smaller, reusable components that ease the creation, testing, and maintenance of software as a whole. Combined with the power of Rust, you can develop robust and scalable applications that fulfill modern day software requirements. This book will help you discover all the Rust features that can be used to build software in a functional way.We begin with a brief comparison of the functional and object-oriented approach to different problems and patterns. We then quickly look at the patterns of control flow, data the abstractions of these unique to functional programming. The next part covers how to create functional apps in Rust; mutability and ownership, which are exclusive to Rust, are also discussed. Pure functions are examined next and you'll master closures, their various types, and currying. We also look at implementing concurrency through functional design principles and metaprogramming using macros. Finally, we look at best practices for debugging and optimization.By the end of the book, you will be familiar with the functional approach of programming and will be able to use these techniques on a daily basis.What you will learnHow Rust supports the use of basic functional programming principlesUse functional programming to handle concurrency with eleganceRead and interpret complex type signatures for types and functionsImplement powerful abstractions using meta programming in RustCreate quality code formulaically using Rust's functional design patternsMaster Rust's complex ownership mechanisms particularly for mutabilityWho This Book Is ForThis book is for Rust developers who are comfortable with the language and now want to improve their coding abilities by learning advanced functional techniques to enhance their skillset and create robust and testable apps.Table of ContentsFunctional Programming - a comparisonFunctional Control FlowFunctional Data StructuresGenerics and PolymorphismCode Organization and Application ArchitectureMutability, Ownership, and Pure FunctionsDesign PatternsImplementing ConcurrencyMetaprogramming,Debugging, and Performance},
title={Hands-On Functional Programming in Rust: Build modular and reactive applications with functional programming techniques in Rust 2018},
pagetotal={292},
}

@book{noauthor_proceedings_2008,
date={2008},
title={Proceedings of the 8th International Conference on New Interfaces for Musical Expression},
isbn={13-978-88-901344-6-3},
}

@book{2015AllenGetting,
title={Getting Things Done: The Art of Stress-Free Productivity},
publisher={Penguin Books},
shorttitle={Getting Things Done},
author={Allen, David and Fallows, James},
abstract={The book Lifehack calls "The Bible of business and personal productivity.""A completely revised and updated edition of the blockbuster bestseller from 'the personal productivity guru'"—Fast CompanySince it was first published almost fifteen years ago, David Allen’s Getting Things Done has become one of the most influential business books of its era, and the ultimate book on personal organization. “GTD” is now shorthand for an entire way of approaching professional and personal tasks, and has spawned an entire culture of websites, organizational tools, seminars, and offshoots. Allen has rewritten the book from start to finish, tweaking his classic text with important perspectives on the new workplace, and adding material that will make the book fresh and relevant for years to come. This new edition of Getting Things Done will be welcomed not only by its hundreds of thousands of existing fans but also by a whole new generation eager to adopt its proven principles.},
edition={Revised edition},
date={2015-03-17},
pagetotal={352},
}

@mvbook{2016HallidayFundamentos,
pagetotal={1045},
author={Halliday, David and Resnick, Robert and Walker, Jearl},
volume={2},
publisher={LTC},
abstract={A nova edição do maior clássico de Física traz o melhor para você! Raspe o código promocional que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-vol2 e ganhe o e-book de Fundamentos de Física - Vol. 2 - Gravitação, Ondas e Termodinâmica + vídeos exclusivos de Física Experimental. Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10ª edição: • Módulos e Objetivos de Aprendizado - Os capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. • Capítulos Reformulados - Para facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. • Novos Exemplos, Perguntas e Problemas - 250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-vol2 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
date={2016-06-30},
volumes={4},
edition={Edição: 10},
title={Fundamentos de Fìsica - Vol. 2 - Gravitação, Ondas e Termodinâmica},
}

@mvbook{2016HallidayFundamentosa,
date={2016-06-30},
publisher={LTC},
volume={1},
abstract={A nova edição do maior clássico de Física traz o melhor para você! Raspe o código promocional que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-vol1 e ganhe o e-book de Fundamentos de Física - Vol. 1 - Mecânica + vídeos exclusivos de Física Experimental. Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10ª edição: • Módulos e Objetivos de Aprendizado - Os capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. • Capítulos Reformulados - Para facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. • Novos Exemplos, Perguntas e Problemas - 250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-vol1 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
author={Halliday, David and Resnick, Robert and Walker, Jearl},
edition={Edição: 10},
volumes={4},
pagetotal={1336},
title={Fundamentos de Física - Vol. 1 - Mecânica},
}

@mvbook{2016HallidayFundamentosb,
publisher={LTC},
edition={Edição: 10},
pagetotal={1379},
volumes={4},
abstract={A nova edição do maior clássico de Física traz o melhor para você! Raspe o código promocional que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-vol1 e ganhe o e-book de Fundamentos de Física - Vol. 4 - Óptica e Física Moderna + vídeos exclusivos de Física Experimental. Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10ª edição: • Módulos e Objetivos de Aprendizado - Os capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. • Capítulos Reformulados - Para facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. • Novos Exemplos, Perguntas e Problemas - 250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-vol4 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
volume={4},
date={2016-06-30},
author={Halliday, David and Resnick, Robert and Walker, Jearl},
title={Fundamentos de Física - Vol. 4 - Óptica e Física Moderna},
}

@mvbook{2016ResnickFundamentos,
volume={3},
date={2016-06-30},
edition={Edição: 10ª, Nova Edição},
author={Resnick, Robert and Walker, Jearl and Halliday, David},
volumes={4},
abstract={A nova edição do maior clássico de Física traz o melhor para você!Na compra do exemplar impresso, raspe o código promocional (PIN) da etiqueta que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-volume-3 e acesse 30 vídeos exclusivos de Física Experimental. E com o mesmo código PIN, oferecemos uma oferta exclusiva! Por apenas R$ 20,00, você poderá adquirir o e-book do volume de “Fundamentos de Física - Vol. 3 - Eletromagnetismo” e mais os vídeos completos (140 vídeos) de Física Experimental que acompanham o livro! Saiba mais em www.halliday.com.br ---------- Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10a edição:- Módulos e Objetivos de AprendizadoOs capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. - Capítulos ReformuladosPara facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. - Novos Exemplos, Perguntas e Problemas250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-volume-3 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
isbn={978-85-216-3037-1},
publisher={LTC},
title={Fundamentos de Física - Vol. 3 - Eletromagnetismo},
}

@book{1979AlexanderTimeless,
location={New York, NY},
pagetotal={552},
abstract={The theory of architecture implicit in our world today, Christopher Alexander believes, is bankrupt. More and more people are aware that something is deeply wrong. Yet the power of present-day ideas is so great that many feel uncomfortable, even afraid, to say openly that they dislike what is happening, because they are afraid to seem foolish, afraid perhaps that they will be laughed at. Now, at last, there is a coherent theory which describes in modern terms an architecture as ancient as human society itself. The Timeless Way of Building is the introductory volume in the Center for Environmental Structure series, Christopher Alexander presents in it a new theory of architecture, building, and planning which has at its core that age-old process by which the people of a society have always pulled the order of their world from their own being. Alexander writes, "There is one timeless way of building. It is thousands of years old, and the same today as it has always been. The great traditional buildings of the past, the villages and tents and temples in which man feels at home, have always been made by people who were very close to the center of this way. And as you will see, this way will lead anyone who looks for it to buildings which are themselves as ancient in their form as the trees and hills, and as our faces are."},
groups={tesse:5},
publisher={Oxford University Press},
isbn={978-0-19-502402-9},
author={Alexander, Christopher},
title={The Timeless Way of Building},
date={1979},
}

@book{2016SeverinoMetodologia,
isbn={978-85-249-2448-4},
publisher={Cortez},
date={2016},
abstract={Este livro tornou-se referência entre alunos e professores universitários, sendo uma das obras mais conceituadas sobre o tema. Amplamente indicado como leitura em instituições de ensino e cursos de todo país, trata-se de uma iniciação teórica, metodológica e prática ao trabalho científico, auxiliando desde a organização dos estudos e pesquisas científicas, até na elaboração de trabalhos acadêmicos. Em sua nova edição, traz noções básicas do uso do computador como ferramenta de elaboração de textos, de intercâmbio entre pesquisadores e de busca de referências, além de um capítulo sobre a contribuição da Internet à pesquisa.},
author={Severino, Antônio Joaquim},
location={São Paulo - SP},
title={Metodologia do Trabalho Cientifico},
}

@misc{2006HillerCarbon,
author={Hiller, Jonathan},
date={2006},
title={Carbon fiber vs wood as an acoustic guitar soundboard},
groups={tesse:5},
}

@misc{2006HillerComposite,
date={2006},
title={Composite acoustic guitar project},
groups={tesse:5},
author={Hiller, Jonathan},
}

@book{2016StrangIntroduction,
date={2016-06-10},
author={Strang, Gilbert},
publisher={Wellesley-Cambridge Press},
title={Introduction to Linear Algebra, Fifth Edition},
edition={Fifth Edition edition},
pagetotal={584},
isbn={978-0-9802327-7-6},
abstract={Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. This new fifth edition has become more than a textbook for the basic linear algebra course. That is its first purpose and always will be. The new chapters about applications of the SVD, probability and statistics, and Principal Component Analysis in finance and genetics, make it also a textbook for a second course, plus a resource at work. Linear algebra has become central in modern applied mathematics. This book supports the value of understanding linear algebra. Introduction to Linear Algebra, Fifth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by eight applications: differential equations in engineering, graphs and networks, statistics, Fourier methods and the FFT, linear programming, computer graphics, cryptography, Principal Component Analysis, and singular values. Audience: Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject. This text is for readers everywhere, with support from the websites and video lectures. Every chapter begins with a summary for efficient review. Contents: Chap. 1: Introduction to Vectors; Chap. 2: Solving Linear Equations; Chap. 3: Vector Spaces and Subspaces; Chap. 4: Orthogonality; Chap. 5: Determinants; Chap. 6: Eigenvalues and Eigenvectors; Chap. 7: Singular Value Decomposition; Chap. 8: Linear Transformations; Chap. 9: Complex Vectors and Matrices; Chap. 10: Applications; Chap. 11: Numerical Linear Algebra; Chap. 12: Linear Algebra in Probability and Statistics; Matrix Factorizations; Index; Six Great Theorems.},
location={Wellesley, MA},
}

@book{2013HallidayFundamentals,
date={2013-08-13},
edition={10 edition},
groups={tesse:5},
title={Fundamentals of Physics Extended},
isbn={978-1-118-23072-5},
author={Halliday, David and Resnick, Robert and Walker, Jearl},
pagetotal={1448},
publisher={Wiley},
location={Hoboken, NJ},
abstract={The 10th edition of Halliday's Fundamentals of Physics, Extended building upon previous issues by offering several new features and additions. The new edition offers most accurate, extensive and varied set of assessment questions of any course management program in addition to all questions including some form of question assistance including answer specific feedback to facilitate success. The text also offers multimedia presentations (videos and animations) of much of the material that provide an alternative pathway through the material for those who struggle with reading scientific exposition. Furthermore, the book includes math review content in both a self-study module for more in-depth review and also in just-in-time math videos for a quick refresher on a specific topic. The Halliday content is widely accepted as clear, correct, and complete. The end-of-chapters problems are without peer. The new design, which was introduced in 9e continues with 10e, making this new edition of Halliday the most accessible and reader-friendly book on the market. WileyPLUS sold separately from text.},
}

@book{2010BurnetteHello,
title={Hello, Android: Introducing Google's Mobile Development Platform},
location={Raleigh, N.C},
abstract={Google's Android is shaking up the mobile market in a big way. With Android, you can write programs that run on any compatible cell phone or tablet in the world. It's a mobile platform you can't afford not to learn, and this book gets you started. Hello, Android has been updated to Android 2.3.3, with revised code throughout to reflect this updated version. That means that the book is now up-to-date for tablets such as the Kindle Fire. All examples were tested for forwards and backwards compatibility on a variety of devices and versions of Android from 1.5 to 4.0. (Note: the Kindle Fire does not support home screen widgets or wallpaper, so those samples couldn't be tested on the Fire.)Android is an operating system for mobile phones and tablets. It's inside millions of cell phones and other devices, including the hugely popular Amazon Kindle Fire, making Android the foremost platform for mobile application developers. That could be your own program running on all those devices.Within minutes, Hello, Android will get you started creating your first working application: Android's version of "Hello, World." From there, you'll build up a more substantial example: an Android Sudoku game. By gradually adding features to the game, you'll learn the basics of Android programming. You'll also see how to build in audio and video support, add graphics using 2D and 3D OpenGL, network with web pages and web services, and store data with SQLite. You'll also learn how to publish your applications to the Android Market.The ///////\\#1 book for learning Android is now in its third edition. Every page and example was reviewed and updated for compatibility with the latest versions. Freshly added material covers installing applications to the SD card, supporting multi-touch, and creating live wallpaper. You'll also find plenty of real-world advice on how to support all major Android versions in use today.If you'd rather be coding than reading about coding, this book is for you.},
publisher={Pragmatic Bookshelf},
date={2010-08-07},
author={Burnette, Ed},
shorttitle={Hello, Android},
isbn={978-1-934356-56-2},
edition={Third edition},
pagetotal={280},
}

@book{2009KothariResearch,
edition={2nd revised edition edition},
publisher={New Age International Publishers},
groups={tesse:5},
author={Kothari, C. R.},
date={2009},
title={Research Methodology Methods and Techniques},
}

@book{2000DudaPattern,
location={New York},
publisher={Wiley-Interscience},
isbn={978-0-471-05669-0},
date={2000-11-09},
title={Pattern Classification},
pagetotal={688},
edition={2 edition},
author={Duda, Richard O. and Hart, Peter E. and Stork, David G.},
abstract={The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics.},
groups={tesse:5},
}

@book{2018GriffithsIntroduction,
publisher={Cambridge University Press},
author={Griffiths, David J. and Schroeter, Darrell F.},
isbn={978-1-107-18963-8},
edition={3 edition},
abstract={Changes and additions to the new edition of this classic textbook include a new chapter on symmetries, new problems and examples, improved explanations, more numerical problems to be worked on a computer, new applications to solid state physics, and consolidated treatment of time-dependent potentials.},
pagetotal={508},
groups={tesse:5, Quantum Mechanics},
date={2018-08-16},
title={Introduction to Quantum Mechanics},
location={Cambridge},
}

@book{2012AndersonStatistics,
publisher={Cengage Learning},
groups={tesse:5, Probability},
author={Anderson, Thomas A. Williams David R., Dennis J. Sweeney},
title={Statistics for Business and Economics},
isbn={978-1-285-02755-5},
date={2012},
}

@book{2014WhitePhysics,
abstract={This foundational text is written for students who want to go beyond the perceptual stage of music to learn how musical sound is created and perceived. It surveys a wide range of topics related to acoustics, beginning with a brief history of the art and science of music. Succeeding chapters explore the general principles of sound, musical scales, the primary ways in which sound can be generated, the characteristics of instruments, the use of mechanical and electronic recording devices, hi-fi stereophonic and quadraphonic sound, the design of electronic musical instruments, and architectural acoustics.Comprehensive yet accessible, Physics and Music includes over 300 diagrams, photographs, and tables. Each chapter concludes with questions, problems, and projects, in addition to references for further study. 1980 edition.},
edition={Reprint edition},
title={Physics and Music: The Science of Musical Sound},
publisher={Dover Publications},
isbn={978-0-486-77934-8},
pagetotal={448},
author={White, Harvey E. and White, Donald H.},
shorttitle={Physics and Music},
location={Mineola, New York},
date={2014-06-18},
}

@book{2005SurowieckiWisdom,
author={Surowiecki, James},
abstract={In this fascinating book, New Yorker business columnist James Surowiecki explores a deceptively simple idea: Large groups of people are smarter than an elite few, no matter how brilliant—better at solving problems, fostering innovation, coming to wise decisions, even predicting the future. With boundless erudition and in delightfully clear prose, Surowiecki ranges across fields as diverse as popular culture, psychology, ant biology, behavioral economics, artificial intelligence, military history, and politics to show how this simple idea offers important lessons for how we live our lives, select our leaders, run our companies, and think about our world.},
date={2005-08-16},
title={The Wisdom of Crowds},
edition={Reprint edition},
groups={tesse:5},
pagetotal={336},
publisher={Anchor},
isbn={978-0-385-72170-7},
location={New York, NY},
}

@book{2011KotlerPrinciples,
abstract={Learn how to create value and gain loyal customers. Today’s marketing challenge is to create vibrant, interactive communities of consumers who make products and brands a part of their daily lives. To help readers understand how to create value and gain loyal customers, Principles of Marketing presents fundamental marketing information in a comprehensive format, organized around an innovative customer-value framework. The fourteenth edition includes coverage on sustainability and a focus on marketing in today's challenging economic climate.},
pagetotal={744},
isbn={978-0-13-216712-3},
date={2011-02-06},
author={Kotler, Philip and Armstrong, Gary},
location={Boston},
publisher={Prentice Hall},
groups={tesse:5},
title={Principles of Marketing},
edition={14 edition},
}

@article{norman_innovation_2003,
author={Norman, E. W. L.},
shorttitle={Innovation in design and technology},
title={Innovation in design and technology: the polymer acoustic guitar and the case for the relegation of'the design process'},
date={2003},
}

@book{1999KotlerMarketing,
title={Marketing Management: Millennium Edition},
shorttitle={Marketing Management},
pagetotal={784},
edition={10 edition},
isbn={978-0-13-012217-9},
publisher={Prentice Hall},
date={1999-07-19},
abstract={This world-wide best-selling book highlights the most recent trends and developments in global marketingwith an emphasis on the importance of teamwork between marketing and all the other functions of the business. It introduces new perspectives in successful strategic market planning, and presents additional company examples of creative, market-focused, and customer-driven action. Coverage includes a focus on marketing in the 21st Century that introduces the new ideas, tools and practices companies will need to successfully operate in the New Millenium. Chapter topics discuss building customer satisfaction, market-oriented strategic planning, analyzing consumer markets and buyer behavior, dealing with the competition, designing pricing strategies and programs, and managing the sales force. For marketing managers who want to increase their understanding of the major issues of strategic, tactical, and administrative marketingalong with the opportunities and needs of the marketplace in the years ahead.},
location={Upper Saddle River, N.J},
author={Kotler, Philip},
}

@book{JolleySummation,
publisher={Dover Publications Inc.},
author={Jolley, L. B. W.},
location={New York},
abstract={Over 1,100 common series, all grouped for easy reference. Arranged by category, these series include arithmetical and geometrical progressions, powers and products of natural numbers, figurate and polygonal numbers, inverse natural numbers, exponential and logarithmic series, binomials, simple inverse products, factorials, trigonometrical and hyperbolic expansions, and additional series. 1961 edition.},
groups={tesse:2, Mathematics},
isbn={978-0-486-60023-9},
title={Summation of Series},
pagetotal={251},
edition={Second Revised Edition edition},
}

@article{arcanjo_o_2013,
url={https://www.academia.edu/4924894/O///////\\_Violao///////\\_de///////\\_Heitor///////\\_Villa-Lobos///////\\_entre///////\\_a///////\\_Belle///////\\_Epoque///////\\_e///////\\_as///////\\_rodas///////\\_de///////\\_choro},
urldate={2019-04-08},
langid={english},
date={2013},
abstract={O Violao de Heitor Villa-Lobos entre a Belle Epoque e as rodas de choro.},
title={O Violao de Heitor Villa-Lobos entre a Belle Epoque carioca e as rodas de choro.},
author={Arcanjo, Loque},
}

@inproceedings{2013RibeiroNumerical,
groups={tesse:5},
eventtitle={22nd International Congress of Mechanical Engineering},
booktitle={22nd International Congress of Mechanical Engineering},
title={Numerical Analysis of Acoustic Guitars Soundboards},
author={Ribeiro, Roseli},
date={2013},
}

@article{2016LeeMathematical,
author={Lee, Meng Koon and Hosseini Fouladi, Mohammad and Namasivayam, Satesh Narayana},
pages={1--10},
title={Mathematical Modelling and Acoustical Analysis of Classical Guitars and Their Soundboards},
url={https://www.hindawi.com/journals/aav/2016/6084230/},
urldate={2019-04-08},
volume={2016},
issn={1687-6261, 1687-627X},
langid={english},
date={2016},
journaltitle={Advances in Acoustics and Vibration},
doi={10.1155/2016/6084230},
}

@collection{2007BottouLarge,
series={Neural information processing series},
title={Large-scale kernel machines},
note={OCLC: ocm79002103},
publisher={The MIT Press},
editor={Bottou, Léon},
location={Cambridge, Mass},
groups={tesse:5},
pagetotal={396},
date={2007},
isbn={978-0-262-02625-3},
}

@article{yau_is_2013,
author={Yau, Alex and Murphy, Christian},
date={2013},
title={Is a Rigorous Agile Methodology the Best Development Strategy for Small Scale Tech Startups?},
pages={10},
abstract={Recently, Agile development processes have become popular in the software development community, and have been shown to be effective in large organizations. However, given that the communication and cooperation dynamics in startup companies are very different from that of larger, more established companies, and the fact that the initial focus of a startup might be significantly different from its ultimate goal, it is questionable whether a rigid process model that works for larger companies is appropriate in tackling the problems faced by a startup. When we scale down even further and observe the small scale startup with only a few members, many of the same problems that Agile methodology sets out to solve do not even exist. Then, for a small scale startup, is it still worth putting the resources into establishing a process model? Do the benefits of adopting an Agile methodology outweigh the opportunity cost of spending the resources elsewhere? This paper examines the advantages and disadvantages of adopting an Agile methodology in a small scale tech startup and compares it to other process models, such as the Waterfall model and Lean Startup. In determining whether a rigorous agile methodology is the best development strategy for small scale tech startups, we consider the metrics of cost, time, quality, and scope in light of the particular needs of small startup organizations, and present a case study of a company that has needed to answer this very question.},
langid={english},
}

@article{2018GhezziAgile,
url={https://linkinghub.elsevier.com/retrieve/pii/S014829631830300X},
title={Agile Business Model Innovation in Digital Entrepreneurship: Lean Startup Approaches},
abstract={Digital startups in the early stages of their development frequently undergo innovation to their value architecture and Business Model. A set of pragmatic methods drawing on lean and agile principles has recently been proposed to support digital entrepreneurs facing Business Model Innovation (BMI), known as Lean Startup Approaches (LSAs). However, the theoretical and practical relationship between BMI and LSAs in dynamic digital environments has seldom been investigated. To fill this gap, our study draws on an exploratory multiple-case study based on three digital multisided platform startups to craft a unified framework that can disclose the relationship between BMI, LSAs and Agile Development (AD), within the context of Strategic Agility. Our findings, which emerge from the unified framework, show that LSAs can be employed as agile methods to enable Business Model Innovation in Digital Entrepreneurship. These findings are then organized around a set of propositions, with the aim of developing a research agenda directed towards integrating BMI, LSAs and AD processes and methods.},
shorttitle={Agile Business Model Innovation in Digital Entrepreneurship},
pages={S014829631830300X},
date={2018-06},
author={Ghezzi, Antonio and Cavallo, Angelo},
journaltitle={Journal of Business Research},
urldate={2019-04-08},
langid={english},
doi={10.1016/j.jbusres.2018.06.013},
issn={0148-2963},
}

@incollection{2016DucMinimum,
doi={10.1007/978-3-319-33515-5///////\\_10},
location={Cham},
pages={118--130},
booktitle={Agile Processes, in Software Engineering, and Extreme Programming},
author={Duc, Anh Nguyen and Abrahamsson, Pekka},
urldate={2019-04-08},
shorttitle={Minimum Viable Product or Multiple Facet Product?},
date={2016},
title={Minimum Viable Product or Multiple Facet Product? The Role of MVP in Software Startups},
editor={Sharp, Helen and Hall, Tracy},
volume={251},
publisher={Springer International Publishing},
isbn={978-3-319-33514-8 978-3-319-33515-5},
}

@article{2014FlyvbjergWhat,
shorttitle={What You Should Know About Megaprojects and Why},
author={Flyvbjerg, Bent},
rights={© 2014 by the Project Management Institute},
number={2},
pages={6--19},
journaltitle={Project Management Journal},
doi={10.1002/pmj.21409},
issn={1938-9507},
title={What You Should Know About Megaprojects and Why: An Overview},
urldate={2019-04-08},
abstract={This paper takes stock of megaproject management, an emerging and hugely costly field of study, by first answering the question of how large megaprojects are by measuring them in the units of mega, giga, and tera, and concluding with how we are presently entering a new “tera era” of trillion-dollar projects. Second, total global megaproject spending is assessed, at US$6 to US$9 trillion annually, or 8///////\\% of the total global gross domestic product (GDP), which denotes the biggest investment boom in human history. Third, four “sublimes”—political, technological, economic, and aesthetic—are identified and used to explain the increased size and frequency of megaprojects. Fourth, the “iron law of megaprojects” is laid out and documented: Over budget, over time, over and over again. Moreover, the “break–fix model” of megaproject management is introduced as an explanation of the iron law. Fifth, Albert O. Hirschman's theory of the “Hiding Hand” is revisited and critiqued as unfounded and corrupting for megaproject thinking in both the academy and policy. Sixth, it is shown how megaprojects are systematically subject to “survival of the unfittest,” which explains why the worst projects get built rather than the best. Finally, it is argued that the conventional way of managing megaprojects has reached a “tension point,” in which tradition is being challenged and reform is emerging.},
langid={english},
date={2014},
volume={45},
}

@article{2008OruetaNew,
issn={1468-2427},
rights={© 2009 The Authors. Journal Compilation © 2009 Joint Editors and Blackwell Publishing Ltd},
volume={32},
doi={10.1111/j.1468-2427.2008.00829.x},
shorttitle={The New Mega-Projects},
title={The New Mega-Projects: Genesis and Impacts},
abstract={Critiques of urban renewal and large-scale developments were prominent in the period 1960–80. In particular, they emphasized the negative environmental and social consequences of these schemes and especially attacked them for displacing low-income and ethnically different populations. In the 1980s and 1990s, we saw a decline in such projects in many places, responding to popular protest and intellectual dissent, along with a new emphasis on preservation. More recently, however, we see the revival of mega-projects, often connected with tourism and sports development and incorporating the designs of world-famous architects. Frequently these are on landfill or abandoned industrial sites. The symposium for which this is an introduction shows the growing convergence of North American and European projects. This convergence is visible in their physical form, their financing, and in the role played by the state in a world marked by neoliberalism. At the same time, the new projects do display a greater environmental sensitivity and commitment to urbanity than the modernist schemes of an earlier epoch. Résumé Dans la période 1960–1980, les critiques sur les aménagements à grande échelle et les grandes rénovations urbaines étaient fréquentes. Elles soulignaient notamment les conséquences environnementales et sociales néfastes de ces programmes, en leur reprochant en particulier de déplacer les populations à faible revenu ou d'appartenance ethnique différente. Dans les années 1980 et 1990, ces projets se sont faits plus rares dans bien des endroits, répondant à la contestation populaire et au désaccord des intellectuels, parallèlement à une préoccupation nouvelle pour la préservation. Dernièrement, pourtant, les mégaprojets ont réapparu, souvent associés à un aménagement touristique ou sportif et intégrant des créations d'architectes de renommée mondiale. Ils se situent fréquemment sur le site d'anciennes décharges ou usines abandonnées. Le symposium dont ce texte sert d'introduction montre la convergence croissante des projets nord-américains et européens, convergence que l'on constate dans leur forme physique, leur financement et dans le rôle que joue l'État dans un monde empreint de néolibéralisme. En même temps, les nouveaux projets affichent une sensibilitéà l'environnement et un engagement vis-à-vis de l'urbanité plus marqués que les programmes modernistes antérieurs.},
pages={759--767},
author={Orueta, Fernando Diaz and Fainstein, Susan S.},
number={4},
journaltitle={International Journal of Urban and Regional Research},
urldate={2019-04-08},
langid={english},
date={2008},
}

@book{2013GreimanMegaproject,
title={Megaproject Management: Lessons on Risk and Project Management from the Big Dig},
langid={english},
shorttitle={Megaproject Management},
isbn={978-1-118-67109-2 978-1-118-11547-3},
doi={10.1002/9781118671092},
urldate={2019-04-08},
location={Hoboken, NJ, USA},
date={2013-06-13},
publisher={John Wiley ////////\\\& Sons, Inc.},
author={Greiman, Virginia A.},
}

@online{2008DoriaO,
titleaddon={Estadão},
abstract={Uma negociação a portas fechadas, no tempo dos militares, volta para assombrar o Brasil},
urldate={2019-04-08},
title={O verdadeiro preço de Itaipu},
author={Doria, Pedro},
date={2008},
langid={brazil},
url={https://alias.estadao.com.br/noticias/geral,o-verdadeiro-preco-de-itaipu,163784},
}

@online{2013JeronimoPonte,
date={2013-02-07},
langid={brazil},
author={Jeronimo, Josie},
url={https://istoe.com.br/274208///////\\_A+PONTE+DE+R+7+BILHOES/},
title={A ponte de R$ 7 bilhões},
abstract={Há muitas dúvidas sobre a necessidade da obra que ligará Salvador a Itaparica. A certeza é que o valor pago pelo governo da Bahia está superdimensionado},
titleaddon={ISTOÉ Independente},
urldate={2019-04-08},
}

@online{2014OtavioO,
title={O Globo - Ponte Rio Niterói},
url={http://infograficos.oglobo.globo.com/pais/ponte-rio-niteroi.html},
date={2014},
urldate={2019-04-08},
author={Otávio, Chico and Góes, Bruno},
titleaddon={O Globo},
}

@online{2018PrestesPrestes,
author={Prestes, Monica},
titleaddon={Folha de São Paulo},
urldate={2019-04-08},
langid={brazil},
title={Prestes a ser concluída, Belo Monte é criticada por atingidos e especialistas},
abstract={Mudança na vazão dos rios para abastecer Belo Monte muda fauna e hábitos///////\\&},
date={2018},
url={https://temas.folha.uol.com.br/projeto-amazonia/hidreletricas/prestes-a-ser-concluida-belo-monte-e-criticada-por-atingidos-e-especialistas.shtml},
}

@online{noauthor_em_2010,
title={Em dinheiro de hoje, Brasília custaria US$ 83 bilhões - Brasília 50 anos - iG},
url={https://ultimosegundo.ig.com.br/brasilia50anos/em-dinheiro-de-hoje-brasilia-custaria-us-83-bilhoes/n1237588758783.html},
urldate={2019-04-08},
abstract={Valor equivale a quase seis vezes o que o Brasil pretende investir nos Jogos Olímpicos de 2016},
date={2010-04-09},
titleaddon={Último Segundo},
langid={brazil},
}

@inreference{noauthor_estadio_2019,
langid={portuguese},
title={Estádio do Maracanã},
url={https://pt.wikipedia.org/w/index.php?title=Est///////\\%C3///////\\%A1dio///////\\_do///////\\_Maracan///////\\%C3///////\\%A3///////\\&oldid=54720810},
date={2019-04-05},
urldate={2019-04-08},
booktitle={Wikipédia, a enciclopédia livre},
note={Page Version ID: 54720810},
rights={Creative Commons Attribution-ShareAlike License},
abstract={Estádio Jornalista Mário Filho, mais conhecido como Maracanã, o popular Maraca ("semelhante a um chocalho" em tupi-guarani, devido ao som de pássaros que viviam por ali), é um estádio de futebol localizado na Zona Norte do Rio de Janeiro e inaugurado em 1950, durante o mandato do então General de Divisão e Prefeito do Distrito Federal do Rio de Janeiro Marechal Ângelo Mendes de Moraes, tendo sido utilizado na Copa do Mundo de Futebol daquele ano. Desde então, o Maracanã foi palco de grandes momentos do futebol brasileiro e mundial, como o milésimo gol de Pelé, finais do Campeonato Brasileiro, Carioca de Futebol, Taça Libertadores da América e do primeiro Campeonato Mundial de Clubes da FIFA, além de competições internacionais e partidas da Seleção Brasileira. O estádio foi um dos locais de competição dos Jogos Pan-Americanos de 2007, recebendo o futebol, as cerimônias de abertura e de encerramento. Sediou futebol e as cerimônias de abertura e encerramento dos Jogos Olímpicos de 2016, que foram realizados na cidade do Rio de Janeiro. Foi também o palco da partida final da Copa das Confederações de 2013 e da Copa do Mundo FIFA de 2014Ao longo do tempo, no entanto, o estádio passou a assumir caráter de espaço multiúso ao receber outros eventos como espetáculos e partidas de outros esportes, como o voleibol em uma oportunidade. Após diversas obras de modernização, a capacidade do estádio é de 78 838 espectadores, sendo o maior estádio do Brasil.},
}

@online{noauthor_maracana:_2013,
shorttitle={Maracanã},
titleaddon={VEJA.com},
date={2013},
title={Maracanã: ‘preço final’ é R$ 1,19 bi, 69///////\\% acima do previsto},
langid={brazil},
url={https://veja.abril.com.br/esporte/maracana-preco-final-e-r-119-bi-69-acima-do-previsto/},
urldate={2019-04-08},
abstract={Governo afirma que não haverá outras surpresas indesejadas para contribuinte},
}

@online{2018OttaTransposicao,
url={https://economia.estadao.com.br/noticias/geral,transposicao-custara-r-800-milhoes-ao-ano,70002268817},
author={Otta, Lu},
title={Transposição custará R$ 800 milhões ao ano - Economia},
titleaddon={Estadão},
urldate={2019-04-08},
date={2018},
langid={brazil},
abstract={Ministério da Transparência diz que se projeto de transposição do Rio São Francisco não se autossustentar conta será paga pelo Tesouro},
}

@online{noauthor_projeto_2018,
langid={portuguese},
date={2018-12-18},
url={https://www.huffpostbrasil.com/2018/12/18/projeto-de-integracao-do-rio-sao-francisco-uma-obra-que-ja-entrou-para-a-historia///////\\_a///////\\_23621697/},
urldate={2019-04-08},
titleaddon={HuffPost Brasil},
abstract={O projeto de integração do Rio São Franscisco é a mais importante iniciativa do governo federal em temos da Política Nacional de Recursos Hídricos.},
shorttitle={Projeto de Integração do Rio São Francisco},
title={Projeto de Integração do Rio São Francisco: uma obra que já entrou para a história},
}

@online{noauthor_brasil_2015,
title={Brasil tem 6 das 100 obras mais importantes do mundo},
date={2015-01-20},
titleaddon={Brasil 247},
url={//www.brasil247.com/pt/247/brasil/167233/Brasil-tem-6-das-100-obras-mais-importantes-do-mundo.htm},
urldate={2019-04-08},
abstract={Seis grandes obras de infraestrutura do Brasil estão entre as 100 mais importantes do mundo, de acordo com lista feita pela consultoria internacional KPMG; quatro delas têm o carimbo do Programa de Aceleração do Crescimento (PAC), do governo federal, Seis grandes obras de infraestrutura do Brasil estão entre as 100 mais importantes do mundo, de acordo com lista feita pela consultoria internacional KPMG; quatro delas têm o carimbo do Programa de Aceleração do Crescimento (PAC), do governo federal},
langid={brazil},
}

@inreference{noauthor_ef-354_2019,
langid={portuguese},
date={2019-03-03},
urldate={2019-04-08},
url={https://pt.wikipedia.org/w/index.php?title=EF-354///////\\&oldid=54416427},
title={EF-354},
note={Page Version ID: 54416427},
booktitle={Wikipédia, a enciclopédia livre},
abstract={A Ferrovia Transcontinental - EF-354 (também referida como Ferrovia Transoceânica), é o projeto de uma ferrovia firmado entre os governos do Brasil e Peru, que busca conectar o Oceano Atlântico, no litoral brasileiro ao Oceano Pacífico no litoral peruano, atravessando de Leste a Oeste o continente Sul-americano. A extensão total é estimada em 4.400 km em solo brasileiro.},
rights={Creative Commons Attribution-ShareAlike License},
}

@online{2018MaiaFerrovia,
urldate={2019-04-08},
title={Ferrovia Transoceânica começa a sair do papel},
author={Maia, Tião},
date={2018},
url={https://www.expressoamazonia.com.br/index.php/economia/413-ferrovia-transoceanica-comeca-a-sair-do-papel.html},
titleaddon={Expresso Amazônia},
}

@online{2013NederCusto,
abstract={Valor foi estimado por consultoria internacional e é quatro vezes superior à projeção da ANP},
date={2013},
url={https://economia.estadao.com.br/noticias/geral,custo-para-explorar-pre-sal-de-libra-pode-chegar-a-us-400-bilhoes,169408e},
title={Custo para explorar pré-sal de Libra pode chegar a US$ 400 bilhões - Economia},
urldate={2019-04-09},
author={Neder, Vinícius},
titleaddon={Estadão},
langid={brazil},
}

@online{noauthor_angra_2000,
urldate={2019-04-09},
url={https://www1.folha.uol.com.br/fsp/dinheiro/fi2307200009.htm},
title={Angra 2 começa a funcionar após gastar R$ 12 bilhões},
date={2000},
titleaddon={Folha de São Paulo},
}

@inreference{noauthor_usina_2019,
urldate={2019-04-09},
note={Page Version ID: 54490770},
url={https://pt.wikipedia.org/w/index.php?title=Usina///////\\_Hidrel///////\\%C3///////\\%A9trica///////\\_Santo///////\\_Ant///////\\%C3///////\\%B4nio///////\\&oldid=54490770},
langid={portuguese},
title={Usina Hidrelétrica Santo Antônio},
booktitle={Wikipédia, a enciclopédia livre},
abstract={A Hidrelétrica Santo Antônio está localizada no Rio Madeira, na cidade de Porto Velho, capital de Rondônia. Possui 50 turbinas do tipo Bulbo para geração de energia elétrica com potência de cerca de 71,6 megawatts (MW) cada uma, totalizando 3.568,3 MW de potência instalada e 2.424 MW de energia assegurada. É a quarta maior hidrelétrica em operação no Brasil e uma das maiores do mundo. A concessionária responsável pela hidrelétrica é a Santo Antônio Energia, atualmente quarta maior geradora hídrica do país, formada pelas empresas Odebrecht Energia do Brasil, SAAG Investimentos, Furnas Centrais Elétricas, Cemig e Caixa FIP Amazônia Energia. A hidrelétrica, juntamente com a de Jirau, no mesmo rio, são consideradas fundamentais para o suprimento de energia elétrica no Brasil e estiveram entre as obras mais importantes do Governo Federal entre 2008 e 2016.O leilão de concessão foi realizado em dezembro de 2007. Os estudos de inventário e viabilidade aconteceram previamente entre os anos de 2001 e 2006. Em 2008 as obras foram iniciadas. Em 30 de março a hidrelétrica recebeu autorização da Agência Nacional de Energia Elétrica (Aneel) para iniciar sua operação. As obras de construção foram concluías em dezembro de 2016.},
rights={Creative Commons Attribution-ShareAlike License},
date={2019-03-12},
}

@article{2012Wang“leagile”,
url={https://linkinghub.elsevier.com/retrieve/pii/S0164121212000404},
volume={85},
langid={english},
pages={1287--1299},
shorttitle={“Leagile” software development},
journaltitle={Journal of Systems and Software},
date={2012-06},
issn={0164-1212},
title={“Leagile” software development: An experience report analysis of the application of lean approaches in agile software development},
urldate={2019-04-09},
abstract={In recent years there has been a noticeable shift in attention from those who use agile software development toward lean software development, often labelled as a shift “from agile to lean”. However, the reality may not be as simple or linear as this label implies. To provide a better understanding of lean software development approaches and how they are applied in agile software development, we have examined 30 experience reports published in past agile software conferences in which experiences of applying lean approaches in agile software development were reported. The analysis identified six types of lean application. The results of our study show that lean can be applied in agile processes in different manners for different purposes. Lean concepts, principles and practices are most often used for continuous agile process improvement, with the most recent introduction being the kanban approach, introducing a continuous, flow-based substitute to time-boxed agile processes.},
number={6},
author={Wang, Xiaofeng and Conboy, Kieran and Cawley, Oisin},
doi={10.1016/j.jss.2012.01.061},
}

@misc{2013SirkiaeLean,
title={Lean and agile financial planning},
author={Sirkiä, Rami and Laanti, Maarit},
date={2013},
url={http://www.scrummaster.dk/lib/AgileLeanLibrary/Topics////////\\_ScalingScrumAgile/Whitepaper///////\\_///////\\%20Lean-Agile///////\\%20Financial///////\\%20Planning///////\\%20(Dec///////\\%202013).pdf},
urldate={2019-04-09},
}

@book{1994DudequeHistoria,
isbn={85-85132-85-X},
date={1994},
groups={tesse:5},
title={História do violão},
author={Dudeque, Norton},
}

@book{2012CostaIntroducao,
date={2012},
title={Introdução Ilustrada à Estatística},
publisher={Harbra},
abstract={No Brasil, poucos livros, principalmente de Estatística, passam da 2ª ou 3ª edição. O sucesso desta obra, agora em 5.ª edição, totalmente revista, deve-se, indiscutivelmente, à preocupação do autor com a organização e apresentação da matéria e à calorosa acolhida que a obra vem recebendo por parte dos usuários desde seu lançamento em 1988. Continua disponível, aos usuários que contatem a Editora por meio de formulário incluso no livro, o gabarito da Curva Normal, instrumento grandemente facilitador na resolução de alguns exercícios e problemas que dependam da visualização do gráfico, além das respostas aos exercícios propostos no livro. Aos professores estão disponíveis as soluções dos exercícios propostos no livro do aluno, além de exercícios e problemas adicionais. O gabarito com a Curva Normal também faz parte desse material. Sem dúvida, uma obra que prioriza a compreensão por parte dos alunos.},
isbn={978-85-294-0419-6},
author={Costa, S. F.},
edition={Edição: 1ª},
}

@inproceedings{2018KarrasProgressive,
booktitle={ICLR 2018},
groups={tesse:5},
title={Progressive Growing Of Gans For Improved Quality, Stability, And Variation},
date={2018},
pages={26},
abstract={We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly ﬁne details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
langid={english},
year={2018},
author={Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
}

@article{2018EslamiNeural,
pages={1204--1210},
groups={tesse:5},
abstract={Scene representation—the process of converting visual sensory data into concise descriptions—is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
doi={10.1126/science.aar6170},
author={Eslami, S. M. Ali and Jimenez Rezende, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
urldate={2019-04-16},
title={Neural scene representation and rendering},
issn={0036-8075, 1095-9203},
volume={360},
langid={english},
journaltitle={Science},
number={6394},
date={2018-06-15},
}

@inproceedings{2017ChivukulaAdversarial,
doi={10.1109/IJCNN.2017.7966196},
location={Anchorage, AK, USA},
publisher={IEEE},
url={http://ieeexplore.ieee.org/document/7966196/},
eventtitle={2017 International Joint Conference on Neural Networks (IJCNN)},
title={Adversarial learning games with deep learning models},
urldate={2019-04-16},
date={2017-05},
isbn={978-1-5090-6182-2},
pages={2758--2767},
author={Chivukula, Aneesh Sreevallabh and Liu, Wei},
booktitle={2017 International Joint Conference on Neural Networks (IJCNN)},
}

@article{2018TuylsSymmetric,
date={2018-12},
journaltitle={Scientific Reports},
number={1},
url={http://www.nature.com/articles/s41598-018-19194-4},
urldate={2019-04-16},
issn={2045-2322},
title={Symmetric Decomposition of Asymmetric Games},
doi={10.1038/s41598-018-19194-4},
volume={8},
author={Tuyls, Karl and Pérolat, Julien and Lanctot, Marc and Ostrovski, Georg and Savani, Rahul and Leibo, Joel Z. and Ord, Toby and Graepel, Thore and Legg, Shane},
pages={1015},
groups={tesse:5, Game Theory},
langid={english},
}

@article{2019ParkSemantic,
author={Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
journaltitle={arXiv:1903.07291 [cs]},
eprint={1903.07291},
abstract={We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style as synthesizing images. Code will be available at https://github.com/NVlabs/SPADE .},
date={2019-03-18},
title={Semantic Image Synthesis with Spatially-Adaptive Normalization},
url={http://arxiv.org/abs/1903.07291},
urldate={2019-04-16},
groups={tesse:5},
eprinttype={arxiv},
}

@book{1984CremerPhysics,
abstract={This major work covers almost all that has been learned about the acoustics of stringed instruments from Helmholtz's 19th-century theoretical elaborations to recent electroacoustic and holographic measurements. Many of the results presented here were uncovered by the author himself (and by his associates and students) over a 20-year period of research on the physics of instruments in the violin family. Lothar Cremer is one of the world's most respected authorities on architectural acoustics and, not incidentally, an avid avocational violinist and violist.The book―which was published in German in 1981―first of all meets the rigorous technical standards of specialists in musical acoustics. But it also serves the needs and interests of two broader groups: makers and players of stringed instruments are expressly addressed, since the implications of the mathematical formulations are fully outlined and explained; and acousticians in general will find that the work represents a textbook illustration of the application of fundamental principles and up-to-date techniques to a specific problem. The first―and longest―of the book's three parts investigates the oscillatory responses of bowed (and plucked) strings. The natural nonlinearities that derive from considerations of string torsion and bending stiffness are deftly handled and concisely modeled. The second part deals with the body of the instrument. Special attention is given to the bridge, which transmits the oscillations of the strings to the wooden body and its air cavity. In this case, linear modeling proves serviceable for the most part―a simplification that would not be possible with lute―like instruments such as the guitar. The radiation of sound from the body into the listener's space, which is treated as an extension of the instrument itself, is the subject of the book's final part.},
author={Cremer, Lothar},
isbn={978-0-262-52707-1},
title={The Physics of the Violin},
publisher={The MIT Press},
edition={UK ed. edition},
date={1984-11-14},
pagetotal={474},
translator={Allen, John S.},
}

@article{2016FriedPerspective,
pages={1--10},
number={4},
author={Fried, Ohad and Shechtman, Eli and Goldman, Dan B. and Finkelstein, Adam},
url={http://dl.acm.org/citation.cfm?doid=2897824.2925933},
abstract={This paper introduces a method to modify the apparent relative pose and distance between camera and subject given a single portrait photo. Our approach ﬁts a full perspective camera and a parametric 3D head model to the portrait, and then builds a 2D warp in the image plane to approximate the effect of a desired change in 3D. We show that this model is capable of correcting objectionable artifacts such as the large noses sometimes seen in “selﬁes,” or to deliberately bring a distant camera closer to the subject. This framework can also be used to re-pose the subject, as well as to create stereo pairs from an input portrait. We show convincing results on both an existing dataset as well as a new dataset we captured to validate our method.},
doi={10.1145/2897824.2925933},
journaltitle={ACM Transactions on Graphics},
issn={0730-0301},
urldate={2019-04-18},
volume={35},
groups={tesse:5},
langid={english},
title={Perspective-aware manipulation of portrait photos},
date={2016-07-11},
}

@article{tahvanainen_modelling_nodate,
abstract={In this thesis, it is shown that the modiﬁed kantele includes vibrational modes of both the freely vibrating top plate and the enclosed air. Thus, it has a higher mode density than the traditional kanteles. Because of the coupling of the enclosed air modes to the body, the modiﬁed kantele radiates more omni-directionally than the traditional kanteles. Consequently, the modiﬁed kantele has a higher radiation eﬃciency than the traditional kanteles when the size of the air gap is small (1-3 mm).},
author={Tahvanainen, Henna},
title={Modelling body vibration and sound radiation of a modiﬁed kantele},
pages={73},
langid={english},
}

@software{2019VdumoulinTechnical,
shorttitle={A technical report on convolution arithmetic in the context of deep learning},
author={vdumoulin},
title={A technical report on convolution arithmetic in the context of deep learning: vdumoulin/convarithmetic},
rights={MIT},
note={original-date: 2016-02-24T15:18:33Z},
urldate={2019-04-18},
date={2019-04-18},
url={https://github.com/vdumoulin/conv///////\\_arithmetic},
}

@inproceedings{2014WangEnergy,
date={2014-03},
pages={1--2},
title={Energy efficient neural networks for big data analytics},
author={Wang, Y. and Li, B. and Luo, R. and Chen, Y. and Xu, N. and Yang, H.},
eventtitle={2014 Design, Automation Test in Europe Conference Exhibition (DATE)},
doi={10.7873/DATE.2014.358},
abstract={The world is experiencing a data revolution to discover knowledge in big data. Large scale neural networks are one of the mainstream tools of big data analytics. Processing big data with large scale neural networks includes two phases: the training phase and the operation phase. Huge computing power is required to support the training phase. And the energy efficiency (power efficiency) is one of the major considerations of the operation phase. We first explore the computing power of GPUs for big data analytics and demonstrate an efficient GPU implementation of the training phase of large scale recurrent neural networks (RNNs). We then introduce a promising ultrahigh energy efficient implementation of neural networks' operation phase by taking advantage of the emerging memristor technique. Experiment results show that the proposed GPU implementation of RNNs is able to achieve 2 11× speed-up compared with the basic CPU implementation. And the scaled-up recurrent neural network trained with GPUs realizes an accuracy of 47 /\\% on the Microsoft Research Sentence Completion Challenge, the best result achieved by a single RNN on the same dataset. In addition, the proposed memristor-based implementation of neural networks demonstrates power efficiency of > 400 GFLOPS/W and achieves energy savings of 22× on the HMAX model compared with its pure digital implementation counterpart.},
groups={tesse:5},
booktitle={2014 Design, Automation Test in Europe Conference Exhibition (DATE)},
}

@inproceedings{2014KapralovaBig,
booktitle={Conference of the International Speech Communication Association (Interspeech)},
author={Kapralova, Olga and Alex, John and Weinstein, Eugene and Moreno, Pedro and Siohan, Olivier},
title={A big data approach to acoustic model training corpus selection},
groups={tesse:5},
date={2014},
}

@inproceedings{2018LiptonBbq,
author={Lipton, Zachary and Li, Xiujun and Gao, Jianfeng and Li, Lihong and Ahmed, Faisal and Deng, Li},
title={Bbq-networks: Efficient exploration in deep reinforcement learning for task-oriented dialogue systems},
shorttitle={Bbq-networks},
date={2018},
booktitle={Thirty-Second AAAI Conference on Artificial Intelligence},
}

@inproceedings{2018KimKorean,
publisher={International Speech Communication Association},
title={Korean Singing Voice Synthesis System based on an LSTM Recurrent Neural Network},
groups={tesse:5},
booktitle={INTERSPEECH 2018},
date={2018},
author={Kim, Juntae and Choi, Heejin and Park, Jinuk and Kim, Sangjin and Kim, Jongjin and Hahn, Minsoo},
}

@inproceedings{2016NishimuraSinging,
author={Nishimura, Masanari and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
booktitle={Interspeech},
title={Singing Voice Synthesis Based on Deep Neural Networks.},
pages={2478--2482},
date={2016},
}

@inproceedings{2014ZenDeep,
eventtitle={ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn={978-1-4799-2893-4},
booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date={2014-05},
title={Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis},
pages={3844--3848},
location={Florence, Italy},
url={http://ieeexplore.ieee.org/document/6854321/},
publisher={IEEE},
doi={10.1109/ICASSP.2014.6854321},
urldate={2019-04-18},
abstract={Statistical parametric speech synthesis (SPSS) using deep neural networks (DNNs) has shown its potential to produce naturally-sounding synthesized speech. However, there are limitations in the current implementation of DNN-based acoustic modeling for speech synthesis, such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations, this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech.},
langid={english},
author={Zen, Heiga and Senior, Andrew},
}

@article{2017BlaauwNeurala,
journaltitle={Applied Sciences},
author={Blaauw, Merlijn and Bonada, Jordi},
title={A neural parametric singing synthesizer modeling timbre and expression from natural songs},
date={2017},
pages={1313},
number={12},
volume={7},
}

@article{fosler-lussier_markov_nodate,
abstract={This tutorial gives a gentle introduction to Markov models and Hidden Markov models as mathematical abstractions, and relates them to their use in automatic speech recognition. This material was developed for the Fall 1995 semester of CS188: Introduction to Arti cial Intelligence at the University of California, Berkeley. It is targeted for introductory AI courses basic knowledge of probability theory (e.g. Bayes' Rule) is assumed. This version is slightly updated from the original, including a few minor error corrections, a short /Further Reading" section, and exercises that were given as a homework in the Fall 1995 class.},
title={Markov Models and Hidden Markov Models: A Brief Tutorial},
author={Fosler-Lussier, Eric},
pages={9},
langid={english},
}

@article{2019GafniVid2game,
journaltitle={arXiv:1904.08379 [cs, stat]},
url={http://arxiv.org/abs/1904.08379},
author={Gafni, Oran and Wolf, Lior and Taigman, Yaniv},
date={2019-04-17},
abstract={We are given a video of a person performing a certain activity, from which we extract a controllable model. The model generates novel image sequences of that person, according to arbitrary user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. The method is based on two networks. The first network maps a current pose, and a single-instance control signal to the next pose. The second network maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes.},
urldate={2019-04-21},
groups={tesse:5},
shorttitle={Vid2Game},
eprint={1904.08379},
eprinttype={arxiv},
title={Vid2Game: Controllable Characters Extracted from Real-World Videos},
}

@book{2018SubramanianDeep,
date={2018},
location={Birmingham, UK},
publisher={Packt Publishing},
shorttitle={Deep learning with PyTorch},
isbn={978-1-78862-607-1},
langid={english},
author={Subramanian, Vishnu},
title={Deep learning with PyTorch a practical approach to building neural network models using PyTorch},
url={http://proxy2.hec.ca/login?url=http://proquestcombo.safaribooksonline.com/?uiCode=hecmontreal///////\\&xmlId=9781788624336},
note={OCLC: 1078352321},
urldate={2019-03-26},
}

@book{2003LakatosFundamentos,
date={2003},
note={OCLC: 53849497},
groups={tesse:5},
title={Fundamentos de metodologia científica},
publisher={Atlas São Paulo},
author={Lakatos, Eva Maria and Marconi, Marina de Andrade},
langid={portuguese},
location={São Paulo},
isbn={978-85-224-3397-1},
}

@electronic{2014Berkeley,
url={http://bair.berkeley.edu/},
date={2014},
title={The Berkeley Artificial Intelligence Research Blog},
urldate={2018-06-30},
groups={tesse:4},
howpublished={online},
}

@electronic{2018Caffe,
title={Caffe Deep Learning Framework},
urldate={2018-06-30},
date={2018},
howpublished={online},
url={http://caffe.berkeleyvision.org/},
}

@electronic{2018Making,
date={2018},
title={Making music using new sounds generated with machine learning},
url={https://www.blog.google/technology/ai/making-music-using-new-sounds-generated-machine-learning/},
urldate={2018-06-30},
howpublished={online},
publisher={<},
groups={tesse:4},
location={Disponvel em},
}

@electronic{2015Ivy,
date={2015},
url={http://www.ivyaudio.com/},
groups={tesse:4},
howpublished={online},
urldate={2018-06-30},
title={Ivy Audio},
}

@electronic{2018Keras,
title={Keras: The Python Deep Learning library},
urldate={2018-06-30},
publisher={<},
location={Keras Documentation, [s.d.]. Disponvel em},
date={2018},
url={https://keras.io/},
howpublished={online},
}

@electronic{2018Magenta,
url={https://magenta.tensorflow.org/},
groups={tesse:4},
publisher={<},
title={Magenta},
location={Magenta Disponvel em},
date={2018},
howpublished={online},
urldate={2018-06-30},
}

@electronic{2011University,
url={http://theremin.music.uiowa.edu/MIS.html},
unidentified={Disponvel em: <},
title={University of Iowa Electronic Music Studios},
groups={tesse:4},
volume={2018},
date={2011},
urldate={2018-06-30},
howpublished={online},
}

@electronic{2018Neon,
date={2018},
url={https://ai.intel.com/neon/},
urldate={2018-06-30},
title={Neon},
groups={tesse:4},
howpublished={online},
}

@electronic{2018Nsynthsuper,
date={2018},
publisher={<},
title={NSynthSuper},
url={https://nsynthsuper.withgoogle.com/},
urldate={2018-06-30},
location={GoogleGoogle Disponvel em},
groups={tesse:4},
howpublished={online},
}

@article{1986RumelhartLearning,
volume={323},
date={1986-10-09},
author={Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
title={Learning representations by back-propagating errors},
day={09},
journaltitle={Nature},
publisher={Nature Publishing Group SN -},
doi={10.1038/323533a0},
pages={533 EP -},
}

@electronic{2018Torch,
date={2018},
title={Torch, a scientific computing framework for LuaJIT},
url={http://torch.ch/},
urldate={2018-06-30},
howpublished={online},
}

@electronic{2018Philharmonia,
urldate={2018-06-30},
groups={tesse:4},
date={2018},
howpublished={online},
unidentified={Disponvel em: <},
volume={2018},
url={http://www.philharmonia.co.uk/explore/sound///////\\_samples},
title={Philharmonia Orchestra},
}

@electronic{2018Tensorflow,
groups={tesse:4},
urldate={2018-06-30},
location={TensorFlow Disponvel em},
url={https://www.tensorflow.org/},
date={2018},
howpublished={online},
publisher={<},
title={TensorFlow},
}

@electronic{2018Repositorio,
title={Repositório Dissertação},
howpublished={online},
url={https://github.com/tesserato/tesserato.github.io},
date={2018},
urldate={2018-06-30},
}

@electronic{2018Theano,
title={Theano 1.0.0 documentation},
howpublished={online},
urldate={2018-06-30},
date={2018},
url={http://deeplearning.net/software/theano/},
}

@electronic{2012Rmsprop,
volume={4},
url={https://www.coursera.org/lecture/neural-networks/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude-YQHki},
date={2012},
urldate={2018-06-30},
howpublished={online},
title={Rmsprop: Divide the gradient by a running average of its recent magnitude.},
journaltitle={COURSERA: Neural networks for machine learning},
}

@article{2017HutchingsTalking,
keywords={generative music,music generation,percussion,rnn,state of the art,translation},
eprinttype={arXiv},
volume={1},
mendeley-tags={music generation,state of the art},
url={http://arxiv.org/abs/1706.09558},
pages={43--47},
number={1},
eprint={1706.09558},
date={2017},
journaltitle={arXiv preprint arXiv:1706.09558},
author={Hutchings, P.},
title={Talking Drums: Generating drum grooves with neural networks},
abstract={Presented is a method of generating a full drum kit part for a provided kick-drum sequence. A sequence to sequence neural network model used in natural language translation was adopted to encode multiple musical styles and an online survey was developed to test different techniques for sampling the output of the softmax function. The strongest results were found using a sampling technique that drew from the three most probable outputs at each subdivision of the drum pattern but the consistency of output was found to be heavily dependent on style.},
arxivid={1706.09558},
}

@article{1999JiangImagea,
number={9},
author={Jiang, J},
publisher={Elsevier},
volume={14},
journaltitle={Signal Processing: Image Communication},
pages={737--760},
date={1999},
title={Image compression with neural networks--a survey},
}

@article{2007BengioScaling,
issn={0009-9104},
groups={tesse:5},
title={Scaling Learning Algorithms towards AI},
doi={10.1.1.72.4580},
number={1},
abstract={One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
isbn={1002620262},
journaltitle={Large Scale Kernel Machines},
keywords={theory},
mendeley-tags={theory},
pmid={11359439},
date={2007},
pages={321--360},
author={Bengio, Yoshua and LeCun, Yann and Lecun, Yann},
}

@article{1999YaoEvolvinga,
pages={1423--1447},
title={Evolving artificial neural networks},
arxivid={1108.1530},
isbn={9780470287194},
eprint={1108.1530},
volume={87},
mendeley-tags={evolutive,seminal},
journaltitle={Proceedings of the IEEE},
abstract={Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
issn={0018-9219},
doi={10.1109/5.784219},
author={Yao, Xin},
publisher={IEEE},
keywords={evolutionary computation,evolutive,intelligent systems,neu-,seminal},
number={9},
pmid={9821520},
eprinttype={arXiv},
date={1999},
}

@article{2016KrishnaEyeriss,
title={Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks The MIT Faculty has made this article openly available . Please share Citation " Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Publisher Version},
date={2016},
keywords={hardware},
author={Krishna, Tushar and Emer, Joel and Sze, Vivienne and Conference, International Solid-state Circuits and Francisco, San and Chen, Yu-hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
mendeley-tags={hardware},
}

@inproceedings{2012KasabovNeucube,
author={Kasabov, Nikola},
booktitle={ANNPR},
organization={Springer},
date={2012},
title={NeuCube EvoSpike Architecture for Spatio-temporal Modelling and Pattern Recognition of Brain Signals.},
pages={225--243},
}

@article{2013StilgoeDeveloping,
pages={1568--1580},
volume={42},
keywords={Emerging technologies,Ethics,Geoengineering,Governance,Responsible innovation},
title={Developing a framework for responsible innovation},
number={9},
author={Stilgoe, Jack and Owen, Richard and Macnaghten, Phil},
doi={10.1016/j.respol.2013.05.008},
issn={0048-7333},
date={2013-11},
url={http://linkinghub.elsevier.com/retrieve/pii/S0048733313000930},
abstract={The governance of emerging science and innovation is a major challenge for contemporary democracies. In this paper we present a framework for understanding and supporting efforts aimed at 'responsible innovation'. The framework was developed in part through work with one of the first major research projects in the controversial area of geoengineering, funded by the UK Research Councils. We describe this case study, and how this became a location to articulate and explore four integrated dimensions of responsible innovation: anticipation, reflexivity, inclusion and responsiveness. Although the framework for responsible innovation was designed for use by the UK Research Councils and the scientific communities they support, we argue that it has more general application and relevance. ?? 2013 Elsevier B.V. All rights reserved.},
journaltitle={Research Policy},
publisher={Elsevier B.V.},
}

@article{MorseMagnitude,
author={Morse, Bryan},
title={Magnitude and Phase The Fourier Transform : Examples , Properties , Common Pairs CS 450 : Introduction to Digital Signal and Image Processing Example : Fourier Transform of a Cosine Example : Fourier Transform of a Cosine Odd and Even Functions Sinusoids},
}

@article{2017LouizosL,
author={Louizos, Christos and Welling, Max},
title={L EARNING S PARSE N EURAL N ETWORKS THROUGH L 0 R EGULARIZATION},
pages={1--13},
eprint={arXiv:1712.01312v1},
groups={tesse:2},
arxivid={arXiv:1712.01312v1},
date={2017},
eprinttype={arXiv},
}

@misc{Unknown,
title={Unknown - Unknown - Leaps and bounds.pdf.pdf},
}

@article{2014DonahueSynthesizing,
eprint={arXiv:1802.04208v1},
title={Synthesizing Audio with Generative Adversarial Networks},
arxivid={arXiv:1802.04208v1},
author={Donahue, Chris and Mcauley, Julian and Puckette, Miller},
date={2014},
eprinttype={arXiv},
}

@article{1942No,
title={No Title},
pages={1--9},
groups={tesse:4},
date={1942},
}

@article{2014WeinmanTop,
date={2014},
title={Top of page},
author={Weinman, Jaime J},
pages={261277},
groups={tesse:4},
number={May 2012},
}

@article{2013LedfordStart,
author={Ledford, B Y Heidi},
date={2013},
title={START-UP},
}

@article{1998SamplerRedefining,
volume={19},
abstract={We are entering a new competitive age in which the basis of competition is being fundamentally altered through the introduction of advanced information technologies and public communication infrastructures, such as the Internet. In these environments, the nature and locus of competition will radically alter, as information becomes an increasingly important resource. This paper develops ideas around the strategic characteristics of information-information separability and industry concentration, related diversification, and innovation for firms competing in the Information Age.},
keywords={industry boundary,industry structure,information,information age,information separability},
publisher={Wiley Online Library},
title={Redefining industry structure for the information age},
author={Sampler, J.},
number={4},
url={http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0266(199804)19:4{////////\\%}3C343::AID-SMJ975{////////\\%}3E3.0.CO;2-G/abstract},
pages={343--355},
date={1998},
journaltitle={Strategic Management Journal},
}

@article{XuSupertagging,
journaltitle={CL (},
title={Supertagging with a Recurrent Neural Network},
author={Xu, W and Auli, M and Clark, S Ccg},
volume={2},
}

@book{TheNo,
title={No Title},
author={The, Rossing},
isbn={0060189878},
groups={tesse:4},
}

@article{GrinsteinNo,
title={No Title},
eprinttype={arXiv},
eprint={arXiv:1710.11385v1},
groups={tesse:4},
author={Grinstein, Eric and Duong, Ngoc Q K and Ozerov, Alexey and Patrick, P},
arxivid={arXiv:1710.11385v1},
}

@misc{1962,
title={1962 On Estimation of a Probability Density Function and Mode.pdf},
}

@misc{1988,
title={1988 A theoretical framework for back-propagation.pdf},
}

@inproceedings{2016WuInvestigatinga,
pages={5140--5144},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
organization={IEEE},
author={Wu, Zhizheng and King, Simon},
date={2016},
title={Investigating gated recurrent networks for speech synthesis},
}

@unpublished{No,
url={https://github.com/tesserato/tesserato.github.io{////////\\%}3E},
groups={tesse:4},
title={No Title},
}

@article{TheDead,
number={March 2002},
author={The, I S},
title={DEAD ?},
}

@article{2016LeonetiUtility,
issn={2214-7160},
volume={3},
date={2016},
doi={10.1016/j.orp.2016.04.001},
publisher={Elsevier Ltd},
author={Leoneti, Alexandre Bevilacqua},
title={Utility Function for modeling Group Multicriteria Decision Making problems as games},
pages={21--26},
journaltitle={Operations Research Perspectives},
keywords={Utility function,Group Multicriteria Decision,MAUT},
}

@article{2006DodgsonRole,
date={2006},
title={The role of technology in the shift towards open innovation : the case},
author={Dodgson, Mark and Gann, David and Salter, Ammon},
pages={333--346},
}

@misc{Estelles,
title={Estelles-Arolas, Gonzalez-Ladron-de-Guevara - 2012 - Towards an integrated crowdsourcing definition.pdf},
}

@article{2014MerriProperties,
author={Merri, Bart Van},
eprinttype={arXiv},
title={On the Properties of Neural Machine Translation: Encoder–Decoder Approaches},
date={2014},
arxivid={arXiv:1409.1259v2},
eprint={arXiv:1409.1259v2},
}

@article{Noa,
arxivid={arXiv:1709.08243v2},
title={No Title},
eprinttype={arXiv},
groups={tesse:4},
eprint={arXiv:1709.08243v2},
}

@article{1994Lee-mortimerStrategic,
date={1994},
number={2},
title={Strategic Design},
pages={31--34},
author={Lee-mortimer, Andrew},
volume={1},
}

@article{Modelling,
title={MODELLING THE DECAY OF PIANO SOUNDS Tian Cheng , Simon Dixon , Matthias Mauch Centre for Digital Music , Queen Mary University of London , London , United Kingdom},
volume={1},
}

@article{2008Digital,
pages={43--55},
volume={7},
date={2008},
title={Digital Synthesis of and Plucked-String Timbres},
number={2},
}

@article{2016SalihSecond,
title={Second-Order Wave Equation d ' Alembert ' s Solution in Infinite Domain},
number={December},
date={2016},
author={Salih, A},
pages={1--24},
}

@article{2014DippolitoImportance,
pages={1--15},
author={D'Ippolito, Beatrice},
keywords={Literature Review Process},
url={http://www.sciencedirect.com/science/article/pii/S016649721400008X},
publisher={Elsevier},
date={2014-02},
title={The importance of design for firms׳ competitiveness: A review of the literature},
journaltitle={Technovation},
abstract={Scholars dedicated increasing attention towards appreciating how design has changed individuals׳ perception of new products, firms׳ understanding and formulation of strategy, or other relevant actors׳ approach to innovation and technology management. By emphasising the importance of design for the definition of consumers׳ needs, the restructuring of firms׳ organisational structures and strategies, and the evolution of firms׳ value creation processes, this review paper identifies relevant research gaps and questions that would benefit from future scholarly attention. In particular, it is suggested that such effort should address the analysis of how design consumption can help better comprehend consumers׳ needs; what are the implications of design thinking on the skill sets of design professionals; the organisational structure of firms, including the reconfiguration of other business functions, and their strategy; and whether and how design thinking can shape firms׳ value creation processes and contribute to the formalisation of design tasks.},
issn={0166-4972},
doi={10.1016/j.technovation.2014.01.007},
}

@article{2012FosterCreative,
author={Foster, Richard N},
title={Creative Destruction Whips through Corporate America},
date={2012},
keywords={Kodak, Radio Shack, Bear Stearns},
}

@book{2018Nsynthsupera,
publisher={<},
groups={tesse:4},
date={2018},
url={https://nsynthsuper.withgoogle.com/{////////\\%}3E},
location={GoogleGoogle Disponvel em},
title={NSynthSuper},
}

@article{MasriUsing,
volume={44},
author={Masri, P},
title={USING DIGITAL WAVEGUIDES},
number={0},
}

@article{2012Bettis-outlandDecision,
number={6},
issn={0148-2963},
journaltitle={Journal of Business Research},
title={Decision-making's impact on organizational learning and information overload},
date={2012-06},
doi={10.1016/j.jbusres.2010.12.021},
pages={814--820},
url={http://www.sciencedirect.com/science/article/pii/S0148296310002845},
volume={65},
abstract={Although an abundance of academic literature positions organizational information processing as antecedent to decision making, little attention is paid to the possibility that decision making can be antecedent to certain elements of organizational information processing. Specifically, does the decision making process impact the type of organizational learning that takes place? Do different approaches to decision making alter the amount and variety of information made available to the organization, that is, the level of information overload? This paper examines incremental and comprehensive decision making to understand the effects of different decision making types on organizational learning and information overload. Incrementalism suggests that decision making should take place in small steps or increments. This approach analyzes only a few scenarios to make decisions resulting in few, if any, major organizational changes. However, comprehensive decision making requires the consideration of all possible scenarios and potential outcomes, resulting in a major overhaul of traditions and procedures within the organization. Consequently, each decision making approach has a different impact on organizational learning and information overload. (C) 2011 Elsevier Inc. All rights reserved.},
keywords={Decision making,Incrementalism,Information overload,Organizational learning},
publisher={Elsevier Inc.},
author={Bettis-Outland, H},
}

@article{2007BengioScalinga,
title={Scaling Learning Algorithms towards AI To appear in “ Large-Scale Kernel Machines ”},
number={1},
pages={1--41},
author={Bengio, Yoshua and Lecun, Yann},
date={2007},
}

@article{GOVINDAN20161,
doi={https://doi.org/10.1016/j.ejor.2015.07.019},
volume={250},
url={http://www.sciencedirect.com/science/article/pii/S0377221715006529},
title={ELECTRE: A comprehensive literature review on methodologies and applications},
number={1},
keywords={Multiple criteria decision aiding (MCDA), Outranking, ELECTRE, Literature review},
pages={1 - 29},
author={Govindan, Kannan and Jepsen, Martin Brandt},
abstract={Multi-criteria decision analysis (MCDA) is a valuable resource within operations research and management science. Various MCDA methods have been developed over the years and applied to decision problems in many different areas. The outranking approach, and in particular the family of ELECTRE methods, continues to be a popular research field within MCDA, despite its more than 40 years of existence. In this paper, a comprehensive literature review of English scholarly papers on ELECTRE and ELECTRE-based methods is performed. Our aim is to investigate how ELECTRE and ELECTRE-based methods have been considered in various areas. This includes area of applications, modifications to the methods, comparisons with other methods, and general studies of the ELECTRE methods. Although a significant amount of literature on ELECTRE is in a language different from English, we focus only on English articles, because many researchers may not be able to perform a study in some of the other languages. Each paper is categorized according to its main focus with respect to ELECTRE, i.e. if it considers an application, performs a review, considers ELECTRE with respect to the problem of selecting an MCDA method or considers some methodological aspects of ELECTRE. A total of 686 papers are included in the review. The group of papers considering an application of ELECTRE consists of 544 papers, and these are further categorized into 13 application areas and a number of sub-areas. In addition, all papers are classified according to the country of author affiliation, journal of publication, and year of publication. For the group of applied papers, the distribution by ELECTRE version vs. application area and ELECTRE version vs. year of publication are provided. We believe that this paper can be a valuable source of information for researchers and practitioners in the field of MCDA and ELECTRE in particular.},
issn={0377-2217},
journal={European Journal of Operational Research},
year={2016},
}

@book{2018GaziUnderstanding,
title={Understanding digital signal processing},
publisher={Springer},
author={Gazi, O.},
date={2018},
groups={tesse:5, DSP},
location={Traducao. [s.l.]},
}

@article{2015CourtneyArxiv,
eprint={arXiv:1507.01832v1},
author={Courtney, Elya and Courtney, Michael},
date={2015},
eprinttype={arXiv},
arxivid={arXiv:1507.01832v1},
title={arXiv : 1507 . 01832v1 [ physics . data-an ] 6 Jul 2015},
}

@article{1995Unclassified,
date={1995},
title={UNCLASSIFIED LIMITATION CHANGES TO : FROM :},
}

@article{2018Theanoa,
date={2018},
url={http://deeplearning.net/software/theano/{////////\\%}3E},
title={Theano},
volume={1},
journaltitle={Multilayer Perceptron - DeepLearning 0.},
}

@book{2018Magentaa,
date={2018},
url={https://magenta.tensorflow.org/{////////\\%}3E},
title={Magenta},
publisher={<},
groups={tesse:4},
location={Magenta Disponvel em},
}

@article{SamplesUniversity,
groups={tesse:4},
author={Samples, Musical Instrument},
title={University of Iowa Electronic Music Studios},
url={http://theremin.music.uiowa.edu/MIS.html{////////\\%}3E},
volume={2018},
}

@article{Literature,
title={A literature review is a description of the literature relevant to a particular field or topic. It gives an overview of:},
abstract={literature review},
journaltitle={2016},
}

@article{RumelhartLearning,
journaltitle={[s.l.] California Univ San Diego La Jolla Inst for Cognitive Science},
volume={1985},
title={Learning internal representations by error propagation.},
author={Rumelhart, D E and Hinton, G E and Williams, R J},
}

@article{2017EisenachN,
pages={1--23},
title={N ONPARAMETRICALLY L EARNING A CTIVATION F UNCTIONS IN D EEP N EURAL N ETS},
author={Eisenach, Carson and Wang, Zhaoran},
date={2017},
}

@book{PapertPerceptrons,
author={Papert, Seymour A},
isbn={0262631113},
title={Perceptrons},
}

@misc{2002DewickTechnological,
pages={1--31},
date={2002},
author={Dewick, Paul and Green, Ken and Miozzo, Marcela},
abstract={This paper seeks to contribute towards the construction and application of a method to assess the long-term impact of the development of their technological technologies on the environment. The paper identifies the effect of three pervasive technologies – biotechnology, information technology and nanotechnology – on the production of a range of sectors and their consequent environmental effects. The sectors are selected according to taxonomies of characteristics. The technological impact is assessed qualitatively in terms of changes in production scale and resource intensity and their resulting impact on industrial greenhouse gas emissions},
number={January},
title={Technological Change, Industry Structure and the Environment},
}

@misc{Wang,
title={Wang et al. - 2011 - Rapid parametric design methods for shoe-last customization.pdf},
}

@article{MARTTUNEN20171,
author={Marttunen, Mika and Lienert, Judit and Belton, Valerie},
journal={European Journal of Operational Research},
keywords={Problem structuring, Multiple Criteria Decision Analysis, Multi-methodology, Multi-stakeholder decision-making},
year={2017},
number={1},
doi={https://doi.org/10.1016/j.ejor.2017.04.041},
abstract={Structuring problems for Multi-Criteria Decision Analysis (MCDA) has attracted increasing attention over the past 20 years from both a conceptual and a practical perspective. This is reflected in a significant growth in the number of published applications which use a formal approach to problem structuring in combination with an analytic method for multi-criteria analysis. The problem structuring approaches (PSMs) include general methodologies such as Checkland's Soft Systems Method (SSM), Eden and Ackermann's Strategic Options Design and Analysis (SODA) and other methods that focus on a particular aspect. We carried out a literature review that covers eight PSMs (Cognitive and Causal Maps, DPSIR, Scenario Planning, SSM, Stakeholder Analysis, Strategic Choice Approach, SODA and SWOT) and seven MCDA methods (AHP, ANP, ELECTRE, MAUT, MAVT, PROMETHEE and TOPSIS). We first identified and analysed 333 articles published during 2000-2015, then selected 68 articles covering all PSM-MCDA combinations, which were studied in detail to understand the associated processes, benefits and challenges. The three PSMs most commonly combined with MCDA are SWOT, Scenario Planning and DPSIR. AHP was by far the most commonly applied MCDA method. Combining PSMs with MCDA produces a richer view of the decision situation and enables more effective support for different phases of the decision-making process. Some limitations and challenges in combining PSMs and MCDA are also identified, most importantly relating to building a value tree and assigning criteria weights.},
issn={0377-2217},
pages={1 - 17},
title={Structuring problems for Multi-Criteria Decision Analysis in practice: A literature review of method combinations},
url={http://www.sciencedirect.com/science/article/pii/S0377221717303880},
volume={263},
}

@article{2008BrabhamCrowdsourcing,
author={Brabham, D. C.},
issn={1354-8565},
date={2008-02},
title={Crowdsourcing as a Model for Problem Solving: An Introduction and Cases},
doi={10.1177/1354856507084420},
volume={14},
keywords={about human ingenuity,challenge,collective intelligence,crowdsourcing,designer,distributed problem solving,goldcorp,innocentive,is going on,istockphoto,its unfolding is to,of client,open source,reject the binary notion,right now,story to be told,the first step to,the next step is,there is an incredible,threadless,to look to what,wisdom of crowds},
journaltitle={Convergence: The International Journal of Research into New Media Technologies},
number={1},
pages={75--90},
abstract={Crowdsourcing is an online, distributed problem-solving and production model that has emerged in recent years. Notable examples of the model include Threadless, iStockphoto, Inno- Centive, the Goldcorp Challenge, and user-generated advertising contests. This article provides an introduction to crowdsourcing, both its theoretical grounding and exemplar cases, taking care to distinguish crowdsourcing from open source production. This article also explores the possibilities for the model, its potential to exploit a crowd of innovators, and its potential for use beyond for- profit sectors. Finally, this article proposes an agenda for research into crowdsourcing.},
isbn={1354856507084},
}

@article{2015StefanoState,
number={January 2016},
author={Stefano, Nara Medianeira and Stefano, N M and Filho, N Casarotto and Vergara, L G L and Rocha, R U G},
date={2015},
title={State of the Art Research and its Applications COPRAS ( Complex Proportional Assessment ): State of the Art Research and its Applications},
doi={10.1109/TLA.2015.7404925},
}

@article{AudioIvy,
groups={tesse:4},
volume={2018},
author={Audio, Ivy},
title={Ivy Audio},
url={http://www.ivyaudio.com/{////////\\%}3E},
}

@article{2014SathyanarayanaGentlea,
author={Sathyanarayana, Shashi and Ph, D},
date={2014},
title={A Gentle Introduction to Backpropagation What is so difficult about designing a neural},
pages={1--15},
}

@book{EckMaking,
location={Disponvel em},
title={Making music using new sounds generated with machine learning},
publisher={<},
url={https://www.blog.google/technology/ai/making-music-using-new-sounds-generated-machine-learning/{////////\\%}3E},
groups={tesse:4},
author={Eck, D},
}

@article{1987ScottFive,
title={Five Stages of Growth Business in Small},
date={1987},
author={Scott, Mel and Bruce, Richard},
number={3},
volume={20},
}

@misc{Poverty,
title={Poverty and profits in the information age.pdf},
}

@article{2015HarrisonEdinburgh,
title={Edinburgh Research Explorer An algorithm for a valved brass instrument synthesis environment using finite-difference time-domain methods with performance optimisation AN ALGORITHM FOR A VALVED BRASS INSTRUMENT SYNTHESIS ENVIRONMENT USING FINITE-DIFFERENCE},
date={2015},
author={Harrison, Reginald L and Bilbao, Stefan},
}

@article{2006DewickModelling,
abstract={Future disruptive, pervasive technologies will have important consequences for industrial structure, economic growth and the environment. Drawing on theories of technological diffusion, industrial evolution and long-term technological change this paper explores the effect of the development and diffusion of two future pervasive technologies on five industrial sectors in three regions during the 21st century in terms of their effect on economic structural change. Through semi-structured interviews with over 100 experts in the two technologies, the paper quantifies the effects of future biotechnologies and nanotechnologies on the industrial structure of the EU, USA and China in 2020 and 2050. The paper finds that as a result of the development and diffusion of future biotechnologies and nanotechnologies, some industries grow whilst others decline and some new ones emerge. The evidence suggests that the effect is different across countries and time; whereas the experts commonly believe that effect of the technologies on the industrial structure of the EU and US is likely to be similar, the effect in China is considered to be less by 2020 but the same as in the EU and US by 2050. This finding has important implications for the location of production, economic growth and energy demand in the future. ?? 2006 Elsevier Inc. All rights reserved.},
volume={73},
title={Modelling creative destruction: Technological diffusion and industrial structure change to 2050},
journaltitle={Technological Forecasting and Social Change},
url={http://linkinghub.elsevier.com/retrieve/pii/S0040162506000862},
date={2006-11},
number={9},
issn={0040-1625},
doi={10.1016/j.techfore.2006.04.002},
pages={1084--1106},
keywords={Biotechnologies,Energy,Industrial structure,Nanotechnologies,Technological diffusion},
author={Dewick, Paul and Green, Ken and Fleetwood, Toby and Miozzo, Marcela},
}

@unpublished{Torch,
title={Torch},
url={http://torch.ch/{////////\\%}3E},
}

@article{2013BernroiderEpub,
title={ePub WU Institutional Repository A technological , organisational , and environmental analysis},
author={Bernroider, Edward and Schmöllerl, Patrick},
date={2013},
}

@article{2000MontiNo,
date={2000},
author={Monti, Giuliano and Sandler, Mark},
number={1},
pages={7--10},
groups={tesse:4},
title={No Title},
}

@book{2018Tensorflowa,
date={2018},
location={TensorFlow Disponvel em},
url={https://www.tensorflow.org/{////////\\%}3E},
publisher={<},
groups={tesse:4},
title={TensorFlow},
}

@article{SteinPrinceton,
title={Princeton lectures in analysis.},
volume={2003},
author={Stein, E M and Shakarchi, R},
journaltitle={Traducao. [s.l.] Princeton University Press},
}

@misc{Unknowna,
title={Unknown - Unknown - Democratizing Innovation.pdf.pdf},
}

@book{2006SmithBasic,
title={basic introduction to digital waveguide synthesis (for the technically inclined)},
location={Stanford University. stanford. edu/ jos/swgt},
author={Smith, J A},
publisher={Center for Computer Research in Music and Acoustics (CCRMA)},
url={http://ccrma},
date={2006},
}

@misc{Democratizing,
title={Democratizing innovation.pdf},
}

@misc{1983,
title={Extensions of the Karplus-Strong Plucked String Algorithm},
}

@article{2009BrunetteReviewa,
title={review of artificial intelligence},
volume={2009},
date={2009},
pages={4},
author={Brunette, E S and Flemmer, R C and Flemmer, C L A},
}

@article{2001Jacquet-lagrPreference,
author={Jacquet-lagr, Eric},
date={2001},
pages={233--245},
keywords={1,criteria,general philosophy,goal programming,in decision-making involving multiple,introduction and background,multicriteria analysis,preference disaggregation,regression},
volume={130},
title={Preference disaggregation : 20 years of MCDA experience},
}

@book{AdlerDesign,
author={Adler, Isabel K},
title={Design Thinking Design Thinking Inovação em negócios},
isbn={9788565424004},
}

@book{AnalysisNo,
title={No Title},
groups={tesse:4},
author={Analysis, Multi-criteria Decision},
isbn={9781119974079},
}

@article{1994SchumpeterThomas,
date={1994},
author={Schumpeter, Joseph A and Mccraw, Thomas and Mirowski, Phillip},
pages={1--8},
title={Thomas K . McCraw , Cambridge : Harvard University Press , 719 pages },
}

@article{ZwickerPsychoacoustics,
title={Psychoacoustics: Facts and models.},
volume={2013},
journaltitle={Traducao. [s.l.] Springer Science ////////\\\& Business Media},
author={Zwicker, E and Fastl, H},
}

@article{VincentEfficient,
pages={1--15},
title={Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets [ Technical report ]},
eprinttype={arXiv},
arxivid={arXiv:1412.7091v3},
eprint={arXiv:1412.7091v3},
author={Vincent, Pascal and Bouthillier, Xavier},
}

@article{2012Estelles-arolasTowards,
journaltitle={Journal of Information Science},
title={Towards an integrated crowdsourcing definition},
number={2},
author={Estelles-Arolas, E. and Gonzalez-Ladron-de-Guevara, F.},
date={2012},
doi={10.1177/016555150000000},
pages={189--200},
volume={38},
keywords={crowdsourcing,definition,innovation},
isbn={0165551500000},
abstract={Crowdsourcing is a relatively recent concept that encompasses many practices. This diversity leads to the blurring of the limits of crowdsourcing that may be identified virtually with any type of Internet-based collaborative activity, such as co-creation or user innovation. Varying definitions of crowdsourcing exist and therefore, some authors present certain specific examples of crowdsourcing as paradigmatic, while others present the same examples as the opposite. In this paper, existing definitions of crowdsourcing are analyzed to extract common elements and to establish the basic characteristics of any crowdsourcing initiative. Based on these existing definitions, an exhaustive and consistent definition for crowdsourcing is presented and contrasted in eleven cases.},
}

@article{1997DavidcacchioneAmerican,
title={American Scientist},
number={2},
author={Cacchione, David A.},
issn={0003-0996},
pages={108--112},
volume={85},
doi={10.1511/2011.89.106},
isbn={2136240900},
date={1997},
}

@article{2008PisanoWhich,
author={Pisano, Gary P. and Verganti, Roberto},
url={http://search.ebscohost.com/login.aspx?direct=true{////////\\&}db=buh{////////\\&}AN=35387060{////////\\&}site=ehost-live{////////\\%}5Cnhttp://content.ebscohost.com/ContentServer.asp?T=P{////////\\&}P=AN{////////\\&}K=35387060{////////\\&}S=R{////////\\&}D=buh{////////\\&}EbscoContent=dGJyMMvl7ESep7Q4wtvhOLCmr0uep65Ssqu4TLGWxWXS{////////\\&}ContentCustomer=dGJyMPGotk{////////\\%}2BxrLZQuePfgeyx44Dt6fIA},
number={12},
volume={86},
date={2008},
abstract={Nowadays, virtually no companies innovate alone. Firms team up with a variety of partners, in a wide number of ways, to create new technologies, products, and services. But what is the best way to leverage the power of outsiders? To help executives answer that question, Pisano, of Harvard Business School, and Verganti, of Politecnico di Milano, developed a simple framework focused on two questions: Given your strategy, how open or closed should your network of collaborators be? And who should decide which problems to tackle and which solutions to adopt? There are four basic modes of collaboration, say the authors. An elite circle is a closed network with a hierarchical governance: One company selects the participants, defines the problem, and chooses the solution. For instance, Alessi, an Italian home-products company, invited 200 outside experts in postmodern architecture to contribute ideas for new home-product designs. An innovation mall is hierarchical but open: Anyone can post a problem or propose solutions in it, but the company posting the problem chooses the solution. An example is InnoCentive.com, an eBay-like site where companies post scientific challenges. An innovation community is open and decentralized: Anyone can propose problems, offer solutions, and decide which ideas to use -- as happens in the Linux open-source software community. A consortium is a private group of participants that operate as equals and jointly select problems, decide how to conduct work, and choose solutions. IBM has set up a number of consortia with other companies to develop next-generation semiconductor technologies. No one approach is superior; each involves strategic trade-offs. When choosing among modes, firms must weigh their advantages and challenges, and assess which will work best with their strategy, capabilities, structure, and assets. INSETS: The Four Ways to Collaborate;How to Choose the Best Mode of Collaboration.},
title={Which Kind of Collaboration Is Right for You?},
journaltitle={Harvard Business Review},
pages={78--86},
}

@article{2000InformationChapter,
author={Information, Background and Of, Description and Mcdm, Some},
date={2000},
title={Chapter 2 MULTI-CRITERIA DECISION MAKING METHODS 2.1},
}

@article{2001LairdPhysicalb,
date={2001},
number={November},
author={Laird, Joel Augustus},
title={THE PHYSICAL MODELLING OF DRUMS USING DIGITAL},
}

@article{2017MixReliability,
number={March 2016},
date={2017},
title={Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Power Mix},
keywords={energy storage systems,intermittent sources,mix,multi-criteria decision analysis,power,renewable sources},
author={Mix, Power and Dester, Mauricio and Dester, M},
doi={10.1109/TLA.2016.7459613},
}

@book{Licensed,
isbn={9781111221294},
title={Licensed to : CengageBrain User Licensed to : CengageBrain User},
}

@article{2006ReiffensteinPatents,
journaltitle={The Canadian Geographer/Le Gographe canadien, v.},
title={patents and the geography of knowledge transfer in the electronic musical instrument industry},
volume={50},
date={2006},
author={Reiffenstein, T Codification},
}

@article{2007BerthonWhen,
issn={0007-6813},
date={2007-01},
volume={50},
doi={10.1016/j.bushor.2006.05.005},
number={1},
journaltitle={Business Horizons},
title={When customers get clever: Managerial approaches to dealing with creative consumers},
pages={39--47},
url={http://linkinghub.elsevier.com/retrieve/pii/S0007681306000796},
abstract={Creative consumers (defined as customers who adapt, modify, or transform a proprietary offering) represent an intriguing paradox for business. On one hand, they can signify a black hole for future revenue, with breach of copyright and intellectual property. On the other hand, they represent a gold mine of ideas and business opportunities. Central to business is the need to create and capture value, and creative consumers demand a shift in the mindsets and business models of how firms accomplish both. Based upon their attitude and action toward customer innovation, we develop a typology of firms' stances toward creative consumers. We then consider the implications of the stances model for corporate strategy and examine a three-step approach to dealing with creative consumers: awareness, analysis, and response. ?? 2006 Kelley School of Business, Indiana University.},
author={Berthon, Pierre R. and Pitt, Leyland F. and McCarthy, Ian and Kates, Steven M.},
keywords={Creative customers,Diagnostics,Firm stance,Strategic response},
}

@article{2016LeiteProcess,
keywords={Innovation,Product development,Startups},
number={7},
volume={37},
date={2016},
title={The process of product development for startups based on creative innovation},
author={Leite, M.L.G. and Purcidonio, P.M. and Tarjano, C.},
abstract={The systematic innovation has been considered one of the most important practices in the business environment, increasingly uncertain and changeable. Understanding how changes they are occurring in society motivated by advances in information technology have impacted the innovation process, through the lens of startups. it is essential for the economic growth of a country, since most new products come these processes. The aim of this study is to develop a new model of development products in startups and small organizations seeking to develop physical products from innovations based on creativity.},
issn={0798-1015},
journaltitle={Espacios},
}

@article{1998SolomonMulti,
author={Solomon, Anthony and Wishart, Nicole and Dublish, Sandipa},
volume={2217},
keywords={multiple criteria analysis},
number={97},
date={1998},
title={Multi-attribute decision making : A simulation comparison of select methods},
}

@article{2017ElectronicWave,
keywords={00a65,2010,35g16,35l05,dissonance,inharmonic spectrum,msc,musical scale,stiffness,string,vibrating,wave equation},
author={Electronic, A N and Of, Journal and Catalana, Societat and Atiques, D E Matem},
volume={3},
date={2017},
title={The wave equation for stiff strings and piano tuning},
doi={10.2436/20.2002.02.11.1},
pages={1--16},
groups={Acoustics},
}

@article{1994ChaigneNumerical,
author={Chaigne, Antoine and Cedex, Paris},
title={Numerical simulations of piano strings . I . A physical model for a struck string using finite difference methods},
date={1994},
pages={1112--1118},
number={February},
volume={95},
}

@book{2015HeMultimodal,
groups={tesse:4},
edition={Proceeding},
author={He, L and Others},
title={Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks},
publisher={AnaisACM},
date={2015},
}

@article{2000Copyright,
date={2000},
title={Copyright 2000. All Rights Reserved.},
}

@article{2016PhumrattanaprapinMachine,
author={Phumrattanaprapin, Khanittha},
pages={196--204},
volume={10},
number={2},
date={2016},
keywords={chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
title={Machine with Multilayer Perceptron},
}

@article{AngellWinners,
title={Winners and Losers in the Information Age},
author={Angell, Ian},
}

@misc{Leaps,
title={Leaps and bounds},
}

@article{2011WangRapid,
doi={10.1007/s00170-010-3144-y},
title={Rapid parametric design methods for shoe-last customization},
volume={54},
keywords={deformation,form reuse,interactive,parametric design,shoe-last},
journaltitle={The International Journal of Advanced Manufacturing Technology},
issn={0268-3768},
date={2011-01},
pages={173--186},
author={Wang, Jin and Zhang, Haining and Lu, Guodong and Liu, Zheng},
number={1-4},
abstract={With the development of computer-aided design (CAD) technology and increasing demands of customized footwear, shoe-lasts are requested to be designed rapidly so as to speed-up the process of footwear manufacturing. Thus, this study presents a CAD system for shoe-last rapid customized design based on the piecewise reconstruction to realize the interactive deformation and separate/global shoe-last form reuse. First, piecewise remodeling method is proposed based on the multi-layer parametric definition and contour curves are extracted from the mesh. Then, five types of proper constraints to support surface manipulation are proposed, and the draft-driven deformation by the contour curve bending can realize the interactive local surface design in free angle of view. Finally, shoe-last styles can be saved and reused globally or separately to share design results between different shoe-lasts. Experimental examples show that customized shoe-lasts can be easily and rapidly generated by adopting the parametric design methods.},
}

@article{1993PoczosPerceptron,
pages={1--10},
date={1993},
number={1982},
author={Poczos, Lecturer Barnabas},
title={Perceptron History of Artificial Neural Networks The Neuron},
}

@article{RobelSignals,
author={Robel, Axel},
title={Signals},
}

@article{1998GuitouniTentative,
volume={109},
title={Tentative guidelines to help choosing an appropriate MCDA method},
keywords={aggregation procedure,behavioural considerations,comparative analysis,decision making situation,multicriteria analysis,multicriterion,multicriterion decision aid method,preferences modelling},
date={1998},
pages={501--521},
author={Guitouni, Adel and Martel, Jean-marc},
}

@article{Boulanger-lewandowskiPhone,
number={5},
title={PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS Universit ´ e de Montr ´ Montr ´ Jasha Droppo Mike Seltzer Dong Yu One Microsoft Way},
author={Boulanger-lewandowski, Nicolas},
}

@article{2013HutchinsTop,
date={2013},
title={Top of page},
pages={649206},
author={Hutchins, Aaron},
groups={tesse:4},
}

@article{2015DeheExpert,
keywords={ahp,analytical hierarchy process,er,evidential reasoning,mcda,multiple criteria decision analysis},
doi={10.1016/j.eswa.2015.04.059},
author={Dehe, Benjamin and Bamford, David},
volume={42},
publisher={Elsevier Ltd},
pages={6717--6727},
title={Expert Systems with Applications Development , test and comparison of two Multiple Criteria Decision Analysis ( MCDA ) models : A case of healthcare infrastructure location},
journaltitle={Expert Systems With Applications},
issn={0957-4174},
date={2015},
number={19},
}

@article{2011NobleElevating,
number={3},
journaltitle={Journal of Product Innovation Management},
pages={389--393},
title={On Elevating Strategic Design Research*},
abstract={While the popular understanding of the influence of design is growing, academic research has largely been restricted to considering consumer-level responses to design elements. This paper reviews this past work and proposes a more strategic research agenda for the field, with the potential to explicate linkages between design elements and strategies and outcomes related to innovation and corporate performance.},
publisher={Blackwell Publishing Inc},
doi={10.1111/j.1540-5885.2011.00808.x},
volume={28},
date={2011},
author={Noble, Charles H},
}

@article{KlambauerSelf,
title={Self-normalizing neural networksAdvances in Neural Information Processing Systems},
author={Klambauer, G and Others},
journaltitle={Anais},
volume={2017},
}

@article{2006PillerToolkits,
date={2006},
title={Toolkits for idea competitions: a novel method to integrate users in new product development},
number={3},
doi={10.1111/j.1467-9310.2006.00432.x},
abstract={Research has shown that many innovations originate not in the manufacturer but the user domain. Internet-based toolkits for idea competitions (TIC) are a novel way for manufacturers to access innovative ideas and solutions from users. Idea competitions build on the nature of competition as a means to encourage users to participate at an open innovation process, to inspire their creativity, and to increase the quality of the submissions. When the contest ends, submissions are evaluated by an expert panel. Users whose submissions score highest receive an award from the manufacturer, which is often granted in exchange for the right to exploit the solution in its domain. Following the idea of evolutionary prototyping, we developed a TIC in cooperation with a manufacturer of sports goods. The TIC was launched as a pilot in one of the company's markets. Submissions were evaluated using the consensual assessment technique. The evaluation of this study provides suggestions for further research, but also implications for managers willing to explore TIC in their organization.},
author={Piller, Frank T. and Walcher, Dominik},
journaltitle={R////////\\\&D Management},
pages={307--318},
volume={36},
}

@article{2012AndersonCompetition,
author={Anderson, Simon P. and de Palma, André},
volume={43},
abstract={The Information Age has a surfeit of information received relative to what is processed. We model multiple sectors competing for consumer attention, with competition in price within each sector. Sector advertising levels follow a constant elasticity of substitution (CES) form, and within-sector prices are dispersed with a truncated Pareto distribution. The “information hump” shows highest ad levels for intermediate attention levels. Overall, advertising is excessive, although the allocation across sectors is optimal. The blame for information overload falls most on product categories with low information transmission costs and low profits.},
number={1},
journaltitle={The RAND Journal of Economics},
doi={10.1111/j.1756-2171.2011.00155.x},
title={Competition for attention in the Information (overload) Age},
date={2012},
pages={1--25},
}

@article{2003___,
volume={114},
date={2003},
journaltitle={The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
}

@book{2018Caffea,
title={Caffe},
publisher={<},
url={http://caffe.berkeleyvision.org/{////////\\%}3E},
date={2018},
location={Caffe | Model Zoo Disponvel em},
}

@article{2011OrdaniniCrowd,
pages={443--470},
abstract={Purpose – The purpose of this paper is to analyze the emerging crowd-funding phenomenon, that is a collective effort by consumers who network and pool their money together, usually via the internet, in order to invest in and support efforts initiated by other people or organizations. Successful service businesses that organize crowd-funding and act as intermediaries are emerging, attesting to the viability of this means of attracting investment. Design/methodology/approach – The research employs a “grounded theory” approach, performing an in-depth qualitative analysis of three cases involving crowd-funding initiatives: SellaBand in the music business, Trampoline in financial services, and Kapipal in non-profit services. These cases were selected to represent a diverse set of crowd-funding operations that vary in terms of risk/return for the investor and the type of payoff associated to the investment. Findings – The research addresses two research questions: how and why do consumers turn into crowd-funding participants? and how and why do service providers set up a crowd-funding initiative? Concerning the first research question, the authors' findings reveal purposes, characteristics, roles and tasks, and investment size of crowd-funding activity from the consumer's point of view. Regarding the second research question, the authors' analysis reveals purposes, service roles, and network effects of crowd-funding activity investigated from the point of view of the service organization that set up the initiative. Practical implications – The findings also have implications for service managers interested in launching and/or managing crowd-funding initiatives. Originality/value – The paper addresses an emerging phenomenon and contributes to service theory in terms of extending the consumer's role from co-production and co-creation to investment.},
volume={22},
journaltitle={Journal of Service Management},
issn={1757-5818},
author={Ordanini, Andrea and Miceli, Lucia and Pizzetti, Marta and Parasuraman, a.},
keywords={crowd-funding,customer-investors,customers,investments,paper type research paper,service innovation},
number={4},
doi={10.1108/09564231111155079},
title={Crowd-funding: transforming customers into investors through innovative service platforms},
date={2011},
}

@book{2018Neona,
location={Intel AI Disponvel em},
date={2018},
publisher={<},
title={Neon},
url={https://ai.intel.com/neon/{////////\\%}3E},
groups={tesse:4},
}

@misc{Matthieu,
title={Matthieu Ricard, Trinh Xuan Thuan-The quantum and the lotus a journey to the frontiers where science and Buddhism meet-Three Rivers Press (2004).pdf},
}

@article{2012TielemanRmsprop,
author={Tieleman, T and Hinton, G Lecture 6.5-rmsprop:},
journaltitle={COURSERA: Neural networks for machine learning, v.},
title={rmsprop: Divide the gradient by a running average of its recent magnitude},
date={2012},
volume={4},
}

@article{1875ArthurIs,
author={Arthur, By W Brian},
title={Is the Information Revolution Dead ?},
date={1875},
}

@article{2010FiniOutside,
pages={1060--1069},
title={Inside or outside the IP system? Business creation in academia},
doi={10.1016/j.respol.2010.05.014},
abstract={Research and public policy on academic entrepreneurship are largely based on the assumption that faculty members start businesses to commercialize inventions that have been disclosed to university administrators and have been patented. In this paper, we analyze a sample of 11,572 professors and find that much academic entrepreneurship occurs outside the university intellectual property system. Specifically, about 2/3 of businesses started by academics are not based on disclosed and patented inventions. Moreover, we show that individual characteristics, departmental and organizational affiliations, and time allocation of academics that have started business outside the IP system are different from those of academics that have started businesses to exploit disclosed and patented inventions. We discuss the implications for research on and the practice of academic entrepreneurship. 2010 Elsevier B.V. All rights reserved.},
author={Fini, Riccardo and Lacetera, Nicola and Shane, Scott},
number={8},
volume={39},
issn={0048-7333},
isbn={0048-7333},
journaltitle={Research Policy},
keywords={Academic entrepreneurship,Business creation,Knowledge transfer},
date={2010},
}

@article{2017RisiNeuroevolutiona,
volume={9},
author={Risi, S and Togelius, J},
journaltitle={IEEE Transactions on Computational Intelligence and AI in Games, v.},
date={2017},
title={Neuroevolution in games:State of the art and open challenges},
}

@article{1996HaganNeural,
author={Hagan, M T and Others},
journaltitle={Traducao. [s.l.] Pws Pub},
volume={20},
groups={tesse:4},
title={Neural network design},
date={1996},
}

@book{1969MinskiIntroduction,
publisher={MIT Press, Cambridge},
location={MA},
author={Minski, M. L. and Papert, S. A. Perceptrons:},
groups={tesse:5},
date={1969},
title={an introduction to computational geometry},
}

@article{JesusMulti,
author={Jesus, T O De and Soares, M S},
title={A Multi-Criteria Analysis of Techniques and Tools for Tracing Software Requirements},
keywords={management,requirements engineering,requirements traceability},
}

@article{2012PoetzValue,
journaltitle={Journal of Product Innovation Management},
number={2},
author={Poetz, Marion K. and Schreier, Martin},
date={2012-03},
title={The Value of Crowdsourcing: Can Users Really Compete with Professionals in Generating New Product Ideas?},
pages={245--256},
issn={0737-6782},
volume={29},
doi={10.1111/j.1540-5885.2011.00893.x},
abstract={Generating ideas for new products used to be the exclusive domain of marketers, engineers, and/or designers. Users have only recently been recognized as an alternative source of new product ideas. Whereas some have attributed great potential to outsourcing idea generation to the “crowd” of users (“crowdsourcing”), others have clearly been more skeptical. The authors join this debate by presenting a real-world comparison of ideas actually generated by a firm's professionals with those generated by users in the course of an idea generation contest. Both professionals and users provided ideas to solve an effective and relevant problem in the consumer goods market for baby products. Executives from the underlying company evaluated all ideas (blind to their source) in terms of key quality dimensions including novelty, customer benefit, and feasibility. The study reveals that the crowdsourcing process generated user ideas that score significantly higher in terms of novelty and customer benefit, and somewhat lower in terms of feasibility. However, the average values for feasibility—in sharp contrast to novelty and customer benefit—tended to be relatively high overall, meaning that feasibility did not constitute a narrow bottleneck in this study. Even more interestingly, it is found that user ideas are placed more frequently than expected among the very best in terms of novelty and customer benefit. These findings, which are quite counterintuitive from the perspective of classic new product development (NPD) literature, suggest that, at least under certain conditions, crowdsourcing might constitute a promising method to gather user ideas that can complement those of a firm's professionals at the idea generation stage in NPD.},
}

@article{DoctoralNo,
title={No Title},
groups={tesse:4},
author={Doctoral, Helping and Write, Students},
}

@article{2016Zamani-sabziStatistical,
title={Statistical and analytical comparison of multi-criteria decision-making techniques under fuzzy environment},
author={Zamani-sabzi, Hamed and Phillip, James and Gard, Charlotte C and Abudu, Shalamu},
publisher={Elsevier Ltd},
keywords={Statistical analysis of ranking methods,Fuzzy envi,methods,statistical analysis of ranking},
pages={92--117},
date={2016},
issn={2214-7160},
volume={3},
journaltitle={Operations Research Perspectives},
doi={10.1016/j.orp.2016.11.001},
}

@article{2010BehzadianPromethee,
journaltitle={European Journal of Operational Research},
date={2010},
publisher={Elsevier B.V.},
doi={10.1016/j.ejor.2009.01.021},
pages={198--215},
author={Behzadian, Majid and Kazemzadeh, R B and Albadvi, A and Aghdasi, M},
volume={200},
issn={0377-2217},
number={1},
title={PROMETHEE : A comprehensive literature review on methodologies and applications},
}

@article{CorporationL,
pages={978--986},
title={notitle},
author={Corporation, Westinghouse Electric and Pittsburgh, East and Arbor, Ann},
number={4},
}

@article{SamplesPhilharmonia,
title={Philharmonia Orchestra},
author={Samples, Sound},
url={http://www.philharmonia.co.uk/explore/sound{////////\\_}samples{////////\\%}3E},
volume={2018},
groups={tesse:4},
}

@article{2016BahrampourC,
title={C OMPARATIVE S TUDY OF C AFFE , N EON , T HEANO , AND T ORCH FOR D EEP L EARNING},
author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
date={2016},
pages={1--11},
}

@article{2007DonosoF,
title={A f ´ ısica do violino},
date={2007},
number={December},
keywords={acoustics,helmholtz,musical instruments,resonance,violin},
author={Donoso, Pedro and Tann, Alberto and Guimar, Francisco},
}

@article{1985Diego862,
date={1985},
author={Diego, S A N},
number={V},
title={862 18 120},
}

@article{2015ReyesSupersymmetric,
eprinttype={arXiv},
author={Reyes, Marco A and Arcos-olalla, Rafael},
date={2015},
number={2},
eprint={arXiv:1510.03735v1},
arxivid={arXiv:1510.03735v1},
title={Supersymmetric features of the Error and Dawson ' s functions arXiv : 1510 . 03735v1 [ math-ph ] 13 Oct 2015},
keywords={02,05,1,10,11,30,dawson,error function,gp,ln,mv,pacs numbers,pb,s function,supersymmetry,the error function,the integral,which is defined by},
pages={1--13},
}

@article{2005SawhneyCollaborating,
abstract={In the networked world, firms are recognizing the power of the Internet as a platform for co-creating value with customers.We focus on how the Internet has impacted the process of collaborative innovation—a key process in value co-creation.We outline the distinctive capabilities of the Internet as a platform for customer engagement, including interactivity, enhanced reach, persistence, speed, and flexibility, and suggest that firms can use these capabilities to engage customers in collaborative product innovation through a variety of Internet-based mechanisms.We discuss how these mechanisms can facilitate collaborative innovation at different stages of the New Product Development process (back end vs. front end stages) and for differing levels of customer involvement (high reach vs. high richness).We present two detailed exploratory case studies to illustrate the integrated and systematic usage of Internetbased collaborative innovation mechanisms—Ducati from the motorbike industry and Eli Lilly from the pharmaceutical industry.We derive implications for managerial practice and academic research on collaborative innovation.},
number={4},
publisher={Elsevier},
pages={4--17},
author={Sawhney, Mohanbir and Verona, Gianmario and Prandelli, Emanuela},
journaltitle={Journal of Interactive Marketing},
title={Collaborating to create: The Internet as a platform for customer engagement in product innovation},
doi={10.1002/dir.20046},
issn={1094-9968},
url={http://linkinghub.elsevier.com/retrieve/pii/S1094996805700785},
date={2005-01},
volume={19},
}

@article{1996SerafinVirtual,
title={Virtual Reality Musical Instruments : State of the Art , Design Principles , and Future Directions},
author={Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C and Nordahl, Rolf and Gables, Coral},
date={1996},
doi={10.1162/COMJ},
}

@article{2017EphratLooking,
eprint={arXiv:1804.03619v1},
date={2017},
eprinttype={arXiv},
author={Ephrat, Ariel and Hebrew, The and Freeman, William T and Rubinstein, Michael and Jon, Only and Rory, Only},
arxivid={arXiv:1804.03619v1},
title={Looking to Listen at the Cocktail Party : A Speaker-Independent Audio-Visual Model for Speech Separation},
}

@article{2013ElliotCross,
title={Cross-Cultural Creativity: Conceptualization and propositions for global new product development},
abstract={In today's global business environment, where multinational companies are pressed to increase revenues in order to survive, creativity may hold the key to ensuring their new product development (NPD) efforts lead to innovations with worldwide appeal, such as Apple's iPad and Gillette's Fusion Razor. To leverage creativity for effective global NPD, businesses want to know how cultures differ in their concepts of creativity and the impact of those differences on approaches to developing new products. Because global new products are increasingly developed in, by, and for multiple cultures, a particular need is for a culturally reflective understanding, or conceptualization, of creativity. While creativity is believed to be culturally tied, the dominant framework of creativity used in business and management assumes that creativity is culturally indifferent or insensitive. This knowledge gap is addressed by studying the role of creativity in NPD practices in a cross-cultural or global context. The study begins by first developing a culturally anchored conceptualization of creativity. Called cross-cultural creativity, the concept draws on creativity insights from the field of art and aesthetics. The concept specifies two modes of creativity, neither of which is superior to the other, called the spontaneous or S route and the divergent or D route. The S route emphasizes adaptiveness, processes, intuitiveness, and metamorphism, while the D route focuses on disruptiveness, results, rationality, and literalism. Next, this new concept is applied to NPD by positing how creativity in distinct cultures may shape NPD practices, as illustrated by Japanese and U.S. firms. Research propositions are formulated to capture these patterns, and thereafter, theoretical and practical implications of the framework and propositions are discussed. The implications center on global NPD, which is a complex enterprise involving typically more than one culture to design and develop new products for several geographic markets. The study is of interest to researchers needing a globally situated, culturally attached framework of creativity for international NPD studies, and managers seeking to exploit creativity in multinational and multicultural innovation projects.},
author={Elliot, E.a. and Nakata, C.},
doi={10.1111/jpim.12066},
pages={110--125},
journaltitle={Journal of Product Innovation Management},
volume={30},
date={2013-12},
issn={0737-6782},
}

@misc{Revisao,
title={A revisão da bibliografia em teses e dissertações.pdf},
}

@article{Tao,
number={Xxx},
title={tão simples e compacta quanto possível , adotando-se , para tanto , a notação matricial . Embora matematicamente equivalente às derivações apresentadas em ( XXX ), optou-se por uma abordagem direta com a intenção de tornar mais intuitivo o entendimento do},
}

@article{2002ZopounidisMulticriteria,
author={Zopounidis, Constantin and Doumpos, Michael},
pages={229--246},
keywords={classification,decision rules,multiple criteria analysis,outranking relations,preference,sorting,utility functions},
volume={138},
date={2002},
title={Multicriteria classification and sorting methods : A literature review},
}

@book{2011UlrichDesign,
title={Design: Creation of Artifacts in Society},
publisher={University of Pennsylvania},
year={2011},
author={Ulrich, Karl},
url={https://www.amazon.com/Design-Creation-Artifacts-Karl-Ulrich-ebook/dp/B005S4EO1Y?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=B005S4EO1Y},
isbn={9780983648703},
}

@article{Percussion,
title={Percussion Instrument Modelling In 3D : Sound Synthesis Through Time Domain Numerical Simulation University of Edinburgh},
}

@article{schaffner2017towards,
title={Towards Edge-Aware Spatio-Temporal Filtering in Real-Time},
number={1},
year={2017},
pages={265--280},
publisher={IEEE},
journal={IEEE Transactions on Image Processing},
author={Schaffner, Michael and Scheidegger, Florian and Cavigelli, Lukas and Kaeslin, Hubert and Benini, Luca and Smolic, Aljosa},
volume={27},
}

@article{Institute,
title={Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. ® www.jstor.org},
}

@article{2015HighlanderArxiv,
eprinttype={arXiv},
date={2015},
pages={1--9},
arxivid={arXiv:1601.06815v1},
author={Highlander, Tyler},
title={arXiv : 1601 . 06815v1 [ cs . NE ] 25 Jan 2016 Very Efficient Training of Convolutional Neural Networks using Fast Fourier},
eprint={arXiv:1601.06815v1},
}

@article{2017IshizakaAre,
author={Ishizaka, Alessio and Siraj, Sajid},
publisher={Elsevier},
journaltitle={European Journal of Operational Research},
doi={10.1016/j.ejor.2017.05.041},
title={Are multi-criteria decision-making tools useful? An experimental comparative study of three methods},
date={2017},
keywords={AHP,Decision analysis,Experimental evaluation,MACBETH,SMART},
issn={0377-2217},
}

@unpublished{1985ParkerLearninga,
title={Learning logic},
author={Parker, D B},
date={1985},
}

@article{OliveiraSmart,
author={Oliveira, G A Q S M and Seleme, R and Zattar, I C},
keywords={multicriteria,project management,roadmap,smart grids},
title={Smart Grid Performance Assessment Via Approach Method},
}

@article{2002DewickTyndall,
date={2002},
author={Dewick, Paul and Green, Ken and Miozzo, Marcela},
title={Tyndall ˚ Centre and the Environment Technological Change , Industry Structure and the Environment},
number={January},
}

@book{NormanEveryday,
author={Norman, Don},
title={of EVERYDAY THINGS THE DESIGN OF EVERYDAY},
isbn={9780465050659},
}

@article{BahrampourComparative,
title={Comparative Study of Deep Learning Software Frameworks},
eprinttype={arXiv},
arxivid={arXiv:1511.06435v3},
author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
eprint={arXiv:1511.06435v3},
}

@article{2017ZappiShader,
pages={145--150},
title={Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments},
date={2017},
author={Zappi, Victor and Allen, Andrew and Fels, Sidney},
groups={tesse:4},
}

@article{JaderbergDecoupled,
title={Decoupled Neural Interfaces using Synthetic},
author={Jaderberg, Max and Graves, Alex},
volume={1},
arxivid={arXiv:1608.05343v2},
eprinttype={arXiv},
eprint={arXiv:1608.05343v2},
}

@article{2014Polyphonic,
title={POLYPHONIC PIANO TRANSCRIPTION USING NON-NEGATIVE MATRIX FACTORISATION WITH GROUP SPARSITY Ken O ' Hanlon and Mark D . Plumbley Queen Mary University of London},
number={May},
pages={3136--3140},
volume={1},
date={2014},
}

@article{1997AlbertsInformation,
author={Alberts, David S and Papp, Daniel S},
date={1997},
title={The Information Age : An Anthology on Its Impact and Consequences Table of Contents},
}

@book{ThePython,
author={The, Keras:},
title={Python Deep Learning library},
url={https://keras.io/{////////\\%}3E},
publisher={<},
location={Keras Documentation, [s.d.]. Disponvel em},
}

@misc{._sawhney,
title={.Sawhney, Verona e Prandelli2005(JIM)COLLABORATING TO CREATE- THE INTERNET AS A PLATFORM FOR CUSTOMER ENGAGEMENT IN PRODUCT INNOVATION.pdf},
}

@article{2018CarayannisTechnological,
date={2018},
title={Technological Forecasting ////////\\\& Social Change MCDA in knowledge-based economies : Methodological developments and real world applications},
author={Carayannis, Elias G and Ferreira, João J M and Jalali, Marjan S and Ferreira, Fernando A F},
doi={10.1016/j.techfore.2018.01.028},
number={xxxx},
publisher={Elsevier},
journaltitle={Technological Forecasting ////////\\\& Social Change},
issn={0040-1625},
pages={0--1},
}

@article{2008ArtsPhysical,
date={2008},
author={Arts, Sonic},
title={Physical modelling of the piano : An investigation into the e ff ect of string sti ff ness on the hammer-string interaction},
number={September},
}

@article{2013IndexTechnology,
author={Index, Industrial},
title={Technology Is Wiping Out Companies Faster than Ever},
pages={2013--2014},
date={2013},
}

@article{LabBerkeley,
volume={2018},
url={http://bair.berkeley.edu/{////////\\%}3E},
groups={tesse:4},
author={Lab, Berkeley Artificial Intelligence Research},
title={The Berkeley Artificial Intelligence Research Blog},
}

@book{maeda2006laws,
publisher={MIT press},
author={Maeda, John},
title={The laws of simplicity},
year={2006},
}

@article{NayebiGruv,
author={Nayebi, Aran and Vitelli, Matt},
title={GRUV : Algorithmic Music Generation using Recurrent Neural Networks},
pages={1--6},
}

@inproceedings{1990Widrow30,
publisher={v. 78},
journaltitle={Proceedings of the IEEE},
doi={10.1109/5.58323},
urldate={2019-04-07},
url={http://ieeexplore.ieee.org/document/58323/},
title={30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation},
shorttitle={30 years of adaptive neural networks},
booktitle={Proceedings of the IEEE},
issn={0018-9219},
date={1990-09},
year={1990},
volume={78},
number={9},
langid={english},
pages={1415--1442},
author={Widrow, Bernard and Lehr, Michael A.},
location={n. 9, p. 14151442},
}

@article{2018RobertsHierarchical,
eprinttype={arXiv},
url={http://arxiv.org/abs/1803.05428},
volume={abs/1803.05428},
title={A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
bibsource={dblp computer science bibliography, https://dblp.org},
groups={tesse:5},
journaltitle={CoRR},
journal={CoRR},
year={2018},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1803-05428},
author={Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
eprint={1803.05428},
timestamp={Mon, 13 Aug 2018 16:47:28 +0200},
date={2018},
}

@article{2012ZeilerAdadelta,
year={2012},
journal={CoRR},
eprint={1212.5701},
volume={abs/1212.5701},
bibsource={dblp computer science bibliography, https://dblp.org},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1212-5701},
journaltitle={CoRR},
title={ADADELTA: An Adaptive Learning Rate Method},
author={Zeiler, Matthew D.},
eprinttype={arXiv},
date={2012},
timestamp={Mon, 13 Aug 2018 16:45:57 +0200},
url={http://arxiv.org/abs/1212.5701},
}

@article{2014KingmaAdam,
url={http://arxiv.org/abs/1412.6980},
title={Adam: A Method for Stochastic Optimization},
date={2014},
eprint={1412.6980},
timestamp={Mon, 13 Aug 2018 16:47:35 +0200},
year={2014},
volume={abs/1412.6980},
bibsource={dblp computer science bibliography, https://dblp.org},
biburl={https://dblp.org/rec/bib/journals/corr/KingmaB14},
journaltitle={CoRR},
author={Kingma, Diederik P. and Ba, Jimmy},
journal={CoRR},
eprinttype={arXiv},
}

@inproceedings{2000FontanaUsing,
groups={tesse:5},
date={2000-12},
volume={2000},
author={Fontana, Federico and Rocchesso, Davide and Apollonio, Enzo},
journaltitle={Anais},
title={Using the waveguide mesh in modelling 3D resonators},
pages={229--232},
booktitle={Proceedings of the International Conference on Digital Audio Effects (DAFx)},
}

@article{1991HornikApproximation,
pmid={25246403},
number={2},
journaltitle={Neural Networks},
date={1991},
arxivid={arXiv:1011.1669v3},
publisher={Elsevier},
isbn={0893-6080},
eprinttype={arXiv},
doi={10.1016/0893-6080(91)90009-T},
volume={4},
issn={0893-6080},
pages={251--257},
abstract={We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
title={Approximation capabilities of multilayer feedforward networks},
author={Hornik, Kurt},
eprint={arXiv:1011.1669v3},
groups={tesse:5},
}

@inproceedings{2015NguyenDeep,
author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
date={2015},
booktitle={Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
organization={IEEE},
publisher={IEEE},
url={http://www.evolvingai.org/fooling},
}

@inproceedings{2018DonahueSynthesizing,
title={Synthesizing Audio with GANs},
note={Published: Workshop},
author={Donahue, Chris and McAuley, Julian and Puckette, Miller},
date={2018},
booktitle={Proceedings of the Sixth International Conference on Learning Representations},
url={https://openreview.net/forum?id=r1RwYIJPM},
groups={tesse:5},
howpublished={Workshop},
}

@article{2011DuchiAdaptive,
author={Duchi, John and Hazan, Elad and Singer, Yoram},
issn={1532-4435},
acmid={2021068},
url={http://dl.acm.org/citation.cfm?id=1953048.2021068},
groups={tesse:5},
numpages={39},
volume={12},
date={2011-07},
publisher={JMLR.org},
title={Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
journaltitle={Journal of Machine Learning Research},
pages={2121--2159},
}

@article{1990ElmanFinding,
pages={179--211},
abstract={Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
volume={14},
title={Finding Structure in Time},
eprint={https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402 /\\_1},
author={Elman, Jeffrey L.},
groups={tesse:5},
doi={10.1207/s15516709cog1402///////\\_1},
date={1990},
number={2},
journaltitle={Cognitive Science},
}

@article{2017GrinsteinAudio,
eprinttype={arXiv},
volume={abs/1710.11385},
date={2017},
bibsource={dblp computer science bibliography, https://dblp.org},
timestamp={Mon, 13 Aug 2018 16:48:00 +0200},
url={http://arxiv.org/abs/1710.11385},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1710-11385},
author={Grinstein, Eric and Duong, Ngoc Q. K. and Ozerov, Alexey and Pérez, Patrick},
journaltitle={CoRR},
title={Audio style transfer},
eprint={1710.11385},
groups={tesse:5},
}

@inproceedings{2015HeMultimodala,
isbn={978-1-4503-3743-4},
organization={ACM},
publisher={ACM Press},
groups={tesse:4},
doi={10.1145/2808196.2811641},
date={2015},
booktitle={Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge - AVEC /textquotesingle15},
title={Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks},
location={New York, NY, USA},
note={event-place: Brisbane, Australia},
pages={73--80},
series={AVEC '15},
keywords={audio and video features, DBLSTM-RNN, multimodal fusion, offset, physiological feature},
author={He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
}

@article{1993LeshnoMultilayer,
volume={6},
keywords={(μ) approximation, Activation functions, Multilayer feedforward networks, Role of threshold, Universal approximation capabilities},
publisher={Elsevier},
issn={0893-6080},
doi={10.1016/S0893-6080(05)80131-5},
journaltitle={Neural Networks},
author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
number={6},
date={1993},
title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
url={http://www.sciencedirect.com/science/article/pii/S0893608005801315},
pages={861--867},
}

@article{2013RigaudParametric,
author={Rigaud, François and David, Bertrand and Daudet, Laurent},
volume={133},
date={2013-05},
journaltitle={The Journal of the Acoustical Society of America},
doi={10.1121/1.4799806},
number={5},
pages={3107--3118},
title={A parametric model and estimation techniques for the inharmonicity and tuning of the piano},
}

@book{1999SmithScientist,
url={http://www.dspguide.com/},
isbn={0-9660176-4-1},
groups={tesse:5, DSP},
title={The Scientist and Engineer's Guide to Digital Signal Processing},
author={Smith, Steven W.},
date={1999},
}

@article{2000VaughnMusic,
date={2000},
issn={00218510, 15437809},
url={http://www.jstor.org/stable/3333641},
author={Vaughn, Kathryn},
volume={34},
number={3},
journaltitle={Journal of Aesthetic Education},
title={Music and Mathematics: Modest Support for the Oft-Claimed Relationship},
publisher={University of Illinois Press},
pages={149--166},
}

@book{2015YadavIntroduction,
title={An Introduction to Neural Network Methods for Differential Equations},
doi={10.1007/978-94-017-9816-7},
author={Yadav, Neha and Yadav, Anupam and Kumar, Manoj},
date={2015-02-26},
publisher={Springer Netherlands},
groups={tesse:5},
}

@article{2017ZhangTowards,
url={http://arxiv.org/abs/1701.02720},
bibsource={dblp computer science bibliography, https://dblp.org},
title={Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks},
volume={abs/1701.02720},
timestamp={Mon, 13 Aug 2018 16:49:16 +0200},
eprinttype={arXiv},
date={2017},
biburl={https://dblp.org/rec/bib/journals/corr/ZhangPBZLBC17},
author={Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon and Zhang, Saizheng and Laurent, César and Bengio, Yoshua and Courville, Aaron C.},
groups={tesse:5},
journaltitle={CoRR},
eprint={1701.02720},
}

@article{2017CaoSpatial,
eprint={1712.05293},
groups={tesse:5},
url={http://arxiv.org/abs/1712.05293},
date={2017},
eprinttype={arXiv},
title={Spatial-temporal wind field prediction by Artificial Neural Networks},
author={Cao, Jianan and Farnham, David J. and Lall, Upmanu},
volume={abs/1712.05293},
bibsource={dblp computer science bibliography, https://dblp.org},
timestamp={Mon, 13 Aug 2018 16:48:46 +0200},
journaltitle={CoRR},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1712-05293},
}

@article{1995TabeiSimple,
journaltitle={IEEE transactions on signal processing},
title={A simple estimator for frequency and decay rate},
groups={tesse:5},
number={6},
publisher={IEEE},
doi={10.1109/78.506615},
author={Tabei, Makoto and Musicus, Bruce R.},
issn={1053-587X},
volume={44},
date={1995},
pages={1504--1511},
}

@mastersthesis{2005GaerderPhysical,
year={2005},
langid={english},
date={2005},
title={Physical modeling of percussion instruments},
institution={Chalmers University of Technology},
pages={80},
groups={tesse:5},
author={Gärder, Anders},
}

@book{2010PetreUnwritten,
title={The unwritten rules of PhD research},
doi={10.1049/em:20040508},
isbn={978-0-335-23702-9},
groups={tesse:5},
langid={english},
pmid={1275585},
issn={1472-4677},
pages={320},
publisher={Open University Press},
note={OCLC: 934394471},
pagetotal={272},
author={Petre, Marian and Rugg, Gordon},
url={http://books.google.com/books?id={////////\\_}DDwCqx6wpcC{////////\\&}printsec=frontcover{////////\\&}dq=unwritten+rules+of+phd+research{////////\\&}hl={////////\\&}cd=1{////////\\&}source=gbs{////////\\_}api{////////\\%}255Cnpapers2://publication/uuid/48967E01-55F9-4397-B941-310D9C5405FA{////////\\%}255Cnhttp://medcontent.metapress.com/index/A65RM03P4874243N.p},
series={Open up study skills},
location={Maidenhead},
booktitle={Vasa},
date={2010},
edition={Second edition},
}

@article{2014RehmanImage,
pages={656--672},
doi={10.19026/rjaset.7.303},
volume={7},
date={2014-01-27},
shorttitle={Image Compression},
issn={20407459, 20407467},
journaltitle={Research Journal of Applied Sciences, Engineering and Technology},
url={http://maxwellsci.com/jp/mspabstract.php?jid=RJASET///////\\&doi=rjaset.7.303},
groups={tesse:5},
isbn={9233351788872},
title={Image Compression: A Survey},
author={Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
urldate={2019-03-28},
number={4},
abstract={Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction. Maxwell Scientific Organization, 2014.},
langid={english},
publisher={Maxwell Science Publishing},
keywords={Compression,Image,Lossless,Lossy,Review,image compression- lossy},
}

@article{2002Egmont-petersenImage,
doi={10.1016/S0031-3203(01)00178-9},
publisher={Elsevier},
author={Egmont-Petersen, M. and de Ridder, D. and Handels, H.},
title={Image processing with neural networks—a review},
number={10},
urldate={2019-04-05},
langid={english},
journaltitle={Pattern Recognition},
pages={2279--2301},
abstract={We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopÿeld neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension speciÿes the type of task performed by the algorithm: preprocessing, data reduction=feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses speciÿc constraints to a neural-based approach. These speciÿc conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and speciÿcally to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments. ? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
issn={0031-3203},
url={http://linkinghub.elsevier.com/retrieve/pii/S0031320301001789},
volume={35},
groups={tesse:5},
date={2002-10},
}

@inproceedings{2004CollobertLinks,
eventtitle={Twenty-first international conference},
urldate={2019-04-05},
groups={tesse:5},
pages={23},
url={http://portal.acm.org/citation.cfm?doid=1015330.1015415},
organization={ACM},
abstract={We propose to study links between three important classiﬁcation algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs). We ﬁrst study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs. After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function. We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin. These ideas are extended afterward to the case of MLPs. Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space. Finally, we present a very simple MLP based on the previous ﬁndings, which yields better performances in generalization and speed than the other models.},
title={Links between perceptrons, MLPs and SVMs},
location={Banff, Alberta, Canada},
publisher={ACM Press},
author={Collobert, Ronan and Bengio, Samy},
langid={english},
date={2004},
booktitle={Twenty-first international conference on Machine learning - ICML '04},
doi={10.1145/1015330.1015415},
}

@article{2013HayesFirst,
author={Hayes, Brian},
journaltitle={American Scientist},
date={2013},
number={April},
groups={tesse:5},
title={First Links in the Markov Chain},
pages={7},
langid={english},
}

@article{2012HintonDeep,
author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
date={2012-11},
doi={10.1109/MSP.2012.2205597},
url={http://ieeexplore.ieee.org/document/6296526/},
issn={1053-5888},
urldate={2019-04-07},
volume={29},
langid={english},
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
publisher={IEEE},
number={6},
pages={82--97},
shorttitle={Deep Neural Networks for Acoustic Modeling in Speech Recognition},
journaltitle={IEEE Signal Processing Magazine},
}

@book{2006BishopPattern,
location={New York, NY},
date={2006-08-17},
isbn={0-387-31073-8},
series={Information science and statistics},
url={https://www.ebook.de/de/product/5324937/christopher///////\\_m///////\\_bishop///////\\_pattern///////\\_recognition///////\\_and///////\\_machine///////\\_learning.html},
pagetotal={738},
langid={english},
author={Bishop, Christopher M.},
title={Pattern Recognition and Machine Learning},
edition={Corrected at 8th printing 2009},
note={OCLC: 845772798},
groups={tesse:5},
publisher={Springer-Verlag New York Inc.},
}

@article{2017OmarPredicting,
abstract={This study explores the effectiveness of an Artificial Neural Network (ANN) in predicting fraudulent financial reporting in small market capitalization companies in Malaysia. Design/methodology/approach Based on the concepts of ANN, a mathematical model is developed to compare non-fraud and fraud companies selected from among small market capitalization companies in Malaysia; the fraud companies had already been charged by the Securities Commission for the falsification of financial statements. Ten financial ratios are used as fraud risk indicators to predict fraudulent financial reporting using ANN. Findings Indicate that the proposed ANN methodology outperforms other statistical techniques widely used for predicting fraudulent financial reporting. Originality/value The study is one of few to adopt the ANN approach to the prediction of financial reporting fraud.},
volume={24},
doi={10.1108/jfc-11-2015-0061},
isbn={1359079051062},
issn={1359-0790},
title={Predicting fraudulent financial reporting using artificial neural network},
number={2},
date={2017},
url={http://dx.doi.org/10.1108/eb025814{////////\\%}5Cnhttp://},
pages={362--387},
groups={tesse:5},
author={Omar, Normah and Johari, Zulaikha `Amirah and Smith, Malcolm},
keywords={security},
journaltitle={Journal of Financial Crime},
}

@inproceedings{2018ReddiConvergence,
author={Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
title={On the Convergence of Adam and Beyond},
groups={tesse:5},
booktitle={Proceedings of the International Conference on Learning Representations},
url={https://openreview.net/forum?id=ryQu7f-RZ},
date={2018},
}

@article{2018PangConvolution,
groups={tesse:5},
author={Pang, Y. and Sun, M. and Jiang, X. and Li, X.},
title={Convolution in Convolution for Network in Network},
issn={2162-237X},
journaltitle={IEEE Transactions on Neural Networks and Learning Systems},
pages={1587--1597},
doi={10.1109/TNNLS.2017.2676130},
number={5},
publisher={IEEE Transactions on Neural Networks and Learning Systems},
date={2018-05},
volume={29},
}

@article{2016GoldbergPrimer,
journaltitle={J. Artif. Intell. Res.(JAIR)},
volume={57},
title={A Primer on Neural Network Models for Natural Language Processing.},
author={Goldberg, Yoav},
date={2016},
pages={345--420},
groups={tesse:5},
timestamp={Mon, 13 Aug 2018 16:48:41 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/Goldberg15c},
eprint={1510.00726},
eprinttype={arXiv},
url={http://arxiv.org/abs/1510.00726},
bibsource={dblp computer science bibliography, https://dblp.org},
}

@article{2016RuderOverview,
author={Ruder, Sebastian},
volume={abs/1609.04747},
title={An overview of gradient descent optimization algorithms},
pages={1--14},
bibsource={dblp computer science bibliography, https://dblp.org},
groups={tesse:5},
timestamp={Mon, 13 Aug 2018 16:48:10 +0200},
date={2016},
url={http://arxiv.org/abs/1609.04747},
journaltitle={CoRR},
arxivid={arXiv:1609.04747v2},
biburl={https://dblp.org/rec/bib/journals/corr/Ruder16},
eprint={arXiv:1609.04747v2},
eprinttype={arXiv},
}

@article{2017MitalTime,
date={2017},
number={Nips},
eprinttype={arXiv},
volume={abs/1711.11160},
bibsource={dblp computer science bibliography, https://dblp.org},
biburl={https://dblp.org/rec/bib/journals/corr/abs-1711-11160},
journaltitle={CoRR},
groups={tesse:5},
eprint={arXiv:1711.11160v1},
arxivid={arXiv:1711.11160v1},
title={Time Domain Neural Audio Style Transfer},
timestamp={Mon, 13 Aug 2018 16:47:15 +0200},
author={Mital, Parag K.},
url={http://arxiv.org/abs/1711.11160},
}

@article{2016AljumahNovel,
groups={tesse:5},
pages={132--138},
date={2016},
author={Aljumah, A. and Ahamad, T. A.},
title={A novel approach for detecting DDoS using artificial neural networks},
journaltitle={International Journal of Computer Science and Network Security},
volume={16},
}

@inproceedings{2018HintonMatrix,
author={Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
year={2018},
booktitle={Proceedings of the International Conference on Learning Representations},
date={2018},
url={https://openreview.net/forum?id=HJWLfGWRb},
pages={1--15},
groups={tesse:5},
title={Matrix capsules with EM routing},
}

@book{2010LaiIntroduction,
langid={english},
author={Lai, W. Michael and Rubin, David and Krempl, Erhard},
pagetotal={556},
title={Introduction to continuum mechanics},
location={Amsterdam},
edition={3. ed., reprint},
publisher={Elsevier [u.a.]},
date={2010},
isbn={978-0-7506-2894-5},
groups={tesse:5},
note={OCLC: 832831309},
}

@article{2016ShiImproving,
pages={7},
title={Improving CNN Performance with Min-Max Objective},
issn={1045-0823},
journaltitle={IJCAI International Joint Conference on Artificial Intelligence},
groups={tesse:5},
date={2016},
author={Shi, Weiwei and Gong, Yihong and Wang, Jinjun},
volume={2016-Janua},
keywords={Machine Learning,object detection},
mendeley-tags={object detection},
abstract={In this paper, we propose a novel method to improve object recognition accuracies of convolutional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The MinMax objective explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin between different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computation cost. Experiments with shallow and deep models on four benchmark datasets including CIFAR10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max objective achieve remarkable performance improvements compared to the corresponding baseline models.},
langid={english},
}

@article{2016ZhangImproving,
abstract={In this paper, we choose to learn useful cues from object recognition mechanisms of the human visual cortex, and propose a DCNN performance improvement method without the need for increasing the network complexity. Inspired by the categoryselective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As experimental results show, when applying the proposed method to the “Quick” model and NIN models, image classiﬁcation performances are remarkably improved on four widely used benchmark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
date={2016},
journaltitle={IJCAI International Joint Conference on Artificial Intelligence},
issn={1045-0823},
keywords={Machine Learning,image synthesis},
author={Zhang, Shizhou and Gong, Yihong and Wang, Jinjun},
title={Improving DCNN Performance with Sparse Category-Selective Objective Function},
groups={tesse:5},
mendeley-tags={image synthesis},
volume={2016-Janua},
pages={7},
langid={english},
}

@article{2014YosinskiHow,
eprint={1411.1792},
issn={1049-5258},
keywords={theory},
langid={english},
groups={tesse:2},
author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
title={How transferable are features in deep neural networks?},
eprinttype={arXiv},
url={http://arxiv.org/abs/1411.1792},
abstract={Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the ﬁrst layer they learn features similar to Gabor ﬁlters and color blobs. Such ﬁrst-layer features appear not to be speciﬁc to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to speciﬁc by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus speciﬁcity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difﬁculties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A ﬁnal surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after ﬁne-tuning to the target dataset.},
arxivid={1411.1792},
mendeley-tags={theory},
pages={9},
date={2014},
}

@inproceedings{2013Boulanger-lewandowskiHigh,
author={Boulanger-Lewandowski, N. and Bengio, Y. and Vincent, P.},
date={2013-05},
organization={IEEE},
title={High-dimensional sequence transduction},
publisher={AnaisIEEE},
keywords={Accuracy, global distribution mode, Hidden Markov models, high dimensional output sequence, high dimensional sequence transduction, musically plausible transcription, Noise, polyphonic audio music, polyphonic transcription, probabilistic model, realistic output distribution, recurrent neural network, Recurrent neural networks, restricted Boltzmann machine, Sequence transduction, Smoothing methods, symbolic notation, test error rate, Training, Vectors},
groups={tesse:5},
booktitle={Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
pages={3178--3182},
doi={10.1109/ICASSP.2013.6638244},
}

@article{2015LeeGeneralizing,
journaltitle={Artificial Intelligence and Statistics},
url={http://arxiv.org/abs/1509.08985},
keywords={optimization},
eprinttype={arXiv},
eprint={1509.08985},
volume={51},
abstract={We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling ﬁlters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These beneﬁts come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.},
mendeley-tags={optimization},
booktitle={Artificial Intelligence and Statistics},
pmid={67101},
issn={0162-8828},
pages={9},
doi={10.1109/TPAMI.2017.2703082},
title={Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree},
arxivid={1509.08985},
author={Lee, Chen-Yu and Gallagher, Patrick W and Tu, Zhuowen},
langid={english},
date={2015},
}

@inproceedings{2015SakFast,
author={Sak, Hasim and Senior, Andrew W. and Rao, Kanishka and Beaufays, Françoise},
abstract={We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.},
date={2015},
booktitle={Interspeech 2015},
eprint={1507.06947},
pages={1468--1472},
url={https://www.isca-speech.org/archive/interspeech///////\\_2015/i15///////\\_1468.html},
keywords={speech recognition},
pmid={1000285842},
eprinttype={arXiv},
arxivid={1507.06947},
journaltitle={arXiv preprint arXiv:1507.06947},
title={Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition},
mendeley-tags={speech recognition},
}

@inproceedings{2016PhurattanaprapinExtended,
isbn={978-1-5090-2033-1},
doi={10.1109/JCSSE.2016.7748874},
pages={1--5},
date={2016-07},
volume={10},
keywords={ELM,chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
groups={tesse:5},
publisher={IEEE},
mendeley-tags={ELM},
urldate={2019-04-05},
booktitle={2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
author={Phurattanaprapin, Khanittha and Horata, Punyaphol},
langid={english},
location={Khon Kaen, Thailand},
url={http://ieeexplore.ieee.org/document/7748874/},
title={Extended Hierarchical Extreme Learning Machine with Multilayer Perceptron},
abstract={The Deep Learning approach provides a high performance of classiﬁcation, especially when invoking image classiﬁcation problems. However, a shortcomming of the traditional Deep Learning method is the large time scale of training. The hierarchical extreme learning machine (H-ELM) framework was based on the hierarchical learning architecture of multilayer perceptron to address the problem. H-ELM is composed of two parts; the ﬁrst entails unsupervised multilayer encoding, and the second is the supervised feature classiﬁcation. H-ELM can give a higher accuracy rate than the traditional ELM. However, there still remains room to enhance its classiﬁcation performance. This paper therefore proposes a new method termed the extending hierarchical extreme learning machine (EH-ELM), which extends the number of layers in the supervised portion of the H-ELM from a single layer to multiple layers. To evaluate the performance of the EH-ELM, the various classiﬁcation datasets were studied and compared with the H-ELM and the multilayer ELM, as well as various state-of-the-art such deep architecture methods. The experimental results show that the EH-ELM improved the accuracy rates over most other methods.},
eventtitle={2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
number={2},
journaltitle={ECTI Transactions on Computer and Information Technology},
}

@inproceedings{bonada_expressive_2016,
date={2016-09-08},
eventtitle={Interspeech 2016},
pages={1230--1234},
keywords={[Electronic Manuscript]},
abstract={Sample and statistically based singing synthesizers typically require a large amount of data for automatically generating expressive synthetic performances. In this paper we present a singing synthesizer that using two rather small databases is able to generate expressive synthesis from an input consisting of notes and lyrics. The system is based on unit selection and uses the Wide-Band Harmonic Sinusoidal Model for transforming samples. The ﬁrst database focuses on expression and consists of less than 2 minutes of free expressive singing using solely vowels. The second one is the timbre database which for the English case consists of roughly 35 minutes of monotonic singing of a set of sentences, one syllable per beat. The synthesis is divided in two steps. First, an expressive vowel singing performance of the target song is generated using the expression database. Next, this performance is used as input control of the synthesis using the timbre database and the target lyrics. A selection of synthetic performances have been submitted to the Interspeech Singing Synthesis Challenge 2016, in which they are compared to other competing systems.},
doi={10.21437/Interspeech.2016-872},
langid={english},
url={http://www.isca-speech.org/archive/Interspeech///////\\_2016/abstracts/0872.html},
title={Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016},
author={Bonada, Jordi and Umbert, Martí and Blaauw, Merlijn},
urldate={2019-03-26},
}

@article{2015WuDrum,
groups={tesse:5},
mendeley-tags={music transcription,tesis},
doi={10.1109/EUSIPCO.2015.7362590},
pages={1281--1285},
isbn={9780992862633},
langid={english},
journaltitle={2015 23rd European Signal Processing Conference, EUSIPCO 2015},
abstract={In this paper, a drum transcription algorithm using partially fixed non-negative matrix factorization is presented. The proposed method allows users to identify percussive events in complex mixtures with a minimal training set. The algorithm decomposes the music signal into two parts: percussive part with pre-defined drum templates and harmonic part with undefined entries. The harmonic part is able to adapt to the music content, allowing the algorithm to work in polyphonic mixtures. Drum event times can be simply picked from the percussive activation matrix with onset detection. The system is efficient and robust even with a minimal training set. The recognition rates for the ENST dataset vary from 56.7 to 78.9 /\\% for three percussive instruments extracted from polyphonic music.},
title={Drum transcription using partially fixed non-negative matrix factorization},
author={Wu, Chih Wei and Lerch, Alexander},
keywords={Automatic Music Transcription,Drum Transcription,MIR,NMF,music transcription,tesis},
date={2015},
}

@electronic{2018WikiDrumgizmo,
date={2018},
urldate={30.06.2018},
volume={2018},
author={Wiki, DrumGizmo},
title={DrumGizmo Wiki},
url={https://www.drumgizmo.org/wiki/doku.php},
howpublished={online},
}

@article{2014SrivastavaDropout,
isbn={1532-4435},
keywords={deep learning,model combination,neural networks,regularization,seminal},
journaltitle={Journal of Machine Learning Research},
title={Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
pages={1929--1958},
volume={15},
mendeley-tags={seminal},
author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
eprinttype={arXiv},
eprint={1102.4807},
langid={english},
date={2014},
abstract={Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
issn={1533-7928},
arxivid={1102.4807},
doi={10.1214/12-AOS1000},
}

@article{2004KarjalainenDigital,
publisher={Springer Nature},
volume={2004},
author={Matti Karjalainen and Cumhur Erkut},
keywords={acoustic signal processing,and phrases,digital waveguides,fdtd model structures,hybrid models,scattering},
month={jun},
title={Digital Waveguides versus Finite Difference Structures: Equivalence and Mixed Modeling},
date={2004},
number={7},
pages={978--989},
doi={10.1155/s1110865704401176},
journaltitle={Journal on Advances in Signal Processing (EURASIP)},
journal={EURASIP} Journal on Advances in Signal Processing},
year={2004},
}

@article{2011DudaDft,
number={11},
langid={english},
title={DFT-based Estimation of Damped Oscillation Parameters in Low-Frequency Mechanical Spectroscopy},
date={2011-11},
url={http://ieeexplore.ieee.org/document/6022793/},
doi={10.1109/TIM.2011.2113124},
issn={0018-9456, 1557-9662},
author={Duda, Krzysztof and Magalas, Leszek B. and Majewski, Mariusz and Zielinski, Tomasz P.},
pages={3608--3618},
urldate={2019-03-28},
journaltitle={IEEE Transactions on Instrumentation and Measurement},
volume={60},
abstract={In this paper, we analyze and compare the properties of different well-known and also new nonparametric discrete Fourier transform (DFT)-based methods for resonant frequency and logarithmic decrement estimation in application to mechanical spectroscopy. We derive a new DFT interpolation algorithm for a signal analyzed with Rife–Vincent class-I windows and also propose new formulas that extend Bertocco and Yoshida methods. We study errors of the resonant frequency and logarithmic decrement estimation in realistic conditions that include measurement noise and a zero-point drift. We also investigate the systematic errors of the estimation methods of interest. A nonlinear least squares time-domain parametric signal ﬁtting is used to determine the boundaries of statistical efﬁciency in all tests.},
}

@misc{2000FeldmanDerivation,
author={Feldman, Joel},
groups={tesse:5},
title={Derivation of the Wave Equation},
url={http://www.math.ubc.ca//textasciitilde feldman/m256/wave.pdf},
pages={1--2},
date={2000},
}

@article{2017PengDeepmimic,
number={4},
doi={10.1145/3197517.3201311},
volume={37},
issn={0730-0301},
shorttitle={DeepMimic},
abstract={A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.},
arxivid={arXiv:1804.02717v1},
eprint={arXiv:1804.02717v1},
journaltitle={ACM Transactions on Graphics},
eprinttype={arXiv},
date={2017},
url={http://arxiv.org/abs/1804.02717},
title={DeepMimic : Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
author={Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
langid={english},
pages={1--14},
urldate={2019-03-26},
}

@article{2016HadjeresDeepbach,
shorttitle={DeepBach},
url={http://arxiv.org/abs/1612.01010},
eprint={1612.01010},
eprinttype={arxiv},
langid={english},
abstract={This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and speciﬁcally hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
title={DeepBach: a Steerable Model for Bach Chorales Generation},
arxivid={arXiv:1612.01010v2},
date={2016-12-03},
urldate={2019-03-28},
journaltitle={arXiv:1612.01010 [cs]},
groups={tesse:5},
author={Hadjeres, Gaëtan and Pachet, François and Nielsen, Frank},
}

@article{2017ArikDeep,
langid={english},
eprinttype={arxiv},
url={http://arxiv.org/abs/1702.07825},
date={2017-02-24},
shorttitle={Deep Voice},
eprint={1702.07825},
journaltitle={arXiv:1702.07825 [cs]},
author={Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
title={Deep Voice: Real-time Neural Text-to-Speech},
urldate={2019-03-26},
abstract={We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises ﬁve major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classiﬁcation (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more ﬂexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
}

@book{2017PattersonDeep,
url={https://www.ebook.de/de/product/23640784/adam///////\\_gibson///////\\_josh///////\\_patterson///////\\_deep///////\\_learning///////\\_the///////\\_definitive///////\\_guide.html},
volume={2017},
edition={1},
date={2017-08-11},
author={Patterson, Josh and Gibson, Adam},
title={Deep Learning: A Practitioner's Approach},
isbn={978-1-4919-1425-0},
publisher={O'Reilly Media, Inc.},
}

@article{2015SainathDeep,
journaltitle={Neural Networks},
date={2015-04},
author={Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and Mohamed, Abdel-rahman and Dahl, George and Ramabhadran, Bhuvana},
title={Deep Convolutional Neural Networks for Large-scale Speech Tasks},
doi={10.1016/j.neunet.2014.08.005},
pages={39--48},
volume={64},
groups={tesse:5},
}

@inproceedings{xu_deep_2017,
journaltitle={Advanced Science and Technology Letters},
pages={199--204},
keywords={aspect-based sentiment,convolution neural network,sentiment analysis,word2vec},
volume={143},
abstract={Sentiment analysis is an important task in natural language processing and has a wide range of applications. This paper describes our deep learning approach to multilingual aspect-based sentiment analysis. Our model use a deep convolutional neural network for both aspect extraction and aspect-based sentiment analysis. We take aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. For the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Experiments result shows that our system performs comparably well on the Yelp reviews.},
number={Ast},
title={Deep Convolutional Neural Network based Approach for Aspect-based Sentiment Analysis},
langid={english},
url={http://onlinepresent.org/proceedings/vol143///////\\_2017/41.pdf},
date={2017-02-12},
urldate={2019-04-05},
eventtitle={Advanced Science and Technology 2017},
author={Xu, Lamei and Lin, Jin and Wang, Lina and Yin, Chunyong and Wang, Jin},
doi={10.14257/astl.2017.143.41},
}

@inproceedings{2008CecottiConvolutional,
booktitle={2008 19th International Conference on Pattern Recognition},
date={2008-12},
title={Convolutional Neural Network with embedded Fourier Transform for EEG classification},
pages={1--4},
publisher={IEEE},
location={Tampa, FL, USA},
doi={10.1109/ICPR.2008.4761638},
journaltitle={2008 19th International Conference on Pattern Recognition (ICPR)},
isbn={978-1-4244-2174-9},
langid={english},
urldate={2019-03-26},
eventtitle={2008 19th International Conference on Pattern Recognition (ICPR)},
author={Cecotti, Hubert and Graeser, Axel},
url={http://ieeexplore.ieee.org/document/4761638/},
abstract={In BCI (Brain - Computer Interface) systems, brain signals must be processed to identify distinct activities that convey different mental states. We propose a new technique for the classiﬁcation of electroencephalographic (EEG) Steady-State Visual Evoked Potential (SSVEP) activity for non-invasive BCI. The proposed method is based on a Convolutional Neural Network that includes a Fourier transform between hidden layers in order to switch from the time domain to the frequency domain analysis in the network. The ﬁrst step allows the creation of different channels. The second step is dedicated to the transformation of the signal in the frequency domain. The last step is the classiﬁcation. It uses a hybrid rejection strategy that uses a junk class for the mental transition states and thresholds for the conﬁdence values. The presented results with ofﬂine processing are obtained with 6 electrodes on 2 subjects with a time segment of 1s. The system is reliable for both subjects over 95///////\\%, with rejection criterion.},
}

@article{2016EsserConvolutional,
langid={english},
author={Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Berg, David J. and Mckinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and Nolfo, Carmelo and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
abstract={Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-eﬃciency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that i) approach state-of-the-art classiﬁcation accuracy across 8 standard datasets, encompassing vision and speech, ii) perform inference while preserving the hardware’s underlying energy-eﬃciency and high throughput, running on the aforementioned datasets at between 1200 and 2600 frames per second and using between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be speciﬁed and trained using backpropagation with the same ease-of-use as contemporary deep learning. For the ﬁrst time, the algorithmic power of deep learning can be merged with the eﬃciency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
title={Convolutional Networks for Fast , Energy-Efficient Neuromorphic Computing},
volume={113},
issn={0027-8424, 1091-6490},
url={http://arxiv.org/abs/1603.08270},
eprint={arXiv:1603.08270v2},
number={Figure 1},
groups={tesse:5},
date={2016-10-11},
journaltitle={Proceedings of the National Academy of Sciences},
eprinttype={arXiv},
pages={1--7},
urldate={2019-03-26},
arxivid={arXiv:1603.08270v2},
doi={10.1073/pnas.1604850113},
}

@article{2005BensaComputational,
author={Bensa, Julien and Bilbao, Stefan and Kronland-Martinet, Richard and Smith, Julius and Voinier, Thierry},
year={2005},
journaltitle={Acta Acustica united with Acustica},
pages={289--298},
url={https://hal.archives-ouvertes.fr/hal-00088061},
date={2005},
journal={Acta Acustica united with Acustica},
volume={91},
keywords={digital waveguides, physical modeling, finite difference},
publisher={Hirzel Verlag},
title={Computational modeling of stiff piano strings using digital waveguides and finite difference},
}

@article{2017SoteloChar2wav,
mendeley-tags={speech synthesis},
pages={6},
langid={english},
groups={tesse:5},
date={2017},
title={Char2Wav: End-to-End Speech Synthesis},
number={October},
journaltitle={Iclr},
url={https://openreview.net/pdf?id=B1VWyySKx},
abstract={We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoderdecoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
keywords={speech synthesis},
author={Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Santos, Joao Felipe and Kastner, Kyle and Courville, Aaron and Bengio, Yoshua},
}

@inproceedings{2014DalgleishBlurring,
date={2014},
publisher={Proceedings of the 9th Conference on Interdisciplinary Musicology. CIM14. Berlin, Germany},
volume={14},
langid={english},
year={2014},
author={Dalgleish, Mat and Foster, Chris and Spencer, Steve},
booktitle={Proceedings of the 9th Conference on Interdisciplinary Musicology – CIM14},
url={http://www.academia.edu/9198495/BLURRING///////\\_THE///////\\_LINES///////\\_AN///////\\_INTEGRATED///////\\_COMPOSITIONAL///////\\_MODEL///////\\_FOR///////\\_DIGITAL///////\\_MUSICAL///////\\_INSTRUMENT///////\\_DESIGN},
abstract={Computer-based interactive music systems date back as far as the late 1960s, but increasingly accessible technologies have prompted significant growth in interest in digital musical instruments (DMIs) over the last decade. To date, the designers of DMIs have generally borrowed paradigms from acoustic instruments or the field of Human-Computer Interaction (HCI). However, it can be argued that DMIs are a fundamentally different case and the suitability of these paradigms is debatable at best. For instance, DMIs lack the haptic feedback of acoustic instruments. Musical instruments are also highly specialized rather than general-purpose tools, and musical performance is not typically task-based. Additionally, Jordà notes that their designers have tended to focus on isolated parts of the problem, to the detriment of instrumental cohesion and character. While a few authors have considered DMIs as more fully rounded constructions, and the term ‘composed instruments’ has been used to describe the specification of the input-output relationship as an intentional act of composition, we argue that this is insufficient. Drawing on theories of affordances and ecological music creation, we describe an alternative model that considers DMI design as part of a broader compositional process that also includes text and hybrid acoustic-digital space. The traditionally distinct roles of designer, composer and performer are seen to blur, and the notion of composition-specific instruments is discussed. As an example of the model in practice, the interdisciplinary collaborative piece Desire Lines is described. This serves to aid an initial assessment of the model and its implementation, and informs some remarks around its limitations and future possibilities.},
journaltitle={. Berlin},
title={Blurring the Lines: An Integrated Compositional Model for Digital Music Instrument Design},
pages={6},
}

@phdthesis{1974WerbosRegression,
month={aug},
author={Werbos, Paul John},
title={Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
type={phdthesis},
date={1974},
institution={Harvard University},
url={https://www.researchgate.net/publication/35055330///////\\_Beyond///////\\_regression///////\\_new///////\\_tools///////\\_for///////\\_prediction///////\\_and///////\\_analysis///////\\_in///////\\_the///////\\_behavior///////\\_sciences///////\\_microform},
year={1974},
}

@inproceedings{2016ShiBenchmarking,
doi={10.1109/CCBD.2016.029},
booktitle={Proceedings of the 2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
eprint={arXiv:1608.07249v7},
author={Shi, S. and Wang, Q. and Xu, P. and Chu, X.},
edition={Cloud Comp},
date={2016-11},
pages={99--104},
eprinttype={arXiv},
publisher={AnaisIEEE},
arxivid={arXiv:1608.07249v7},
keywords={Training, Neural networks, Convolutional Neural Networks, Deep Learning, Recurrent Neural Networks, Machine learning, Graphics processing units, Benchmark testing, Caffe, CNTK, CPU platform, deep network training, Feed-forward Neural Networks, GPU, GPU-accelerated deep learning software tools, hardware platforms, Instruction sets, machine learning method, open-source deep learning software tool benchmarking, running time performance evaluation, TensorFlow, Tools, Torch},
title={Benchmarking State-of-the-Art Deep Learning Software Tools},
}

@article{2015IoffeBatch,
arxivid={arXiv:1502.03167v3},
title={Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift},
langid={english},
url={http://arxiv.org/abs/1502.03167},
author={Ioffe, Sergey and Szegedy, Christian},
abstract={Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.9///////\\% top-5 validation error (and 4.8///////\\% test error), exceeding the accuracy of human raters.},
urldate={2019-03-26},
shorttitle={Batch Normalization},
eprint={arXiv:1502.03167v3},
date={2015-02-10},
eprinttype={arXiv},
journaltitle={arXiv:1502.03167 [cs]},
}

@misc{2006MakinBackpropagation,
author={Makin, J. G.},
title={Backpropagation},
url={http://www.cs.cornell.edu/courses/cs5740/2016sp/resources/backprop.pdf},
pages={1--8},
date={2006},
groups={tesse:5},
}

@inproceedings{2016SouthallAutomatic,
isbn={978-0-692-75506-8},
groups={tesse:5},
booktitle={Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016, New York City, United States, August 7-11, 2016},
volume={2016},
date={2016},
pages={591--597},
keywords={music transcription,tesis},
journaltitle={Proceedings of International Society for Music Information Retrieval Conference (ISMIR)},
author={Southall, Carl and Stables, Ryan and Hockman, Jason},
title={Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks},
url={https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/217///////\\_Paper.pdf},
abstract={Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in-struments in audio recordings. Neural networks have al-ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro-pose the use of neural networks for ADT in order to ex-ploit their ability to capture a complex configuration of fea-tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu-ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit-able for online operation. In both systems, a separate net-work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis-ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec-tively. The results demonstrate the effectiveness of the pre-sented methods for solo percussion and a capacity for iden-tifying snare drums, which are historically the most diffi-cult drum class to detect.},
mendeley-tags={music transcription,tesis},
}

@article{2016PersioArtiﬁcial,
langid={english},
author={Persio, Luca Di and Honchar, Oleksandr},
pages={403--413},
journaltitle={International Journal of Circuits, Systems and Signal Processing},
keywords={Artificial neural networks,Convolutional neural network,Deep Learning,Financial forecasting,Long shortterm memory,Multi-layer neural network,Recurrent neural network,Stock markets analysis,Time series analysis,forecasting},
date={2016},
issn={1998-4464},
mendeley-tags={forecasting},
title={Artiﬁcial Neural Networks architectures for stock price prediction: comparisons and applications},
abstract={We present an Artiﬁcial Neural Network (ANN) approach to predict stock market indices, particularly with respect to the forecast of their trend movements up or down. Exploiting different Neural Networks architectures, we provide numerical analysis of concrete ﬁnancial time series. In particular, after a brief re´sume´ of the existing literature on the subject, we consider the Multi-layer Perceptron (MLP), the Convolutional Neural Networks (CNN), and the Long Short-Term Memory (LSTM) recurrent neural networks techniques. We focus on the importance of choosing the correct input features, along with their preprocessing, for the speciﬁc learning algorithm one wants to use. Eventually, we consider the S///////\\&P500 historical time series, predicting trend on the basis of data from the past days, and proposing a novel approach based on combination of wavelets and CNN, which outperforms the basic neural networks ones. We show, that neural networks are able to predict ﬁnancial time series movements even trained only on plain time series data and propose more ways to improve results.},
volume={10},
}

@book{2016RussellArtificial,
publisher={Prentice Hall},
volume={2016},
shorttitle={Artificial intelligence},
series={Prentice Hall series in artificial intelligence},
groups={tesse:5},
ean={9781292153964},
location={Upper Saddle River},
isbn={9781292153964},
edition={3rd ed},
author={Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
date={2016},
url={https://www.ebook.de/de/product/25939961/stuart///////\\_russell///////\\_peter///////\\_norvig///////\\_artificial///////\\_intelligence///////\\_a///////\\_modern///////\\_approach///////\\_global///////\\_edition.html},
pagetotal={1132},
title={Artificial intelligence: a modern approach},
langid={english},
year={2016},
keywords={Artificial intelligence},
}

@inproceedings{2017GullyArticulatory,
title={Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network},
doi={10.21437/Interspeech.2017-900},
year={2017},
date={2017-08},
month={aug},
author={Amelia J. Gully and Takenori Yoshimura and Damian T. Murphy and Kei Hashimoto and Yoshihiko Nankaku and Keiichi Tokuda},
booktitle={Proceedings of the Interspeech 2017},
keywords={[Electronic Manuscript]},
publisher={ISCA},
pages={234--238},
}

@inproceedings{2017ParkAnalysis,
volume={10112},
abstract={Regularizing neural networks is an important task to reduce overﬁtting. Dropout [1] has been a widely-used regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization eﬀect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the eﬀect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image ﬁlters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout [2].},
isbn={978-3-319-54183-9 978-3-319-54184-6},
pages={189--204},
editor={Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
publisher={Springer International Publishing},
urldate={2019-04-05},
title={Analysis on the Dropout Effect in Convolutional Neural Networks},
location={Cham},
langid={english},
booktitle={Computer Vision – ACCV 2016},
doi={10.1007/978-3-319-54184-6///////\\_12},
date={2017},
author={Park, Sungheon and Kwak, Nojun},
groups={tesse:5},
organization={Springer},
}

@article{2001MuellerIntroduction,
journaltitle={IEEE TRANSACTIONS ON NEURAL NETWORKS},
pmid={18244377},
groups={tesse:5},
isbn={1045-9227},
author={Müller, Klaus-Robert and Mika, Sebastian and Rätsch, Gunnar and Tsuda, Koji and Schölkopf, Bernhard},
title={An Introduction to Kernel-Based Learning Algorithms},
number={2},
doi={10.1109/72.914517},
issn={1045-9227},
keywords={Boosting,Fisher's discriminant,Kernel PCA,Kernel methods,Mathematical programming machines,Mercer kernels,Principal component analysis (PCA),Single-class classification,Support vector machines (SVMs),seminal},
mendeley-tags={seminal},
pages={21},
abstract={This paper provides an introduction to support vector machines (SVMs), kernel Fisher discriminant analysis, and kernel principal component analysis (PCA), as examples for successful kernel-based learning methods. We first give a short background about Vapnik–Chervonenkis (VC) theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by finally discussing applications such as optical character recognition (OCR) and DNA analysis.},
langid={english},
volume={12},
date={2001},
}

@inproceedings{2006TuohyEvolved,
pages={576--579},
title={An Evolved Neural Network/HC Hybrid for Tablature Creation in GA- based Guitar Arranging},
isbn={////////\\%}(},
number={January 2006},
journaltitle={CMC},
keywords={GA,fingeringprediction,guitar,music transcription},
url={http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2006.119},
volume={2006},
mendeley-tags={GA,music transcription},
booktitle={ICMC},
year={2006},
author={Tuohy, D. R. and Potter, W. D.},
date={2006},
abstract={In this paper we describe a technique for creating guitar tablature using a neural network. Training data was parsed from an online repository of human-created tablatures. The contents of both the input layer and the set of training data have been optimized through genetic search in order to maximize the accuracy of the network. The output of the network is im- proved upon with a local heuristic hill-climber (HC). We implement this model in an existing system for generating guitar arrangements via genetic algorithm (GA). When compared to the original system for generating tablature, we note modest improvement in tablature quality and drastic improvements in execution time.},
}

@inproceedings{2017ParvatSurvey,
title={A survey of deep-learning frameworks},
pages={1--7},
keywords={Training, Mathematical model, Neural networks, Computational modeling, Machine learning, artificial neural network, Artificial neural networks, Deep learning, deep-learning frameworks, Graphics processing units, Libraries, machine learning, Software libraries},
doi={10.1109/ICISC.2017.8068684},
groups={tesse:5},
date={2017-01},
author={Parvat, A. and Chavan, J. and Kadam, S. and Dev, S. and Pathak, V.},
booktitle={Proceedings of the 2017 International Conference on Inventive Systems and Control (ICISC)},
publisher={AnaisIEEE},
edition={Inventive},
}

@article{2008HuangReview,
title={A review on Hilbert-Huang transform: Method and its applications to geophysical studies},
author={Huang, Norden E and Wu, Zhaohua},
issn={8755-1209},
date={2008},
shorttitle={A review on Hilbert-Huang transform},
pages={1--23},
number={2007},
urldate={2019-03-28},
volume={46},
journaltitle={Reviews of Geophysics},
langid={english},
doi={10.1029/2007RG000228.1.INTRODUCTION},
}

@inproceedings{he_praat-based_2016,
eventtitle={Interspeech 2016},
title={A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform},
doi={10.21437/Interspeech.2016-1447},
abstract={A speech signal can be viewed as a high frequency carrier signal containing the temporal fine structure (TFS) that is modulated by a low frequency envelope (ENV). A widely used method to decompose a speech signal into the TFS and ENV is the Hilbert transform. Although this method has been available for about one century and is widely applied in various kinds of speech processing tasks (e.g. speech chimeras), there are only very few speech processing packages that contain readily available functions for the Hilbert transform, and there is very little textbook type literature tailored for speech scientists to explain the processes behind the transform. With this paper we provide the code for carrying out the Hilbert operation to obtain the TFS and ENV in the widely used speech processing software Praat, and explain the basics of the procedure. To verify our code, we compare the Hilbert transform in Praat with a widely applied function for the same purpose in MATLAB (“hilbert(...)”). We can confirm that both methods arrive at identical outputs.},
pages={530--534},
number={September},
author={He, Lei and Dellwo, Volker},
date={2016-09-08},
url={http://www.isca-speech.org/archive/Interspeech///////\\_2016/abstracts/1447.html},
urldate={2019-03-26},
langid={english},
}

@article{2003IiiBasic,
year={2006},
title={A Basic Introduction to Digital Waveguide Synthesis (for the Technically Inclined)},
date={2006},
author={Iii, Julius O Smith},
pages={1--5},
url={https://ccrma.stanford.edu/ jos/swgt/},
address={Stanford University. stanford. edu/ jos/swgt},
publisher={Center for Computer Research in Music and Acoustics (CCRMA)},
location={Stanford University. stanford. edu/ jos/swgt},
}

@article{2015HouBlind,
date={2015},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6872541},
author={Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong and Member, Senior and Tao, Dacheng and Member, Senior},
number={6},
urldate={2019-04-05},
issn={2162-237X, 2162-2388},
groups={tesse:5},
publisher={IEEE},
title={Blind Image Quality Assessment via Deep Learning},
keywords={image classification,sentiment analysis},
doi={10.1109/TNNLS.2014.2336852},
pages={1275--1286},
langid={english},
journaltitle={IEEE transactions on neural networks and learning systems},
mendeley-tags={image classification,sentiment analysis},
abstract={This paper investigates how to blindly evaluate the visual quality of an image by learning rules from linguistic descriptions. Extensive psychological evidence shows that humans prefer to conduct evaluations qualitatively rather than numerically. The qualitative evaluations are then converted into the numerical scores to fairly benchmark objective image quality assessment (IQA) metrics. Recently, lots of learning-based IQA models are proposed by analyzing the mapping from the images to numerical ratings. However, the learnt mapping can hardly be accurate enough because some information has been lost in such an irreversible conversion from the linguistic descriptions to numerical scores. In this paper, we propose a blind IQA model, which learns qualitative evaluations directly and outputs numerical scores for general utilization and fair comparison. Images are represented by natural scene statistics features. A discriminative deep model is trained to classify the features into ﬁve grades, corresponding to ﬁve explicit mental concepts, i.e., excellent, good, fair, poor, and bad. A newly designed quality pooling is then applied to convert the qualitative labels into scores. The classiﬁcation framework is not only much more natural than the regression-based models, but also robust to the small sample size problem. Thorough experiments are conducted on popular databases to verify the model’s effectiveness, efﬁciency, and robustness.},
volume={26},
}

@article{2015KulkarniDeep,
title={Deep Convolutional Inverse Graphics Network},
journaltitle={Advances in Neural Information Processing Systems 28},
abstract={This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
groups={tesse:5},
year={2015},
volume={2015},
eprint={1503.03167},
url={http://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf},
issn={1089-7550},
arxivid={1503.03167},
date={2015},
eprinttype={arXiv},
keywords={image synthesis,tesis},
mendeley-tags={image synthesis,tesis},
doi={10.1063/1.4914407},
editor={Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
author={Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Josh},
publisher={Curran Associates, Inc.},
booktitle={Advances in Neural Information Processing Systems 28},
pages={2539--2547},
}

@inproceedings{2017SabourDynamic,
author={Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
eprint={arXiv:1710.09829v1},
pages={3856--3866},
booktitle={Advances in Neural Information Processing Systems 30},
publisher={Curran Associates, Inc.},
title={Dynamic Routing Between Capsules},
groups={tesse:5},
editor={Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
journaltitle={Advances in Neural Information Processing Systems 30},
year={2017},
number={Nips},
eprinttype={arXiv},
arxivid={arXiv:1710.09829v1},
volume={2017},
date={2017},
url={http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},
}

@book{2004CoppinArtificial,
langid={english},
edition={1st ed},
date={2004-04-11},
keywords={Artificial intelligence},
location={Boston},
publisher={JONES //\\\& BARTLETT PUB INC},
url={https://www.ebook.de/de/product/4345313/ben///////\\_coppin///////\\_artificial///////\\_intelligence///////\\_illuminated.html},
author={Coppin, Ben},
title={Artificial intelligence illuminated},
ean={9780763732301},
pagetotal={740},
groups={tesse:5},
isbn={0-7637-3230-3},
year={2004},
volume={2004},
}

@inproceedings{2015XuCcg,
pages={250--255},
isbn={9781941643730},
url={http://www.aclweb.org/anthology/P15-2041},
groups={tesse:5},
date={2015},
number={2014},
publisher={Association for Computational Linguistics},
title={CCG Supertagging with a Recurrent Neural Network},
doi={10.3115/v1/p15-2041},
note={event-place: Beijing, China},
journaltitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
booktitle={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
keywords={sentence classification},
author={Xu, Wenduan and Auli, Michael and Clark, Stephen},
}

@inproceedings{2016ZhangColorful,
eprinttype={arXiv},
volume={abs/1603.08511},
timestamp={Mon, 13 Aug 2018 16:46:30 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/ZhangIE16},
organization={Springer},
title={Colorful Image Colorization},
author={Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
location={Anais},
date={2016},
booktitle={European Conference on Computer Vision},
pages={649--666},
url={http://arxiv.org/abs/1603.08511},
bibsource={dblp computer science bibliography, https://dblp.org},
groups={tesse:5},
journaltitle={CoRR},
publisher={Springer},
eprint={1603.08511},
}

@book{2016GoodfellowDeep,
groups={tesse:5},
title={Deep Learning},
date={2016},
note={http://www.deeplearningbook.org},
volume={1},
author={Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
publisher={MIT Press},
}

@inproceedings{1999JensenEnvelope,
date={1999},
booktitle={PROCEEDINGS OF THE DAFX},
doi={10.1109/MMSP.2001.962718},
abstract={This paper presents a model of the envelope of the additive parameters of isolated musical sounds, along with a new method for the estimation of the important envelope split- point times. The model consists of start, attack, sustain, release, and end segments with variable split-point amplitude and time. The estimation of the times is done using smoothed derivatives of the envelopes. The estimated split-point values can be used together with a curve-form model introduced in this paper in the analysis/synthesis of musical sounds. The envelope model can recreate noise-less musical sounds with good fidelity, and the method for the estimation of the envelope times performs significantly better than the classical percentage- based method.},
pages={9--12},
title={Envelope Model Of Isolated Musical Sounds},
author={Jensen, Kristoffer},
journaltitle={Audio},
number={1},
groups={tesse:5},
}

@inproceedings{2013GravesSpeech,
edition={Acoustics},
date={2013-05},
author={Graves, A. and Mohamed, A. and Hinton, G.},
title={Speech recognition with deep recurrent neural networks},
booktitle={Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
doi={10.1109/ICASSP.2013.6638947},
pages={6645--6649},
publisher={AnaisIEEE},
keywords={Noise, Recurrent neural networks, Training, Vectors, Acoustics, recurrent neural networks, speech recognition, Speech recognition, connectionist temporal classification, deep neural networks, deep recurrent neural networks, end-to-end training methods, long short-term memory RNN architecture, sequential data},
}

@incollection{2012LecunEfficient,
title={Efficient backprop},
doi={10.1007/978-3-642-35289-8///////\\_3},
author={Lecun, Y. A. and Others},
pages={9--48},
date={2012},
publisher={Springer},
location={Traducao. [s.l.] p. 948},
groups={tesse:5},
booktitle={Neural networks: Tricks of the trade},
}

@phdthesis{2014SocherRecursive,
date={2014},
groups={tesse:5},
publisher={Citeseer},
school={Stanford University},
type={phdthesis},
url={https://nlp.stanford.edu//textasciitilde socherr/thesis.pdf},
author={Socher, Richard},
year={2014},
title={Recursive deep learning for natural language processing and computer vision},
institution={Stanford University},
}

@inproceedings{2016ChoiAutomatic,
keywords={music classification},
url={http://arxiv.org/abs/1606.00298},
arxivid={1606.00298},
month={August},
address={New York City, United States},
bibsource={dblp computer science bibliography, https://dblp.org},
author={Choi, Keunwoo and Fazekas, George and Sandler, Mark},
journaltitle={arXiv preprint arXiv:1606.00298},
biburl={https://dblp.org/rec/bib/conf/ismir/ChoiFS16},
abstract={We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
booktitle={Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016},
date={2016},
eprinttype={arXiv},
location={New York City, United States},
pages={805--811},
title={Automatic tagging using deep convolutional neural networks},
mendeley-tags={music classification},
eprint={1606.00298},
timestamp={Thu, 08 Sep 2016 13:32:51 +0200},
year={2016},
}

@article{2016OordConditional,
biburl={https://dblp.org/rec/bib/journals/corr/OordKVEGK16},
eprint={1606.05328},
date={2016},
title={Conditional Image Generation with PixelCNN Decoders},
journaltitle={CoRR},
url={http://arxiv.org/abs/1606.05328},
volume={abs/1606.05328},
eprinttype={arXiv},
bibsource={dblp computer science bibliography, https://dblp.org},
timestamp={Mon, 13 Aug 2018 16:46:40 +0200},
author={Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
}

@article{1958RosenblattPerceptron,
date={1958},
title={The perceptron: A probabilistic model for information storage and organization in the brain.},
pages={386--408},
mendeley-tags={seminal},
abstract={To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
pmid={13602029},
number={6},
volume={65},
author={Rosenblatt, F.},
issn={0033-295X},
doi={10.1037/h0042519},
isbn={0033-295X},
keywords={seminal},
journaltitle={Psychological Review},
url={http://content.apa.org/journals/rev/65/6/386},
}

@incollection{2008JuliusO.SmithDigital,
groups={tesse:5},
pages={399--417},
date={2008},
author={Julius O. Smith, I. I. I.},
booktitle={Handbook of Signal Processing in Acoustics},
title={Digital Waveguide Architectures for Virtual Musical Instruments},
publisher={Springer},
year={2008},
location={Traducao. [s.l.] p. 399417},
doi={10.1007/978-0-387-30441-0_25},
}

@article{1990SerraSpectral,
title={Spectral Modeling Synthesis: A Sound Analysis/Synthesis System Based on a Deterministic Plus Stochastic Decomposition},
year={1990},
shorttitle={Spectral Modeling Synthesis},
date={1990},
urldate={2019-03-26},
volume={14},
author={Serra, Xavier and Smith, Julius},
issn={0148-9267},
number={4},
pages={12},
url={https://www.jstor.org/stable/3680788?origin=crossref},
publisher={JSTOR},
journaltitle={Computer Music Journal},
journal={Computer Music Journal},
doi={10.2307/3680788},
groups={tesse:5},
langid={english},
}

@book{1996RojasNeural,
location={Berlin ; New York},
isbn={978-3-540-60505-8},
publisher={Springer},
ean={9783540605058},
pagetotal={502},
abstract={Neural networks are a computing paradigm that is finding increasing attention among computer scientists. In this book, theoretical laws and models previously scattered in the literature are brought together into a general theory of artificial neural nets. Always with a view to biology and starting with the simplest nets, it is shown how the properties of models change when more general computing elements and net topologies are introduced. Each chapter contains examples, numerous illustrations, and a bibliography. The book is aimed at readers who seek an overview of the field or who wish to deepen their knowledge. It is suitable as a basis for university courses in neurocomputing.},
shorttitle={Neural Networks},
doi={10.1007/978-3-642-61068-4},
date={1996-07-12},
edition={1 edition},
author={Rojas, Raúl},
url={https://www.ebook.de/de/product/1427891/raul///////\\_rojas///////\\_neural///////\\_networks.html},
title={Neural Networks: A Systematic Introduction},
year={1996},
groups={tesse:5},
}

@inproceedings{2013DengNew,
title={New types of deep neural network learning for speech recognition and related applications: an overview},
pages={8599--8603},
urldate={2019-04-05},
booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
organization={IEEE},
url={http://ieeexplore.ieee.org/document/6639344/},
shorttitle={New types of deep neural network learning for speech recognition and related applications},
isbn={978-1-4799-0356-6},
doi={10.1109/ICASSP.2013.6639344},
location={Vancouver, BC, Canada},
langid={english},
abstract={In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed.},
author={Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
eventtitle={ICASSP 2013 - 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
publisher={IEEE},
date={2013-05},
}

@article{2017JingNeural,
langid={english},
url={http://arxiv.org/abs/1705.04058},
urldate={2019-04-05},
author={Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Yu, Yizhou and Song, Mingli},
groups={tesse:5},
eprint={1705.04058},
title={Neural Style Transfer: A Review},
eprinttype={arxiv},
abstract={The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
journaltitle={arXiv:1705.04058 [cs, eess, stat]},
shorttitle={Neural Style Transfer},
date={2017-05-11},
arxivid={arXiv:1705.04058v1},
}

@inproceedings{2014KarpathyLarge,
doi={10.1109/CVPR.2014.223},
pages={1725--1732},
keywords={Training, convolutional neural networks, Feature extraction, action, classification, CNN, Computational modeling, Computer architecture, convolutional, dataset, feature-based baselines, image recognition problems, large-scale, local spatiotemporal information, network, neural, recognition, Spatial resolution, spatiotemporal networks, sports, Streaming media, UCF-101 action recognition dataset, UCF-101 baseline model, video, video classification, YouTube videos},
booktitle={Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
date={2014-06},
journaltitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
groups={tesse:5},
author={Karpathy, A. and Toderici, G. and Shetty, S. and Leung, T. and Sukthankar, R. and Fei-Fei, L.},
title={Large-Scale Video Classification with Convolutional Neural Networks},
volume={2014},
}

@article{2014MoonWhat,
author={Moon, Il Joon and Hong, Sung Hwa},
publisher={The Korean Audiological Society},
title={What Is Temporal Fine Structure and Why Is It Important ?},
url={http://ejao.org/journal/view.php?doi=10.7874/kja.2014.18.1.1},
urldate={2019-03-28},
date={2014},
year={2014},
issn={2092-9862, 2093-3797},
keywords={hearing,speech perception,temporal envelope,temporal fine structure},
number={1},
journaltitle={Korean Journal of Audiology},
volume={18},
langid={english},
doi={10.7874/kja.2014.18.1.1},
pages={1--7},
}

@report{2015BaharevExact,
abstract={Given a directed graph G, a feedback arc set of G is a subset of its edges containing at least one edge of every cycle in G. Finding a feedback arc set of minimum cardinality is the minimum feedback arc set problem. The present paper focuses on large and sparse graphs. The minimum set cover formulation of the minimum feedback arc set problem is practical as long as all the simple cycles in G can be enumerated. Unfortunately, even sparse graphs can have Ω(2n) simple cycles, and such graphs appear in practice. An exact method is proposed that enumerates simple cycles in a lazy fashion, and extends an incomplete cycle matrix iteratively in the hope that only a tractable number of cycles has to be enumerated until a minimum feedback arc set is found. Numerical results are given on a test set containing large and sparse test graphs relevant for industrial applications.},
date={2015},
groups={tesse:5},
year={2015},
pages={34},
type={resreport},
author={Baharev, Ali and Schichl, Hermann and Neumaier, Arnold},
institution={University of Vienna},
langid={english},
title={An exact method for the minimum feedback arc set problem},
}

@misc{2017RamachandranSearching,
eprint={1710.05941},
url={https://arxiv.org/abs/1710.05941},
title={Searching for Activation Functions},
eprintclass={cs.NE},
eprinttype={arXiv},
groups={tesse:5},
pages={1--13},
date={2017},
author={Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
}

@book{2015RaschkaPython,
year={2015},
title={Python machine learning: unlock deeper insights into machine learning with this vital guide to cutting-edge predictive analytics},
pagetotal={425},
author={Raschka, Sebastian},
isbn={978-1-78355-513-0},
location={Birmingham Mumbai},
shorttitle={Python machine learning},
publisher={Packt Publishing open source},
langid={english},
series={Community experience distilled},
url={https://www.ebook.de/de/product/25100042/sebastian///////\\_raschka///////\\_python///////\\_machine///////\\_learning.html},
note={OCLC: 927507196},
date={2016},
ean={9781783555130},
}

@book{2016SalsaPartial,
publisher={Springer},
year={2016},
shorttitle={Partial differential equations in action},
series={UNITEXT - La Matematica per il 3+2},
title={Partial differential equations in action: from modelling to theory},
langid={english},
number={volume 99},
note={OCLC: 986521541},
ean={9783319312385},
pagetotal={686},
location={Cham},
date={2016},
url={https://www.ebook.de/de/product/27952550/sandro///////\\_salsa///////\\_partial///////\\_differential///////\\_equations///////\\_in///////\\_action.html},
groups={tesse:5},
author={Salsa, Sandro},
edition={Third edition},
isbn={978-3-319-31237-8 978-3-319-31238-5},
}

@book{2010LyonsUnderstanding,
date={2011},
title={Understanding Digital Signal Processing, 3/E},
url={https://www.ebook.de/de/product/11024296/richard///////\\_g///////\\_lyons///////\\_understanding///////\\_digital///////\\_signal///////\\_processing.html},
ean={9780137027415},
isbn={0137027419},
pagetotal={992},
publisher={Traducao. [s.l.] Pearson Education India},
location={Upper Saddle River, NJ},
author={Lyons, R G},
year={2010},
edition={3 edition},
}

@thesis{2018PfalzGenerating,
date={2018-08},
year={2018},
type={phdthesis},
author={Pfalz, Andrew},
url={https://digitalcommons.lsu.edu/cgi/viewcontent.cgi?article=5621///////\\&context=gradschool///////\\_dissertations},
month={August},
school={Louisiana State University},
institution={Louisiana State University},
title={Generating Audio Using Recurrent Neural Networks},
groups={tesse:5},
}

@inproceedings{1993DuynePhysical,
edition={Proceeding},
author={Duyne, Scott A. Van and Julius O. Smith, I. I. I.},
pages={40--47},
date={1993},
publisher={INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
url={https://ccrma.stanford.edu/ jos/pdf/mesh.pdf},
organization={INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
groups={tesse:5},
title={Physical modeling with the 2-D digital waveguide mesh},
booktitle={Proceedings of the International Computer Music Conference},
}

@inproceedings{1995DuyneTetrahedral,
groups={tesse:5},
booktitle={Proceedings of 1995 Workshop on Applications of Signal Processing to Audio and Accoustics},
title={The tetrahedral digital waveguide mesh},
author={Duyne, Scott A. Van and Julius O. Smith, I. I. I.},
date={1995},
doi={10.1109/ASPAA.1995.482998},
volume={1995},
journaltitle={Applications of Signal Processing to Audio and Acoustics},
pages={234--237},
}

@phdthesis{2001LairdPhysical,
type={phdthesis},
date={2001},
author={Laird, Joel Augustus},
url={https://research-information.bristol.ac.uk/en/theses/the-physical-modelling-of-drums-using-digital-waveguides(ebd75b4b-bcdd-4cc7-b153-a6e0007682aa).html},
title={The physical modelling of drums using digital waveguides},
journaltitle={University of Bristol},
groups={tesse:5, Digital Waveguides},
volume={2001},
keywords={digital waveguides, physical modeling},
institution={University of Bristol},
}

@misc{2014KopparapuOptimal,
url={http://arxiv.org/abs/1406.3172},
urldate={2019-03-26},
eprinttype={arxiv},
title={Optimal Gaussian Filter for Effective Noise Filtering},
author={Kopparapu, Sunil and Satish, M.},
arxivid={arXiv:1406.3172v1},
groups={tesse:5},
langid={english},
date={2014-06-12},
number={2},
pages={1--7},
eprint={1406.3172},
journaltitle={arXiv:1406.3172 [cs]},
abstract={In this paper we show that the knowledge of noise statistics contaminating a signal can be effectively used to choose an optimal Gaussian ﬁlter to eliminate noise. Very speciﬁcally, we show that the additive white Gaussian noise (AWGN) contaminating a signal can be ﬁltered best by using a Gaussian ﬁlter of speciﬁc characteristics. The design of the Gaussian ﬁlter bears relationship with the noise statistics and also some basic information about the signal. We ﬁrst derive a relationship between the properties of the Gaussian ﬁlter, noise statistics and the signal and later show through experiments that this relationship can be used effectively to identify the optimal Gaussian ﬁlter that can effectively ﬁlter noise.},
}

@report{2016DozatIncorporating,
institution={Stanford University},
url={http://cs229.stanford.edu/proj2015/054///////\\_report.pdf},
title={Incorporating Nesterov Momentum into Adam},
groups={tesse:5},
author={Dozat, Timothy},
type={resreport},
year={2016},
date={2016},
}

@inproceedings{2016GatysImage,
doi={10.1109/CVPR.2016.265},
volume={2016},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
author={Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
groups={tesse:5},
journaltitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
keywords={convolutional neural networks, Feature extraction, arbitrary photograph, artistic style, high level image synthesis, image content, image processing, Image reconstruction, Image representation, image representations, image style transfer, neural algorithm, Neural networks, Neuroscience, object recognition, rendering, semantic content, semantic information, Semantics, Visualization},
title={Image Style Transfer Using Convolutional Neural Networks},
date={2016},
pages={2414--2423},
}

@inproceedings{2016MollahosseiniGoing,
booktitle={Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
journaltitle={2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
eprint={1511.04110},
date={2016},
title={Going deeper in facial expression recognition using deep neural networks},
author={Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},
pages={1--10},
groups={tesse:5},
organization={IEEE},
abstract={Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classiﬁer’s hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not signiﬁcant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Speciﬁcally, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classiﬁes them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.},
doi={10.1109/WACV.2016.7477450},
urldate={2019-03-28},
eprinttype={arxiv},
langid={english},
url={http://arxiv.org/abs/1511.04110},
}

@report{2016StaudtDevelopment,
date={2016},
groups={tesse:5},
title={Development of a Digital Musical Instrument with Embedded Sound Synthesis},
publisher={Technische Universität Berlin},
year={2016},
institution={Technische Universität Berlin},
type={resreport},
url={https://www2.ak.tu-berlin.de//textasciitilde akgroup/ak///////\\_pub/abschlussarbeiten/2016/Staudt///////\\_Expose.pdf},
author={Staudt, Pascal},
}

@report{2018HorowitzStrategic,
url={https://www.cnas.org/publications/reports/strategic-competition-in-an-era-of-artificial-intelligence},
date={2018},
type={techreport},
author={Horowitz, Michael C. and Allen, Gregory C. and Kania, Elsa B. and Scharre, Paul},
institution={Center for a New American Security},
title={Strategic Competition in an Era of Artificial Intelligence},
groups={tesse:5},
year={2018},
}

@article{2016DumoulinGuide,
intrahash={f00abfcfc0627c65ea10b68ecef24cac},
author={Dumoulin, Vincent and Visin, Francesco},
title={A guide to convolution arithmetic for deep learning.},
volume={abs/1603.07285},
keywords={dblp},
url={http://dblp.uni-trier.de/db/journals/corr/corr1603.html///////\\#DumoulinV16},
groups={tesse:5},
ee={http://arxiv.org/abs/1603.07285},
location={mar},
biburl={https://www.bibsonomy.org/bibtex/2f00abfcfc0627c65ea10b68ecef24cac/dblp},
timestamp={2018-08-14T13:29:40.000+0200},
interhash={47f037084e2155cec24f09bf4897dcba},
journaltitle={CoRR},
date={2016},
publisher={ArXiv e-prints},
}

@inproceedings{2014SarroffMusical,
title={Musical Audio Synthesis Using Autoencoding Neural Nets},
date={2014},
journaltitle={Proceedings of the International Computer Music Conference},
pages={14--20},
groups={tesse:5},
institution={Dartmouth College},
type={resreport},
booktitle={Joint 40th International Computer Music Conference (ICMC) and 11th Sound //\\\& Music Computing conference (SMC)},
isbn={9789604661374},
author={Sarroff, Andy M. and Casey, Michael},
number={September},
abstract={With an optimal network topology and tuning of hyperpa-rameters, artificial neural networks (ANNs) may be trained to learn a mapping from low level audio features to one or more higher-level representations. Such artificial neu-ral networks are commonly used in classification and re-gression settings to perform arbitrary tasks. In this work we suggest repurposing autoencoding neural networks as musical audio synthesizers. We offer an interactive musi-cal audio synthesis system that uses feedforward artificial neural networks for musical audio synthesis, rather than discriminative or regression tasks. In our system an ANN is trained on frames of low-level features. A high level representation of the musical audio is learned though an autoencoding neural net. Our real-time synthesis system allows one to interact directly with the parameters of the model and generate musical audio in real time. This work therefore proposes the exploitation of neural networks for creative musical applications.},
volume={1},
}

@inproceedings{2012BockPolyphonic,
title={Polyphonic Piano Note Transcription With Recurrent Neural Networks},
groups={tesse:5},
author={Bock, Sebastian and Schedl, Markus and Böck, Sebastian and Schedl, Markus},
isbn={9781467300469},
doi={10.1109/ICASSP.2012.6287832},
publisher={AnaisIEEE},
date={2012},
booktitle={Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
keywords={music transcription},
mendeley-tags={music transcription},
year={2012},
pages={121--124},
institution={IEEE},
journaltitle={Network},
}

@inproceedings{2014Boulanger-lewandowskiPhone,
doi={10.1109/ICASSP.2014.6854638},
institution={IEEE},
journaltitle={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
keywords={speech recognition},
date={2014},
groups={tesse:5},
title={Phone Sequence Modeling with Recurrent Neural Networks},
mendeley-tags={speech recognition},
booktitle={Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn={9781479928934},
pages={5454--5458},
number={2},
author={Boulanger-lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong},
}

@report{2016HwangImage,
author={Hwang, Jeff and Zhou, You},
journaltitle={Cs231N.Stanford.Edu},
title={Image Colorization with Deep Convolutional Neural Networks},
type={resreport},
abstract={We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
keywords={image synthesis},
url={http://cs231n.stanford.edu/reports/2016/pdfs/219///////\\_Report.pdf},
date={2016},
institution={Stanford University},
}

@article{2015TheisGenerative,
keywords={image synthesis},
location={Cambridge, MA, USA},
abstract={Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
address={Cambridge, MA, USA},
issn={1049-5258},
publisher={MIT Press},
mendeley-tags={image synthesis},
acmid={2969455},
url={http://dl.acm.org/citation.cfm?id=2969442.2969455},
pages={1927--1935},
eprint={1506.03478},
journaltitle={Advances in Neural Information Processing Systems},
title={Generative Image Modeling Using Spatial LSTMs},
booktitle={Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
numpages={9},
note={event-place: Montreal, Canada},
author={Theis, Lucas and Bethge, Matthias},
date={2015},
series={NIPS'15},
eprinttype={arXiv},
arxivid={1506.03478},
}

@incollection{2016LarssonLearning,
author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
booktitle={Computer Vision (ECCV 2016)},
title={Learning Representations for Automatic Colorization},
location={Anais},
pages={577--593},
publisher={Springer International Publishing},
date={2016},
edition={European C},
doi={10.1007/978-3-319-46493-0///////\\_35},
}

@article{2016VeitResidual,
arxivid={1605.06431},
pages={1--9},
author={Veit, Andreas and Wilber, Michael J. and Belongie, Serge},
note={event-place: Barcelona, Spain},
booktitle={Proceedings of the 30th International Conference on Neural Information Processing Systems},
journaltitle={Advances in Neural Information Processing Systems},
eprinttype={arXiv},
publisher={Curran Associates Inc.},
keywords={theory},
date={2016},
eprint={1605.06431},
series={NIPS'16},
abstract={In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
isbn={978-1-5108-3881-9},
url={http://arxiv.org/abs/1605.06431},
location={USA},
issn={1049-5258},
groups={tesse:5},
mendeley-tags={theory},
title={Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
}

@article{2016ZhuGenerative,
abstract={Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
date={2016},
eprint={1609.03552},
location={Anais},
pages={597--613},
mendeley-tags={image synthesis},
isbn={9783319464534},
booktitle={Proceedings of European Conference on Computer Vision (ECCV)},
eprinttype={arXiv},
publisher={Springer},
doi={10.1007/978-3-319-46454-1///////\\_36},
edition={European C},
issn={1611-3349},
author={Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros, Alexei A.},
volume={9909 LNCS},
keywords={image synthesis},
title={Generative Visual Manipulation on the Natural Image Manifold},
institution={Springer},
journaltitle={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
arxivid={1609.03552},
pmid={4520227},
}

@article{2017EricksonToolkits,
author={Erickson, Bradley J. and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy and Philbrick, Kenneth},
date={2017-03},
pages={400--405},
doi={10.1007/s10278-017-9965-6},
volume={30},
journaltitle={Journal of Digital Imaging},
title={Toolkits and Libraries for Deep Learning},
number={4},
groups={tesse:5},
}

@book{2016BovermannMusical,
groups={tesse:5},
langid={english},
title={Musical Instruments in the 21st Century},
publisher={Springer},
url={https://www.ebook.de/de/product/26880613/musical///////\\_instrument///////\\_in///////\\_the///////\\_21st///////\\_century.html},
author={Bovermann, T. and Others},
location={Traducao. [s.l.]},
year={2016},
isbn={978-981-10-2950-9},
date={2016},
ean={9789811029509},
}

@inproceedings{2012HooverGenerating,
booktitle={ICCC},
pages={111--118},
journaltitle={International Conference on Computational Creativity},
publisher={Citeseer},
groups={tesse:5},
isbn={9781905254668},
title={Generating a Complete Multipart Musical Composition from a Single Monophonic Melody with Functional Scaffolding.},
date={2012},
author={Hoover, Amy K. and Szerlip, Paul A. and Norton, Marie E. and Brindle, Trevor A. and Merritt, Zachary and Stanley, Kenneth O.},
mendeley-tags={music generation},
keywords={music generation},
}

@inproceedings{2016OordPixel,
url={http://proceedings.mlr.press/v48/oord16.html},
abstract={Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
eprinttype={arXiv},
isbn={9781510829008},
keywords={image synthesis},
volume={48},
author={van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
eprint={1601.06759},
title={Pixel Recurrent Neural Networks},
mendeley-tags={image synthesis},
booktitle={Proceedings of The 33rd International Conference on Machine Learning},
publisher={PMLR},
date={2016},
arxivid={1601.06759},
editor={Balcan, Maria Florina and Weinberger, Kilian Q.},
journaltitle={arXiv preprint arXiv:1601.06759},
location={New York, New York, USA},
series={Proceedings of Machine Learning Research},
pages={1747--1756},
}

@inproceedings{2016ChoiConvolutional,
url={http://arxiv.org/abs/1609.04243},
doi={10.1.1.302.7795},
journaltitle={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
abstract={We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
mendeley-tags={music classification},
issn={1520-9210},
organization={IEEE},
institution={IEEE},
date={2016},
keywords={music classification},
pages={1--5},
author={Choi, Keunwoo and Fazekas, George György and Sandler, Mark and Cho, Kyunghyun},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
eprint={1609.04243},
isbn={9789881701282},
eprinttype={arXiv},
arxivid={1609.04243},
title={Convolutional Recurrent Neural Networks for Music Classification},
groups={tesse:5},
}

@inproceedings{2002UnciniSound,
institution={University of Rome "La Sapienza"},
type={resreport},
booktitle={Italian Workshop on Neural Nets},
pages={168--177},
publisher={Springer},
date={2002},
title={Sound synthesis by flexible activation function recurrent neural networks},
groups={tesse:5},
author={Uncini, Aurelio},
}

@article{2016IizukaLet,
issn={0730-0301},
isbn={9781450342797},
abstract={We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
groups={tesse:5},
journaltitle={ACM Transactions on Graphics},
pages={1--11},
keywords={colorization,computing methodologies,convolutional neural network concepts,image processing,image synthesis,neural net-},
url={http://dl.acm.org/citation.cfm?doid=2897824.2925974},
volume={35},
doi={10.1145/2897824.2925974},
number={4},
author={Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
mendeley-tags={image synthesis},
date={2016},
title={Let there be color! :Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification},
}

@article{2019StollCarbon,
author={Stoll, Christian and Klaa{/ss}en, Lena and Gallersdörfer, Ulrich},
doi={10.1016/j.joule.2019.05.012},
groups={Neural Networks and Sustainability},
title={The Carbon Footprint of Bitcoin},
year={2019},
journal={Joule},
month={jun},
publisher={Elsevier {BV},
}

@article{2000MaoProbabilistic,
abstract={Network structure determination is an important issue in pattern classification based on a probabilistic neural network. In this study, a supervised network structure determination algorithm is proposed. The proposed algorithm consists of two parts and runs in an iterative way. The first part identifies an appropriate smoothing parameter using a genetic algorithm, while the second part determines suitable pattern layer neurons using a forward regression orthogonal algorithm. The proposed algorithm is capable of offering a fairly small network structure with satisfactory classification accuracy.},
doi={10.1109/72.857781},
mendeley-tags={evolutive,seminal},
title={Probabilistic neural network structure determination for pattern classification},
number={4},
issn={1045-9227},
groups={tesse:5},
publisher={IEEE},
keywords={evolutive,seminal},
pages={1009--1016},
journaltitle={IEEE Transactions on Neural Networks},
date={2000},
url={http://ieeexplore.ieee.org/document/857781/},
urldate={2019-04-05},
volume={11},
author={Mao, K. Z. and Tan, K. C. and Ser, W.},
langid={english},
}

@article{2016ZweigAdvances,
url={http://arxiv.org/abs/1609.05935},
eprinttype={arXiv},
edition={Acoustics},
year={2016},
title={Advances in all-neural speech recognition},
volume={abs/1609.05935},
date={2017},
journal={CoRR},
author={Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
timestamp={Mon, 13 Aug 2018 16:47:53 +0200},
biburl={https://dblp.org/rec/bib/journals/corr/ZweigYDS16},
publisher={AnaisIEEE},
journaltitle={CoRR},
bibsource={dblp computer science bibliography, https://dblp.org},
eprint={1609.05935},
}

@inproceedings{2017YangTensor,
journaltitle={arXiv preprint arXiv:1707.01786},
keywords={video classification},
arxivid={1707.01786},
author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
mendeley-tags={video classification},
eprinttype={arXiv},
title={Tensor-Train Recurrent Neural Networks for Video Classification},
eprint={1707.01786},
date={2017},
url={https://arxiv.org/pdf/1707.01786.pdf},
abstract={The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
groups={tesse:5},
booktitle={Proceedings of the International Conference on Machine Learning},
}

@article{2017SangkloyScribbler,
eprint={1612.00835},
eprinttype={arXiv},
shorttitle={Scribbler},
author={Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
pages={6836--6845},
arxivid={1612.00835},
url={http://ieeexplore.ieee.org/document/8100206/},
isbn={978-1-5386-0457-1},
abstract={Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.},
booktitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
location={Honolulu, HI},
doi={10.1109/CVPR.2017.723},
urldate={2019-04-05},
eventtitle={2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords={image synthesis,state of the art},
title={Scribbler: Controlling Deep Image Synthesis with Sketch and Color},
groups={tesse:5},
journaltitle={Computer Vision and Pattern Recognition (CVPR)},
publisher={IEEE},
langid={english},
date={2017-07},
}

@inproceedings{1992BoashashEstimating,
author={Boashash, Boualem},
title={Estimating and Interpreting the Instantaneous Frequency of a Signal-Part 1: Fundamentals},
pages={19},
volume={80},
langid={english},
date={1992},
groups={tesse:5},
booktitle={PROCEEDINGS OF THE IEEE},
}

@article{2018VanDenOordWavenet,
journaltitle={CoRR},
title={WaveNet: A Generative Model for Raw Audio},
url={https://deepmind.com/blog/wavenet-generative-model-raw-audio/},
groups={tesse:5},
bibsource={dblp computer science bibliography, https://dblp.org},
eprint={1609.03499},
urldate={2018-06-30},
biburl={https://dblp.org/rec/bib/journals/corr/OordDZSVGKSK16},
location={DeepMind Disponvel em},
howpublished={online},
timestamp={Mon, 13 Aug 2018 16:49:15 +0200},
eprinttype={arXiv},
volume={abs/1609.03499},
publisher={<},
author={Van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew W. and Kavukcuoglu, Koray},
date={2018},
}

@article{MizutaniDerivation,
booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN 2000). Neural Computing: New Challenges and Perspectives for the New Millennium},
title={On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application},
volume={2},
doi={10.1109/IJCNN.2000.857892},
date={2000-07},
keywords={Neurons, Backpropagation, BP derivative computation process, Cost function, Discrete time systems, discrete-time optimal control theory, dynamic programming, Dynamic programming, Gradient methods, Industrial training, Kelley-Bryson optimal-control gradient formula, Laboratories, MLP backpropagation, multilayer perceptron learning, Multilayer perceptrons, Nonhomogeneous media, Optimal control, Optimized production technology, Poles and towers, Training data},
pages={167--172 vol.2},
journaltitle={Neural Networks},
author={Mizutani, E. and Dreyfus, S. E. and Nishio, K.},
}

@inproceedings{2016GregorTowards,
date={2016},
journaltitle={Advances In Neural Information Processing Systems},
publisher={Curran Associates, Inc.},
url={http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf},
keywords={image compression - conceptual,state of the art},
abstract={We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
title={Towards Conceptual Compression},
booktitle={Advances in Neural Information Processing Systems 29},
pages={3549--3557},
urldate={2019-04-18},
eprinttype={arXiv},
groups={tesse:5},
issn={1049-5258},
number={Nips},
mendeley-tags={image compression - conceptual,state of the art},
eprint={1604.08772},
editor={Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
author={Gregor, Karol and Besse, Frederic and Jimenez Rezende, Danilo and Danihelka, Ivo and Wierstra, Daan},
arxivid={1604.08772},
}

@inproceedings{2003LeungTuning,
author={Leung, Frank Hung-Fat F and Lam, Hak-Keung K and Ling, Sai-Ho H and Tam, Peter Kwong-Shun S},
eventtitle={IECON'01. 27th Annual Conference of the IEEE Industrial Electronics Society},
journaltitle={IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
date={2003},
doi={10.1109/TNN.2002.804317},
number={1},
title={Tuning of the structure and parameters of a neural network using an improved genetic algorithm.},
pages={79--88},
urldate={2019-04-05},
pmid={18237992},
publisher={IEEE},
url={http://ieeexplore.ieee.org/document/976448/},
isbn={1045-9227 (Print)$/backslash$r1045-9227 (Linking)},
abstract={This paper presents the tuning of the structure and parameters of a neural network using an improved genetic algorithm (GA). It is also shown that the improved GA performs better than the standard GA based on some benchmark test functions. A neural network with switches introduced to its links is proposed. By doing this, the proposed neural network can learn both the input-output relationships of an application and the network structure using the improved GA. The number of hidden nodes is chosen manually by increasing it from a small number until the learning performance in terms of fitness value is good enough. Application examples on sunspot forecasting and associative memory are given to show the merits of the improved GA and the proposed neural network.},
location={Denver, CO, USA},
booktitle={IECON'01. 27th Annual Conference of the IEEE Industrial Electronics Society (Cat. No.37243)},
volume={14},
issn={1045-9227},
langid={english},
keywords={evolutive},
mendeley-tags={evolutive},
}

@article{2008AagerfalkOutsourcing,
booktitle={MIS Quarterly},
title={Outsourcing to an Unknown Workforce: Exploring Opensourcing as a Global Sourcing Strategy},
keywords={crowdsourcing,global software development,multi-method research,offshoreing,open source,opensourcing,outsourcing},
isbn={02767783},
author={Ågerfalk, Pär J. and Fitzgerald, Brian and Agerfalk, Par J and Fitzgerald, Brian},
volume={32},
abstract={This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy-opensourcing, as we term it here-whereby commerical companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product. We followed this with a large-scale survey involving additional exemplars of the phenomenon. The study identifies a number of symmetrical and complementary customer and community obligations that are associated with opensourcing success. We also identify a number of tension points on which customer and community perceptions tend to vary. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness: The customer and community need to establish a trusted partnership of shared responsibility in building an overall opensourcing ecosystem. The study reveals an ongoing shift from OSS as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises. It also reveals that opensourcing provides ample opportunity for companies to headhunt top developers, hence moving from outsourcing to a largely unknown OSS workforce toward recruitment of developers from a global open source community whose talents have become known as a result of the opensourcing experience.},
pages={385--409},
date={2008},
issn={0276-7783},
pmid={31831011},
shorttitle={OUTSOURCING TO AN UNKNOWN WORKFORCE},
doi={Article},
number={2},
journaltitle={MIS Quarterly},
}

@article{2017GabrielliIntroducing,
author={Gabrielli, Leonardo and Tomassetti, Stefano and Squartini, Stefano and Zinato, Carlo},
title={Introducing Deep Machine Learning for Parameter Estimation in Physical Modelling},
langid={english},
groups={tesse:2},
date={2017},
pages={6},
abstract={One of the most challenging tasks in physically-informed sound synthesis is the estimation of model parameters to produce a desired timbre. Automatic parameter estimation procedures have been developed in the past for some speciﬁc parameters or application scenarios but, up to now, no approach has been proved applicable to a wide variety of use cases. A general solution to parameters estimation problem is provided along this paper which is based on a supervised convolutional machine learning paradigm. The described approach can be classiﬁed as “end-to-end” and requires, thus, no speciﬁc knowledge of the model itself. Furthermore, parameters are learned from data generated by the model, requiring no effort in the preparation and labeling of the training dataset. To provide a qualitative and quantitative analysis of the performance, this method is applied to a patented digital waveguide pipe organ model, yielding very promising results.},
}

@article{2014LillicrapRandom,
pages={1--27},
urldate={2019-03-26},
groups={tesse:5},
abstract={The brain processes information through many layers of neurons. This deep architecture is representationally powerful1,2,3,4, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made1,5. In machine learning, the backpropagation algorithm1 assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron’s axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain 1,6,7,8,9,10,11,12,13,14. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
journaltitle={arXiv:1411.0247 [cs, q-bio]},
date={2014-11-02},
url={http://arxiv.org/abs/1411.0247},
eprinttype={arxiv},
author={Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
arxivid={arXiv:1411.0247v1},
title={Random feedback weights support learning in deep neural networks},
langid={english},
eprint={1411.0247},
}

@inproceedings{2017BlaauwNeural,
abstract={We present a new model for singing synthesis based on a modiﬁed version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the inﬂuence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and signiﬁcantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overﬁtting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefﬁcient, using a smart algorithm we can greatly speed up the process and obtain a system that’s competitive in both speed and quality.},
urldate={2019-03-28},
eprinttype={arxiv},
title={A Neural Parametric Singing Synthesizer},
date={2017-04-12},
langid={english},
booktitle={INTERSPEECH},
url={http://arxiv.org/abs/1704.03809},
eprint={1704.03809},
groups={tesse:5},
journaltitle={arXiv:1704.03809 [cs]},
author={Blaauw, Merlijn and Bonada, Jordi},
}

@article{2017EstevaDermatologist,
volume={542},
author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
journaltitle={Nature},
number={7639},
pages={115--118},
doi={10.1038/nature21056},
groups={tesse:5},
date={2017-01},
title={Dermatologist-level classification of skin cancer with deep neural networks},
}

@inproceedings{2017ZhangVery,
isbn={9781509041176},
eprint={1610.03022},
arxivid={1610.03022},
mendeley-tags={speech recognition,state of the art},
pages={4845--4849},
keywords={Hidden Markov models, Training, Acoustics, speech recognition, word error rate, recurrent neural nets, Computational modeling, convolution, Biological neural networks, Automatic Speech Recognition, batch normalization, Brain modeling, convolutional LSTM, end-to-end ASR models, end-to-end speech recognition, End-to-End Speech Recognition, feature space, Logic gates, network-in-network principles, residual connections, sequence-to-sequence models, shallow acoustic encoder networks, spectral structure, very deep convolutional networks, Very Deep Convolutional Neural Networks, very deep recurrent structures},
title={Very Deep Convolutional Networks for End-to-End Speech Recognition},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
url={http://arxiv.org/abs/1610.03022},
abstract={Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5///////\% word error rate without any dictionary or language using a 15 layer deep network.},
eprinttype={arXiv},
doi={10.1109/ICASSP.2017.7953077},
author={Zhang, Yu and Chan, William and Jaitly, Navdeep},
organization={IEEE},
date={2017},
institution={IEEE},
journaltitle={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
groups={tesse:5},
}

@article{2000EdmundsProblem,
volume={20},
journaltitle={International Journal of Information Management},
doi={10.1016/S0268-4012(99)00051-1},
author={Edmunds, Angela and Morris, Anne},
number={1},
isbn={0268-4012},
abstract={This paper reviews the literature on the problem of information overload, with particular reference to business organisations. The literature reveals that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognised and experienced. Both perceptions and the actual effects of information overload have been exacerbated by the rapid advances made in information and communication technology, although it is not clear cut as to whether the Internet has worsened or improved the situation. A theme stressed in the literature is the paradoxical situation that, although there is an abundance of information available, it is often difficult to obtain useful, relevant information when it is needed. Some solutions put forward to reduce information overload are: a reduction in the duplication of information found in the professional literature; the adoption of personal information management strategies, together with the integration of software solutions such as push technology and intelligent agents; and the provision of value-added information (filtered by software or information specialists). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
booktitle={International Journal of Information Management},
issn={0268-4012},
title={The problem of information overload in business organisations: a review of the literature},
url={http://www.sciencedirect.com/science/article/pii/S0268401299000511},
pmid={10272},
pages={17--28},
keywords={infoglut,information fatigue syndrome,information overload},
date={2000},
}

@inproceedings{2016BellOutsidea,
doi={10.1109/CVPR.2016.314},
pages={2874--2883},
publisher={IEEE},
shorttitle={Inside-Outside Net},
date={2016-06},
urldate={2019-03-28},
author={Bell, Sean and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross},
title={Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks},
langid={english},
location={Las Vegas, NV, USA},
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
url={http://ieeexplore.ieee.org/document/7780683/},
isbn={978-1-4673-8851-1},
eventtitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
abstract={It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9///////\\% to 77.9///////\\% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7///////\\% to 33.1///////\\% mAP. In the 2015 MS COCO Detection Challenge, our ION model won “Best Student Entry” and ﬁnished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
}

@report{1960WidrowAdaptive,
author={Widrow, Bernard and Hoff, Marcian E.},
booktitle={Technical Report No. 1553-1},
title={Adaptive Switching Circuits},
comment={Reprinted in anderson:neurocomputing},
institution={STANFORD UNIV CA STANFORD ELECTRONICS LABS},
type={techreport},
groups={tesse:5},
journaltitle={[s.l.] STANFORD UNIV CA STANFORD ELECTRONICS LABS},
volume={1960},
url={http://www.dtic.mil/dtic/tr/fulltext/u2/241531.pdf},
location={New York},
date={1960},
}

@article{2018TomarAmplitude,
journaltitle={IEEE Transactions on Instrumentation and Measurement},
groups={tesse:5, Acoustics},
url={http://ieeexplore.ieee.org/document/8063390/},
date={2018-01},
pages={229--237},
doi={10.1109/TIM.2017.2755998},
number={1},
issn={0018-9456, 1557-9662},
author={Tomar, Shikha and Sumathi, Parasuraman},
title={Amplitude and Frequency Estimation of Exponentially Decaying Sinusoids},
volume={67},
abstract={An online method for amplitude and frequency estimation of exponentially decaying sinusoids is proposed with a moving-window discrete Fourier transform (MWDFT) ﬁlter and frequency-locked loop. The tuned ﬁlter characteristics of MWDFT is modiﬁed into more ﬂat characteristic around the center frequency with negative feedback, which increases the bandwidth of the ﬁlter. An adaptive sampling pulse adjustment mechanism is incorporated in the proposed structure for online estimation of frequency. Hence, the frequency error was exploited to achieve synchronization between in-phase component of MWDFT and input signal of estimation. The amplitude is estimated in online from the in-phase and quadrature-phase components of MWDFT. The performance of the proposed method is compared with the existing techniques and experimentally validated on single-link ﬂexible manipulator system for the online estimation of frequency and amplitude of tip deﬂection signal. The experimental investigation prove that the proposed online technique performs well over the existing techniques.},
langid={english},
urldate={2019-03-26},
}

@electronic{2017Neural,
title={The Neural Network Zoo},
url={http://www.asimovinstitute.org/neural-network-zoo/},
author={Veen, F Van.},
date={2017},
urldate={2018-06-30},
howpublished={online},
publisher={<},
}

@misc{MoscaritoloPebble,
author={Moscaritolo, Autores and Angelamoscaritolopcmagcom, Angela},
title={Pebble Smartwatch Sells Out , Collects $ 10 Million on Kickstarter},
}

@report{parker_learning-logic:_1985,
author={Parker, D.B.},
institution={Massachusetts Institute of Technology, Center for Computational Research in Economics and Management Science},
title={Learning-logic: Casting the Cortex of the Human Brain in Silicon},
date={1985},
url={https://books.google.com.br/books?id=2kS9GwAACAAJ},
}

@article{2015SmithCyclical,
eprinttype={arxiv},
url={http://arxiv.org/abs/1506.01186},
groups={tesse:5},
eprint={1506.01186},
journaltitle={arXiv:1506.01186 [cs]},
urldate={2019-03-26},
langid={english},
author={Smith, Leslie N.},
title={Cyclical Learning Rates for Training Neural Networks},
date={2015-06-03},
number={April},
abstract={It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally ﬁnd the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of ﬁxed values achieves improved classiﬁcation accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate “reasonable bounds” – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
arxivid={arXiv:1506.01186v6},
}

@article{klaus_comparison_nodate,
author={Klaus, Leonard},
abstract={For the analysis of rotational damping measurements in the time domain, two different identiﬁcation procedures are compared. The ﬁrst procedure investigated incorporates a Hilbert transform of the data, which enables an analysis by a linear regression calculation. The second approach is a direct nonlinear regression calculation of a damped sine function. The two approaches are compared using both simulated data and measurement data. The results of the comparison are presented.},
langid={english},
volume={1},
date={2015},
pages={4},
title={Comparison of hilbert transform and sine fit approaches for the determination of damping parameters},
number={4},
}

@inproceedings{2018RobertsLearninga,
author={Roberts, Adam and Engel, Jesse and Oore, Sageev and Eck, Douglas},
date={2018},
url={http://ceur-ws.org/Vol-2068/milc7.pdf},
booktitle={Proceedings of the 2018 ACM Workshop on Intelligent Music Interfaces for Listening and Creation, MILC@IUI 2018},
title={Learning Latent Representations of Music to Generate Interactive Musical Palettes},
}

@inproceedings{2013SutskeverImportance,
series={Proceedings of Machine Learning Research},
author={Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
location={Atlanta, Georgia, USA},
booktitle={Proceedings of the 30th International Conference on Machine Learning},
number={3},
abstract={Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
date={2013-06-17},
title={On the importance of initialization and momentum in deep learning},
editor={Dasgupta, Sanjoy and McAllester, David},
journaltitle={International conference on machine learning},
groups={tesse:5},
volume={2013},
publisher={PMLR},
pages={1139--1147},
url={http://proceedings.mlr.press/v28/sutskever13.html},
}

@article{2016BahrampourComparative,
journaltitle={CoRR},
location={neon, theano, and torch for deep learning},
bibsource={dblp computer science bibliography, https://dblp.org},
volume={abs/1511.06435},
publisher={Comparative study of caffe},
groups={tesse:5},
biburl={https://dblp.org/rec/bib/journals/corr/BahrampourRSS15},
timestamp={Mon, 13 Aug 2018 16:47:26 +0200},
title={Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning},
url={http://arxiv.org/abs/1511.06435},
date={2016},
eprint={1511.06435},
eprinttype={arXiv},
author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
}

@inproceedings{2007SerraState,
pages={2--5},
groups={tesse:5},
date={2007-10},
booktitle={Proceedings of the IEEE 9th Workshop on Multimedia Signal Processing},
doi={10.1109/MMSP.2007.4412805},
author={Serra, Xavier},
title={State of the Art and Future Directions in Musical Sound Synthesis},
journaltitle={Multimedia Signal Processing},
keywords={Instruments, music, acoustic signal processing, corpus-based concatenative methods, Humans, Multiple signal classification, Music, musical sound synthesis, perceptual characteristics, physical models, Physics computing, Psychoacoustic models, Psychology, Signal processing algorithms, signal synthesis, Signal synthesis, Sliding mode control, sound and music computing, sound source, sound synthesis},
volume={2007},
}

@report{2008BerdahlPlucked,
groups={tesse:5},
institution={Stanford University},
type={resreport},
author={Berdahl, Edgar J. and Julius O. Smith, I. I. I.},
date={2008},
title={Plucked String Digital Waveguide Model},
pages={14},
langid={english},
}

@inproceedings{2016KhorramiHow,
edition={(ICIP), 20},
arxivid={1602.07377},
abstract={We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
doi={10.1109/ICIP.2016.7532431},
author={Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
title={How Deep Neural Networks Can Improve Emotion Recognition on Video Data},
groups={tesse:5},
keywords={Recurrent neural networks, recurrent neural networks, Convolution, convolutional neural networks, Feature extraction, deep neural networks, Audio/Visual+Emotion Challenge, AV+EC2015, Convolutional Neural Networks, deep learning, Deep Learning, emotion recognition, Emotion recognition, Emotion Recognition, Face, Predictive models, Recurrent Neural Networks, temporal neural network models, Video Processing},
eprinttype={arXiv},
booktitle={Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP)},
publisher={AnaisIEEE},
date={2016-09},
url={http://arxiv.org/abs/1602.07377},
pages={619--623},
mendeley-tags={sentiment analysis,video classification},
eprint={1602.07377},
}

@article{2016KalchbrennerNeural,
groups={tesse:5},
langid={english},
journaltitle={arXiv:1610.10099 [cs]},
date={2016-10-31},
eprint={1610.10099},
author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
url={http://arxiv.org/abs/1610.10099},
arxivid={arXiv:1610.10099v2},
abstract={We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efﬁcient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive ﬁeld. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We ﬁnd that the latent alignment structure contained in the representations reﬂects the expected alignment between the tokens.},
eprinttype={arxiv},
urldate={2019-03-26},
title={Neural Machine Translation in Linear Time},
}

@article{1992SmithPhysical,
journaltitle={Computer Music Journal},
number={4},
date={1992-12},
groups={tesse:5, Digital Waveguides},
pages={74--91},
author={Smith, Julius O.},
volume={16},
title={Physical Modeling Using Digital Waveguides},
keywords={digital waveguides, physical modeling},
url={https://ccrma.stanford.edu/ jos/pmudw/},
}

@inproceedings{2015HeDeep,
title={Deep Residual Learning for Image Recognition},
eprint={1512.03385},
booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
pages={770--778},
url={http://arxiv.org/abs/1512.03385},
urldate={2019-04-05},
abstract={Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57 /\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
langid={english},
eprinttype={arxiv},
groups={tesse:5},
journaltitle={arXiv:1512.03385 [cs]},
date={2015-12-10},
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
}

@article{2017BelloNeural,
author={Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
urldate={2019-03-26},
eprint={1709.07417},
journaltitle={arXiv:1709.07417 [cs, stat]},
title={Neural Optimizer Search with Reinforcement Learning},
url={http://arxiv.org/abs/1709.07417},
arxivid={arXiv:1709.07417v2},
number={2002},
groups={tesse:5},
eprinttype={arxiv},
langid={english},
date={2017-09-21},
abstract={We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain speciﬁc language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classiﬁcation and Google’s neural machine translation system.},
}

@article{1991LeshnoMultilayer,
number={21},
author={Leshno, Moshe and Schocken, Shimon},
journaltitle={Neural Networks},
title={Multilayer Feedforward Networks with Non-Polynomial Activation Functions Can Approximate Any Function},
groups={tesse:5},
mendeley-tags={theory},
abstract={Several researchers characterized the activation functions under which multilayer feedforward networks can act as universal approximators. We show that all the characterizations that were reported thus far in the literature ark special cases of the following general result: a standard multilayer feedforward network can approximate any continuous function to any degree of accuracy if and only if the network's activation functions are not polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem doesn't hold.},
langid={english},
year={1991},
url={https://archive.nyu.edu/bitstream/2451/14384/1/IS-91-26.pdf},
keywords={theory},
date={1991},
pages={17},
}

@article{2013PingpingZhuKernel,
location={Vancouver, BC, Canada},
date={2013-05},
pages={3572--3576},
url={http://ieeexplore.ieee.org/document/6638323/},
doi={10.1109/ICASSP.2013.6638323},
groups={tesse:5},
isbn={978-1-4799-0356-6},
abstract={This paper presents a kernelized version of recurrent systems (KRS) and develops a kernel real-time recurrent learning (KRTRL) algorithm to train KRS. To avoid instabilities during training, the teacher forcing technique is adopted to modify the KRTRL learning. The proposed algorithm is compared with the KLMS in Lorenz time series prediction. The prediction performances of the proposed algorithm outperform the KLMS signiﬁcantly.},
langid={english},
urldate={2019-04-05},
mendeley-tags={forecasting},
publisher={IEEE},
eventtitle={ICASSP 2013 - 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
author={Pingping Zhu, José and Príncipe, e C. and Zhu, Pingping and Pr'incipe, José C.},
booktitle={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
keywords={forecasting},
journaltitle={Computer Engineering},
title={Kernel recurrent system trained by real-time recurrent learning algorithm},
}

@article{2003RitchieOptimizationa,
mendeley-tags={evolutive},
isbn={1471-2105 (Electronic)$/backslash$r1471-2105 (Linking)},
publisher={BioMed Central},
title={Optimization of neural network architecture using genetic programming improves detection and modeling of gene-gene interactions in studies of human diseases},
url={http://www.ncbi.nlm.nih.gov/pubmed/12846935},
date={2003},
langid={english},
volume={4},
doi={10.1186/1471-2105-4-28},
pages={14},
keywords={*Epistasis,*Models,*Neural Networks (Computer),*Software,Algorithms,Artificial Intelligence,Gene Expression Regulation/*genetics,Genetic,Humans,Molecular Epidemiology/*methods/*trends,Polymorphism,Predictive Value of Tests,Research Design,Single Nucleotide/genetics,Software Validation,evolutive},
number={1},
journaltitle={BMC Bioinformatics},
issn={1471-2105},
author={Ritchie, Marylyn D and White, Bill C and Parker, Joel S and Hahn, Lance W and Moore, Jason H},
abstract={Background: Appropriate definition of neural network architecture prior to data analysis is crucial for successful data mining. This can be challenging when the underlying model of the data is unknown. The goal of this study was to determine whether optimizing neural network architecture using genetic programming as a machine learning strategy would improve the ability of neural networks to model and detect nonlinear interactions among genes in studies of common human diseases. Results: Using simulated data, we show that a genetic programming optimized neural network approach is able to model gene-gene interactions as well as a traditional back propagation neural network. Furthermore, the genetic programming optimized neural network is better than the traditional back propagation neural network approach in terms of predictive ability and power to detect gene-gene interactions when non-functional polymorphisms are present. Conclusion: This study suggests that a machine learning strategy for optimizing neural network architecture may be preferable to traditional trial-and-error approaches for the identification and characterization of gene-gene interactions in common, complex human diseases.},
pmid={12846935},
}

@phdthesis{2006MullenPhysical,
url={http://www-users.york.ac.uk/ dtm3/Download/JackThesis.pdf},
date={2006-04},
author={Mullen, Jack},
volume={2006},
title={Physical modelling of the vocal tract with the 2D digital waveguide mesh.},
groups={tesse:5},
institution={The University of York},
journaltitle={[s.l.] University of York},
type={phdthesis},
}

@inproceedings{2013MaasRectifier,
title={Rectifier nonlinearities improve neural network acoustic models},
volume={2013},
booktitle={Proceedings of the International Conference on Machine Learning},
groups={tesse:5},
abstract={Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2 /\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
keywords={speech recognition},
pages={6},
journaltitle={ICML. Anais},
mendeley-tags={speech recognition},
number={1},
url={https://web.stanford.edu/{/textasciitilde }awni/papers/relu{////////\\_}hybrid{////////\\_}icml2013{////////\\_}final.pdf},
author={Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
date={2013},
}

@inproceedings{2001SerafinBanded,
pages={2--5},
title={The banded digital waveguide mesh},
organization={Citeseer},
booktitle={Workshop on Future Directions of Computer Music (Mosart-01)},
url={http://imi.aau.dk/ sts/publications/mosart.pdf},
author={Serafin, Stefania and Huang, Patty and Iii, Julius O. Smith},
volume={1},
groups={tesse:5},
journaltitle={Workshop on Future Directions of Computer Music (Mosart-0},
publisher={Citeseer},
date={2001},
}

@article{2008ForgeardPracticing,
groups={tesse:5},
date={2008-10},
title={Practicing a Musical Instrument in Childhood is Associated with Enhanced Verbal Ability and Nonverbal Reasoning},
pages={e3566},
number={10},
author={Forgeard, Marie and Winner, Ellen and Norton, Andrea and Schlaug, Gottfried},
doi={10.1371/journal.pone.0003566},
journaltitle={PLoS ONE},
editor={Fitch, Tecumseh},
volume={3},
}

@inproceedings{2014OhanlonPolyphonic,
url={http://ieeexplore.ieee.org/document/6854173/},
title={Polyphonic piano transcription using non-negative Matrix Factorisation with group sparsity},
pages={3112--3116},
eventtitle={ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
urldate={2019-04-05},
volume={1},
number={May},
location={Florence, Italy},
publisher={IEEE},
langid={english},
booktitle={2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
author={O'Hanlon, Ken and Plumbley, Mark D.},
doi={10.1109/ICASSP.2014.6854173},
isbn={978-1-4799-2893-4},
date={2014-05},
}

@inproceedings{2017SelfridgeReal,
langid={english},
date={2017},
author={Selfridge, Rod and Moffat, David J. and Reiss, Joshua D. and Avital, Eldad J.},
journaltitle={ICSV24},
pages={1--8},
title={Real-time physical model of an aeolian harp},
abstract={A real-time physical sound synthesis model of an Aeolian harp is presented. The model uses semiempirical ﬂuid dynamics equations to inform its operation, providing suitable parameters for users to interact. A basic wind model is included as well as an interface allowing user adjustable parameters. Sounds generated by the model were subject to objective measurements against real-world recordings, which showed that many of the physical properties of the harp were replicated in our model, but a possible link between harmonics and vibration amplitude was not. A perceptual test was performed, where participants were asked to rate sounds in terms of how plausible they were in comparison with spectral modelling synthesis and recorded Aeolian Harp samples. Evaluation showed that our model performed as well as an alternative non-physical synthesis method, but was not as authentic as actual recorded samples.},
groups={tesse:5, Acoustics},
keywords={aeolian harp,physical model,real-time,sound synthesis},
}

@article{2003BensaSimulation,
date={2003-08},
number={2},
doi={10.1121/1.1587146},
volume={114},
title={The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
pages={1095--1107},
author={Bensa, Julien and Bilbao, Stefan and Kronland-Martinet, Richard and Smith, Julius O.},
keywords={digital waveguides, physical modeling, finite difference},
journaltitle={The Journal of the Acoustical Society of America},
}

@phdthesis{2012SpeedVoice,
school={The University of York (United Kingdom},
author={Speed, Matthew David Adam},
adsurl={http://adsabs.harvard.edu/abs/2012PhDT.......545S},
date={2012},
urldate={2019-03-28},
adsnote={Provided by the SAO/NASA Astrophysics Data System},
title={Voice Synthesis Using the Three-Dimensional Digital Waveguide Mesh},
abstract={The acoustic response of the vocal tract is fundamental to our interpretation of voice production. As an acoustic filter, it shapes the spectral envelope of vocal fold vibration towards resonant modes, or formants, whose behaviours form the most basic building blocks of phonetics. Physical models of the voice exploit this effect by modelling the nature of wave propagation in abstracted cylindrical constructs. Whilst effective, the accuracy of such approaches is limited due to their limited geometrical analogue. Developments in numerical acoustics modelling meanwhile have seen the formalisation of higher dimensionality configurations of the same technologies, allowing a much closer geometrical representation of an acoustic field. The major focus of this thesis is the application of such a technique to the vocal tract, and comparison of its performance with lower dimensionality approaches. To afford the development of such models, a body of data is collected from Magnetic Resonance Imaging for a range of subjects, and procedures are developed for the decomposition of this imaging into suitable, efficient data structures for simulation. The simulation technique is exhaustively validated using a combination of bespoke measurement/inversion techniques and analytical determination of lower frequency behaviours. Finally, voice synthesis based on each numerical model is compared with acoustic recordings of the subjects involved and with equivalent simulations from lower dimensionality methods. It is found that application of a higher dimensionality method typically yields a more accurate frequency-domain representation of the voice, although in some cases lower dimensionality equivalents are seen to perform better at low frequencies.},
url={http://etheses.whiterose.ac.uk/2800/},
institution={University of York},
type={phdthesis},
groups={tesse:5},
}

@article{2017KlambauerSelf,
date={2017},
eprint={arXiv:1706.02515v5},
eprinttype={arXiv},
title={Self-Normalizing Neural Networks},
url={http://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf},
arxivid={arXiv:1706.02515v5},
booktitle={Advances in Neural Information Processing Systems 30},
groups={tesse:5},
pages={971--980},
journaltitle={Advances in Neural Information Processing Systems 30},
author={Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
publisher={Curran Associates, Inc.},
editor={Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
}

@inproceedings{2015ZenUnidirectionala,
langid={english},
urldate={2019-04-05},
organization={IEEE},
abstract={Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the concerns for applying them to text-to-speech applications is its effect on latency. To address this concern, this paper proposes a low-latency, streaming speech synthesis architecture using unidirectional LSTMRNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of output acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch processing.},
eventtitle={ICASSP 2015 - 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
institution={IEEE},
title={Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis},
issn={9781467369978},
isbn={978-1-4673-6997-8},
journaltitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
date={2015},
author={Zen, Heiga and Sak, Hacsim},
location={South Brisbane, Queensland, Australia},
keywords={speech synthesis},
mendeley-tags={speech synthesis},
doi={10.1109/ICASSP.2015.7178816},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
pages={4470--4474},
publisher={IEEE},
url={http://ieeexplore.ieee.org/document/7178816/},
}

@electronic{2015Open,
date={2015},
urldate={2018-06-30},
title={The Open Source Drumkit},
url={https://github.com/crabacus/the-open-source-drumkit},
howpublished={online},
}

@book{noauthor_spline_2015,
note={OCLC: 906187660},
publisher={Springer Berlin Heidelberg},
isbn={978-3-319-22302-5},
date={2015},
author={Averbuch, Amir Z and Zheludev, Valery A and Neittaanmäki, P and Kitchener and Foundation, Waterloo Community},
location={New York, NY},
langid={english},
title={Spline and spline wavelet methods with applications to signal and image processing},
}

@book{,
}

@book{2009BilbaoNumericala,
isbn={978-0-470-74901-2 978-0-470-51046-9},
publisher={John Wiley {\&} Sons, Ltd},
title={Numerical Sound Synthesis},
doi={10.1002/9780470749012},
groups={tesse:5},
urldate={2019-03-31},
url={https://www.ebook.de/de/product/9338750/stefan///////\\_bilbao///////\\_numerical///////\\_sound///////\\_synthesis.html},
langid={english},
month={oct},
location={Chichester, UK},
ean={9780470510469},
year={2009},
author={Stefan Bilbao},
date={2009-10-23},
pagetotal={456},
}

@inproceedings{2017EngelNeural,
eprinttype={arXiv},
month={06--11 Aug},
bibsource={dblp computer science bibliography, https://dblp.org},
journaltitle={CoRR},
date={2017},
pages={1068--1077},
groups={tesse:5},
url={http://proceedings.mlr.press/v70/engel17a.html},
author={Jesse Engel and Cinjon Resnick and Adam Roberts and Sander Dieleman and Mohammad Norouzi and Douglas Eck and Karen Simonyan},
title={Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders},
arxivid={arXiv:1704.01279v1},
series={Proceedings of Machine Learning Research},
eprint={arXiv:1704.01279v1},
editor={Doina Precup and Yee Whye Teh},
publisher={PMLR},
abstract={Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
timestamp={Mon, 13 Aug 2018 16:47:29 +0200},
address={International Convention Centre, Sydney, Australia},
biburl={https://dblp.org/rec/bib/journals/corr/EngelRRDESN17},
volume={70},
year={2017},
booktitle={Proceedings of the 34th International Conference on Machine Learning},
}

@book{Tarjano2019,
isbn={9783030304898},
issn={16113349},
volume={11730 LNCS},
keywords={Acoustic modeling,Digital musical instruments,Neural networks,Real-time audio synthesis},
doi={10.1007/978-3-030-30490-4_30},
booktitle={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
abstract={\textcopyright} 2019, Springer Nature Switzerland AG. Two main approaches are currently prevalent in the digital emulation of musical instruments: manipulation of pre-recorded samples and techniques of real-time synthesis, generally based on physical models with varying degrees of accuracy. Concerning the first, while the processing power of present-day computers enables their use in real-time, many restrictions arising from this sample-based design persist; the huge on disk space requirements and the stiffness of musical articulations being the most prominent. On the other side of the spectrum, pure synthesis approaches, while offering greater flexibility, fail to capture and reproduce certain nuances central to the verisimilitude of the generated sound, offering a dry, synthetic output, at a high computational cost. We propose a method where ensembles of lightweight neural networks working in parallel are learned, from crafted frequency-domain features of an instrument sound spectra, an arbitrary instrument's voice and articulations realistically and efficiently. We find that our method, while retaining perceptual sound quality on par with sampled approaches, exhibits 1/10 of latency times of industry standard real-time synthesis algorithms, and 1/100 of the disk space requirements of industry standard sample-based digital musical instruments. This method can, therefore, serve as a basis for more efficient implementations in dedicated devices, such as keyboards and electronic drumkits and in general purpose platforms, like desktops and tablets or open-source hardware like Arduino and Raspberry Pi. From a conceptual point of view, this work highlights the advantages of a closer integration of machine learning with other subjects, especially in the endeavor of new product development. Exploiting the synergy between neural networks, digital signal processing techniques and physical modelling, we illustrate the proposed method via the implementation of two virtual instruments: a conventional grand piano and a hibrid stringed instrument.},
year={2019},
title={Neuro-Spectral Audio Synthesis: Exploiting Characteristics of the Discrete Fourier Transform in the Real-Time Simulation of Musical Instruments Using Parallel Neural Networks},
author={Tarjano, C. and Pereira, V.},
}

@article{ISI:000183263200010,
issn={0018-9162},
year={2003},
month={jun},
number={6},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
publisher={IEEE COMPUTER SOC},
title={Agile software development: It's about feedback and change},
author={Williams, L and Cockburn, A},
doi={10.1109/MC.2003.1204373},
type={Editorial Material},
pages={39--43},
volume={36},
journal={COMPUTER},
}

@article{ISI:000268944000023,
type={Article},
doi={10.1051/0004-6361/200810527},
pages={995--1013},
address={17, AVE DU HOGGAR, PA COURTABOEUF, BP 112, F-91944 LES ULIS CEDEX A, FRANCE},
issn={1432-0746},
number={3},
publisher={EDP SCIENCES S A},
year={2009},
title={The AGILE Mission},
journal={ASTRONOMY {\&} ASTROPHYSICS},
keywords={instrumentation: detectors; techniques: high angul},
volume={502},
abstract={Context. AGILE is an Italian Space Agency mission dedicated to observingthe gamma-ray Universe. The AGILE's very innovative instrumentation forthe first time combines a gamma-ray imager (sensitive in the energyrange 30 MeV-50 GeV), a hard X-ray imager (sensitive in the range 18-60keV), a calorimeter (sensitive in the range 350 keV-100 MeV), and ananticoincidence system. AGILE was successfully launched on 2007 April 23from the Indian base of Sriharikota and was inserted in an equatorialorbit with very low particle background.Aims. AGILE provides crucial data for the study of active galacticnuclei, gamma-ray bursts, pulsars, unidentified gamma-ray sources,galactic compact objects, supernova remnants, TeV sources, andfundamental physics by microsecond timing.Methods. An optimal sky angular positioning (reaching 0.1 degrees ingamma- rays and 1-2 arcmin in hard X-rays) and very large fields of view(2.5 sr and 1 sr, respectively) are obtained by the use of Silicondetectors integrated in a very compact instrument.Results. AGILE surveyed the gamma- ray sky and detected many Galacticand extragalactic sources during the first months of observations.Particular emphasis is given to multifrequency observation programs ofextragalactic and galactic objects.Conclusions. AGILE is a successful high-energy gamma-ray mission thatreached its nominal scientific performance. The AGILE Cycle-1 pointingprogram started on 2007 December 1, and is open to the internationalcommunity through a Guest Observer Program.},
month={aug},
author={Tavani, M and Barbiellini, G and Argan, A and Boffelli, F and Bulgarelli, A and Caraveo, P and Cattaneo, P W and Chen, A W and Cocco, V and Costa, E and D'Ammando, F and {Del Monte}, E and {De Paris}, G and {Di Cocco}, G and {Di Persio}, G and Donnarumma, I and Evangelista, Y and Feroci, M and Ferrari, A and Fiorini, M and Fornari, F and Fuschino, F and Froysland, T and Frutti, M and Galli, M and Gianotti, F and Giuliani, A and Labanti, C and Lapshov, I and Lazzarotto, F and Liello, F and Lipari, P and Longo, F and Mattaini, E and Marisaldi, M and Mastropietro, M and Mauri, A and Mauri, F and Mereghetti, S and Morelli, E and Morselli, A and Pacciani, L and Pellizzoni, A and Perotti, F and Piano, G and Picozza, P and Pontoni, C and Porrovecchio, G and Prest, M and Pucella, G and Rapisarda, M and Rappoldi, A and Rossi, E and Rubini, A and Soffitta, P and Traci, A and Trifoglio, M and Trois, A and Vallazza, E and Vercellone, S and Vittorini, V and Zambra, A and Zanello, D and Pittori, C and Preger, B and Santolamazza, P and Verrecchia, F and Giommi, P and Colafrancesco, S and Antonelli, A and Cutini, S and Gasparrini, D and Stellato, S and Fanari, G and Primavera, R and Tamburelli, F and Viola, F and Guarrera, G and Salotti, L and D'Amico, F and Marchetti, E and Crisconio, M and Sabatini, P and Annoni, G and Alia, S and Longoni, A and Sanquerin, R and Battilana, M and Concari, P and Dessimone, E and Grossi, R and Parise, A and Monzani, F and Artina, E and Pavesi, R and Marseguerra, G and Nicolini, L and Scandelli, L and Soli, L and Vettorello, V and Zardetto, E and Bonati, A and Maltecca, L and D'Alba, E and Patane, M and Babini, G and Onorati, F and Acquaroli, L and Angelucci, M and Morelli, B and Agostara, C and Cerone, M and Michetti, A and Tempesta, P and D'Eramo, S and Rocca, F and Giannini, F and Borghi, G and Garavelli, B and Conte, M and Balasini, M and Ferrario, I and Vanotti, M and Collavo, E and Giacomazzo, M},
}

@article{ISI:000238141400003,
abstract={This article explores how agile practices can reduce three kinds of``distance{\{}''{\}} - temporal, geographical, and sociocultural - in globalsoftware development (GSD). On the basis of two in-depth case studies,specific Scrum and eXtreme Programming (XP) practices are found to beuseful for reducing communication, coordination, and control problemsthat have been associated with GSD.},
publisher={AUERBACH PUBLICATIONS},
type={Article},
doi={10.1201/1078.10580530/46108.23.3.20060601/93703.2},
journal={INFORMATION SYSTEMS MANAGEMENT},
year={2006},
volume={23},
pages={7--18},
title={Agile practices reduce distance in global software development},
number={3},
author={Holmstrom, Helena and Fitzgerald, Brian and Agerfalk, Par J and Conchuir, Eoin O},
address={C/O CRC PRESS L L C, 2000 CORPORATE BLVD NW, BOCA RATON, FL 33431 USA},
issn={1058-0530},
}

@article{ISI:000298773100005,
year={2011},
journal={JOURNAL OF PURCHASING AND SUPPLY MANAGEMENT},
number={4},
title={A literature review of decision-making models and approaches for partner selection in agile supply chains},
type={Review},
author={Wu, Chong and Barnes, David},
doi={10.1016/j.pursup.2011.09.002},
month={dec},
issn={1478-4092},
volume={17},
publisher={ELSEVIER SCI LTD},
address={THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
pages={256--274},
keywords={Literature review; Partner selection; Agile supply},
abstract={The paper reviews the literature on supply partner decision-makingpublished between 2001 and 2011, a period that has seen a significantincrease in work published in this field. The progress made indeveloping new models and methods that can be applied to this task isassessed in the context of the previous literature. Particular attentionis given to those methods that are especially relevant for use in agilesupply chains. The paper uses a classification framework that enablesmodels intended for similar purposes to be compared and tracked overtime. It is also used to identify a number of gaps in the literature.The findings highlight an on-going need to develop methods that are ableto meet the combination of qualitative and quantitative objectives thatare typically found in partner selection problems in practice. (C) 2011Elsevier Ltd. All rights reserved.},
}

@article{ISI:000233567300021,
address={1515 BROADWAY, NEW YORK, NY 10036 USA},
author={Augustine, S and Payne, B and Sencindiver, F and Woodcock, S},
journal={COMMUNICATIONS OF THE ACM},
pages={85--89},
title={Agile project management: Steering from the edges},
doi={10.1145/1101779.1101781},
publisher={ASSOC COMPUTING MACHINERY},
type={Article},
month={dec},
year={2005},
issn={0001-0782},
number={12},
volume={48},
}

@article{ISI:000277107200002,
doi={10.1016/j.infsof.2009.11.004},
issn={0950-5849},
year={2010},
volume={52},
publisher={ELSEVIER SCIENCE BV},
author={Moe, Nils Brede and Dingsoyr, Torgeir and Dyba, Tore},
number={5, SI},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
journal={INFORMATION AND SOFTWARE TECHNOLOGY},
keywords={Agile software development; Scrum; Software engine},
month={may},
type={Article},
pages={480--491},
abstract={Context Software development depends significantly on team performance,as does any process that involves human interactionObjective Most current development methods argue that teams shouldself-manage Our objective is thus to provide a better understanding ofthe nature of self-managing agile teams, and the teamwork challengesthat arise when introducing such teamsMethod We conducted extensive fieldwork for 0 months in a softwaredevelopment company that introduced Scrum. We focused on the humansensemaking, on how mechanisms of teamwork were understood by the peopleinvolvedResults We describe a project through Dickinson and McIntyre's teamworkmodel, focusing on the interrelations between essential teamworkcomponents Problems with team orientation, team leadership andcoordination in addition to highly specialized skills and correspondingdivision of work were important barriers for achieving teameffectivenessConclusion Transitioning from individual work to self-managing teamsrequires a reorientation not only by developers but also by managementThis transition takes time and resources, but should not be neglected Inaddition to Dickinson and McIntyre's teamwork components, we found trustand shared mental models to be of fundamental importance (C) 2009Elsevier B V All rights reserved},
title={A teamwork model for understanding an agile team: A case study of a Scrum project},
}

@article{ISI:000084793700002,
abstract={Feature recognition, from low level geometric entities of product designrepresentations within a CAD model to facilitate process planning andmanufacturing activities, has been of significant importance in computerintegrated manufacturing (CIM). However, the emerging paradigm of AgileManufacturing has imposed additional requirements of `'neutralformat{\{}''{\}} so that form-feature information can be readily shared amongmultiple partners of a virtual enterprise. Recently, the STandard forthe Exchange of Product model data (STEP) has emerged as the means forneutral form exchange of product related data. The ``STEP efforts{\{}''{\}}have broken down the domain of manufacturing related activities in theform of application protocols (APs) target for specific functions whichinclude drafting, configuration control and feature-based processplanning to mention a few. Efforts are still on to increase theacceptance and use of this international standard (IS). This paperfocuses on our efforts to support the STEP standard with the developmentof a standards-oriented form-feature extraction system. The developedfeature extraction system takes as a input a STEP file defining thegeometry and topology of a part and generates as output a STEP file withform-feature information in AP224 format for form feature-based processplanning. The system can also be interfaced with a recent IGES to AP202translator {\{}[{\}}M.P. Bhandarkar, B. Downie, M. Hardwick, R. Nagi,Migration from ICES to STEP: one-to-one translation of IGES drawing toSTEP drafting data, accepted by Computers in Industry, July, 1999; M.P.Bhandarkar, Satisfying information needs in Agile Manufacturing throughtranslation and feature extraction into STEP product data models, MSThesis, State University of New York at Buffalo, 1997.] to allowconversion of legacy data. The feature recognition algorithm isboundary-representation (B-Rep) based and follows a sequential approachthrough an existing classification of features. Properties of eachfeature class are exploited to enable their extraction. The algorithm iscurrently developed for prismatic solids produced by milling operationsand that contain elementary shapes such as plane and cylindricalsurfaces (possibly using non-uniform rational B-splines (NURBS)).Special attention has been paid to implementation issues. We demonstratethe efficacy of the system using representative parts. (C) 2000Published by Elsevier Science B.V. All rights reserved.},
issn={0166-3615},
journal={COMPUTERS IN INDUSTRY},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
number={1},
month={jan},
pages={3--24},
volume={41},
publisher={ELSEVIER SCIENCE BV},
keywords={STEP; form feature; feature extraction},
doi={10.1016/S0166-3615(99)00040-8},
author={Bhandarkar, M P and Nagi, R},
title={STEP-based feature extraction from STEP geometry for agile manufacturing},
type={Article},
year={2000},
}

@article{ISI:000225756200019,
author={Lindvall, M and Muthig, D and Dagnino, A and Wallin, C and Stupperick, M and Kiefer, D and May, J and Kahkonen, T},
number={12},
title={Agile software development in large organizations},
doi={10.1109/MC.2004.231},
abstract={Developers need evidence that a new technology works in a certaincontext before they promote and deploy it on a larger scale. This needlooms greater in large organizations because of their complexity and theneed to integrate new technologies and processes with existing ones.To further evaluate agile methods and their underlying softwaredevelopment practices, several Software Experience Center membercompanies initiated a series of activities to discover if agilepractices match their organizations' needs. Based on the experiences ofthese organizations, researchers concluded that agile practices matchthe needs of large organizations, but integrating new practices withexisting processes and quality systems that govern the conduct ofsoftware development requires further tailoring. The challenge here liesnot in applying agile practices to a project, but in efficientlyintegrating the agile project into its environment.},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
issn={0018-9162},
journal={COMPUTER},
publisher={IEEE COMPUTER SOC},
type={Article},
volume={37},
pages={26+},
year={2004},
month={dec},
}

@article{ISI:000309058000012,
issn={1070-9932},
pages={69--79},
volume={19},
month={sep},
doi={10.1109/MRA.2012.2205617},
number={3},
author={Palunko, Ivana and Cruz, Patricio and Fierro, Rafael},
type={Article},
address={445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
journal={IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
year={2012},
title={Agile Load Transportation Safe and Efficient Load Manipulation with Aerial Robots},
}

@article{ISI:000272058100007,
number={4},
year={2010},
author={Pan, Feng and Nagi, Rakesh},
publisher={PERGAMON-ELSEVIER SCIENCE LTD},
month={apr},
abstract={This paper considers a supply chain design problem for a new marketopportunity with uncertain demand in an agile manufacturing setting. Weconsider the integrated optimization of logistics and production costsassociated with the supply chain members. These problems routinely occurin a wide variety of industries including semiconductor manufacturing,multi-tier automotive supply chains, and consumer appliances to name afew. There are two types of decision variables: binary variables forselection of companies to form the supply chain and continuous variablesassociated with production planning. A scenario approach is used tohandle the uncertainty of demand. The formulation is a robustoptimization model with three components in the objective function:expected total costs, cost variability due to demand uncertainty, andexpected penalty for demand unmet at the end of the planning horizon.The increase of computational time with the numbers of echelons andmembers per echelon necessitates a heuristic. A heuristic based on ak-shortest path algorithm is developed by using a surrogate distance todenote the effectiveness of each member in the supply chain. Theheuristic can find an optimal solution very quickly in some small- andmedium-size cases. For large problems, a ``good{\{}''{\}} solution with asmall gap relative to our lower bound is obtained in a shortcomputational time. (C) 2009 Elsevier Ltd. All rights reserved.},
keywords={Supply chain formation; Integrated costs; Uncertai},
address={THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
title={Robust supply chain design under uncertain demand in agile manufacturing},
doi={10.1016/j.cor.2009.06.017},
journal={COMPUTERS {\&} OPERATIONS RESEARCH},
pages={668--683},
issn={0305-0548},
type={Article},
volume={37},
}

@article{ISI:000165846400007,
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month={nov},
abstract={Many enterprises have pursued the lean thinking paradigm to improve theefficiency of their business processes. More recently, the agilemanufacturing paradigm has been highlighted as an alternative to, andpossibly an improvement on, leanness. In pursuing such arguments inisolation, the power of each paradigm may be lost, which is basicallythat agile manufacturing is adopted where demand is volatile, and leanmanufacturing adopted where there is a stable demand. However, in somesituations it is advisable to utilize a different paradigm on eitherside of the material flow de-coupling point to enable a total supplychain strategy. This approach we have termed the Leagile Paradigm. Thispaper therefore considers the effect of the marketplace environment onstrategy selection to ensure optimal supply chain performance.Real-world case studies in the mechanical precision products, carpetmaking, and electronic products market sectors demonstrate the newapproach to matching supply chain design to the actual needs of themarketplace.},
year={2000},
number={17},
author={Mason-Jones, R and Naylor, B and Towill, D R},
title={Lean, agile or leagile? Matching your supply chain to the marketplace},
doi={10.1080/00207540050204920},
volume={38},
issn={0020-7543},
annote={15th International Conference of Production Research(ICPR-15), UNIVLIMERICK, LIMERICK, IRELAND, AUG, 1999},
publisher={TAYLOR {\&} FRANCIS LTD},
address={11 NEW FETTER LANE, LONDON EC4P 4EE, ENGLAND},
type={Article; Proceedings Paper},
pages={4061--4070},
}

@article{ISI:000170758100024,
type={Article},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
year={2001},
author={Highsmith, J and Cockburn, A},
issn={0018-9162},
pages={120--122},
title={Agile software development: The business of innovation},
volume={34},
number={9},
doi={10.1109/2.947100},
journal={COMPUTER},
month={sep},
publisher={IEEE COMPUTER SOC},
}

@article{ISI:000254515800002,
doi={10.1051/0004-6361:200809552},
title={Multifrequency monitoring of the blazar 0716+714 during the GASP-WEBT-AGILE campaign of 2007},
publisher={EDP SCIENCES S A},
volume={481},
number={2},
address={17, AVE DU HOGGAR, PA COURTABOEUF, BP 112, F-91944 LES ULIS CEDEX A, FRANCE},
issn={0004-6361},
author={Villata, M and Raiteri, C M and Larionov, V M and Kurtanidze, O M and Nilsson, K and Aller, M F and Tornikoski, M and Volvach, A and Aller, H D and Arkharov, A A and Bach, U and Beltrame, P and Bhatta, G and Buemi, C S and Boettcher, M and Calcidese, P and Carosati, D and Castro-Tirado, A J and {Da Rio}, D and {Di Paola}, A and Dolci, M and Forne, E and Frasca, A and Hagen-Thorn, V A and Heidt, J and Hiriart, D and Jelinek, M and Kimeridze, G N and Konstantinova, T S and Kopatskaya, E N and Lanteri, L and Leto, P and Ligustri, R and Lindfors, E and Lahteenmaki, A and Marilli, E and Nieppola, E and Nikolashvili, M G and Pasanen, M and Ragozzine, B and Ros, J A and Sigua, L A and Smart, R L and Sorcia, M and Takalo, L O and Tavani, M and Trigilio, C and Turchetti, R and Uckert, K and Umana, G and Vercellone, S and Webb, J R},
pages={L79--L82},
month={apr},
abstract={Aims. Since the CGRO operation in 1991-2000, one of the primaryunresolved questions about the blazar gamma-ray emission has been itspossible correlation with the low-energy (in particular optical)emission. To help answer this problem, the Whole Earth Blazar Telescope(WEBT) consortium has organized the GLAST-AGILE Support Program (GASP)to provide the optical-to-radio monitoring data to be compared with thegamma-ray detections by the AGILE and GLAST satellites. This new WEBTproject started in early September 2007, just before a strong gamma-raydetection of 0716+714 by AGILE.Methods. We present the GASP-WEBT optical and radio light curves of thisblazar obtained in July-November 2007, about various AGILE pointings atthe source. We construct NIR-to-UV spectral energy distributions (SEDs),by assembling GASP-WEBT data together with UV data from the Swift ToOobservations of late October.Results. We observe a contemporaneous optical-radio outburst, which is arare and interesting phenomenon in blazars. The shape of the SEDs duringthe outburst appears peculiarly wavy because of an optical excess and aUV drop- and-rise. The optical light curve is well sampled during theAGILE pointings, showing prominent and sharp flares. A futurecross-correlation analysis of the optical and AGILE data will shed lighton the expected relationship between these flares and the gamma-rayevents.},
journal={ASTRONOMY {\&} ASTROPHYSICS},
keywords={galaxies : active; galaxies : BL Lacertae objects},
type={Article},
year={2008},
}

@article{ISI:000303626300001,
number={6},
month={jun},
title={A decade of agile methodologies: Towards explaining agile software development},
issn={0164-1212},
publisher={ELSEVIER SCIENCE INC},
year={2012},
type={Article},
volume={85},
journal={JOURNAL OF SYSTEMS AND SOFTWARE},
keywords={Agile software development; Theory; Software engin,XP; Scrum; Lean software development; Crystal met},
address={360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
pages={1213--1221},
author={Dingsoyr, Torgeir and Nerur, Sridhar and Balijepally, VenuGopal and Moe, Nils Brede},
doi={10.1016/j.jss.2012.02.033},
abstract={Ever since the agile manifesto was created in 2001, the researchcommunity has devoted a great deal of attention to agile softwaredevelopment. This article examines publications and citations toillustrate how the research on agile has progressed in the 10 yearsfollowing the articulation of the manifesto. Specifically, we delineatethe conceptual structure underlying agile scholarship by performing ananalysis of authors who have made notable contributions to the field.Further, we summarize prior research and introduce contributions in thisspecial issue on agile software development. We conclude by discussingdirections for future research and urging agile researchers to embrace atheory-based approach in their scholarship. (C) 2012 Elsevier Inc. Allrights reserved.},
}

@article{ISI:000223955900002,
volume={36},
year={2004},
abstract={This paper outlines approaches for assessing and classifyingmanufacturing and service operations in terms of their suitability foruse of cross-trained (flexible) workers. We refer to our overallframework as agile workforce evaluation. The primary contributions ofthis paper are: (i) a strategic assessment framework that structures thekey mechanisms by which cross-training can support organizationalstrategy; (ii) a tactical framework that identifies key factors to guidethe selection of an architecture and worker coordination policy forimplementing workforce agility; (iii) a classification of workforceagility architectures; (iv) a survey of a broad range of archetypicalclasses of worker coordination policies; (v) a survey of the literaturewith an operational perspective on workforce agility; and (vi)identification of opportunities for research and development ofarchitectures for specific production environments.},
doi={10.1080/07408170490487759},
number={10},
title={Agile workforce evaluation: a framework for cross-training and coordination},
month={oct},
address={4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND},
issn={0740-817X},
pages={919--940},
type={Review},
journal={IIE TRANSACTIONS},
author={Hopp, W J and {Van Oyen}, M P},
publisher={TAYLOR {\&} FRANCIS LTD},
}

@article{ISI:000167124500008,
publisher={MCB UNIV PRESS LTD},
type={Article},
issn={0144-3577},
title={Measuring agile capabilities in the supply chain},
keywords={agility; supply chain; audit},
author={van Hoek, R I and Harrison, A and Christopher, M},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
pages={126--147},
volume={21},
abstract={Agility is increasingly mentioned as one of the coming challenges to theinternational business world, given volatile markets and increasinglydynamic performance requirements. Existing literature, however, mainlypresents agility as a general management or a strongly manufacturingbiased concept, but does not explicitly relate the concept to the supplychain as a whole. Research also shows a bias towards the USA. This paperpresents an attempt to establish an audit of agility in the supplychain. The audit is used in an empirical investigation of agilecapabilities in Europe. Using existing streams of supply chain researchas building blocks, a preliminary framework is introduced for creatingan agile supply chain. Based on a survey of agile efforts in the UK andthe Benelux the agile capabilities of companies are assessed andapproaches to outscore the benchmark are suggested.},
number={1-2},
address={60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
year={2001},
}

@article{ISI:000256077900014,
address={MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
publisher={NATURE PUBLISHING GROUP},
number={5},
doi={10.1038/nphoton.2008.52},
title={Experimental demonstration of frequency-agile terahertz metamaterials},
pages={295--298},
year={2008},
author={Chen, Hou-Tong and O'Hara, John F and Azad, Abul K and Taylor, Antoinette J and Averitt, Richard D and Shrekenhamer, David B and Padilla, Willie J},
abstract={Metamaterials exhibit numerous novel effects(1-5) and operate over alarge portion of the electromagnetic spectrum(6-10). Metamaterialdevices based on these effects include gradient-index lenses(11,12),modulators for terahertz radiation(13-15) and compact waveguides(16).The resonant nature of metamaterials results in frequency dispersion andnarrow bandwidth operation where the centre frequency is fixed by thegeometry and dimensions of the elements comprising the metamaterialcomposite. The creation of frequency-agile metamaterials would extendthe spectral range over which devices function and, further, enable themanufacture of new devices such as dynamically tunable notch filters.Here, we demonstrate such frequency-agile metamaterials operating in thefar-infrared by incorporating semiconductors in critical regions ofmetallic split-ring resonators. For this first-generation device,external optical control results in tuning of the metamaterial resonancefrequency by similar to 20{\%}. Our approach is integrable with currentsemiconductor technologies and can be implemented in other regions ofthe electromagnetic spectrum.},
issn={1749-4885},
volume={2},
journal={NATURE PHOTONICS},
month={may},
type={Article},
}

@article{ISI:000171519800001,
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
volume={39},
year={2001},
abstract={About a decade ago, the agile manufacturing paradigm was formulated inresponse to the constantly changing `new economy' and as a basis forreturning to global competitiveness. While agility means differentthings to different enterprises under different contexts, the followingelements capture its essential concept: agility is characterized bycooperativeness and synergism (possibly resulting in virtualcorporations), by a strategic vision that enables thriving in face ofcontinuous and unpredictable change, by the responsive creation anddelivery of customer-valued, high quality and mass customizedgoods/services, by nimble organization structures of a knowledgeable andempowered workforce, and facilitated by an information infrastructurethat links constituent partners in a unified electronic network. Duringthis period, a significant amount of attention from both the academicand industrial communities has produced a large body of results inresearch and development related to this topic. Each contribution hastackled a different aspect of this large field. In this paper, we reviewa wide range of recent literature on agile manufacturing. About 73papers from premier scientific journals and conferences have beenreviewed, and a classification scheme to organize these is proposed. Wecritique these bodies of work and suggest directions for additionalresearch and identify topics where fruitful opportunities exist.},
type={Review},
issn={0020-7543},
doi={10.1080/00207540110068790},
address={11 NEW FETTER LANE, LONDON EC4P 4EE, ENGLAND},
month={nov},
author={Sanchez, L M and Nagi, R},
pages={3561--3600},
publisher={TAYLOR {\&} FRANCIS LTD},
number={16},
title={A review of agile manufacturing systems},
}

@article{ISI:000185086300001,
address={HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
year={2003},
abstract={This paper is based on longitudinal case studies of research intostrategy formulation within six plants from large firms - three in thecar industry and three from the computer industry - that have embarkedon mass customisation. The core theme of this paper is that, in spite ofthe increasing attention given to manufacturing strategy from theseminal work of Skinner through to the plethora of articles in recenttimes, little is mentioned about its application to paradigms of agilityor mass customisation. As a consequence firms attempt to become agileand to pursue mass customisation without appreciating the contributionof plant-specific manufacturing strategies that might enable them toachieve these aspirations. We examine the enablers and strategicblockages in pursuing mass customisation, via a mapping process, andreveal reasons why some firms remain unable to devise and implementmanufacturing strategies.},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords={strategic planning; mass customization; agile prod},
pages={707--730},
type={Article},
title={The manufacturing strategy-capabilities links in mass customisation and agile manufacturing - an exploratory study},
number={7-8},
doi={10.1108/01443570310481522},
author={Brown, S and Bessant, J},
issn={0144-3577},
publisher={EMERALD GROUP PUBLISHING LTD},
volume={23},
}

@article{ISI:000183263200013,
volume={36},
year={2003},
doi={10.1109/MC.2003.1204376},
journal={COMPUTER},
author={Boehm, B and Turner, R},
abstract={Both agile and plan-driven approaches have situation-dependentshortcomings that, if not addressed, can lead to project failure. Thechallenge is to balance the two approaches to take advantage of theirstrengths in a given situation while compensating for their weaknesses.The authors present a risk-based approach for structuring projects toincorporate both agile and plan-driven approaches in proportion to aproject's needs.},
number={6},
type={Article},
publisher={IEEE COMPUTER SOC},
issn={0018-9162},
month={jun},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
pages={57+},
title={Using risk to balance agile and plan-driven methods},
}

@article{ISI:000175277300004,
address={4 PARK SQUARE, MILTON PARK,, ABINGDON OX14 4RN, OXON, ENGLAND},
abstract={Agile Manufacturing (AM) is a relatively new operations concept that isintended to improve the competitiveness of firms. Manufacturing/serviceprocesses based on AM are characterized by customer-supplier integratedprocesses for product design, manufacturing, marketing, and supportservices. Agile manufacturing requires enriching of the customer;cooperating with competitors; organizing to manage change, uncertaintyand complexity; and leveraging people and information. In recent years,a number of research papers have been published in the area of AM. Theterm `agile' was coined in 1991. However, there are still some seriousconcerns that prevent companies from taking an entirely differentdirection from AM. Considering the potential importance of agilemanufacturing in 21st century manufacturing competitiveness, an attempthas been made in this paper to re-examine the scope, definitions andstrategies of AM. In addition, a framework has been presented as a basisfor understanding the major strategies and relevant technologies of AM.},
number={6},
author={Gunasekaran, A and Yusuf, Y Y},
doi={10.1080/00207540110118370},
pages={1357--1385},
title={Agile manufacturing: a taxonomy of strategic and technological imperatives},
type={Article},
volume={40},
publisher={TAYLOR {\&} FRANCIS LTD},
year={2002},
month={apr},
issn={0020-7543},
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
}

@article{ISI:000085104500005,
number={1},
publisher={ELSEVIER SCIENCE INC},
doi={10.1016/S0019-8501(99)00110-8},
pages={37--44},
title={The agile supply chain - Competing in volatile markets},
type={Article},
volume={29},
author={Christopher, M},
year={2000},
issn={0019-8501},
abstract={Turbulent and volatile markets are becoming the norm as life cyclesshorten and global economic and competitive forces create additionaluncertainty. The risk attached to lengthy and slow-moving logistics``pipelines{\{}''{\}} has become unsustainable, forcing organizations to lookagain at how their supply chains are structured and managed. This papersuggests that the key to survival in these changed conditions is through``agility,{\{}''{\}} in particular by the creation of responsive supplychains. A distinction is drawn between the philosophies of``leanness{\{}''{\}} and ``agility, `` and the appropriate application ofthese ideas is (C) 2000 Elsevier Science Inc. All rights reserved.},
journal={INDUSTRIAL MARKETING MANAGEMENT},
month={jan},
address={655 AVENUE OF THE AMERICAS, NEW YORK, NY 10010 USA},
}

@article{ISI:000247547600012,
abstract={This paper proposes a small-scale agile wall-climbing robot, which isable to climb on smooth vertical surfaces using flat adhesive elastomermaterials for attachment. Using two actuated legs with rotary motion andtwo passive revolute joints at each foot, this robot can climb and steerin any orientation. Due to its compact design, a high degree ofminiaturization is possible. It has onboard power, computing, andwireless communication, which allow for semiautonomous operation.Various aspects of a functioning prototype design and performance arediscussed in detail, including leg and foot design and gait dynamics. Amodel for the adhesion requirements and performance is developed andverified through experiments. Using an adhesive elastomer (Vytaflex 10),the current prototype can climb 90 slopes at a speed of up to 6 cm/s andsteer to any angle reliably on a,smooth acrylic surface as well astransition from floor walking to wall climbing. This robot is intendedfor inspection and surveillance applications, and ultimately, for spacemissions.},
month={jun},
address={445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
title={Waalbot: An agile small-scale wall-climbing robot utilizing dry elastomer adhesives},
volume={12},
journal={IEEE-ASME TRANSACTIONS ON MECHATRONICS},
author={Murphy, Michael P and Sitti, Metin},
pages={330--338},
issn={1083-4435},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
number={3},
keywords={dry adhesives; mechatronics; miniature robotics; m},
type={Article},
year={2007},
doi={10.1109/TMECH.2007.897277},
}

@article{ISI:000251148000017,
month={nov},
volume={581},
author={Feroci, M and Costa, E and Soffitta, P and {Del Monte}, E and {Di Persio}, G and Donnarumma, I and Evangelista, Y and Frutti, M and Lapshov, I and Lazzarotto, F and Mastropietro, M and Morelli, E and Pacciani, L and Porrovecchio, G and Rapisarda, M and Rubini, A and Tavani, M and Argan, A},
title={SuperAGILE: The hard X-ray imager for the AGILE space mission},
year={2007},
issn={0168-9002},
keywords={high energy astrophysics; X-ray detectors; microst},
doi={10.1016/j.nima.2007.07.147},
journal={NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
type={Article},
publisher={ELSEVIER SCIENCE BV},
number={3},
abstract={SuperAGILE is a coded mask experiment based on silicon microstripdetectors. It operates in the 15-45 keV nominal energy range, providingcrossed one-dimensional images of the X-ray sky with an on-axis angularresolution of 6 arcmin, over a field of view in excess of 1 sr. It wasdesigned as the hard X-ray monitor of the AGILE space mission, a smallsatellite of the Italian Space Agency devoted to image the gamma-ray skyin the 30 MeV-50 GeV energy band. The AGILE mission was launched in alow-earth orbit on 23rd April 2007. In this paper we describe theSuperAGILE experiment, its construction and test processes, and itsperformance before flight, based on the on-ground test and calibrations.(c) 2007 Elsevier B.V. All rights reserved.},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
pages={728--754},
}

@article{ISI:000269983900004,
journal={INFORMATION SYSTEMS RESEARCH},
year={2009},
volume={20},
publisher={INFORMS},
title={A Control Theory Perspective on Agile Methodology Use and Changing User Requirements},
abstract={In this paper, we draw on control theory to understand the conditionsunder which the use of agile practices is most effective in improvingsoftware project quality. Although agile development methodologies offerthe potential of improving software development outcomes, limitedresearch has examined how project managers can structure the softwaredevelopment environment to maximize the benefits of agile methodologyuse during a project. As a result, project managers have little guidanceon how to manage teams who are using agile methodologies. Arguing thatthe most effective control modes are those that provide teams withautonomy in determining the methods for achieving project objectives, wepropose hypotheses related to the interaction between control modes,agile methodology use, and requirements change. We test the model in afield study of 862 software developers in 110 teams. The model explainssubstantial variance in four objective measures of project quality-bugseverity, component complexity, coordinative complexity, and dynamiccomplexity. Results largely support our hypotheses, highlighting theinterplay between project control, agile methodology use, andrequirements change. The findings contribute to extant literature byintegrating control theory into the growing literature on agilemethodology use and by identifying specific contingencies affecting theefficacy of different control modes. We discuss the theoretical andpractical implications of our results.},
issn={1047-7047},
doi={10.1287/isre.1090.0238},
keywords={agile methodologies; agility; control theory; requ},
author={Maruping, Likoebe M and Venkatesh, Viswanath and Agarwal, Ritu},
pages={377--399},
type={Article},
month={sep},
address={5521 RESEARCH PARK DR, SUITE 200, CATONSVILLE, MD 21228 USA},
number={3},
}

@article{ISI:A1996VZ99700005,
issn={1070-9932},
keywords={parallel mechanisms; dynamic control; camera-orien},
title={On the development of the Agile Eye},
type={Article},
year={1996},
author={Gosselin, C M and StPierre, E and Gagne, M},
month={dec},
abstract={The `'Agile Eye'' is a high-performance mechanism capable of orienting acamera within a workspace larger than that of a human eye and withvelocities and accelerations larger than those of the human eye. Themechanical design, control issues, and experimental results arepresented.},
doi={10.1109/100.556480},
number={4},
address={345 E 47TH ST, NEW YORK, NY 10017-2394},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
volume={3},
journal={IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
pages={29--37},
}

@article{ISI:000269428600001,
author={Abrahamsson, Pekka and Conboy, Kieran and Wang, Xiaofeng},
number={4},
publisher={PALGRAVE MACMILLAN LTD},
doi={10.1057/ejis.2009.27},
pages={281--284},
year={2009},
type={Editorial Material},
address={BRUNEL RD BLDG, HOUNDMILLS, BASINGSTOKE RG21 6XS, HANTS, ENGLAND},
month={aug},
issn={0960-085X},
journal={EUROPEAN JOURNAL OF INFORMATION SYSTEMS},
title={`Lots done, more to do': the current state of agile systems development research},
volume={18},
}

@article{ISI:000180587100029,
publisher={SPRINGER-VERLAG},
type={Article},
number={6-7},
address={175 FIFTH AVE, NEW YORK, NY 10010 USA},
month={nov},
abstract={A swept-wavelength source is created by connecting four elements inseries: a femtosecond fiber laser at 1.56 mum, a non-linear fiber, adispersive fiber and a tunable spectral bandpass filter. The 1.56-mumpulses are converted to supercontinuum (1.1-2.2 mum) pulses by thenon-linear fiber, and these broadband pulses are stretched and arrangedinto wavelength scans by the dispersive fiber. The tunable bandpassfilter is used to select a portion of the super-continuum as ascan-wavelength output. A variety of scan characteristics are possibleusing this approach. As an example, an output with an effectivelinewidth of approximately 1 cm(-1) is scanned from 1350-1550 nm every20 us. Compared to previous scanning benchmarks of approximately 1nm/mus, such broad, rapid scans offer new capabilities: a gas sensingapplication is demonstrated by monitoring absorption bands of H2O, CO2,C2H2 and C2H6O at a pressure of 10 bar.},
journal={APPLIED PHYSICS B-LASERS AND OPTICS},
author={Sanders, S T},
pages={799--802},
volume={75},
doi={10.1007/s00340-002-1044-z},
title={Wavelength-agile fiber laser using group-velocity dispersion of pulsed super-continua and application to broadband absorption spectroscopy},
issn={0946-2171},
year={2002},
}

@article{ISI:000235093400003,
keywords={intelligent manufacturing control; holonic manufac},
publisher={ELSEVIER SCIENCE BV},
type={Article},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
volume={57},
author={Leitao, P and Restivo, F},
issn={0166-3615},
doi={10.1016/j.compind.2005.05.005},
journal={COMPUTERS IN INDUSTRY},
month={feb},
abstract={In the last decades significant changes in the manufacturing environmenthave been noticed: moving from a local economy towards a global economy,with markets asking for products with higher quality at lower costs,highly customised and with short life cycle. In these circumstances, thechallenge is to develop manufacturing control systems with intelligencecapabilities, fast adaptation to the environment changes and morerobustness against the occurrence of disturbances. This paper presentsan agile and adaptive manufacturing control architecture that addressesthe need for the fast reaction to disturbances at the shop floor level,increasing the agility and flexibility of the enterprise, when it worksin volatile environments. The proposed architecture introduces anadaptive control that balances dynamically between a more centralisedstructure and a more decentralised one, allowing combining the globalproduction optimisation with agile reaction to unexpected disturbances.(c) 2005 Elsevier B.V. All rights reserved.},
pages={121--130},
year={2006},
number={2},
title={ADACOR: A holonic architecture for agile and adaptive manufacturing control},
}

@article{ISI:000323654900005,
issn={0929-5593},
journal={AUTONOMOUS ROBOTS},
address={VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
year={2013},
volume={35},
author={Kushleyev, Alex and Mellinger, Daniel and Powers, Caitlin and Kumar, Vijay},
pages={287--300},
month={nov},
abstract={We describe a prototype 75 g micro quadrotor with onboard attitudeestimation and control that operates autonomously with an externallocalization system. The motivation for designing quadrotors at thisscale comes from two observations. First, the agility of the robotincreases with a reduction in size, a fact that is supported byexperimental results in this paper. Second, smaller robots are able tooperate in tight formations in constrained, indoor environments. Wedescribe the hardware and software used to operate the vehicle as wellour dynamic model. We also discuss the aerodynamics of vertical flightand the contribution of ground effect to the vehicle performance.Finally, we discuss architecture and algorithms to coordinate a team ofthese quadrotors, and provide experimental results for a team of 20micro quadrotors.},
title={Towards a swarm of agile micro quadrotors},
number={4, SI},
publisher={SPRINGER},
keywords={Micro aerial vehicles; Quadrotors; Trajectory gene},
doi={10.1007/s10514-013-9349-9},
type={Article},
}

@article{ISI:000184714000004,
publisher={ELSEVIER SCIENCE BV},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
annote={16th International Conference on Production Research (ICPR), CZECH TECHUNIV, PRAGUE, CZECH REPUBLIC, AUG, 2001},
month={sep},
volume={85},
issn={0925-5273},
author={Prince, J and Kay, J M},
type={Article; Proceedings Paper},
number={3},
institution={Czech Assoc Sci {\&} Tech Soc},
doi={10.1016/S0925-5273(03)0118-X},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
abstract={The paper presents the background to why some manufacturingorganisations require a combination of agile and lean characteristics intheir manufacturing organisations. The paper also describes thedevelopment of the virtual group (VG) concept, which is the applicationof virtual cells to functional layouts. VGs enable the appropriateapplication of lean and agile concepts to different stages of productionwithin a factory. The identification of VGs is achieved through the useof a methodology called enhanced production flow analysis (EPFA), whichis described together with how it differs from Burbidge's PFA. Finallythe results of two case studies are presented which tested the abilityof EFPA to identify VGs, and assess its usability. (C) 2003 ElsevierB.V. All rights reserved.},
title={Combining lean and agile characteristics: Creation of virtual groups by enhanced production flow analysis},
pages={305--318},
year={2003},
keywords={functional layout; lean; agile; virtual groups},
}

@article{ISI:000255490900012,
year={2008},
doi={10.1016/j.nima.2008.01.023},
annote={1st Roma International Conference on Astroparticle Physics, Rome, ITALY,JUN 20-22, 2007},
month={apr},
number={1-2},
title={The AGILE space mission},
author={Tavani, M and Barbiellini, G and Argan, A and Bulgarelli, A and Caraveo, P and Chen, A and Cocco, V and Costa, E and {De Paris}, G and {Del Monte}, E and {Di Cocco}, G and Donnarumma, I and Feroci, M and Florini, M and Froysland, T and Fuschino, F and Galli, M and Gianotti, F and Giuliani, A and Evangelista, Y and Labanti, C and Lapshov, I and Lazzarotto, F and Lipari, P and Longo, F and Marisaldi, M and Mastropietro, M and Mauri, F and Mereghetti, S and Morelli, E and Morselli, A and Pacciani, L and Pellizzoni, A and Perotti, F and Picozza, P and Pontoni, C and Porrovecchio, G and Prest, M and Pucella, G and Rapisarda, M and Rossi, E and Rubini, A and Soffitta, P and Trifoglio, M and Trois, A and Vallazza, E and Vercellone, S and Zarnbra, A and Zanello, D and Giommi, P and Antonelli, A and Pittori, C},
volume={588},
abstract={AGILE is an Italian Space Agency mission dedicated to the exploration ofthe gamma-ray Universe. The AGILE, very innovative instrument, combinesfor the first time a gamma-ray imager (sensitive in the range 30 MeV-50GeV) and a hard X-ray imager (sensitive in the range 18-60 keV). Anoptimal angular resolution and very large fields of view are obtained bythe use of state-of-the-art Silicon detectors integrated in a verycompact instrument. AGILE was successfully launched on April 23, 2007from the Indian base of Sriharikota and was inserted in an optimallow-particle background equatorial orbit. AGILE will provide crucialdata for the study of Active Galactic Nuclei, Gamma-Ray Bursts,unidentified gamma-ray sources, galactic compact objects, supernovaremnants, TeV sources, and fundamental physics by microsecond timing.The AGILE Cycle-1 pointing program started on 2007 December 1, and isopen to the international community through a Guest Observer Program.(c) 2008 Elsevier B.V. All rights reserved.},
pages={52--62},
publisher={ELSEVIER SCIENCE BV},
type={Article; Proceedings Paper},
issn={0168-9002},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
keywords={astronomical and space-research instrumentation; a},
journal={NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
}

@article{ISI:000080462600004,
title={Agile manufacturing: The drivers, concepts and attributes},
pages={33--43},
publisher={ELSEVIER SCIENCE BV},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
issn={0925-5273},
abstract={Agile manufacturing, a recently popularised concept, has been advocatedas the 21st century manufacturing paradigm. It is seen as the winningstrategy to be adopted by manufacturers bracing. themselves for dramaticperformance enhancements to become national and international leaders inan increasingly competitive market of fast changing customerrequirements. This paper identifies the drivers of agility and discussesthe portfolio of competitive advantages that have emerged over time as aresult of the changing requirements of manufacturing. The need toachieve the competitive advantages of manufacturing in synergy andwithout trade-offs is fundamental to the agile paradigm. To further theunderstanding of agility, this paper reviews the meaning of agility fromdifferent perspectives and suggests a comprehensive definition which canbe adopted as a working definition by practitioners. Four underliningconcepts of agility has emerged from the working definition and thepaper presents a representation of these concepts and theirinteractions. Finally, the paper highlights some of the key enablers ofagility and identifies potential future research directions. (C) 1999Elsevier Science B.V. All rights reserved.},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
year={1999},
month={may},
type={Article},
volume={62},
author={Yusuf, Y Y and Sarhadi, M and Gunasekaran, A},
keywords={agility; drivers; concepts; attributes; enablers},
doi={10.1016/S0925-5273(98)00219-9},
number={1-2},
}

@article{ISI:000241037900009,
title={Can distributed software development be agile?},
pages={41--46},
issn={0001-0782},
doi={10.1145/1164394.1164418},
journal={COMMUNICATIONS OF THE ACM},
number={10},
publisher={ASSOC COMPUTING MACHINERY},
type={Article},
volume={49},
author={Ramesh, Balasubramaniam and Cao, Lan and Mohan, Kannan and Xu, Peng},
month={oct},
address={1515 BROADWAY, NEW YORK, NY 10036 USA},
year={2006},
}

@inproceedings{ISI:000183140900022,
title={New directions on agile methods: A comparative analysis},
publisher={IEEE COMPUTER SOC},
issn={0270-5257},
organization={IEEE; IEEE Comp Soc, Tech Council Software Engn; ACM; ACM SIGSOFT; IBM; NORTHROP GRUMMAN Space Technol; BMW; NOKIA; SUN Microsyst; DaimlerChrysler; Microsoft Res},
year={2003},
doi={10.1109/ICSE.2003.1201204},
abstract={Agile software development methods have caught the attention of softwareengineers and researchers worldwide. Scientific research is yet scarce.This paper reports results from a study, which aims to organize, analyzeand make sense out of the dispersed field of agile software developmentmethods. The comparative analysis is performed using the method'slife-cycle coverage, project management support, type of practicalguidance, fitness-for-use and empirical evidence as the analyticallenses. The results show that agile software development methods,without rationalization, cover certain/different phases of the softwaredevelopment life-cycle and most of them do not offer adequate supportfor project management. Yet, many methods still attempt to strive foruniversal solutions (as opposed to situation appropriate) and theempirical evidence is still very limited. Based on the results, newdirections are suggested In principal, it is suggested to place emphasison methodological quality - not method quantity.},
isbn={0-7695-1877-X},
booktitle={25TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, PROCEEDINGS},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
annote={25th International Conference on Software Engineering (ICSE 2003),PORTLAND, OR, MAY 03-10, 2003},
author={Abrahamsson, P and Warsta, J and Siponen, M T and Ronkainen, J},
type={Proceedings Paper},
pages={244--254},
series={International Conference on Software Engineering},
}

@article{ISI:000251595500020,
issn={0003-0007},
type={Article},
address={45 BEACON ST, BOSTON, MA 02108-3693 USA},
abstract={Weather radars with conventional antenna cannot provide desired volumescan updates at intervals of one minute or less, which is essential forsignificant improvement in warning lead time of impending storm hazards.The agile-beam multimission phased array radar (MPAR) discussed hereinis one potential candidate that can provide faster scanning. It alsooffers a unique potential for multipurpose use to not only sampleweather, but support air traffic needs and track noncooperativeairplanes, thus making it an affordable option. After introducing thebasic idea behind electronic beam steering, the needs for frequentobservations of convective weather are explained. Then, advantages ofthe phased array radar (PAR) for weather monitoring and improving dataquality are examined. To explore and develop weather-relatedapplications of the PAR, a National Weather Radar Testbed (NWRT) hasbeen established in Norman, Oklahoma. The NWRT's main purpose is toaddress the advanced capabilities anticipated within the next decade sothat these could be projected to a possible network of future weatherradars. Examples of data illustrating advantages of this advanced radarare shown, and forthcoming plans are discussed.},
doi={10.1175/BAMS-88-11-1753},
month={nov},
author={Zrnic, D S and Kimpel, J F and Forsyth, D E and Shapiro, A and Crain, G and Ferek, R and Heimmer, J and Benner, W and McNellis, T J and Vogt, R J},
journal={BULLETIN OF THE AMERICAN METEOROLOGICAL SOCIETY},
number={11},
publisher={AMER METEOROLOGICAL SOC},
title={Agile-beam phased array radar for weather observations},
pages={1753+},
year={2007},
volume={88},
}

@article{ISI:000173405800012,
issn={0731-5090},
number={1},
publisher={AMER INST AERONAUT ASTRONAUT},
pages={96--104},
abstract={Study results of developing an attitude control system for agilespacecraft that require rapid retargeting and fast transient settlingare presented. In particular, a nonlinear feedback control logic isdeveloped for large-angle, rapid multitarget acquisition and pointingmaneuvers subject to various physical constraints, including actuatorsaturation, slew rate limit, and control bandwidth limit. The rapidmultitarget acquisition and pointing capability of the proposed attitudecontrol system is demonstrated for an agile spacecraft equipped withredundant single-gimbal control moment gyros. A realistic case ofpointing the line of sight of an imaging satellite in low Earth orbittoward multiple targets on the ground is also briefly discussed.},
doi={10.2514/2.4854},
author={Wie, B and Bailey, D and Heiberg, C},
title={Rapid multitarget acquisition and pointing control of agile spacecraft},
type={Article},
volume={25},
address={1801 ALEXANDER BELL DRIVE, STE 500, RESTON, VA 22091 USA},
journal={JOURNAL OF GUIDANCE CONTROL AND DYNAMICS},
year={2002},
}

@article{ISI:000080462600013,
pages={155--169},
author={Sharp, J M and Irani, Z and Desai, S},
volume={62},
month={may},
issn={0925-5273},
number={1-2},
doi={10.1016/S0925-5273(98)00228-X},
year={1999},
keywords={manufacturing; agility; conceptual model},
type={Article},
title={Working towards agile manufacturing in the UK industry},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
publisher={ELSEVIER SCIENCE BV},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
abstract={The business environment is one which is ever more demanding oncompanies, due to its sheer dynamism, which means that they areconstantly having to improve their manufacturing performance.Organisations are continuously having to cope with changing markets thatare unpredictable and more diversified, increasing global competitionand ever changing customer demands. Companies now have to be able to notonly predict variations and changes within the market and socio-economicand political environments but must also be able to adapt and change inaccordance with these environments. As a result, this demands that anorganisation develops and sustains an inherent ability to continuouslychange. Such a demand can be met by adopting the management philosophyof agile manufacturing. Tn embracing such an approach, there are a lotof key concepts and enabling technologies that are required to be ableto implement agile manufacturing and many companies do not know how fardown the path they are towards becoming agile manufacturingorganisations. Hence, in providing a deeper understanding, this paperproposes a conceptual model, based on joint research, which has beendeveloped to identify where UK's best practice companies are in theirquest to become agile manufacturing organisations. In support of this, aquestionnaire has been developed and completed by best practitioners ofmanufacturing, to assess the model, and establish whether they aremaking progress to becoming agile manufacturing organisations. (C) 1999Elsevier Science B.V. All rights reserved.},
}

@article{ISI:000173128600021,
year={2002},
title={Get ready for agile methods, with care},
doi={10.1109/2.976920},
issn={0018-9162},
number={1},
publisher={IEEE COMPUTER SOC},
abstract={Although many of their advocates consider the agile and plan-drivensoftware development methods polar opposites, synthesizing the two canprovide developers with a comprehensive spectrum of tools and options.},
journal={COMPUTER},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
month={jan},
author={Boehm, B},
pages={64+},
type={Article},
volume={35},
}

@article{ISI:000222980400008,
journal={EUROPEAN JOURNAL OF OPERATIONAL RESEARCH},
pages={379--392},
number={2},
doi={10.1016/j.ejor.2003.08.022},
type={Article},
abstract={Changing customer and technological requirements force manufacturers todevelop agile supply chain capabilities in order to be competitive.Therefore, several companies are stressing flexibility and agility inorder to respond, real time, to the unique needs of customers andmarkets. However, the resource competencies required are often difficultto mobilise and retain by single companies. It is therefore imperativefor companies to co-operate and leverage complementary competencies. Tothis end, legally separate and spatially distributed companies arebecoming integrated through Internet-based technologies. The paperreviews emerging patterns in supply chain integration. It also exploresthe relationship between the emerging patterns and attainment ofcompetitive objectives. The results reported in the paper are based onthe data collected from a survey using the standard questionnaire. Thesurvey involved 600 companies in the UK, as part of a larger study ofagile manufacturing. The study was driven by a conceptual model, whichrelates supply chain practices to competitive objectives. The studyinvolves the use of factor analysis to reduce research variables to afew principal components. Subsequently, multiple regression wasconducted to study the relationship amongst the selected variables. Theresults validate the proposed conceptual model and lend credence tocurrent thinking that supply chain integration is a vital tool forcompetitive advantage. (C) 2003 Elsevier B.V. All rights reserved.},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
month={dec},
publisher={ELSEVIER SCIENCE BV},
issn={0377-2217},
keywords={agile manufacturing; agile supply chains; enterpri},
year={2004},
author={Yusuf, Y Y and Gunasekaran, A and Adeleye, E O and Sivayoganathan, K},
title={Agile supply chain capabilities: Determinants of competitive objectives},
volume={159},
}

@article{ISI:000245954500003,
number={6},
issn={0098-5589},
volume={33},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
year={2007},
journal={IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
author={Olague, Hector M and Etzkorn, Letha H and Gholston, Sampson and Quattlebaum, Stephen},
pages={402--419},
doi={10.1109/TSE.2007.1015},
type={Article},
publisher={IEEE COMPUTER SOC},
month={jun},
keywords={object-oriented software metrics; object-oriented},
abstract={Empirical validation of software metrics suites to predict faultproneness in object-oriented (OO) components is essential to ensuretheir practical use in industrial settings. In this paper, weempirically validate three OO metrics suites for their ability topredict software quality in terms of fault-proneness: the Chidamber andKemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design(QMOOD). Some CK class metrics have previously been shown to be goodpredictors of initial OO software quality. However, the other two suiteshave not been heavily validated except by their original proposers.Here, we explore the ability of these three metrics suites to predictfault-prone classes using defect data for six versions of Rhino, anopen-source implementation of JavaScript written in Java. We concludethat the CK and QMOOD suites contain similar components and producestatistical models that are effective in detecting error-prone classes.We also conclude that the class components in the MOOD metrics suite arenot good class fault-proneness predictors. Analyzing multivariate binarylogistic regression models across six Rhino versions indicates thesemodels may be useful in assessing quality in OO classes produced usingmodern highly iterative or agile software development processes.},
title={Empirical validation of three software metrics suites to predict fault-proneness of object-oriented classes developed using highly iterative or agile software development processes},
}

@article{ISI:000173405800014,
type={Article},
number={1},
journal={JOURNAL OF GUIDANCE CONTROL AND DYNAMICS},
author={Frazzoli, E and Dahleh, M A and Feron, E},
doi={10.2514/2.4856},
year={2002},
pages={116--129},
abstract={Planning the path of an autonomous, agile vehicle in a dynamicenvironment is a very complex problem, especially when the vehicle isrequired to use its full maneuvering capabilities. Recent efforts aimedat using randomized algorithms for planning the path of kinematic anddynamic vehicles have demonstrated considerable potential forimplementation on future autonomous platforms. This paper builds uponthese efforts by proposing a randomized path planning architecture fordynamical systems in the presence of fixed and moving obstacles. Thisarchitecture addresses the dynamic constraints on the vehicle's motion,and it provides at the same time a consistent decoupling betweenlow-level control and motion planning. The path planning algorithmretains the convergence properties of its kinematic counterparts. Systemsafety is also addressed in the face of finite computation times byanalyzing the behavior of the algorithm when the available onboardcomputation resources are limited, and the planning must be performed inreal time. The proposed algorithm can be applied to vehicles whosedynamics are described either by ordinary differential equations or byhigher-level, hybrid representations. Simulation examples involving aground robot and a small autonomous helicopter are presented anddiscussed.},
volume={25},
address={1801 ALEXANDER BELL DRIVE, STE 500, RESTON, VA 22091-4344 USA},
publisher={AMER INST AERONAUT ASTRONAUT},
issn={0731-5090},
title={Real-time motion planning for agile autonomous vehicles},
}

@article{ISI:000243915300010,
abstract={This paper reports an agile VCO frequency calibration technique and itsapplication on a 10-GHz CMOS integer-N phase-locked loop. The proposedcalibration method accomplishes efficient search for an optimum VCOdiscrete tuning curve among a group of frequency sub-bands. The agilityis attributed to a proposed frequency comparison technique which isbased on measuring the period difference between two signals. Othermixed-signal circuits are also developed to facilitate this approach.The PLL incorporating the proposed calibration technique is implementedin a 0.18-mu m CMOS process. The measured PLL phase noise at 10 GHz is-102 dBc/Hz at 1-MHz offset frequency and the reference spurs are lowerthan -48 dBc. The PLL consumes 44 mW in the low-current mode. Thecalibration time is less than 4 mu s.},
pages={340--349},
doi={10.1109/JSSC.2006.889360},
title={An agile VCO frequency-calibration technique for a 10-GHz CMOS PLL},
address={445 HOES LANE, PISCATAWAY, NJ 08855 USA},
year={2007},
month={feb},
number={2},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
type={Article},
journal={IEEE JOURNAL OF SOLID-STATE CIRCUITS},
author={Lin, Tsung-Hsien and Lai, Yu-Jen},
issn={0018-9200},
volume={42},
keywords={calibration; CMOS integrated circuits; frequency s},
}

@article{ISI:000182753200046,
author={Prest, M and Barbiellini, G and Bordignon, G and Fedel, G and Liello, F and Longo, F and Pontoni, C and Vallazza, E},
doi={10.1016/S0168-9002(02)02047-8},
journal={NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
pages={280--287},
abstract={AGILE (Light Imager for Gamma-ray Astrophysics) is the first smallscientific mission of ASI, the Italian Space Agency. It is a light (100kg for the scientific instrument) satellite for the detection ofgamma-ray sources in the energy range 30 MeV-50 GeV within a large fieldof view (1 of the sky). It is planned to be operational in the years2003-2006, a period in which no other gamma-ray mission in the sameenergy range is foreseen.AGILE is made of a silicon tungsten tracker, a CsI(Tl) minicalorimeter(1.5X(0)), an anticoincidence system of segmented plastic scintillatorsand a X-ray imaging detector sensitive in the 10-40 keV range. Thetracker consists of 14 planes, each of them made of two layers of 16single-sided, AC coupled, 410 mum thick, 9.5 x 9.5 cm(2) silicondetectors with a readout pitch of 242 mum and a floating strip. Thereadout ASIC is the TAA1, an analog-digital, low noise, self-triggeringASIC used in a very low power configuration ({\textless}400 {\&}mu;W/channel) withfull analog readout. The trigger of the satellite is given by thetracker. The total number of readout channels is around 43 000.We present a detailed description of the tracker, its trigger andreadout logic, its assembly procedures and the prototype performance inseveral testbeam periods at the CERN PS. (C) 2002 Elsevier Science B.V.All rights reserved.},
volume={501},
month={mar},
title={The AGILE silicon tracker: an innovative gamma-ray instrument for space},
keywords={satellite; silicon detector; self-triggering; floa},
type={Article; Proceedings Paper},
annote={10th International Workshop on Vertex Detectors, BRUNNEN, SWITZERLAND,SEP 23-28, 2001},
number={1},
issn={0168-9002},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
publisher={ELSEVIER SCIENCE BV},
year={2003},
}

@article{ISI:000283606800005,
author={Luo, Xinxing and Wu, Chong and Rosenberg, Duska and Barnes, David},
journal={JOURNAL OF PURCHASING AND SUPPLY MANAGEMENT},
keywords={Supplier selection; Agile supply chain; Artificial},
issn={1478-4092},
title={Supplier selection in agile supply chains: An information-processing model and an illustration},
number={4},
year={2009},
pages={249--262},
address={THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
publisher={ELSEVIER SCI LTD},
doi={10.1016/j.pursup.2009.05.004},
month={dec},
volume={15},
abstract={Agile supply chains need to be highly flexible in order to reconfigurequickly in response to changes in their environment. An effectivesupplier selection process is essential for this. This paper develops amodel that helps overcome the information-processing difficultiesinherent in screening a large number of potential suppliers in the earlystages of the selection process. Based on radial basis functionartificial neural network (RBF-ANN), the model enables potentialsuppliers to be assessed against multiple criteria using bothquantitative and qualitative measures. Its efficacy is illustrated usingempirical data from the Chinese electrical appliance and equipmentmanufacturing industries. (C) 2009 Elsevier Ltd. All rights reserved.},
type={Article},
}

@article{ISI:000367200400009,
number={1},
publisher={NATURE PUBLISHING GROUP},
type={Article},
volume={10},
journal={NATURE PHOTONICS},
month={jan},
author={Millot, Guy and Pitois, Stephane and Yan, Ming and Hovhannisyan, Tatevik and Bendahmane, Abdelkrim and Haensch, Theodor W and Picque, Nathalie},
year={2016},
address={MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
pages={27--U37},
title={Frequency-agile dual-comb spectroscopy},
doi={10.1038/NPHOTON.2015.250},
abstract={Spectroscopic gas sensing and its applications to, for example, tracedetection or chemical kinetics, require ever more demanding measurementtimes, acquisition rates, sensitivities, precisions and broad tuningranges. Here, we propose a new approach to near-infrared molecularspectroscopy, utilizing advanced concepts of optical telecommunicationsand supercontinuum photonics. We generate, without mode-locked lasers,two frequency combs of slightly different repetition frequencies andmoderate, but rapidly tunable, spectral span. The output of afrequency-agile continuous-wave laser is split and sent into twoelectro-optic intensity modulators. Flat-top low-noise frequency combsare produced by wave-breaking in a nonlinear optical fibre of normaldispersion. With a dual-comb spectrometer, we record Doppler-limitedspectra spanning 60 GHz within 13 mu s and an 80 kHz refresh rate, at atuning speed of 10 nm s(-1). The sensitivity for weak absorption isenhanced by a long gas-filled hollow-core fibre. New opportunities forreal-time diagnostics may be opened up, even outside the laboratory.},
issn={1749-4885},
}

@article{ISI:000077216500002,
type={Article},
issn={0008-1256},
address={GRAD SCH BUSINESS ADMIN, BERKELEY, CA 94720 USA},
title={Agile product development: Managing development flexibility in uncertain environments},
volume={41},
year={1998},
author={Thomke, S and Reinertsen, D},
abstract={As product complexity and the rate of market change have dramaticallyincreased over the last years, firms find it increasingly difficult toforecast product requirements in their development processes. Thisarticle redefines the problem from one of improving forecasting to oneof increasing product development agility and thus reducing the need foraccurate long-term forecasts. It introduces the notion of developmentflexibility, shows how it can be measured, and presents results from alarge empirical study on integrated systems development, which foundthat projects using flexible technologies outperformed projects usinginflexible technologies by a factor of 2.2 (in person-months). Finally,the article proposes three major strategies for introducing flexibilityinto organizations. These strategies can help firms increase theiragility and position themselves to succeed in accelerating and moreturbulent markets.},
doi={10.2307/41165973},
number={1},
pages={8+},
publisher={UNIV CALIF},
journal={CALIFORNIA MANAGEMENT REVIEW},
}

@article{ISI:000256966600004,
type={Article},
month={jun},
abstract={Agile software development practices such as eXtreme Programming (XP)and SCRUM have increasingly been adopted to respond to the challenges ofvolatile business environments, where the markets and technologiesevolve rapidly and present the unexpected. In spite of the encouragingresults so far, little is known about how agile practices affectcommunication. This article presents the results from a study whichexamined the impact of XP and SCRUM practices on communication withinsoftware development teams and within the focal organization. Theresearch was carried out as a case study in F-Secure where two agilesoftware development projects were compared from the communicationperspective. The goal of the study is to increase the understanding ofcommunication in the context of agile software development: internallyamong the developers and project leaders and in the interface betweenthe development team and stakeholders (i.e. customers, testers, otherdevelopment teams). The study shows that agile practices improve bothinformal and formal communication. However, it further indicates that,in larger development situations involving multiple externalstakeholders, a mismatch of adequate communication mechanisms cansometimes even hinder the communication. The study highlights the factthat hurdles and improvements in the communication process can bothaffect the feature requirements and task subtask dependencies asdescribed in coordination theory. While the use of SCRUM and some XPpractices facilitate team and organizational communication of thedependencies between product features and working tasks, the use ofagile practices requires that the team and organization use alsoadditional plan-driven practices to ensure the efficiency of externalcommunication between all the actors of software development.},
title={The impact of agile practices on communication in software development},
year={2008},
doi={10.1007/s10664-008-9065-9},
address={VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
number={3},
journal={EMPIRICAL SOFTWARE ENGINEERING},
issn={1382-3256},
pages={303--337},
keywords={agile software development practices; communicatio},
publisher={SPRINGER},
author={Pikkarainen, M and Haikara, J and Salo, O and Abrahamsson, P and Still, J},
volume={13},
}

@article{ISI:000245650200061,
author={Prieto, Carlos and Rivas, Javier De Las},
address={GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND},
volume={34},
year={2006},
abstract={Agile Protein Interaction DataAnalyzer (APID) is an interactivebioinformatics web tool developed to integrate and analyze in a unifiedand comparative platform main currently known information aboutprotein-protein interactions demonstrated by specific small-scale orlarge-scale experimental methods. At present, the application includesinformation coming from five main source databases enclosing an unifiedsever to explore {\textgreater} 35 000 different proteins and 111 000 differentproven interactions. The web includes search tools to query and browseupon the data, allowing selection of the interaction pairs based incalculated parameters that weight and qualify the reliability of eachgiven protein interaction. Such parameters are for the `proteins':connectivity, cluster coefficient, Gene Ontology ( GO) functionalenvironment, GO environment enrichment; and for the `interactions':number of methods, GO overlapping, iPfam domain-domain interaction. APIDalso includes a graphic interactive tool to visualize selectedsub-networks and to navigate on them or along the whole interactionnetwork. The application is available open access athttp://bioinfow.dep.usal.es/apid/.},
doi={10.1093/nar/gkl128},
issn={0305-1048},
pages={W298--W302},
type={Article},
publisher={OXFORD UNIV PRESS},
month={jul},
number={SI},
journal={NUCLEIC ACIDS RESEARCH},
title={APID: Agile Protein Interaction DataAnalyzer},
}

@article{ISI:000229359800020,
issn={0001-0782},
publisher={ASSOC COMPUTING MACHINERY},
address={1515 BROADWAY, NEW YORK, NY 10036 USA},
type={Article},
author={Nerur, S and Mahapatra, R and Mangalaraj, G},
volume={48},
pages={72--78},
number={5},
title={Challenges of emigrating to agile methodologies},
journal={COMMUNICATIONS OF THE ACM},
doi={10.1145/1060710.1060712},
month={may},
year={2005},
}

@article{ISI:000071490100010,
author={Duguay, C R and Landry, S and Pasin, F},
abstract={In industrial management, the 1980s marked the end of the twentiethcentury, an epoch dominated by US manufacturers, the alleged masters ofmass production. This system has now been outstripped in several dynamicsectors by flexible/agile production. Increases in the pace oftechnological progress, training and aspirations have made the moderncontext so dynamic that firms which manage to harness the creativity andinitiative of a good part of their workforce have an advantage overthose that can only count on the input of their experts and managers. Insectors undergoing relatively broad and rapid change, twenty-firstcentury firms must adopt a more flexible and innovative type oforganization to achieve manufacturing excellence.},
keywords={agile production; flexible manufacturing systems;},
publisher={EMERALD GROUP PUBLISHING LIMITED},
volume={17},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
number={11-12},
pages={1183+},
type={Article},
doi={10.1108/01443579710182936},
issn={0144-3577},
title={From mass production to flexible/agile production},
year={1997},
address={HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
}

@article{ISI:000235785800021,
volume={44},
keywords={microwave radiometry; radio spectrum management},
type={Article},
issn={0196-2892},
address={445 HOES LANE, PISCATAWAY, NJ 08855 USA},
month={mar},
journal={IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
number={3},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title={RFI detection and mitigation for microwave radiometry with an agile digital detector},
year={2006},
abstract={A new type of microwave radiometer detector has been developed that iscapable of identifying high and low levels of radio-frequencyinterference (RFI) and of reducing or eliminating its effect on themeasured brightness temperatures. High-level, localized RFI can beeasily identified by its unnatural appearance in brightness temperatureimagery. Low-level or persistent RFI can be much more difficult toidentify and filter out. The agile digital detector (ADD) candiscriminate between RFI and natural thermal emission signals bydirectly measuring higher order moments of the signal than the variancethat is traditionally measured. After detection, the ADD then usesspectral filtering methods to selectively remove the RFI. ADDperformance is experimentally verified in controlled laboratory testsand in the field near a commercial air traffic control radar. High-levelRFI is easily identified and removed. Very low level RFI contamination,with power levels as low as the radiometric measurement uncertainty ofthe radiometer, is also shown to be reliably detected and removed.},
author={Ruf, C S and Gross, S M and Misra, S},
pages={694--706},
doi={10.1109/TGRS.2005.861411},
}

@article{ISI:000080462600009,
publisher={ELSEVIER SCIENCE BV},
type={Article},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
pages={107--118},
year={1999},
volume={62},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
number={1-2},
author={Naylor, J B and Naim, M M and Berry, D},
issn={0925-5273},
abstract={As the lean thinking and agile manufacturing paradigms have beendeveloped there has been a tendency to view them in a progression and inisolation. This article shows that this is too simplistic a view. Theuse of either paradigm has to be combined with a total supply chainstrategy particularly considering market knowledge and positioning ofthe decoupling point as agile manufacturing is best suited to satisfyinga fluctuating demand and lean manufacturing requires a level schedule.This view is supported by consideration of a PC supply chain case study.(C) 1999 Elsevier Science B.V. All rights reserved.},
month={may},
title={Leagility: Integrating the lean and agile manufacturing paradigms in the total supply chain},
keywords={agile manufacturing; lean thinking; supply chain m},
}

@article{ISI:000231943500006,
volume={16},
year={2005},
author={Erickson, J and Lyytinen, K and Siau, K},
publisher={IGI PUBL},
title={Agile modeling, agile software development, and extreme programming: The state of research},
number={4},
journal={JOURNAL OF DATABASE MANAGEMENT},
address={701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA},
keywords={agile software development; agility; agile modelin},
abstract={While there are many claims for the successful use of extremeprogramming (XP) and agile modeling (AM), and the proponents can oftenbe vocal in the extreme regarding their supposed benefits, researchevidence supporting proponents' claims is somewhat lacking. Currently,the only research appearing to investigate the phenomena consists of twoprominent streams. A small number of case studies and experience reportsthat generally promote the success of XP in various developmentenvironments, and a well-established stream of research into pairprogramming has generated results that in part support the idea of XPResearch into AM appears to be even more sparse than that for XP Casestudies, comparative analyses, and experience reports comprise themajority of the research in the area, while very few empirical researchefforts have been conducted. This article reviews the state of researchin XP and AM, and recommends areas that could benefit from furtherstudy. Since nearly all empirical XP research relates to pairprogramming, a closer look into the unstudied XP core practices would bebeneficial, although interaction between related core practice areascould confound such efforts. It might also be possible to group relatedcore XP concepts and study the groups individually. Finally, there arethose who claim that XP and AM, or even agility in general, are reallynothing more than a repackaging of old concepts. This claim needs to beinvestigated.},
doi={10.4018/jdm.2005100105},
pages={88--100},
type={Review},
issn={1063-8016},
}

@article{ISI:000255824100022,
abstract={Background: Capsule endoscopy (CE) of the small bowel has become astandard diagnostic tool, but there have been concerns regarding therisk of capsule retention in certain high-risk groups. The Agile patencysystem, an ingestible and dissolvable capsule with an external scanner,was developed to allow physicians to perform CE with greater confidencethat the capsule will be safely excreted in patients at risk for capsuleretention.Objective: Our purpose was to assess the ability of the device to helpphysicians identify which patients with known strictures may safelyundergo CE.Design: Patients with known strictures ingested the new patency capsuleand under-went periodic scanning until it was excreted. The intestinaltract was considered to be sufficiently patent if the capsule wasexcreted intact or if the capsule was not detected by the scanner at 30hours after ingestion. if patency was established, then standard CE wasperformed.Setting: International multicenter study.Patients: A total of 106 patients with known strictures.Intervention: Agile patency system.Main Outcome Measurements: Performance and safety of Agile patencysystem.Results: A total of 106 patients ingested the patency capsule.Fifty-nine (56{\%}) excreted it intact and subsequently underwent CE.There were no cases of capsule retention. Significant findings on CEwere found in 24 (41{\%}). There were 3 severe adverse events.Conclusions: These results suggest that the Agile patency system is auseful tool for physicians to use before CE in patients with stricturesto avoid retention. This group of patients may have a high yield ofclinically significant findings at CE. This capsule may determinewhether patients who have a contraindication to CE may safely undergo CEand obtain useful diagnostic information.},
author={Herrerias, Juan M and Leighton, Jonathan A and Costamagna, Guido and Infantolino, Anthony and Eliakim, Rami and Fischer, Doron and Rubin, David T and Manten, Howard D and Scapa, Eitan and Morgan, Douglas R and Bergwerk, Ari J and Koslowsky, Binyamin and Adler, Samuel N},
number={6},
volume={67},
address={360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA},
pages={902--909},
title={Agile patency system eliminates risk of capsule retention in patients with known intestinal strictures who undergo capsule endoscopy},
month={may},
issn={0016-5107},
publisher={MOSBY-ELSEVIER},
journal={GASTROINTESTINAL ENDOSCOPY},
type={Article; Proceedings Paper},
doi={10.1016/j.gie.2007.10.063},
year={2008},
annote={3rd International Symposium on Natural Orifice Translumenal EndoscopicSurgery, San Francisco, CA, JUL 10, 2008},
}

@article{ISI:000073433700004,
author={Gunasekaran, A},
month={may},
address={ONE GUNPOWDER SQUARE, LONDON EC4A 3DE, ENGLAND},
abstract={Tougher competitive situations have led to increasing attention beingpaid to customer satisfaction, of which timely and customized servicesare the key concepts. As the product life cycle becomes shortened, highproduct quality becomes necessary for survival. Markets become highlydiversified and global, and continuous and unexpected change become thekey factors for success. The need for a method of rapidly andcost-effectively developing products, production facilities andsupporting software, including design, process planning and shop floorcontrol system has led to the concept of agile manufacturing.Agile manufacturing can be defined as the capability to survive andprosper in a competitive environment of continuous and unpredictablechange by reacting quickly and effectively to changing markets, drivenby customer-designed products and services. This article details the keyconcepts and enablers of agile manufacturing. The key enablers of agilemanufacturing include: (i) virtual enterprise formation tools/metrics;(ii) physically distributed manufacturing architecture and teams; (iii)rapid partnership formation tools/metrics; (iv) concurrent engineering;(v) integrated product/production/business information system; (vi)rapid prototyping tools; and (vii) electronic commerce. A conceptualframework for the development of an agile manufacturing system andfuture research directions are presented in this paper. This frameworktakes into account the customization and system integration with thehelp of business process redesign, legal issues, concurrent engineering,computer-integrated manufacturing, cost management, total qualitymanagement and information technology.},
doi={10.1080/002075498193291},
number={5},
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
publisher={TAYLOR {\&} FRANCIS LTD},
issn={0020-7543},
title={Agile manufacturing: enablers and an implementation framework},
pages={1223--1247},
type={Article},
volume={36},
year={1998},
}

@article{ISI:000238914800019,
issn={1094-4087},
number={14},
publisher={OPTICAL SOC AMER},
year={2006},
abstract={A novel basis for beam steering with electrowetting microprisms (EMPs)is reported. EMPs utilize electrowetting modulation of liquid contactangle in order to mimic the refractive behavior for various classicalprism geometries. Continuous beam steering through an angle of 14degrees (+/- 7 degrees) has been demonstrated with a liquid index ofn=1.359. Experimental results are well-matched to theoretical behaviorup to the point of electrowetting contact-angle saturation. Projectionsshow that use of higher index liquids (n similar to 1.6) will result insteering through similar to 30 degrees(+/- 15 degrees). Fundamentalfactors defining achievable deflection range, and issues for Ladar use,are reviewed. This approach is capable of good switching speed (similarto ms), polarization independent operation, modulation of beamfield-of-view (lensing), and high steering efficiency that isindependent of deflection angle. (c) 2006 Optical Society of America.},
journal={OPTICS EXPRESS},
month={jul},
address={2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036 USA},
author={Smith, Neil R and Abeysinghe, Don C and Haus, Joseph W and Heikenfeld, Jason},
doi={10.1364/OE.14.006557},
pages={6557--6563},
title={Agile wide-angle beam steering with electrowetting microprisms},
type={Article},
volume={14},
}

@article{ISI:000228020200008,
title={Susceptibility of Italian agile frog populations to an emerging strain of Ranavirus parallels population genetic diversity},
address={9600 GARSINGTON RD, OXFORD OX4 2DG, OXON, ENGLAND},
doi={10.1111/j.1461-0248.2005.00735.x},
journal={ECOLOGY LETTERS},
abstract={Western populations of the Italian agile frog (Rana latastei) experiencewidespread genetic depletion. Based on population genetic theory,molecular models of immunity and previous empirical studies, populationgenetic depletion predicts increased susceptibility of populations toemergent pathogens. We experimentally compared susceptibility of R.latastei populations upon exposure to an emerging strain of Ranavirus,frog virus 3 (FV3), using six populations spanning the geographicalrange and range of population genetic diversity found in nature. Ourfindings confirm this prediction, suggesting that the loss of geneticdiversity accompanying range expansion and population isolation iscoincident with increased mortality risk from an emergent pathogen. Lossof heterozygosity and escape from selection imposed by immunologicallycross-reactive pathogens may potentially generate range-wide variationin disease resistance.},
keywords={amphibian declines; disease emergence; frog virus},
year={2005},
volume={8},
type={Article},
number={4},
issn={1461-023X},
pages={401--408},
month={apr},
publisher={BLACKWELL PUBLISHING LTD},
author={Pearman, P B and Garner, T W J},
}

@article{ISI:000308110200009,
type={Article},
pages={4830--4845},
address={4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND},
year={2012},
abstract={In modern business environments, an effective supply chain management(SCM) is crucial to business continuity. Competition between supplychains (SC) has replaced the traditional competition between companies.Lean, Agile, Resilient and Green (LARG) paradigms are advocated as thefoundation of a competitive SCM. To make a supply chain morecompetitive, capable of responding to the demands of customers withagility and capable of responding effectively to unexpected disturbance,in conjugation with environmental responsibilities and the necessity toeliminate processes that add no value, companies must implement a set ofLARG SCM practices and key performance indicators (KPI) to measure theirinfluence on the SC performance. However, the selection of the best LARGSCM practices and KPIs is a complex problem, involving dependencies andfeedbacks. This paper proposes an integrated LARG analytic networkprocess (ANP) model to support decision-making in choosing the mostappropriate practices and KPIs to be implemented by companies in an SC.To validate the model in an exploratory approach, a case study in anautomaker supply chain is presented.},
keywords={lean; agile; resilient; green; SCM; ANP},
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
issn={0020-7543},
number={17, SI},
volume={50},
author={Cabral, Izunildo and Grilo, Antonio and Cruz-Machado, Virgilio},
doi={10.1080/00207543.2012.657970},
publisher={TAYLOR {\&} FRANCIS LTD},
title={A decision-making model for Lean, Agile, Resilient and Green supply chain management},
}

@article{ISI:000275321000002,
doi={10.1029/2009JA014502},
author={Marisaldi, M and Fuschino, F and Labanti, C and Galli, M and Longo, F and {Del Monte}, E and Barbiellini, G and Tavani, M and Giuliani, A and Moretti, E and Vercellone, S and Costa, E and Cutini, S and Donnarumma, I and Evangelista, Y and Feroci, M and Lapshov, I and Lazzarotto, F and Lipari, P and Mereghetti, S and Pacciani, L and Rapisarda, M and Soffitta, P and Trifoglio, M and Argan, A and Boffelli, F and Bulgarelli, A and Caraveo, P and Cattaneo, P W and Chen, A and Cocco, V and D'Ammando, F and {De Paris}, G and {Di Cocco}, G and {Di Persio}, G and Ferrari, A and Fiorini, M and Froysland, T and Gianotti, F and Morselli, A and Pellizzoni, A and Perotti, F and Picozza, P and Piano, G and Pilia, M and Prest, M and Pucella, G and Rappoldi, A and Rubini, A and Sabatini, S and Striani, E and Trois, A and Vallazza, E and Vittorini, V and Zambra, A and Zanello, D and Antonelli, L A and Colafrancesco, S and Gasparrini, D and Giommi, P and Pittori, C and Preger, B and Santolamazza, P and Verrecchia, F and Salotti, L},
month={mar},
title={Detection of terrestrial gamma ray flashes up to 40 MeV by the AGILE satellite},
issn={0148-0227},
journal={JOURNAL OF GEOPHYSICAL RESEARCH-SPACE PHYSICS},
type={Article},
volume={115},
publisher={AMER GEOPHYSICAL UNION},
address={2000 FLORIDA AVE NW, WASHINGTON, DC 20009 USA},
abstract={We report the detection by the Astrorivelatore Gamma a Immagini Leggero(AGILE) satellite of terrestrial gamma ray flashes (TGFs) obtained withthe minicalorimeter (MCAL) detector operating in the energy range0.3-100 MeV. We select events typically lasting a few milliseconds withspectral and directional selections consistent with the TGFcharacteristics previously reported by other space missions. During theperiod 1 June 2008 to 31 March 2009 we detect 34 high-confidence eventsshowing millisecond durations and a geographical distribution peakedover continental Africa and Southeast Asia. For the first time,AGILE-MCAL detects photons associated with TGF events up to 40 MeV. Wedetermine the cumulative spectral properties of the spectrum in therange 0.5-40 MeV, which can be effectively described by a Bremsstrahlungspectrum. We find that both the TGF cumulative spectral properties andtheir geographical distribution are in good agreement with the ReuvenRamaty High Energy Solar Spectroscopic Imager (RHESSI) results.},
year={2010},
}

@article{ISI:000271681800011,
pages={1869--1890},
year={2009},
address={360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
abstract={Agile software development (ASD) is an emerging approach in softwareengineering, initially advocated by a group of 17 software professionalswho practice a set of ``lightweight{\{}''{\}} methods, and share a common setof values of software development. In this paper, we advance thestate-of-the-art of the research in this area by conducting asurvey-based ex-post-facto study for identifying factors from theperspective of the ASD practitioners that will influence the success ofprojects that adopt ASD practices. In this paper, we describe ahypothetical success factors framework we developed to address ourresearch question, the hypotheses we conjectured, the researchmethodology, the data analysis techniques we used to validate thehypotheses, and the results we obtained from data analysis. The studywas conducted using an unprecedentedly large-scale survey-basedmethodology, consisting of respondents who practice ASD and who hadexperience practicing plan-driven software development in the past. Thestudy indicates that nine of the 14 hypothesized factors havestatistically significant relationship with ``Success{\{}''{\}}. The importantsuccess factors that were found are: customer satisfaction, customercollaboration, customer commitment, decision time, corporate culture,control, personal characteristics, societal culture, and training andlearning. (C) 2009 Elsevier Inc. All rights reserved.},
title={Identifying some important success factors in adopting agile software development practices},
keywords={Success factors; Agile software},
doi={10.1016/j.jss.2009.05.052},
author={Misra, Subhas Chandra and Kumar, Vinod and Kumar, Uma},
journal={JOURNAL OF SYSTEMS AND SOFTWARE},
type={Article},
publisher={ELSEVIER SCIENCE INC},
month={nov},
issn={0164-1212},
number={11},
volume={82},
}

@article{ISI:A1997YE58900002,
month={oct},
volume={29},
journal={IIE TRANSACTIONS},
year={1997},
abstract={Agile manufacturing is a new expression that is used to represent theability of a producer of goods and services to thrive in the face ofcontinuous change. These changes can occur in markets, in technologies,in business relationships and in all facets of the business enterprise.This paper discusses the genesis of several of the Agile ManufacturingResearch Institutes (AMRIs) and their on-going activities and results todale. A vision for agile manufacturing research is articulated andinitial accomplishments identified. Additional research needs are alsodiscussed.},
publisher={CHAPMAN HALL LTD},
number={10},
author={DeVor, R and Graves, R and Mills, J J},
type={Article},
issn={0740-817X},
address={2-6 BOUNDARY ROW, LONDON, ENGLAND SE1 8HN},
title={Agile manufacturing research: accomplishments and opportunities},
pages={813--823},
}

@article{ISI:000231388800019,
number={5},
author={Boehm, B and Turner, R},
pages={30+},
doi={10.1109/MS.2005.129},
abstract={Agile software development processes have shown positive impacts oncost, schedule, and customer satisfaction. However, most implementationsof agile processes have been in smaller-scale, software-onlyenvironments. In March 2004, a group of researchers and practitionersaddressed the implementation of agile processes in largesystems-engineering projects that rely on traditional developmentprocesses and artifacts. They identified three management challengeareas. Here, the authors discuss numerous ways in which to address them.},
title={Management challanges to implementing Agile Processes in traditional development organizations},
volume={22},
year={2005},
issn={0740-7459},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
journal={IEEE SOFTWARE},
type={Article},
publisher={IEEE COMPUTER SOC},
}

@article{ISI:000078667300001,
publisher={TAYLOR {\&} FRANCIS LTD},
author={Meade, L M and Sarkis, J},
issn={0020-7543},
pages={241--261},
doi={10.1080/002075499191751},
year={1999},
abstract={The objective of this paper is to introduce a decision methodology andstructure for manufacturing land organizational) agility improvement.The methodology allows for the evaluation of alternatives (e.g.projects) to help organizations become more agile, with a specificobjective of improving the manufacturing business processes. An agileenterprise is one whose processes are designed to respond effectively tounanticipated change. One of the difficulties in designing and analysingbusiness processes, in general, is that they are operational designsthat need to incorporate strategic attributes. In order to evaluatealternatives that impact the business processes, a networkedhierarchical analysis model based on the various characteristics ofagility, is proposed. This evaluation model will be based on theanalytic network process methodology for solving complex and systemicdecisions. An actual example of a small manufacturing enterpriseprovides some managerial insights into the methodology.},
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
number={2},
month={jan},
address={ONE GUNPOWDER SQUARE, LONDON EC4A 3DE, ENGLAND},
title={Analyzing organizational project alternatives for agile manufacturing processes: an analytical network approach},
type={Article},
volume={37},
}

@article{ISI:000180224400015,
volume={40},
year={2002},
pages={4545--4562},
abstract={In response to changing success factors and environmental pressures,companies are aspiring to break out of mass production and become leanor agile. Whereas performance enhancements of lean practices have beendemonstrated, it is now the case that markets have become increasinglycharacterized by turbulence, a situation in which reliance on leanpractices is insufficient, and that survival requires adoption of agilepractices. As a result, a comparative study of lean and agilemanufacturing with a related survey of current practices in the UK wascarried out, the results of which is presented in this paper. The paperexplored the threats to lean and the drivers of agile manufacturing.Using data from a questionnaire survey, four hypotheses were tested,which was indicative of the benefits of agile manufacturing. In contrastto their lean counterparts, agile companies paid attention to a widerrange of competitive capabilities. They therefore had a lower range ofmean scores on competitive capabilities. Independent sample tests ofsignificant difference in business performance measures revealed thatthe agile companies consistently outperformed their lean competitors onall business performance measures studied. In addition, a wider range ofcompetitive capabilities and performance measures of the agile companiescorrelated significantly and positively whilst such correlation wasobserved for only a narrow range of capabilities and performancemeasures for lean companies. The results suggest that competingsimultaneously on multiple competitive capabilities enhance performancebetter than a rather narrow focus on cost and quality.},
issn={0020-7543},
address={4 PARK SQUARE, MILTON PARK,, ABINGDON OX14 4RN, OXON, ENGLAND},
month={nov},
doi={10.1080/00207540210157141},
author={Yusuf, Y Y and Adeleye, E O},
number={17},
journal={INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
publisher={TAYLOR {\&} FRANCIS LTD},
title={A comparative study of lean and agile manufacturing with a related survey of current practices in the UK},
type={Article},
}

@article{ISI:000080462600008,
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
pages={87--105},
keywords={agile manufacturing; review; future research; deve},
abstract={Agile manufacturing (AM) is a new concept in manufacturing intended toimprove the competitiveness of firms. Manufacturing processes based onAM are characterized by customer-supplier integrated process for productdesign, manufacturing, marketing, and support services. This needsdecision-making at functional knowledge levels, stable unit costs,flexible manufacturing, easy access to integrated data, and modularproduction facilities. Agile manufacturing requires enriching of thecustomer, co-operating with competitors, organizing to manage change,uncertainty and complexity, and leveraging people and information. Inthe recent years, a number of research papers have been published in thearea of AM. However, a framework for the development of AM has notreceived due attention from both researchers and practitioners.Realizing the importance of agile manufacturing in the 21st centurymanufacturing competitiveness, an attempt has been made in this paper toreview the literature available on AM with the objective to: (i)identify key strategies and techniques of AM,(ii) suggest some futureresearch directions and (iii) develop a framework for the development ofagile manufacturing systems (AMSs) along four key dimensions whichinclude strategies, technologies, systems and people. (C) 1999 ElsevierScience B.V. All rights reserved.},
doi={10.1016/S0925-5273(98)00222-9},
month={may},
number={1-2},
publisher={ELSEVIER SCIENCE BV},
issn={0925-5273},
year={1999},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
volume={62},
author={Gunasekaran, A},
type={Article},
title={Agile manufacturing: A framework for research and development},
}

@article{ISI:000265879200001,
journal={ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS},
number={1},
keywords={Design; Experimentation; Performance; Internet app},
issn={1556-4665},
type={Article},
volume={3},
title={Agile Dynamic Provisioning of Multi-Tier Internet Applications},
author={Urgaonkar, Bhuvan and Shenoy, Prashant and Chandra, Abhishek and Goyal, Pawan and Wood, Timothy},
doi={10.1145/1342171.1342172},
annote={2nd International Conference on Autonomic Computing (ICAC 2005),Seattle, WA, JUN 13-16, 2005},
address={2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA},
year={2008},
publisher={ASSOC COMPUTING MACHINERY},
month={mar},
institution={IEEE Comp Soc; Natl Sci Fdn},
abstract={Dynamic capacity provisioning is a useful technique for handling themulti-time-scale variations seen in Internet workloads. In this article,we propose a novel dynamic provisioning technique for multi-tierInternet applications that employs (1) a flexible queuing model todetermine how much of the resources to allocate to each tier of theapplication, and (2) a combination of predictive and reactive methodsthat determine when to provision these resources, both at large andsmall time scales. We propose a novel data center architecture based onvirtual machine monitors to reduce provisioning overheads. Ourexperiments on a forty-machine Xen/Linux-based hosting platformdemonstrate the responsiveness of our technique in handling dynamicworkloads. In one scenario where a flash crowd caused the workload of athree-tier application to double, our technique was able to double theapplication capacity within five minutes, thus maintaining response-timetargets. Our technique also reduced the overhead of switching serversacross applications from several minutes to less than a second, whilemeeting the performance targets of residual sessions.},
}

@article{ISI:000184713800006,
type={Article; Proceedings Paper},
title={The strategic integration of agile and lean supply},
author={Stratton, R and Warburton, R D H},
abstract={Lean supply is closely associated with enabling flow and the eliminationof wasteful variation within the supply chain. However, lean operationsdepend on level scheduling and the growing need to accommodate varietyand demand uncertainty has resulted in the emergence of the concept ofagility. This paper explores the role of inventory and capacity inaccommodating such variation and identifies how TRIZ separationprinciples and TOC tools may be combined in the integrated developmentof responsive and efficient supply chains. A detailed apparel industrycase study is used to illustrate the application of these concepts andtools. (C) 2003 Elsevier Science B.V. All rights reserved.},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
issn={0925-5273},
year={2003},
month={aug},
doi={10.1016/S0925-5273(03)00109-9},
pages={183--198},
institution={Czech Assoc Sci {\&} Tech Soc},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
annote={16th International Conference on Production Research (ICPR), CZECH TECHUNIV, PRAGUE, CZECH REPUBLIC, AUG, 2001},
publisher={ELSEVIER SCIENCE BV},
keywords={agile; trade-offs; lean; quick response},
volume={85},
number={2},
}

@article{ISI:000080462600006,
publisher={ELSEVIER SCIENCE BV},
issn={0925-5273},
volume={62},
keywords={agile; supply chain; information; competitive adva},
pages={61--73},
year={1999},
journal={INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
month={may},
author={Mason-Jones, R and Towill, D R},
abstract={Agility is herein interpreted as using market knowledge and a virtualcorporation to exploit profitable opportunities in a volatile marketplace. This requires the slashing of process lead times throughout thechain. However, as we demonstrate in the paper such action is simply notenough to enable agility. Similar steps must also be taken to reduceinformation lead times, resulting in the concept of the ``informationenriched{\{}''{\}} supply chain. Simulation results obtained on realisticmodels of fashion trade supply chains confirm the superior agilityresulting from information enrichment. The paper concludes with aRoute-Map indicating the steps to be taken in achieving supply chainagility in real world scenarios. (C) 1999 Elsevier Science B.V. Allrights reserved.},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
title={Total cycle time compression and the agile supply chain},
type={Article},
number={1-2},
doi={10.1016/S0925-5273(98)00221-7},
}

@article{ISI:000171886500026,
month={nov},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
issn={0018-9162},
publisher={IEEE COMPUTER SOC},
number={11},
type={Editorial Material},
year={2001},
pages={131--133},
title={Agile software development: The people factor},
volume={34},
doi={10.1109/2.963450},
author={Cockburn, A and Highsmith, J},
journal={COMPUTER},
}

@article{ISI:000234695300029,
title={The AGILE anticoincidence detector},
month={jan},
abstract={AGILE is a gamma-ray astrophysics space mission which will operate,starting from 2006, in the 30 MeV-50 GeV energy range with imagingcapability also in the 15-45 keV energy band. In order to achieve therequired detection sensitivity, all AGILE detectors are surrounded by ananticoincidence detector aimed at charged particle background rejectionwith an inefficiency as low as 10(-4). In this work, the design and thestructure of this anticoincidence detector are presented, as well as itsperformances in terms of charged particles detection inefficiency asderived from extensive calibrations performed at CERN PS. (c) 2005Elsevier B.V. All rights reserved.},
number={1},
issn={0168-9002},
volume={556},
type={Article},
publisher={ELSEVIER SCIENCE BV},
doi={10.1016/j.nima.2005.10.016},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
year={2006},
keywords={scintillation detector; charged particles detector},
journal={NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
author={Perotti, F and Fiorini, M and Incorvaia, S and Mattaini, E and Sant'Ambrogio, E},
pages={228--236},
}

@article{ISI:000178571700015,
author={Skogen, E J and Barton, J S and Denbaars, S P and Coldren, L A},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
volume={8},
address={345 E 47TH ST, NEW YORK, NY 10017-2394 USA},
year={2002},
journal={IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS},
keywords={ion implantation; laser tuning; semiconductor lase},
abstract={Wavelength-agile photonic integrated circuits are fabricated using aone-step ion implantation quantum-well intermixing process. In thispaper, we discuss the issues in processing optimized widely tunablemultisection lasers using this technique and present the resultsachieved using this process. This quantum-well intermixing process isgeneral in its application and can be used to monolithically integrate awide variety of optoelectronic components with widely tunable lasers.},
number={4},
pages={863--869},
title={A quantum-well-intermixing process for wavelength-agile photonic integrated circuits},
doi={10.1109/JSTQE.2002.800849},
type={Article},
issn={1077-260X},
}

@article{ISI:000300611700007,
type={Article},
year={2012},
pages={139--163},
doi={10.1257/jep.26.1.139},
author={Deming, David J and Goldin, Claudia and Katz, Lawrence F},
publisher={AMER ECONOMIC ASSOC},
title={The For-Profit Postsecondary School Sector: Nimble Critters or Agile Predators?},
address={2014 BROADWAY, STE 305, NASHVILLE, TN 37203 USA},
journal={JOURNAL OF ECONOMIC PERSPECTIVES},
number={1},
issn={0895-3309},
abstract={Private for-profit institutions have been the fastest-growing part ofthe U.S. higher education sector. For-profit enrollment increased from0.2 percent to 9.1 percent of total enrollment in degree-grantingschools from 1970 to 2009, and for-profit institutions account for themajority of enrollments in non-degree-granting postsecondary schools. Wedescribe the schools, students, and programs in the for-profit highereducation sector, its phenomenal recent growth, and its relationship tothe federal and state governments. Using the 2004 to 2009 BeginningPostsecondary Students (BPS) longitudinal survey, we assess outcomes ofa recent cohort of first-time undergraduates who attended for-profitsrelative to comparable students who attended community colleges or otherpublic or private non-profit institutions. We find that relative tothese other institutions, for-profits educate a larger fraction ofminority, disadvantaged, and older students, and they have greatersuccess at retaining students in their first year and getting them tocomplete short programs at the certificate and AA levels. But we alsofind that for-profit students end up with higher unemployment and``idleness{\{}''{\}} rates and lower earnings six years after enteringprograms than do comparable students from other schools and that, notsurprisingly, they have far greater default rates on their loans.},
volume={26},
}

@article{ISI:A1956WD31400011,
number={5},
author={GUNTER, S E and KOHN, H I},
issn={0021-9193},
publisher={AMER SOC MICROBIOLOGY},
year={1956},
address={1325 MASSACHUSETTS AVENUE, NW, WASHINGTON, DC 20005-4171},
pages={571--581},
title={THE EFFECT OF X-RAYS ON THE SURVIVAL OF BACTERIA AND YEAST .1. A COMPARATIVE STUDY OF THE DOSE-SURVIVAL CURVES OF AZOTOBACTER-AGILE, ESCHERICHIA-COLI, PSEUDOMONAS-FLUORESCENS, RHODOPSEUDOMONAS-SPHEROIDES, AND SACCHAROMYCES-CEREVISIAE IRRADIATED IN THE RES},
type={Article},
volume={71},
journal={JOURNAL OF BACTERIOLOGY},
}

@article{ISI:000271206100001,
title={Lean and agile manufacturing: external and internal drivers and performance outcomes},
volume={29},
pages={976--999},
address={HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
doi={10.1108/01443570910993456},
abstract={Purpose - Lean and agile manufacturing are two initiatives that are usedby manufacturing plant managers to improve operations capabilities. Thepurpose of this paper is to investigate internal and external factorsthat drive the choice of lean and agile operations capabilities andtheir respective impact on operational performance.Design/methodology/approach - Lean and agile manufacturing are eachconceptualized as a second-order factor and measured through a bundle ofdistinct practices. The competitive intensity of industry and thecompetitive strategy are modeled as potential external and internaldrivers, respectively, and the impact on quality, delivery, cost, andflexibility performance is analyzed using structural equations modeling.The model is tested with data from the high performance manufacturingproject comprising a total of 211 plants from three industries and sevencountries.Findings - The results indicate that lean and agile manufacturing differin terms of drivers and outcomes. The choice of a cost-leadershipstrategy fully mediates the impact of the competitive intensity ofindustry as a driver of lean manufacturing, while agile manufacturing isdirectly affected by both internal and external drivers, i.e. adifferentiation strategy as well as the competitive intensity ofindustry. Agile manufacturing is found to be negatively associated witha cost-leadership strategy, emphasizing the difference between lean andagile manufacturing. The major differences in performance outcomes arerelated to cost and flexibility, such that lean manufacturing has asignificant impact on cost performance (whereas agile manufacturing hasnot), and that agile manufacturing has a stronger relationship withvolume as well as product mix flexibility than does lean manufacturing.Research limitations/implications - Cross-sectional data from threeindustries and seven countries are used, and it would be interesting totest this model for more industries and countries.Practical implications - The results provide insights into the factorsthat influence the choice of lean or agile manufacturing for improvingoperations, and the results that can be obtained.Originality/value - To the authors' knowledge, this is the firstlarge-scale empirical survey of leanness and agility simultaneously,using data from manufacturing firms in Europe, Asia, and North America.The model incorporates a wide perspective on factors related to lean andagile manufacturing, to be able to identify similarities anddifferences.},
number={10},
type={Article},
issn={0144-3577},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
year={2009},
publisher={EMERALD GROUP PUBLISHING LIMITED},
author={Hallgren, Mattias and Olhager, Jan},
keywords={Lean production; Agile production; Operations mana},
}

@article{ISI:000342721500040,
year={2014},
issn={0036-8075},
address={1200 NEW YORK AVE, NW, WASHINGTON, DC 20005 USA},
pages={196--203},
publisher={AMER ASSOC ADVANCEMENT SCIENCE},
title={Biorobotics: Using robots to emulate and investigate agile locomotion},
abstract={The graceful and agile movements of animals are difficult to analyze andemulate because locomotion is the result of a complex interplay of manycomponents: the central and peripheral nervous systems, themusculoskeletal system, and the environment. The goals of bioroboticsare to take inspiration from biological principles to design robots thatmatch the agility of animals, and to use robots as scientific tools toinvestigate animal adaptive behavior. Used as physical models, biorobotscontribute to hypothesis testing in fields such as hydrodynamics,biomechanics, neuroscience, and prosthetics. Their use may contribute tothe design of prosthetic devices that more closely take human locomotionprinciples into account.},
month={oct},
type={Review},
doi={10.1126/science.1254486},
journal={SCIENCE},
volume={346},
author={Ijspeert, Auke J},
number={6206},
}

@article{ISI:000245226000011,
month={apr},
type={Article},
abstract={We explore the price dynamics in a competitive market consisting ofspectrum agile network service providers and users. Here, multipleself,interested spectrum providers operating with different technologiesand costs compete for potential customers. Different buyers or consumersmay evaluate the same seller differently depending on theirapplications, operating technologies and locations. Two different buyerpopulations, the quality-sensitive and the price-sensitive areinvestigated, and the resulting collective price dynamics are studiedusing a combination of analysis and simulations. Various scenarios areconsidered regarding the nature and accuracy of information available tothe sellers. A myopically optimal strategy is studied when fullinformation is available, while a stochastic learning based strategy isconsidered when the information is limited. Cooperating groups may beformed among the sellers which will in-turn influence the group profitfor those participants. Free riding phenomenon is observed under certaincircumstances.},
year={2007},
publisher={IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
issn={0733-8716},
keywords={wireless communication; radio spectrum management;},
journal={IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS},
volume={25},
title={Price dynamics in competitive agile spectrum access markets},
doi={10.1109/JSAC.2007.07041},
address={445 HOES LANE, PISCATAWAY, NJ 08855 USA},
pages={613--621},
number={3},
author={Xing, Yiping and Chandramouli, R and Cordeiro, Carlos},
}

@article{ISI:000251876200013,
pages={60--67},
title={Agile requirements engineering practices: An emprical study},
type={Article},
issn={0740-7459},
year={2008},
doi={10.1109/MS.2008.1},
volume={25},
journal={IEEE SOFTWARE},
author={Cao, Lan and Ramesh, Balasubramaniam},
number={1},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
publisher={IEEE COMPUTER SOC},
}

@article{ISI:000237770500016,
author={Agarwal, A and Shankar, R and Tiwari, M K},
doi={10.1016/j.ejor.2004.12.005},
publisher={ELSEVIER SCIENCE BV},
journal={EUROPEAN JOURNAL OF OPERATIONAL RESEARCH},
number={1},
year={2006},
keywords={agility; flexibility; supply chain; analytic netwo},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
issn={0377-2217},
pages={211--225},
title={Modeling the metrics of lean, agile and leagile supply chain: An ANP-based approach},
type={Article},
month={aug},
volume={173},
abstract={With the emergence of a business era that embraces `change' as one ofits major characteristics, manufacturing success and survival arebecoming more and more difficult to ensure. The emphasis is onadaptability to changes in the business environment and on addressingmarket and customer needs proactively. Changes in the businessenvironment due to varying needs of the customers lead to uncertainty inthe decision parameters. Flexibility is needed in the supply chain tocounter the uncertainty in the decision parameters. A supply chainadapts the changes if it is flexible and agile in nature. A framework ispresented in this paper, which encapsulates the market sensitiveness,process integration, information driver and flexibility measures ofsupply chain performance. The paper explores the relationship amonglead-time, cost, quality, and service level and the leanness and agilityof a case supply chain in fast moving consumer goods business. The paperconcludes with the justification of the framework, which analyses theeffect of market winning criteria and market qualifying criteria on thethree types of supply chains: lean, agile and leagile. (c) 2005 ElsevierB.V. All rights reserved.},
}

@article{ISI:000271514900043,
type={Article},
journal={ASTRONOMY {\&} ASTROPHYSICS},
keywords={gamma rays: observations; catalogs},
doi={10.1051/0004-6361/200911783},
address={17, AVE DU HOGGAR, PA COURTABOEUF, BP 112, F-91944 LES ULIS CEDEX A, FRANCE},
year={2009},
publisher={EDP SCIENCES S A},
number={3},
title={First AGILE catalog of high-confidence gamma-ray sources},
volume={506},
pages={1563--1574},
author={Pittori, C and Verrecchia, F and Chen, A W and Bulgarelli, A and Pellizzoni, A and Giuliani, A and Vercellone, S and Longo, F and Tavani, M and Giommi, P and Barbiellini, G and Trifoglio, M and Gianotti, F and Argan, A and Antonelli, A and Boffelli, F and Caraveo, P and Cattaneo, P W and Cocco, V and Colafrancesco, S and Contessi, T and Costa, E and Cutini, S and D'Ammando, F and {Del Monte}, E and {De Paris}, G and {Di Cocco}, G and {Di Persio}, G and Donnarumma, I and Evangelista, Y and Fanari, G and Feroci, M and Ferrari, A and Fiorini, M and Fornari, F and Fuschino, F and Froysland, T and Frutti, M and Galli, M and Gasparrini, D and Labanti, C and Lapshov, I and Lazzarotto, F and Liello, F and Lipari, P and Mattaini, E and Marisaldi, M and Mastropietro, M and Mauri, A and Mauri, F and Mereghetti, S and Morelli, E and Moretti, E and Morselli, A and Pacciani, L and Perotti, F and Piano, G and Picozza, P and Pilia, M and Pontoni, C and Porrovecchio, G and Preger, B and Prest, M and Primavera, R and Pucella, G and Rapisarda, M and Rappoldi, A and Rossi, E and Rubini, A and Sabatini, S and Santolamazza, P and Scalise, E and Soffitta, P and Stellato, S and Striani, E and Tamburelli, F and Traci, A and Trois, A and Vallazza, E and Vittorini, V and Zambra, A and Zanello, D and Salotti, L},
issn={0004-6361},
month={nov},
abstract={We present the first catalog of high-confidence gamma-ray sourcesdetected by the AGILE satellite during observations performed from July9, 2007 to June 30, 2008. Cataloged sources were detected by merging allthe available data over the entire time period. AGILE, launched in April2007, is an ASI mission devoted to gamma-ray observations in the 30MeV-50 GeV energy range, with simultaneous X-ray imaging capability inthe 18-60 keV band. This catalog is based on Gamma-Ray Imaging Detector(GRID) data for energies greater than 100 MeV. For the first AGILEcatalog, we adopted a conservative analysis, with a high-quality eventfilter optimized to select gamma-ray events within the central zone ofthe instrument field of view (radius of 40 degrees). This is asignificance-limited (4 sigma) catalog, and it is not a completeflux-limited sample due to the non-uniform first-year AGILE skycoverage. The catalog includes 47 sources, 21 of which are associatedwith confirmed or candidate pulsars, 13 with blazars (7 FSRQ, 4 BL Lacs,2 unknown type), 2 with HMXRBs, 2 with SNRs, 1 with a colliding-windbinary system, and 8 with unidentified sources.},
}

@article{ISI:000231943500005,
year={2005},
number={4},
journal={JOURNAL OF DATABASE MANAGEMENT},
publisher={IGI PUBL},
keywords={agile development; assumptions; extreme programmin},
author={Turk, D and France, R and Rumpe, B},
doi={10.4018/jdm.2005100104},
issn={1063-8016},
title={Assumptions underlying agile software-development processes},
abstract={Agile processes focus on the early facilitation and fast production ofworking code, and are based on software-development process models thatsupport iterative, incremental development of software. Although agilemethods have existed for a number of years now, answers to questionsconcerning the suitability of agile processes to particularsoftware-development environments are still often based on anecdotalaccounts of experiences. An appreciation of the (often unstated)assumptions underlying agile processes can lead to a betterunderstanding of the applicability of agile processes to particularsituations. Agile processes are less likely to be applicable insituations in which core assumptions do not hold. This article examinesthe principles and advocated practices of agile processes to identifyunderlying assumptions. It also identifies limitations that may arisefrom these assumptions and outlines how the limitations can be addressedby incorporating other software-development techniques and practicesinto agile development environments.},
type={Review},
address={701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA},
pages={62--87},
volume={16},
}

@article{ISI:000319705100015,
title={Metabolomics Coupled with Proteomics Advancing Drug Discovery toward More Agile Development of Targeted Combination Therapies},
pages={1226--1238},
address={9650 ROCKVILLE PIKE, BETHESDA, MD 20814-3996 USA},
year={2013},
author={Wang, Xijun and Zhang, Aihua and Wang, Ping and Sun, Hui and Wu, Gelin and Sun, Wenjun and Lv, Haitao and Jiao, Guozheng and Xu, Hongying and Yuan, Ye and Liu, Lian and Zou, Dixin and Wu, Zeming and Han, Ying and Yan, Guangli and Dong, Wei and Wu, Fangfang and Dong, Tianwei and Yu, Yang and Zhang, Shuxiang and Wu, Xiuhong and Tong, Xin and Meng, Xiangcai},
issn={1535-9476},
abstract={To enhance the therapeutic efficacy and reduce the adverse effects oftraditional Chinese medicine, practitioners often prescribe combinationsof plant species and/or minerals, called formulae. Unfortunately, theworking mechanisms of most of these compounds are difficult to determineand thus remain unknown. In an attempt to address the benefits offormulae based on current biomedical approaches, we analyzed thecomponents of Yinchenhao Tang, a classical formula that has been shownto be clinically effective for treating hepatic injury syndrome. Thethree principal components of Yinchenhao Tang are Artemisia annua L.,Gardenia jasminoids Ellis, and Rheum Palmatum L., whose major activeingredients are 6,7-dimethylesculetin (D), geniposide (G), and rhein(R), respectively. To determine the mechanisms underlying the efficacyof this formula, we conducted a systematic analysis of the therapeuticeffects of the DGR compound using immunohistochemistry, biochemistry,metabolomics, and proteomics. Here, we report that the DGR combinationexerts a more robust therapeutic effect than any one or two of the threeindividual compounds by hitting multiple targets in a rat model ofhepatic injury. Thus, DGR synergistically causes intensified dynamicchanges in metabolic biomarkers, regulates molecular networks throughtarget proteins, has a synergistic/additive effect, and activates bothintrinsic and extrinsic pathways.},
doi={10.1074/mcp.M112.021683},
month={may},
volume={12},
publisher={AMER SOC BIOCHEMISTRY MOLECULAR BIOLOGY INC},
type={Article},
journal={MOLECULAR {\&} CELLULAR PROTEOMICS},
number={5},
}

@article{ISI:000269983900003,
month={sep},
type={Article},
address={7240 PARKWAY DR, STE 310, HANOVER, MD 21076-1344 USA},
abstract={Despite the popularity of agile methods in software development andincreasing adoption by organizations there is debate about what agilityis and how it is achieved. The debate suffers from a lack ofunderstanding of agile concepts and how agile software development ispracticed. This paper develops a framework for the organization of agilesoftware development that identifies enablers and inhibitors of agilityand the emergent capabilities of agile teams. The work is grounded incomplex adaptive systems (CAS) and draws on three principles ofcoevolving systems: match coevolutionary change rate, maximizeself-organizing, and synchronize exploitation and exploration. Theseprinciples are used to study the processes of two software developmentteams, one a team using eXtreme Programming (XP) and the other a teamusing a more traditional, waterfall-based development cycle. From thecases a framework for the organization of agile software development isdeveloped. Time pacing, self-management with discipline androutinization of exploration are among the agile enablers found in thecases studies while event pacing, centralized management, and lack ofresources allocated to exploration are found to be inhibitors toagility. Emergent capabilities of agile teams that are identified fromthe research include coevolution of business value, sustainable workingwith rhythm, sharing and team learning, and collective mindfulness.},
number={3},
doi={10.1287/isre.1090.0237},
volume={20},
keywords={agile software development; coevolving systems; co},
journal={INFORMATION SYSTEMS RESEARCH},
publisher={INFORMS},
author={Vidgen, Richard and Wang, Xiaofeng},
pages={355--376},
year={2009},
title={Coevolving Systems and the Organization of Agile Software Development},
issn={1047-7047},
}

@article{ISI:000171242900006,
abstract={Traditional approach for the design of missile guidance and autopilotsystems has been to design these subsystems separately and then tointegrate them. Such an approach does not exploit any beneficialrelationships between these and other subsystems. A technique forintegrated design of missile guidance and autopilot systems using thefeedback linearization technique is discussed. Numerical results using asix degree-of-freedom missile simulation are given. Integratedguidance-autopilot systems are expected to result in significantimprovements in missile performance, leading to lower weight andenhanced lethality. These design methods have extensive applications inhigh performance aircraft autopilot and guidance system design. (C) 2001Elsevier Science Ltd. All rights reserved.},
address={THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
pages={1095--1106},
type={Article},
title={Integrated design of agile missile guidance and autopilot systems},
doi={10.1016/S0967-0661(01)00082-X},
publisher={PERGAMON-ELSEVIER SCIENCE LTD},
issn={0967-0661},
year={2001},
month={oct},
number={10},
journal={CONTROL ENGINEERING PRACTICE},
keywords={integrated; guidance; autopilot; feedback lineariz},
author={Menon, P K and Ohlmeyer, E J},
volume={9},
}

@article{ISI:000275074600005,
year={2010},
journal={MIS QUARTERLY},
month={mar},
keywords={Software development agility; agile software devel},
author={Lee, Gwanhoo and Xia, Weidong},
abstract={As business and technology environments change at an unprecedented rate,software development agility to respond to changing user requirementshas become increasingly critical for software development performance.Agile software development approaches, which emphasizesense-and-respond, self-organization, cross-functional teams, andcontinuous adaptation, have been adopted by an increasing number oforganizations to improve their software development agility. However,the agile development literature is largely anecdotal and prescriptive,lacking empirical evidence and theoretical foundation to support theprinciples and practices of agile development. Little research hasempirically examined the software development agility construct in termsof its dimensions, determinants, and effects on software developmentperformance. As a result, there is a lack of understanding about howorganizations can effectively implement an agile development approach.Using an integrated research approach that combines quantitative andqualitative data analyses, this research opens the black box of agiledevelopment by empirically examining the relationships among twodimensions of software development agility (software team responseextensiveness and software team response efficiency), two antecedentsthat can be con trolled (team autonomy and team diversity), and threeaspects of software development performance (on-time completion,on-budget completion, and software functionality). Our PLS results ofsurvey, responses of 399 software project managers suggest that therelationships among these variables are more complex than what has beenperceived by the literature. The results suggest a tradeoff relationshipbetween response extensiveness and response efficiency. These twoagility dimensions impact software development performance differently:response efficiency positively affects all of on-time completion,on-budget completion, and software functionality, whereas responseextensiveness positively affects only software functionality. Theresults also suggest that team autonomy has a positive effect onresponse efficiency and a negative effect on response extensiveness, andthat team diversity has a positive effect on response extensiveness, Weconducted 10 post hoc case studies to qualitatively cross-validate ourPLS results and provide rich, additional insights regarding the complex,dynamic interplays between autonomy, diversity, agility, andperformance. The qualitative analysis also provides explanations forboth supported and unsupported hypotheses. We discuss these qualitativeanalysis results and conclude with the theoretical and practicalimplications of our research findings for agile development approaches.},
number={1},
pages={87--114},
publisher={SOC INFORM MANAGE-MIS RES CENT},
volume={34},
issn={0276-7783},
type={Article},
address={UNIV MINNESOTA-SCH MANAGEMENT 271 19TH AVE SOUTH, MINNEAPOLIS, MN 55455 USA},
title={TOWARD AGILE: AN INTEGRATED ANALYSIS OF QUANTITATIVE AND QUALITATIVE FIELD DATA ON SOFTWARE DEVELOPMENT AGILITY},
}

@article{ISI:000251845500002,
abstract={Purpose - Despite the fact that agile manufacturing has been frequentlypromoted as a means of improving business competitiveness, littleempirical evidence exists in the literature validating its positive linkwith business performance. The purpose of this research paper is toanalyse agile manufacturing in Spain and study whether it is a criticalfactor for success in different industries.Design/methodology/approach - A conceptual model is drawn up, based onthe literature and a previous case study, to relate turbulence in theenvironment with agile manufacturing practices and business performance.The model is tested on a large sample of Spanish manufacturers using asurvey methodology to obtain information and a structural equation modelto analyse the data.Findings - The results obtained show that, in turbulent environments,the integrated use of agile manufacturing practices promotesmanufacturing competitive strength, leading to better operational,market and financial performance.Research limitations/implications - This study has two main limitations.First, it is difficult to determine the most suitable unit of analysiswhen studying agile manufacturing. Second, single respondent bias may beconsidered a limitation.Practical implications - Managers should consider the integratedimplementation of agile manufacturing practices in order to developmanufacturing strength and to outperform competitors in turbulentbusiness environments.Originality/value - This study adopts a systematic approach to theanalysis of agile manufacturing, considering various agility practicesor enablers in an integrated way and relating them not only toenvironmental characteristics but also to business performance. Thisapproach is especially interesting because most of the literature onagile manufacturing deals with agility strategies or techniques in anisolated way. The study also tests the suitability of agilemanufacturing in real organisations - for the first time in the Spanishcontext.},
issn={0144-3577},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords={agile production; operations and production manage},
number={12},
pages={1303--1332},
address={HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
publisher={EMERALD GROUP PUBLISHING LIMITED},
volume={27},
doi={10.1108/01443570710835633},
type={Article},
year={2007},
author={Vazquez-Bustelo, Daniel and Avella, Lucia and Fernandez, Esteban},
title={Agility drivers, enablers and outcomes - Empirical test of an integrated agile manufacturing model},
}

@article{ISI:000244664800021,
title={Theoretical reflections on agile development methodologies - The traditional goal of optimization and control is making way for learning and innovation.},
address={1515 BROADWAY, NEW YORK, NY 10036 USA},
issn={0001-0782},
doi={10.1145/1226736.1226739},
journal={COMMUNICATIONS OF THE ACM},
pages={79--83},
type={Article},
author={Nerur, Sridhar and Balijepally, VanuGopal},
number={3},
publisher={ASSOC COMPUTING MACHINERY},
volume={50},
month={mar},
year={2007},
}

@article{ISI:000251845300005,
volume={27},
doi={10.1108/01443570710830601},
year={2007},
keywords={agile production; partnership; virtual organizatio},
pages={1213--1234},
number={11},
address={HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
abstract={Purpose - This paper aims to provide a practical model usable byorganizations to help form agile virtual enterprises. The model helps tointegrate a variety of factors, tangible and intangible, strategic andoperational, for decision-making purposes.Design/methodology/approach - A comprehensive development of factors isdetermined from the literature and an analytical network process (ANP)methodology is introduced for decision model development. Anillustrative example is presented.Findings - The results provide a robust model that will aid decisionmakers and agile virtual enterprise brokers form partnerships withinthese organizational structures.Research limitations/implications - The paper introduces a conceptualmodel with an illustrative validating example. A practical applicationand reapplication of the model are required to further validate themodel. ANP can require significant managerial input for its application,potentially causing fatigue for decision makers.Practical implications - Practical implications include a partnerselection tool and framework for decision makers. The model may beeasily tweaked by the elimination or addition of decision factors andtheir relationships.Originality/value - The paper is useful to practitioners andorganizations seeking to manage partnership formation of agile virtualenterprises, an emerging organizational form. This work expands thenumber of factors and interrelationships among these factors that noother model has explicitly addressed for the agile virtual enterpriseformation situation.},
author={Sarkis, Joseph and Talluri, Srinivas and Gunasekaran, A},
publisher={EMERALD GROUP PUBLISHING LIMITED},
issn={0144-3577},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
title={A strategic model for agile virtual enterprise partner selection},
type={Article},
}

@article{ISI:000237552000009,
volume={15},
author={Fitzgerald, Brian and Hartnett, Gerard and Conboy, Kieran},
number={2},
title={Customising agile methods to software practices at Intel Shannon},
doi={10.1027/palgrave.ejis.300605},
address={BRUNEL RD BLDG, HOUNDMILLS, BASINGSTOKE RG21 6XS, HANTS, ENGLAND},
month={apr},
journal={EUROPEAN JOURNAL OF INFORMATION SYSTEMS},
issn={0960-085X},
year={2006},
type={Article},
keywords={agile methods; software development; XP; Scrum; me},
abstract={Tailoring of methods is commonplace in the vast majority of softwaredevelopment projects and organisations. However, there is not much knownabout the tailoring and engineering of agile methods, or about how thesemethods can be used to complement each other. This study investigatedtailoring of the agile methods, eXtreme programming (XP) and Scrum, atIntel Shannon, and involved experienced software engineers whocontinuously monitored and reflected on these methods over a 3- yearperiod. The study shows that agile methods may individually beincomplete in supporting the overall development process, but XP andScrum complement each other well, with XP providing support fortechnical aspects and Scrum providing support for project planning andtracking. The principles of XP and Scrum were carefully selected (onlysix of the 12 XP key practices were implemented, for example) andtailored to suit the needs of the development environment at IntelShannon. Thus, the study refutes the suggestion that agile methods arenot divisible or individually selectable but achieve their benefitsthrough the synergistic combination of individual agile practices;rather, this study shows that an a la carte selection and tailoring ofpractices can work very well. In the case of Scrum, some local tailoringhas led to a very committed usage by developers, in contrast to manydevelopment methods whose usage is limited despite being decreedmandatory by management. The agile practices that were applied did leadto significant benefits, including reductions in code defect density bya factor of 7. Projects of 6-month and 1-year duration have beendelivered ahead of schedule, which bodes well for future ability toaccurately plan development projects.},
pages={200--213},
publisher={PALGRAVE MACMILLAN LTD},
}

@article{ISI:000296592600022,
issn={1084-7529},
pages={2400--2413},
address={2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036 USA},
month={nov},
journal={JOURNAL OF THE OPTICAL SOCIETY OF AMERICA A-OPTICS IMAGE SCIENCE AND VISION},
number={11},
title={Code aperture optimization for spectrally agile compressive imaging},
author={Arguello, Henry and Arce, Gonzalo R},
volume={28},
abstract={Coded aperture snapshot spectral imaging (CASSI) provides a mechanismfor capturing a 3D spectral cube with a single shot 2D measurement. Inmany applications selective spectral imaging is sought since relevantinformation often lies within a subset of spectral bands. Capturing andreconstructing all the spectral bands in the observed image cube, tothen throw away a large portion of this data, is inefficient. To thisend, this paper extends the concept of CASSI to a system admittingmultiple shot measurements, which leads not only to higher quality ofreconstruction but also to spectrally selective imaging when thesequence of code aperture patterns is optimized. The aperture codeoptimization problem is shown to be analogous to the optimization of aconstrained multichannel filter bank. The optimal code apertures allowthe decomposition of the CASSI measurement into several subsets, eachhaving information from only a few selected spectral bands. The richtheory of compressive sensing is used to effectively reconstruct thespectral bands of interest from the measurements. A number ofsimulations are developed to illustrate the spectral imagingcharacteristics attained by optimal aperture codes. (C) 2011 OpticalSociety of America},
type={Article},
doi={10.1364/JOSAA.28.002400},
publisher={OPTICAL SOC AMER},
year={2011},
}

@article{ISI:000257529900001,
abstract={Agile software development represents a major departure fromtraditional, plan-based approaches to software engineering. A systematicreview of empirical studies of agile software development up to andincluding 2005 was conducted. The search strategy identified 1996studies, of which 36 were identified as empirical studies. The studieswere grouped into four themes: introduction and adoption, human andsocial factors, perceptions on agile methods, and comparative studies.The review investigates what is currently known about the benefits andlimitations of, and the strength of evidence for, agile methods.Implications for research and practice are presented. The mainimplication for research is a need for more and better empirical studiesof agile software development within a common research agenda. For theindustrial readership, the review provides a map of findings, accordingto topic, that can be compared for relevance to their own settings andsituations. (C) 2008 Elsevier B.V. All rights reserved.},
pages={833--859},
volume={50},
journal={INFORMATION AND SOFTWARE TECHNOLOGY},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
doi={10.1016/j.infsof.2008.01.006},
author={Dyba, Tore and Dingsoyr, Torgeir},
title={Empirical studies of agile software development: A systematic review},
keywords={empirical software engineering; evidence-based sof},
type={Review},
publisher={ELSEVIER SCIENCE BV},
month={aug},
issn={0950-5849},
number={9-10},
year={2008},
}

@article{ISI:000256391400011,
author={Chow, Tsun and Cao, Dac-Buu},
volume={81},
doi={10.1016/j.jss.2007.08.020},
month={jun},
pages={961--971},
number={6},
title={A survey study of critical success factors in agile software projects},
address={360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
journal={JOURNAL OF SYSTEMS AND SOFTWARE},
abstract={While software is so important for all facets of the modern world,software development itself is not a perfect process. Agile softwareengineering methods have recently emerged as a new and different way ofdeveloping software as compared to the traditional methodologies.However, their success has mostly been anecdotal, and research in thissubject is still scant in the academic circles. This research study wasa survey study on the critical success factors of Agile softwaredevelopment projects using quantitative approach.Based on existing literature, a preliminary list of potential criticalsuccess factors of Agile projects were identified and compiled.Subsequently, reliability analysis and factor analysis were conducted toconsolidate this preliminary list into a final set of 12 possiblecritical success factors for each of the four project success categories- Quality, Scope, Time, and Cost.A survey was conducted among Agile professionals, gathering survey datafrom 109 Agile projects from 25 countries across the world. Multipleregression techniques were used, both at the full regression model andat the optimized regression model via the stepwise screening procedure.The results revealed that only 10 out of 48 hypotheses were supported,identifying three critical success factors for Agile softwaredevelopment projects: (a) Delivery Strategy, (b) Agile SoftwareEngineering Techniques, and (c) Team Capability.Limitations of the study are discussed together with interpretations forpractitioners. To ensure success of their projects, managers are urgedto focus on choosing a high-caliber team, practicing Agile engineeringtechniques and following Agile-style delivery strategy. (C) 2007Elsevier Inc. All rights reserved.},
issn={0164-1212},
publisher={ELSEVIER SCIENCE INC},
type={Article},
keywords={software development; agile methods; critical succ},
year={2008},
}

@article{ISI:000178991000007,
abstract={This article concerns the problem of managing the new generation ofAgile Earth Observing Satellites (AEOS). This kind of satellites ispresently studied by the French Centre National d'Etudes Spatiales(PLEIADES project). The mission of an Earth Observing Satellite is toacquire images of specified areas on the Earth surface, in response toobservation requests from customers. Whereas non-agile satellites suchas SPOT5 have only one degree of freedom for acquiring images, the newgeneration satellites have three, giving opportunities for a moreefficient use of the satellite imaging capabilities. Counterwise to thisadvantage, the selection and scheduling of observations becomessignificantly more difficult, due to the larger search space forpotential solutions. Hence, selecting and scheduling observations ofagile satellites is a highly combinatorial problem. This article setsout the overall problem and analyses its difficulties. Then it presentsdifferent methods which have been investigated in order to solve asimplified version of the complete problem: a greedy algorithm, adynamic programming algorithm, a constraint programming approach and alocal search method. (C) 2002 Editions scientifiques et medicalesElsevier SAS. All rights reserved.},
author={Lemaitre, M and Verfaillie, G and Jouhaud, F and Lachiver, J M and Bataille, N},
keywords={Earth Observing Satellite; agile satellite; missio},
publisher={ELSEVIER FRANCE-EDITIONS SCIENTIFIQUES MEDICALES ELSEVIER},
month={sep},
year={2002},
address={23 RUE LINOIS, 75724 PARIS, FRANCE},
issn={1270-9638},
doi={10.1016/S1270-9638(02)01173-2},
volume={6},
number={5},
title={Selecting and scheduling observations of agile satellites},
journal={AEROSPACE SCIENCE AND TECHNOLOGY},
type={Article},
pages={367--381},
}

@article{ISI:000086196000001,
year={2000},
address={84 THEOBALDS RD, LONDON WC1X 8RR, ENGLAND},
journal={TRENDS IN NEUROSCIENCES},
doi={10.1016/S0166-2236(00)01576-9},
issn={0166-2236},
abstract={Since early anatomical descriptions, the existence of dendritic spineshas stimulated intense curiosity and speculation about their regulationand function. Research over the past three decades has described animpressive mutability in dendritic-spine number and morphology under avariety of physiological circumstances. Current evidence favors aproposed model in which two pools of actin filaments, one stable and theother dynamic, support both persistent spine structure and rapid spinemotility. Potential functions of spine motility and dynamic actininclude regulated protein scaffolding, retrograde signaling and synapsestabilization.},
month={apr},
number={4},
author={Halpain, S},
publisher={ELSEVIER SCIENCE LONDON},
pages={141--146},
type={Editorial Material},
title={Actin and the agile spine: how and why do dendritic spines dance?},
volume={23},
}

@article{ISI:A1996UU40400002,
volume={30},
type={Article},
publisher={PERGAMON-ELSEVIER SCIENCE LTD},
month={jul},
author={Cho, H and Jung, M Y and Kim, M},
number={3},
issn={0360-8352},
title={Enabling technologies of agile manufacturing and its related activities in Korea},
doi={10.1016/0360-8352(96)00001-0},
address={THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD, ENGLAND OX5 1GB},
pages={323--334},
year={1996},
journal={COMPUTERS {\&} INDUSTRIAL ENGINEERING},
abstract={As product life cycle becomes shortened, high product quality becomesnecessary for survival, markets become highly diversified and global,and continuous and unexpected change becomes the key factor for success.The need for a method of rapidly and cost-effectively developingproducts, production facilities and supporting software includingdesign, process planning, shop door control systems is becoming urgent.The essence of this concept of manufacturing would be characterized byintroducing a new term agility or rapidity. When compared with computerintegrated manufacturing, agile manufacturing can be defined as thecapability of surviving and prospering in a competitive environment ofcontinuous and unpredictable change by reacting quickly and effectivelyto changing markets, driven by customer-designed products and services.Critical to successfully accomplishing agile manufacturing are a fewenabling technologies such as the standard for the exchange of products(STEP), concurrent engineering, virtual manufacturing, component-basedheterarchical shop floor control system, information and communicationinfrastructure, etc. This article details key concepts of those enablingtechnologies and presents various activities related to agilemanufacturing under development in Korea, especially an agilemanufacturing test-bed at Pohang University of Science and Technologyand a prototype of the life cycle engineering study of a product modelmade in a consumer electronic industry. Copyright (C) 1996 ElsevierScience Ltd.},
}

@unpublished{,
url={https://github.com/tesserato/tesserato.github.io{\%}3E},
title={No Title},
}

@article{ISI:000169462700014,
pages={772--794},
title={Agile manufacturing in practice - Application of a methodology},
year={2001},
type={Article},
volume={21},
issn={0144-3577},
number={5-6},
publisher={MCB U P LIMITED},
doi={10.1108/01443570110390462},
keywords={agile production; methodology; manufacturing},
author={Sharifi, H and Zhang, Z},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
address={60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
abstract={A evolutionary transformation of the business environment, with changeas a main characteristic, is taking place. Manufacturing companies, eventhose operating in relatively stable conditions with good marketpositions, are facing rapid and often unanticipated changes in theirbusiness environment. Agile manufacturing is proposed in response to thecircumstances as a solution and is perceived as a vital characteristicthat manufacturing companies need to have in order to maintain theircompetitive advantages in the new order of world business, Each companywill respond in a specific and different way to the changingcircumstances by deploying its own agile characteristics. Agility inmanufacturing may be achieved through the implementation and integrationof appropriate practices which provide the required abilities for acompany to respond properly to changes. Based on this concept, amethodology is applied in two manufacturing companies and data collectedfrom the applications are used to validate the methodology. This paperprovides a brief summary of the methodology and details itsimplementation and validation in the two case study companies. Practicesare proposed to support the achievement of agility in the twoorganisations.},
}

@article{ISI:000288926200006,
abstract={A structural model incorporating agile manufacturing as the focalconstruct is theorized and tested. The model includes the primarycomponents of JIT (JIT-purchasing and JIT-production) as antecedents andoperational performance and firm performance as consequences to agilemanufacturing. Using data collected from production and operationsmanagers working for large U.S. manufacturers, the model is assessedfollowing a structural equation modeling methodology. The resultsindicate that JIT-purchasing has a direct positive relationship withagile manufacturing while the positive relationship betweenJIT-production and agile manufacturing is mediated by JIT-purchasing.The results also indicate that agile manufacturing has a direct positiverelationship with the operational performance of the firm, that theoperational performance of the firm has a direct positive relationshipwith the marketing performance of the firm, and that the positiverelationship between the operational performance of the firm and thefinancial performance of the firm is mediated by the marketingperformance of the firm. (C) 2010 Elsevier B.V. All rights reserved.},
issn={0272-6963},
volume={29},
keywords={Agile manufacturing; JIT systems; Organizational p},
year={2011},
journal={JOURNAL OF OPERATIONS MANAGEMENT},
author={Inman, R Anthony and Sale, R Samuel and {Green Jr.}, Kenneth W and Whitten, Dwayne},
number={4},
pages={343--355},
type={Article},
address={PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
month={may},
publisher={ELSEVIER SCIENCE BV},
title={Agile manufacturing: Relation to JIT, operational performance and firm performance},
doi={10.1016/j.jom.2010.06.001},
}

@article{ISI:000236672000004,
volume={11},
doi={10.1108/13598540610652492},
publisher={EMERALD GROUP PUBLISHING LIMITED},
year={2006},
abstract={Purpose - The purpose of this article is to investigate the nature ofthe humanitarian aid supply chain and discuss the extent to whichcertain business supply chain concepts, particularly supply chainagility, are relevant to humanitarian aid.Design/methodology/approach - The paper identifies elements of goodpractice in conventional business supply chains and applies them to thehumanitarian aid supply chain, making use of published practice-basedliterature and web sites associated with humanitarian aid. Particularemphasis is placed on the concept of ``agility{\{}''{\}} in supply chainmanagement. A model of an agile supply chain for humanitarian aid isdeveloped.Findings - Humanitarian supply chains have similarities with businesssupply chains, but there are significant differences. Many humanitariansupply chains have a short and unstable existence with an inadequatelink between emergency aid and longer-term developmental aid. Unlikemany business supply chains, typical emergency aid appeals assigninventory to a particular destination at the supply chain source.Practical implications - This research note is a starting-point forempirical studies to test the agile humanitarian supply chain model.Originality/value - This paper seeks to integrate humanitarian aidpractice with concepts in the academic supply chain literature. Inparticular, proposes that humanitarian donors need convincing of thevalue of supply chain processes.},
number={2},
keywords={aid agencies; supply chain management},
pages={115--120},
type={Article},
journal={SUPPLY CHAIN MANAGEMENT-AN INTERNATIONAL JOURNAL},
author={Oloruntoba, R and Gray, R},
issn={1359-8546},
title={Humanitarian aid: an agile supply chain?},
address={60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
}

@article{ISI:000256780400001,
abstract={Wireless networks and devices have rapidly been gaining popularity overtheir wired counterparts. This popularity, in turn, has been generatingan explosive and ever-increasing demand for, and hence creating ashortage of, the radio spectrum. The reason for this foreseen spectrumshortage is reported not to be the scarcity of the radio spectrum butthe inefficiency of current spectrum access methods, thus leavingspectrum opportunities along both the time and frequency dimensions thatwireless devices can exploit. Fortunately, recent technological advanceshave made it possible to build software-defined radios (SDRs), which,unlike traditional radios, can switch from one frequency band to anotherat little or no cost. We propose a MAC protocol, called OpportunisticSpectrum MAC (OS-MAC), for wireless networks equipped with cognitiveradios like SDRs. OS-MAC 1) adaptively and dynamically seeks andexploits opportunities in both licensed and unlicensed spectra and alongboth the time and frequency dimensions, 2) accesses and shares spectrumamong different unlicensed and licensed users, and 3) coordinates withother unlicensed users for better spectrum utilization. Using extensivesimulation, OS-MAC is shown to be far more effective than current accessprotocols from both the network's and the user's perspectives. Bycomparing its performance with an Ideal-MAC protocol, OS-MAC is alsoshown to not only outperform current access protocols, but also achieveperformance very close to that obtainable under the Ideal-MAC protocol.},
author={Hamdaoui, Bechir and Shin, Kang G},
address={10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
journal={IEEE TRANSACTIONS ON MOBILE COMPUTING},
keywords={spectrum agility; opportunistic MAC protocols; sof},
volume={7},
issn={1536-1233},
number={8},
pages={915--930},
publisher={IEEE COMPUTER SOC},
doi={10.1109/TMC.2007.70758},
type={Article},
year={2008},
title={OS-MAC: An efficient MAC protocol for spectrum-agile wireless networks},
month={aug},
}

@article{ISI:000220125400007,
issn={0144-3577},
journal={INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
abstract={The textiles and apparel industry has been neglected in terms of supplychain management research. Recently, the industry has undergone a greatdeal of change, particularly with global sourcing and high levels ofprice competition. In addition, textiles and clothing has marketcharacteristics, such as short product lifecycle, high volatility, lowpredictability, and a high level of impulse purchase, making such issuesas quick response of Paramount importance. This article discussescharacteristics of the textiles and apparel industry and identifies theperspectives of lean, agile and leagility (a combination of these)within existing supply chain literature, which have been proffered assolutions to achieving quick response and reduced lead times. Throughcase studies of textile and apparel companies, different approaches tosupply chain management are illustrated.},
keywords={supply chain management; textile industry},
pages={151--170},
author={Bruce, M and Daly, L and Towers, N},
publisher={EMERALD GROUP PUBLISHING LIMITED},
title={Lean or agile - A solution for supply chain management in the textiles and clothing industry?},
type={Article},
number={1-2},
volume={24},
year={2004},
doi={10.1108/01443570410514867},
address={60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
}

@unpublished{f17,
author={Frans, K},
archivePrefix={arXiv},
annote={preprint},
arxivId={1704.08834},
eprint={1704.08834},
title={Outline Colorization through Tandem Adversarial Networks.},
year={2017},
}

@article{ojs17,
volume={24},
journal={Journal of Financial Crime, v.},
title={Predicting fraudulent financial reporting using artificial neural network},
year={2017},
author={Omar, N and Johari, Z �. and Smith, M},
}

@book{g18,
title={Understanding digital signal processing},
publisher={Springer},
address={Traducao. [s.l.]},
author={Gazi, O},
year={2018},
}

@book{sa97,
author={Smith, S W and Others.},
title={The scientist and engineer�s guide to digital signal processing},
year={1997},
}

@unpublished{cfs16,
title={Automatic tagging using deep convolutional neural networks.},
annote={preprint},
author={Choi, K and Fazekas, G and Sandler, M},
archivePrefix={arXiv},
arxivId={1606.00298},
eprint={1606.00298},
year={2016},
}

@article{s07,
year={2007},
author={Stanley, K O},
volume={8},
title={Compositional pattern producing networks: A novel abstraction of development},
journal={Genetic programming and evolvable machines, v.},
}

@book{bbv13,
title={High-dimensional sequence transduction},
author={Boulanger-lewandowski, N and Bengio, Y and Vincent, P},
publisher={Anais�IEEE},
year={2013},
}

@article{e17,
author={Erickson, B J and Others},
journal={Journal of digital imaging, v.},
title={Toolkits and libraries for deep learning},
volume={30},
year={2017},
}

@article{r,
journal={Traducao. [s.l.] Cornell Aeronautical Laboratory},
title={The perceptron a perceiving and recognizing automaton Project Para.},
volume={1957},
author={Rosenblatt, F},
}

@article{sfh,
volume={2017},
author={Sabour, S and Frosst, N and Hinton, G E},
title={Dynamic routing between capsules},
journal={Advances in Neural Information Processing Systems},
}

@unpublished{g17,
year={2017},
annote={preprint},
author={Grinstein, E and Others},
archivePrefix={arXiv},
arxivId={1710.11385},
eprint={1710.11385},
title={Audio style transfer.},
}

@article{s,
volume={2018},
title={Philharmonia Orchestra},
url={http://www.philharmonia.co.uk/explore/sound{\_}samples{\%}3E},
author={Samples, Sound},
}

@article{18,
journal={Multilayer Perceptron - DeepLearning 0.},
year={2018},
volume={1},
url={http://deeplearning.net/software/theano/{\%}3E},
title={Theano},
}

@article{a,
title={Ivy Audio},
author={Audio, Ivy},
url={http://www.ivyaudio.com/{\%}3E},
volume={2018},
}

@book{18,
address={Intel AI Dispon�vel em},
url={https://ai.intel.com/neon/{\%}3E},
title={Neon},
year={2018},
publisher={\textless},
}

@article{03,
year={2003},
volume={114},
title={The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
}

@book{18,
title={TensorFlow},
publisher={\textless},
url={https://www.tensorflow.org/{\%}3E},
address={TensorFlow Dispon�vel em},
year={2018},
}

@unpublished{,
url={https://github.com/tesserato/tesserato.github.io{\%}3E},
title={No Title},
}

@book{18,
address={GoogleGoogle Dispon�vel em},
title={NSynthSuper},
publisher={\textless},
year={2018},
url={https://nsynthsuper.withgoogle.com/{\%}3E},
}

@book{18,
publisher={\textless},
title={Caffe},
url={http://caffe.berkeleyvision.org/{\%}3E},
year={2018},
address={Caffe | Model Zoo Dispon�vel em},
}

@unpublished{,
url={http://torch.ch/{\%}3E},
title={Torch},
}

@article{58,
title={The perceptron: a probabilistic model for information storage and organization in the brain},
volume={65},
year={1958},
}

@book{18,
address={Magenta Dispon�vel em},
title={Magenta},
url={https://magenta.tensorflow.org/{\%}3E},
year={2018},
publisher={\textless},
}

@article{18,
year={2018},
volume={1},
url={http://deeplearning.net/software/theano/{\%}3E},
title={Theano},
journal={Multilayer Perceptron - DeepLearning 0.},
}

@book{t,
address={Keras Documentation, [s.d.]. Dispon�vel em},
publisher={\textless},
title={Python Deep Learning library},
url={https://keras.io/{\%}3E},
author={The, Keras:},
}

@article{tpc,
journal={CMC},
title={An Evolved Neural Network/HC Hybrid for Tablature Creation in GA-based Guitar Arranging},
author={Tuohy, D R and Potter, W D and Center, A I},
volume={2006},
}

@incollection{08,
address={Traducao. [s.l.] p. 399�417},
booktitle={Handbook of Signal Processing in Acoustics},
publisher={Springer},
year={2008},
title={Digital waveguide architectures for virtual musical instruments},
}

@article{f08,
volume={3},
year={2008},
title={Practicing a musical instrument in childhood is associated with enhanced verbal ability and nonverbal reasoning},
author={Forgeard, M and Others},
journal={PloS one, v.},
}

@book{18,
title={Neon},
address={Intel AI Dispon�vel em},
publisher={\textless},
url={https://ai.intel.com/neon/{\%}3E},
year={2018},
}

@book{lms16,
author={Larsson, G and Maire, M and Shakhnarovich, G},
publisher={Springer},
edition={European C},
address={Anais�},
year={2016},
title={Learning representations for automatic colorization},
}

@article{03,
title={\_}{\_}{\_},
year={2003},
journal={The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
volume={114},
}

@unpublished{h17,
arxivId={1706.09558},
archivePrefix={arXiv},
author={Hutchings, P},
title={Talking Drums: Generating drum grooves with neural networks.},
annote={preprint},
year={2017},
eprint={1706.09558},
}

@book{ha17,
author={Howard, D M and Angus, J},
publisher={Traducao. [s.l.] Focal press},
year={2017},
title={Acoustics and psychoacoustics},
}

@article{s,
title={On the importance of initialization and momentum in deep learning},
volume={2013},
author={Sutskever, I and Others},
journal={International conference on machine learning},
}

@article{k,
author={Kulkarni, T D and Others},
title={Deep convolutional inverse graphics network},
volume={2015},
journal={Advances in Neural Information Processing Systems},
}

@unpublished{gs16,
year={2016},
archivePrefix={arXiv},
arxivId={1603.05516},
author={Gracia, X and Sanz-perela, T},
annote={preprint},
eprint={1603.05516},
title={The wave equation for stiff strings and piano tuning.},
}

@book{mp69,
year={1969},
title={an introduction to computational geometry},
author={Minski, M L and Papert, S A Perceptrons:},
publisher={MIT Press, Cambridge},
address={MA},
}

@book{das93,
author={Duyne, Van and A., S and Smith, J O},
edition={Proceeding},
title={Physical modeling with the 2-D digital waveguide mesh},
publisher={Anais�INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
year={1993},
}

@book{18,
publisher={\textless},
title={TensorFlow},
year={2018},
address={TensorFlow Dispon�vel em},
url={https://www.tensorflow.org/{\%}3E},
}

@book{bs12,
title={Polyphonic piano note transcription with recurrent neural networks},
publisher={Anais�IEEE},
year={2012},
author={B�ck, S and Schedl, M},
}

@article{ke04,
year={2004},
author={Karjalainen, M and Erkut, C},
title={Digital waveguides versus finite difference structures: Equivalence and mixed modeling},
journal={EURASIP Journal on Applied Signal Processing, v.},
volume={2004},
}

@article{rhw,
journal={[s.l.] California Univ San Diego La Jolla Inst for Cognitive Science},
title={Learning internal representations by error propagation.},
author={Rumelhart, D E and Hinton, G E and Williams, R J},
volume={1985},
}

@unpublished{e17,
eprint={1704.01279},
author={Engel, J and Others},
arxivId={1704.01279},
title={Neural audio synthesis of musical notes with wavenet autoencoders.},
year={2017},
annote={preprint},
archivePrefix={arXiv},
}

@book{k16,
edition={(ICIP), 20},
publisher={Anais�IEEE},
title={How deep neural networks can improve emotion recognition on video dataImage Processing},
year={2016},
author={Khorrami, P and Others},
}

@article{shs01,
year={2001},
author={Serafin, S and Huang, P and Smith, J},
volume={1},
journal={Workshop on Future Directions of Computer Music (Mosart-0},
title={The banded digital waveguide mesh},
}

@book{yyk15,
author={Yadav, N and Yadav, A and Kumar, M},
title={An introduction to neural network methods for differential equations},
publisher={Springer},
year={2015},
address={Traducao. [s.l.]},
}

@unpublished{,
title={No Title},
url={https://github.com/tesserato/tesserato.github.io{\%}3E},
}

@inproceedings{g17,
author={Gully, A J and Others},
booktitle={Proc. Interspeech 2017},
title={Articulatory Text-to-Speech Synthesis using the Digital Waveguide Mesh driven by a Deep Neural Network},
publisher={p. 234�238},
year={2017},
}

@unpublished{p85,
title={Learning logic},
year={1985},
author={Parker, D B},
}

@article{th12,
journal={COURSERA: Neural networks for machine learning, v.},
title={rmsprop: Divide the gradient by a running average of its recent magnitude},
author={Tieleman, T and Hinton, G Lecture 6.5-rmsprop:},
volume={4},
year={2012},
}

@book{s16,
address={Traducao. [s.l.] v. 99},
year={2016},
publisher={Springer},
title={Partial differential equations in action: from modelling to theory},
author={Salsa, S},
}

@article{b05,
author={Bensa, J and Others},
title={Computational modeling of stiff piano strings using digital waveguides and finite differences},
journal={Acta Acustica united with Acustica, v.},
volume={91},
year={2005},
}

@book{d16,
year={2016},
author={Dozat, T},
title={Incorporating nesterov momentum into adam},
}

@article{58,
journal={The perceptron: a probabilistic model for information storage and organization in the brain},
volume={65},
year={1958},
title={\_}{\_}{\_},
}

@book{b09,
publisher={Wiley},
address={Traducao. [s.l.] John {\&} Sons},
author={Bilbao, S},
year={2009},
title={Numerical sound synthesis: finite difference schemes and simulation in musical acoustics},
}

@article{95,
volume={1995},
title={The tetrahedral digital waveguide mesh},
year={1995},
journal={Applications of Signal Processing to Audio and Acoustics},
}

@article{fra,
journal={Anais�},
author={Fontana, F and Rocchesso, D and Apollonio, E},
volume={2000},
title={Using the waveguide mesh in modelling 3D},
}

@article{k,
author={Karpathy, A and Others},
journal={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
volume={2014},
title={Large-scale video classification with convolutional neural networks},
}

@article{mp43,
volume={5},
year={1943},
author={Mcculloch, W S and Pitts, W A},
title={logical calculus of the ideas immanent in nervous activity},
journal={The bulletin of mathematical biophysics, v.},
}

@article{xac,
journal={CL (},
volume={2},
author={Xu, W and Auli, M and Clark, S Ccg},
title={Supertagging with a Recurrent Neural Network},
}

@unpublished{,
url={https://github.com/crabacus/the-open-source-drumkit{\%}3E},
title={the open source drumkit},
}

@book{gmh13,
title={Speech recognition with deep recurrent neural networks},
publisher={Anais�IEEE},
author={Graves, A and Mohamed, A.-r. and Hinton, G},
year={2013},
edition={Acoustics},
}

@article{o,
volume={2016},
title={Conditional image generation with pixelcnn decoders},
author={Oord, A Van Den and Others},
journal={Advances in Neural Information Processing Systems},
}

@article{k,
volume={2017},
author={Klambauer, G and Others},
journal={Anais�},
title={Self-normalizing neural networksAdvances in Neural Information Processing Systems},
}

@book{18,
url={https://nsynthsuper.withgoogle.com/{\%}3E},
publisher={\textless},
title={NSynthSuper},
year={2018},
address={GoogleGoogle Dispon�vel em},
}

@article{w,
author={Wiki, DrumGizmo},
url={https://www.drumgizmo.org/wiki/doku.php{\%}3E},
volume={2018},
title={DrumGizmo Wiki},
}

@book{e,
address={Dispon�vel em},
author={Eck, D},
url={https://www.blog.google/technology/ai/making-music-using-new-sounds-generated-machine-learning/{\%}3E},
title={Making music using new sounds generated with machine learning},
publisher={\textless},
}

@book{b16,
address={Traducao. [s.l.]},
author={Bovermann, T and Others},
publisher={Springer},
title={Musical Instruments in the 21st Century},
year={2016},
}

@book{h15,
publisher={Anais�ACM},
author={He, L and Others},
edition={Proceeding},
title={Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks},
year={2015},
}

@unpublished{s15,
annote={preprint},
eprint={1507.06947},
title={Fast and accurate recurrent neural network acoustic models for speech recognition.},
year={2015},
arxivId={1507.06947},
author={Sak, H and Others},
archivePrefix={arXiv},
}

@unpublished{e17,
arxivId={1704.01279},
archivePrefix={arXiv},
annote={preprint},
author={Engel, J and Others},
eprint={1704.01279},
title={Neural audio synthesis of musical notes with wavenet autoencoders.},
year={2017},
}

@book{z17,
title={Advances in all-neural speech recognition},
publisher={Anais�IEEE},
edition={Acoustics},
author={Zweig, G and Others},
year={2017},
}

@book{ck16,
address={Traducao. [s.l.]},
year={2016},
publisher={Springer},
author={Chaigne, A and Kergomard, J},
title={Acoustics of musical instruments},
}

@book{s16,
title={Development of a Digital Musical Instrument with Embedded Sound Synthesis},
year={2016},
author={Staudt, P},
}

@article{bff09,
year={2009},
title={review of artificial intelligence},
author={Brunette, E S and Flemmer, R C and Flemmer, C L A},
pages={4},
volume={2009},
}

@book{16,
title={Comparative study of caffe, neon, theano, and torch for deep learning},
publisher={Comparative study of caffe},
year={2016},
address={neon, theano, and torch for deep learning},
}

@unpublished{o16,
eprint={1609.03499},
title={Wavenet: A generative model for raw audio},
arxivId={1609.03499},
year={2016},
archivePrefix={arXiv},
author={Et al, Van Den Oord A},
annote={. arXiv preprint},
}

@article{s87,
author={Sorensen, H V and Others},
year={1987},
volume={35},
journal={IEEE Transactions on Acoustics, Speech, and Signal Processing, v.},
title={Real-valued fast Fourier transform algorithms},
}

@article{18,
journal={Multilayer Perceptron - DeepLearning 0.},
volume={1},
year={2018},
title={Theano},
url={http://deeplearning.net/software/theano/{\%}3E},
}

@book{s16,
author={Shi, S and Others},
publisher={Anais�IEEE},
year={2016},
edition={Cloud Comp},
title={Benchmarking state-of-the-art deep learning software tools},
}

@article{sc,
title={Musical audio synthesis using autoencoding neural nets},
author={Sarroff, A M and Casey, M A},
journal={ICMC},
volume={2014},
}

@book{zie16,
year={2016},
author={Zhang, R and Isola, P and Efros, A A},
edition={European C},
address={Anais�},
title={Colorful image colorization},
publisher={Springer},
}

@article{rn,
volume={2016},
author={Russell, S J and Norvig, P},
title={Artificial intelligence: a modern approach},
}

@article{s,
volume={2018},
author={Samples, Musical Instrument},
url={http://theremin.music.uiowa.edu/MIS.html{\%}3E},
title={University of Iowa Electronic Music Studios},
}

@unpublished{i16,
annote={preprint},
author={Isola, P and Others},
year={2016},
archivePrefix={arXiv},
arxivId={1611.07004},
title={Image-to-image translation with conditional adversarial networks.},
eprint={1611.07004},
}

@article{l,
volume={2018},
url={http://bair.berkeley.edu/{\%}3E},
author={Lab, Berkeley Artificial Intelligence Research},
title={The Berkeley Artificial Intelligence Research Blog},
}

@article{rdd13,
title={A parametric model and estimation techniques for the inharmonicity and tuning of the piano},
author={Rigaud, F and David, B and Daudet, L},
volume={133},
year={2013},
journal={The Journal of the Acoustical Society of America, v.},
}

@article{dhs11,
author={Duchi, J and Hazan, E and Singer, Y},
year={2011},
volume={12},
journal={Journal of Machine Learning Research, v.},
title={Adaptive subgradient methods for online learning and stochastic optimization},
}

@article{v00,
year={2000},
author={Vaughn, K},
journal={Journal of aesthetic education, v.},
title={Music and mathematics: Modest support for the oft-claimed relationship},
volume={34},
}

@article{a,
title={Ivy Audio},
url={http://www.ivyaudio.com/{\%}3E},
author={Audio, Ivy},
volume={2018},
}

@unpublished{,
title={Torch},
url={http://torch.ch/{\%}3E},
}

@unpublished{z12,
title={Adadelta:an adaptive learning rate method.},
year={2012},
archivePrefix={arXiv},
arxivId={1212.5701},
annote={preprint},
author={Zeiler, M D},
eprint={1212.5701},
}

@article{oa,
volume={2017},
author={Olson, T and Levitt.},
title={Applied Fourier Analysis},
}

@book{p17,
title={A survey of deep-learning frameworks},
year={2017},
publisher={Anais�IEEE},
author={Parvat, A and Others},
edition={Inventive },
}

@article{dsf14,
journal={CIM},
volume={14},
year={2014},
author={Dalgleish, M and Spencer, S and Foster, C},
title={Blurring The Lines: An Integrated Compositional Model For Digital Musical Instrument},
}

@article{rt17,
year={2017},
title={Neuroevolution in games:State of the art and open challenges},
journal={IEEE Transactions on Computational Intelligence and AI in Games, v.},
volume={9},
author={Risi, S and Togelius, J},
}

@unpublished{b15,
archivePrefix={arXiv},
annote={preprint},
arxivId={1511.06435},
author={Bahrampour, S and Others},
eprint={1511.06435},
title={Comparative study of deep learning software frameworks. arXiv},
year={2015},
}

@book{dv16,
title={guide to convolution arithmetic for deep learning},
year={2016},
address={mar},
author={Dumoulin, V and Visin, F A},
publisher={ArXiv e-prints},
}

@article{e17,
author={Esteva, A and Others},
volume={542},
title={Dermatologist-level classification of skin cancer with deep neural networks},
year={2017},
journal={Nature, v.},
}

@article{sbd16,
author={Sigtia, S and Benetos, E and Dixon, S},
year={2016},
volume={24},
title={An end-to-end neural network for polyphonic piano music transcription},
journal={IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), v.},
}

@article{s15,
title={Deep learning in neural networks: An overview},
author={Schmidhuber, J},
volume={61},
year={2015},
journal={Neural networks, v.},
}

@book{w18,
title={WaveNet:, A.Generative Model for Raw Audio},
url={https://deepmind.com/blog/wavenet-generative-model-raw-audio/{\%}3E},
year={2018},
address={DeepMind Dispon�vel em},
publisher={\textless},
}

@article{ssh,
journal={SMIR},
volume={2016},
title={Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks},
author={Southall, C and Stables, R and Hockman, J},
}

@article{g16,
author={Goodfellow, I and Others},
title={Deep learning},
journal={Traducao. [s.l.] MIT press Cambridge v.},
volume={1},
year={2016},
}

@book{hz16,
year={2016},
author={Hwang, J and Zhou, Y},
title={Image Colorization with Deep Convolutional Neural Networks},
}

@article{r06,
volume={50},
title={patents and the geography of knowledge transfer in the electronic musical instrument industry},
year={2006},
author={Reiffenstein, T Codification},
journal={The Canadian Geographer/Le G�ographe canadien, v.},
}

@book{p17,
year={2017},
publisher={IEEE Transactions on Neural Networks and Learning Systems},
author={Pang, Y and Others},
title={Convolution in convolution for network in network},
}

@unpublished{m17,
year={2017},
archivePrefix={arXiv},
arxivId={1711.11160},
eprint={1711.11160},
author={Mital, P K},
title={Time Domain Neural Audio Style Transfer.},
annote={preprint},
}

@book{s06,
url={http://ccrma},
year={2006},
title={basic introduction to digital waveguide synthesis (for the technically inclined)},
author={Smith, J A},
address={Stanford University. stanford. edu/ jos/swgt},
publisher={Center for Computer Research in Music and Acoustics (CCRMA)},
}

@article{r,
volume={2015},
title={Python machine learning},
author={Raschka, S},
}

@article{l,
journal={[s.l.] University of Bristol},
volume={2001},
author={Laird, J A},
title={The physical modelling of drums using digital waveguides.},
}

@article{mdn00,
title={On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application},
author={Mizutani, E and Dreyfus, S E and Nishio, K},
volume={2000},
journal={Neural Networks},
year={2000},
}

@article{pw,
journal={Traducao. [s.l.] {\{}A{\}}merican Mathematical Soc},
title={Harmonic analysis:from Fourier to wavelets.},
volume={2012},
author={Pereyra, M C and Ward, L A},
}

@article{s92,
title={Physical modeling using digital waveguides},
volume={16},
journal={Computer music journal, v.},
author={Smith, J O},
year={1992},
}

@inproceedings{nyc15,
title={Deep neural networks are easily fooled: High confidence predictions for unrecognizable images},
year={2015},
pages={427--436},
author={Nguyen, A and Yosinski, J and Clune, J},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
}

@article{zf,
author={Zwicker, E and Fastl, H},
journal={Traducao. [s.l.] Springer Science {\&} Business Media},
title={Psychoacoustics: Facts and models.},
volume={2013},
}

@article{c,
author={Coppin, B},
volume={2004},
title={Artificial intelligence illuminated},
}

@article{h91,
volume={4},
year={1991},
title={Approximation capabilities of multilayer feedforward networks},
author={Hornik, K},
journal={Neural networks, v.},
}

@article{isi16,
volume={35},
year={2016},
journal={ACM Transactions on Graphics (TOG), v.},
title={Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification},
author={Iizuka, S and Simo-serra, E and Ishikawa, H},
}

@book{18,
title={Magenta},
url={https://magenta.tensorflow.org/{\%}3E},
year={2018},
address={Magenta Dispon�vel em},
publisher={\textless},
}

@book{v,
url={http://www.asimovinstitute.org/neural-network-zoo/{\%}3E},
author={Veen, F Van.},
title={The Neural Network Zoo},
address={Dispon�vel em},
edition={The Asimov},
publisher={\textless},
}

@article{ss,
title={Princeton lectures in analysis.},
journal={Traducao. [s.l.] Princeton University Press},
volume={2003},
author={Stein, E M and Shakarchi, R},
}

@article{s15,
journal={Neural Networks, v.},
volume={64},
year={2015},
title={Deep convolutional neural networks for large-scale speech tasks},
author={Sainath, T N and Others},
}

@book{l11,
publisher={Traducao. [s.l.] Pearson Education India},
title={Understanding Digital Signal Processing, 3/E},
year={2011},
author={Lyons, R G},
}

@book{haks18,
title={Strategic Competition in an Era of Artificial Intelligence},
author={Horowitz, M C and Allen, G C and Kania, E B and Scharre, P},
year={2018},
}

@book{z16,
address={Anais�},
author={Zhu, J.-y. and Others},
publisher={Springer},
year={2016},
title={Generative visual manipulation on the natural image manifold},
edition={European C},
}

@article{wh,
title={Adaptive switching circuits},
journal={[s.l.] STANFORD UNIV CA STANFORD ELECTRONICS LABS},
author={Widrow, B and Hoff, M E},
volume={1960},
}

@book{r18,
year={2018},
author={Roberts, A and Others},
title={Learning Latent Representations of Music to Generate Interactive Musical Palettes},
}

@unpublished{,
title={No Title},
url={https://github.com/tesserato/tesserato.github.io{\%}3E},
}

@unpublished{kb14,
author={Kingma, D P and Ba, J},
year={2014},
annote={preprint},
title={Adam: A method for stochastic optimization.},
archivePrefix={arXiv},
eprint={1412.6980},
arxivId={1412.6980},
}

@article{cos17,
year={2017},
title={An evaluation of Convolutional Neural Networks for music classification using spectrograms},
author={Costa, Y M and Oliveira, L S and Silla, C N},
volume={52},
journal={Applied Soft Computing, v.},
}

@article{hsw89,
year={1989},
author={Hornik, K and Stinchcombe, M and White, H},
title={Multilayer feedforward networks are universal approximators},
journal={Neural networks, v.},
volume={2},
}

@article{e90,
volume={14},
author={Elman, J L},
year={1990},
title={Finding structure in time},
journal={Cognitive science, v.},
}

@unpublished{dmp18,
year={2018},
annote={preprint},
author={Donahue, C and Mcauley, J and Puckette, M},
title={Synthesizing Audio with Generative Adversarial Networks.},
archivePrefix={arXiv},
arxivId={1802.04208},
eprint={1802.04208},
}

@unpublished{r16,
title={An overview of gradient descent optimization algorithms.},
annote={preprint},
arxivId={1609.04747},
author={Ruder, S},
eprint={1609.04747},
year={2016},
archivePrefix={arXiv},
}

@book{rkk18,
year={2018},
author={Reddi, S J and Kale, S and Kumar, S},
title={On the convergence of adam and beyond},
}

@article{aa16,
title={novel approach for detecting DDoS using artificial neural networks},
author={Aljumah, A and Ahamad, T A},
journal={International Journal of Computer Science and Network Security, v.},
year={2016},
volume={16},
}

@article{s07,
volume={2007},
title={State of the art and future directions in musical sound synthesis},
year={2007},
journal={Multimedia Signal Processing},
author={Serra, X},
}

@article{pg,
volume={2017},
author={Patterson, Josh and Gibson, Adam},
title={Deep Learning: A Practitioner's Approach. " O'Reilly Media, Inc."},
}

@book{hsf18,
author={Hinton, G E and Sabour, S and Frosst, N},
year={2018},
title={Matrix capsules with EM routing},
}

@unpublished{z17,
author={Zhang, Y and Others},
year={2017},
archivePrefix={arXiv},
eprint={1701.02720},
annote={preprint},
title={Towards end-to-end speech recognition with deep convolutional neural networks.},
arxivId={1701.02720},
}

@article{vwb,
author={Veit, A and Wilber, M J and Belongie, S},
journal={Advances in Neural Information Processing Systems},
title={Residual networks behave like ensembles of relatively shallow networks},
volume={2016},
}

@unpublished{s16,
archivePrefix={arXiv},
author={Sangkloy, P and Others},
eprint={1612.00835},
title={Scribbler: Controlling deep image synthesis with sketch and color.},
year={2016},
annote={preprint},
arxivId={1612.00835},
}

@book{ykt17,
title={Tensor-Train Recurrent Neural Networks for Video Classification},
author={Yang, Y and Krompass, D and Tresp, V},
year={2017},
}

@book{p18,
title={Generating Audio Using Recurrent Neural Networks},
author={Pfalz, A},
year={2018},
}

@article{geb,
volume={2016},
title={Image style transfer using convolutional neural networks},
author={Gatys, L A and Ecker, A S and Bethge, M},
journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
}

@article{m,
journal={[s.l.] University of York},
title={Physical modelling of the vocal tract with the 2D digital waveguide mesh.},
author={Mullen, J},
volume={2006},
}

@unpublished{okk16,
author={Oord, A Van Den and Kalchbrenner, N and Kavukcuoglu, K},
eprint={1601.06759},
title={Pixel recurrent neural networks.},
annote={preprint},
archivePrefix={arXiv},
arxivId={1601.06759},
year={2016},
}

@article{18,
url={http://deeplearning.net/software/theano/{\%}3E},
year={2018},
journal={Multilayer Perceptron - DeepLearning 0.},
volume={1},
title={Theano},
}

@book{18,
title={Caffe},
url={http://caffe.berkeleyvision.org/{\%}3E},
publisher={\textless},
year={2018},
address={Caffe | Model Zoo Dispon�vel em},
}

@article{mhn,
journal={ICML. Anais�},
author={Maas, A L and Hannun, A Y and Ng, A Y},
title={Rectifier nonlinearities improve neural network acoustic models},
volume={2013},
}

@inproceedings{wl90,
publisher={v. 78},
year={1990},
address={n. 9, p. 1415�1442},
author={Widrow, B and Lehr, M A},
title={30 years of adaptive neural networks: perceptron, madaline, and backpropagation},
booktitle={Proceedings of the IEEE},
}

@article{Poczos1993,
author={Poczos, Lecturer Barnabas},
number={1982},
title={Perceptron History of Artificial Neural Networks The Neuron},
pages={1--10},
year={1993},
}

@article{Mital2017,
author={Mital, Parag K},
number={Nips},
year={2017},
archivePrefix={arXiv},
title={Time Domain Neural Audio Style Transfer},
arxivId={arXiv:1711.11160v1},
eprint={arXiv:1711.11160v1},
}

@article{,
title={UNCLASSIFIED LIMITATION CHANGES TO : FROM :},
year={1995},
}

@article{Rosenblatt,
author={Rosenblatt, F and Nonr-, Contract},
title={THE PERCEPTRON : A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION},
pages={386--408},
volume={65},
number={6},
}

@book{Papert,
isbn={0262631113},
title={Perceptrons},
author={Papert, Seymour A},
}

@article{Diego1985,
year={1985},
number={V},
title={862 18 120},
author={Diego, S A N},
}

@article{Staudt,
author={Staudt, Pascal},
title={Development of a Digital Musical Instrument with Embedded Sound Synthesis},
}

@article{Serra,
keywords={and music composition,experimental psychology and neurosciences,including,most smc research is,psychoacoustics,psychology,quite applied,signal processing and electronics,sound and music computing,sound synthesis},
author={Serra, Xavier},
pages={2--5},
title={State of the Art and Future Directions in Musical Sound Synthesis},
}

@article{,
title={No Title},
}

@article{Harrison2015,
title={Edinburgh Research Explorer An algorithm for a valved brass instrument synthesis environment using finite-difference time-domain methods with performance optimisation AN ALGORITHM FOR A VALVED BRASS INSTRUMENT SYNTHESIS ENVIRONMENT USING FINITE-DIFFERENCE},
year={2015},
author={Harrison, Reginald L and Bilbao, Stefan},
}

@article{Sound1990,
year={1990},
number={4},
volume={14},
title={Spectral Modeling Syrthesis : A Sound Analysis / Sytrthesis System Based on a Detenninistic plus Stochastic Decomposition},
author={Sound, A},
}

@article{Serra2000,
author={Serra, Xavier and Bonada, Jordi},
year={2000},
title={Sound Transformations Based on the SMS High Level Attributes Sound Transformations Based on the SMS High Level Attributes},
number={February 2013},
}

@article{,
title={Percussion Instrument Modelling In 3D : Sound Synthesis Through Time Domain Numerical Simulation University of Edinburgh},
}

@article{Hahn2017,
year={2017},
author={Hahn, Henrik},
title={Expressive sampling synthesis. Learning extended source-filter models from instrument sound databases for expressive sample manipulations},
}

@article{Serafin1996,
year={1996},
doi={10.1162/COMJ},
author={Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C and Nordahl, Rolf and Gables, Coral},
title={Virtual Reality Musical Instruments : State of the Art , Design Principles , and Future Directions},
}

@article{Bonada2016,
author={Bonada, Jordi and Blaauw, Merlijn},
title={Expressive Singing Synthesis based on Unit Selection for the Singing Synthesis Challenge 2016},
year={2016},
pages={1230--1234},
keywords={[Electronic Manuscript]},
}

@article{Selfridge,
title={REAL-TIME PHYSICAL MODEL OF AN AEOLIAN HARP},
pages={1--8},
keywords={aeolian harp,physical model,real-time,sound synthesis},
author={Selfridge, Rod and Moffat, David J and Reiss, Joshua D},
}

@article{Blaauw2010,
archivePrefix={arXiv},
arxivId={arXiv:1704.03809v3},
pages={1--9},
year={2010},
title={A n p s s},
eprint={arXiv:1704.03809v3},
author={Blaauw, Merlijn and Bonada, Jordi and Group, Music Technology and Fabra, Universitat Pompeu},
}

@article{Zappi2017,
year={2017},
pages={145--150},
title={Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments},
author={Zappi, Victor and Allen, Andrew and Fels, Sidney},
}

@article{Unterthiner2017,
author={Unterthiner, Thomas and Sep, L G and Hochreiter, Sepp},
title={Self-Normalizing Neural Networks},
arxivId={arXiv:1706.02515v5},
eprint={arXiv:1706.02515v5},
year={2017},
archivePrefix={arXiv},
}

@article{Bensa2003,
title={Edinburgh Research Explorer The simulation of piano string vibration The simulation of piano string vibration : From physical models to finite difference schemes and digital waveguides},
year={2003},
author={Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Iii, Julius O Smith},
doi={10.1121/1.1587146},
}

@article{Lokki2018,
title={applied sciences Special Issue on “ Sound and Music Computing ”},
doi={10.3390/app8040518},
pages={1--5},
author={Lokki, Tapio},
year={2018},
}

@article{Morse,
author={Morse, Bryan},
title={Magnitude and Phase The Fourier Transform : Examples , Properties , Common Pairs CS 450 : Introduction to Digital Signal and Image Processing Example : Fourier Transform of a Cosine Example : Fourier Transform of a Cosine Odd and Even Functions Sinusoids },
}

@article{,
pages={43--55},
title={Digital Synthesis of and Plucked-String Timbres},
volume={7},
year={2008},
number={2},
}

@article{Karjalainen2004,
author={Karjalainen, Matti},
title={Digital Waveguides versus Finite Difference Structures : Equivalence and Mixed Modeling},
year={2004},
keywords={acoustic signal processing,and phrases,digital waveguides,fdtd model structures,hybrid models,scattering},
pages={978--989},
}

@article{Chaigne1994,
volume={95},
year={1994},
title={Numerical simulations of piano strings . I . A physical model for a struck string using finite difference methods},
pages={1112--1118},
author={Chaigne, Antoine and Cedex, Paris},
number={February},
}

@article{Corporation1959,
year={1959},
pages={978--986},
volume={31},
number={4},
author={Corporation, Westinghouse Electric and Pittsburgh, East and Arbor, Ann},
title={L : L : {\{} L : L : {\{} L : L :},
}

@misc{Dewick2002,
author={Dewick, Paul and Green, Ken and Miozzo, Marcela},
title={Technological Change, Industry Structure and the Environment},
pages={1--31},
abstract={This paper seeks to contribute towards the construction and application of a method to assess the long-term impact of the development of their technological technologies on the environment. The paper identifies the effect of three pervasive technologies – biotechnology, information technology and nanotechnology – on the production of a range of sectors and their consequent environmental effects. The sectors are selected according to taxonomies of characteristics. The technological impact is assessed qualitatively in terms of changes in production scale and resource intensity and their resulting impact on industrial greenhouse gas emissions},
year={2002},
number={January},
}

@misc{,
title={1983 Extensions of the Karplus-Strong Plucked String Algorithm.pdf},
}

@article{Jarvelainen2001,
volume={2},
pages={79--84},
number={April},
doi={10.1121/1.1374756},
author={J{\"{a}}rvel{\"{a}}inen, Hanna and V{\"{a}}lim{\"{a}}ki, Vesa and Karjalainen, Matti},
year={2001},
title={Audibility of the timbral effects of inharmonicity in stringed instrument tones},
}

@article{Masri,
number={0},
volume={44},
title={USING DIGITAL WAVEGUIDES},
author={Masri, P},
}

@article{Laird2001,
author={Laird, Joel Augustus},
year={2001},
title={THE PHYSICAL MODELLING OF DRUMS USING DIGITAL},
number={November},
}

@article{Molteno2004,
title={An experimental investigation into the dynamics of a string},
doi={10.1119/1.1764557},
author={Molteno, Timothy C},
year={2004},
}

@article{Oø,
title={\`{I}}{\'{o}}{\^{u}} {\"{o}} × {\aa} {\o} {\"{o}} ð {\aa}{\'{o}} ðð {\`{o}} {\`{o}} {\`{e}} {\'{y}}× ð {\aa}{\'{o}} ð× {\'{i}}× {\`{o}} {\"{i}} {\'{u}} {\`{u}} ×},
author={\`{O}}{\o}, {\^{O}} {\"{O}} and {\^{U}}{\'{o}}{\"{o}}, {\"{O}} {\O} Ð{\textordmasculine} {\"{O}} {\~{N}} and {\O}{\"{o}}, {\`{O}} {\O} {\`{O}} {\'{O}} {\`{O}} {\^{U}} {\'{O}} {\"{O}} and {\`{O}}{\o}, {\O} {\`{O}} {\O}{\"{o}}{\`{u}}{\~{n}} and {\~{N}}{\'{o}}, Ð {\O} {\O}{\'{y}}{\textordmasculine} {\`{I}} and {\'{O}}{\"{o}}, Ð {\"{O}} {\`{O}}{\o}{\"{o}}{\'{o}} {\`{U}} {\`{O}} {\O} and {\O}, {\"{O}} {\'{O}} {\~{N}} {\O} {\'{O}} {\`{U}} {\O} {\'{O}} {\`{O}} {\'{U}} Ð {\'{O}} and {\"{E}}{\o}{\"{o}}, {\`{O}} {\`{O}} and {\'{O}}{\`{o}}, {\'{A}}{\`{o}}{\o}{\"{o}}{\'{o}} {\`{U}} {\O} and {\'{A}}{\`{o}}, Ð {\`{O}} {\O} and {\'{O}}{\`{o}}, {\O} Ð {\'{O}}{\"{o}}{\~{n}}{\`{u}}ð {\O} and {\O}{\'{o}}{\"{o}}, {\O} {\'{O}}{\`{o}}ð{\'{y}} {\"{O}} and {\`{O}}{\`{u}}, {\`{I}} {\^{O}} {\^{O}} {\"{O}} {\'{U}} {\`{O}} {\O} {\'{O}} {\'{U}} {\O} {\'{O}} {\`{O}} {\`{O}} {\`{O}}{\o}{\"{o}}{\'{o}} {\`{U},
}

@article{Iii2003,
year={2003},
pages={1--5},
title={A Basic Introduction to Digital Waveguide Synthesis ( for the Technically Inclined ) Basics of Digital Waveguide Modeling},
author={Iii, Julius O Smith},
}

@article{Garder2005,
author={G{\"{a}}rder, Anders},
title={Physical modeling of percussion instruments},
year={2005},
}

@article{Arts2008,
number={September},
year={2008},
title={Physical modelling of the piano : An investigation into the e ff ect of string sti ff ness on the hammer-string interaction},
author={Arts, Sonic},
}

@article{Huang2008,
number={2007},
pages={1--23},
title={A REVIEW ON HILBERT-HUANG TRANSFORM : METHOD AND ITS APPLICATIONS},
year={2008},
doi={10.1029/2007RG000228.1.INTRODUCTION},
author={Huang, Norden E and Wu, Zhaohua},
}

@article{Duda2011,
title={DFT-based Estimation of Damped Oscillation Parameters in Low-Frequency Mechanical Spectroscopy},
volume={60},
number={11},
pages={3608--3618},
year={2011},
author={Duda, Krzysztof and Magalas, Leszek B and Majewski, Mariusz},
}

@article{Bensa2006,
author={Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Smith, Julius and Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Smith, Julius and Computa-, Thierry Voinier and Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Iii, Julius O Smith and Voinier, Thierry},
year={2006},
title={Computational modeling of stiff piano strings using digital waveguides and finite difference To cite this version : HAL Id : hal-00088061 Computational Modeling of Stiff Piano Strings Using Digital Waveguides and Finite Differences},
pages={0--10},
}

@article{Jos2009,
number={April},
title={The Acoustics of the Piano},
author={Jos, Juan},
year={2009},
}

@article{David2012,
year={2012},
author={David, Matthew and Speed, Adam},
title={Voice Synthesis Using the Three-Dimensional Digital Waveguide Mesh},
}

@article{Society2012,
author={Society, Acoustical and Identifier, Digital Object and Document, Explorer and Society, Acoustical and Publisher, America and Statement, Rights and Society, Acoustical},
number={1},
year={2012},
title={Edinburgh Research Explorer Time domain simulation and sound synthesis for the snare drum},
doi={10.1121/1.3651240},
pages={914--925},
volume={131},
}

@article{Ptb2015,
title={COMPARISON OF HILBERT TRANSFORM AND SINE FIT APPROACHES FOR THE DETERMINATION OF DAMPING PARAMETERS},
number={4},
author={Ptb, Physikalisch-technische Bundesanstalt},
volume={1},
year={2015},
}

@article{Kartofelev,
title={MODELING A VIBRATING STRING TERMINATED AGAINST A BRIDGE WITH ARBITRARY GEOMETRY},
author={Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-maria},
}

@article{Gr2016,
eprint={arXiv:1603.05516v2},
title={The wave equation for stiff strings and piano tuning},
keywords={inharmonic spectrum,musical,stiffness,vibrating string,wave equation},
archivePrefix={arXiv},
year={2016},
arxivId={arXiv:1603.05516v2},
volume={2},
author={Gr, Xavier},
pages={1--16},
}

@article{,
title={MODELLING THE DECAY OF PIANO SOUNDS Tian Cheng , Simon Dixon , Matthias Mauch Centre for Digital Music , Queen Mary University of London , London , United Kingdom},
volume={1},
}

@article{Serra2014,
author={Serra, Xavier},
title={Spectral Modeling Synthesis : Past and Present Spectral Modeling Synthesis : Past and Present Spectral Analysis / Synthesis},
year={2014},
number={November},
}

@article{Berdahl,
author={Berdahl, Edgar J and Iii, Julius O Smith},
title={Plucked String Digital Waveguide Model},
pages={1--14},
}

@article{Deserio,
number={6},
title={Addendum : The Fourier transform of decaying oscillations},
author={Deserio, Robert},
}

@article{Serafin,
title={The banded digital waveguide mesh},
pages={2--5},
author={Serafin, Stefania and Huang, Patty and Iii, Julius O Smith},
}

@article{Ði,
author={Ð{\"{i}}, {\O} {\'{U}} {\`{U},
title={\AA}{\`{u}}× ð {\'{a}}{\`{o}}×{\o}{\"{o}}{\`{u}}{\~{n}} {\`{o}}{\o} {\aa}{\'{o}} ðð {\`{o}} {\'{i}}× {\`{o}} {\o} ð{\"{i}} {\'{u}} {\`{u}} ×},
}

@article{Iii,
author={Iii, Smith},
title={Physical Modeling Using Digital Waveguides},
}

@article{Bank2010,
doi={10.1109/TASL.2010.2040524},
author={Bank, Balazs and Fontana, Federico},
title={A Modal-Based Real-Time Piano Synthesizer},
number={May 2014},
year={2010},
}

@article{Electronic2017,
title={The wave equation for stiff strings and piano tuning},
volume={3},
year={2017},
pages={1--16},
doi={10.2436/20.2002.02.11.1},
author={Electronic, A N and Of, Journal and Catalana, Societat and Atiques, D E Matem},
keywords={00a65,2010,35g16,35l05,dissonance,inharmonic spectrum,msc,musical scale,stiffness,string,vibrating,wave equation},
}

@article{Iiia,
title={Digital Waveguide Architectures for Virtual Musical Instruments},
author={Iii, Julius O Smith},
}

@article{David2013,
year={2013},
doi={10.1121/1.4799806},
title={A parametric model and estimation techniques for the inharmonicity and tuning of the piano a )},
author={David, Bertrand and Daudet, Laurent},
}

@article{Salih2016,
number={December},
author={Salih, A},
pages={1--24},
title={Second-Order Wave Equation d ' Alembert ' s Solution in Infinite Domain},
year={2016},
}

@article{Feldman2000,
year={2000},
author={Feldman, Joel},
pages={1--2},
title={Derivation of the Wave Equation},
}

@article{h96,
author={Hagan, M T and Others},
title={Neural network design},
journal={Traducao. [s.l.] Pws Pub},
volume={20},
year={1996},
}

@article{tb,
author={Theis, L and Bethge, M},
volume={2015},
journal={Advances in Neural Information Processing Systems},
title={Generative image modeling using spatial LSTMs},
}

@article{hsw89,
journal={Neural networks, v.},
year={1989},
volume={2},
title={Multilayer feedforward networks are universal approximators},
author={Hornik, K and Stinchcombe, M and White, H},
}

@unpublished{gs16,
annote={preprint},
eprint={1603.05516},
year={2016},
author={Gracia, X and Sanz-perela, T},
title={The wave equation for stiff strings and piano tuning.},
arxivId={1603.05516},
archivePrefix={arXiv},
}

@incollection{l12,
author={Lecun, Y A and Others},
booktitle={Neural networks: Tricks of the trade},
publisher={Springer},
title={Efficient backprop},
address={Traducao. [s.l.] p. 9�48},
year={2012},
}

@unpublished{18,
year={2018},
arxivId={1803.05428},
annote={arXiv preprint},
eprint={1803.05428},
archivePrefix={arXiv},
title={A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music.},
}

@article{,
archivePrefix={arXiv},
arxivId={arXiv:1709.08243v2},
eprint={arXiv:1709.08243v2},
title={No Title},
}

@article{Liao,
title={How Important Is Weight Symmetry in Backpropagation ?},
author={Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
}

@article{Ruder2016,
eprint={arXiv:1609.04747v2},
pages={1--14},
author={Ruder, Sebastian},
title={An overview of gradient descent optimization},
arxivId={arXiv:1609.04747v2},
archivePrefix={arXiv},
year={2016},
}

@article{Tomar2017,
author={Tomar, Shikha and Sumathi, Parasuraman and Member, Senior},
title={Amplitude and Frequency Estimation of Exponentially Decaying Sinusoids},
pages={1--9},
year={2017},
}

@article{Smith2015,
arxivId={arXiv:1506.01186v6},
author={Smith, Leslie N},
eprint={arXiv:1506.01186v6},
number={April},
title={Cyclical Learning Rates for Training Neural Networks},
year={2015},
archivePrefix={arXiv},
}

@article{Gully2017,
keywords={[Electronic Manuscript]},
title={Articulatory Text-to-Speech Synthesis using the Digital Waveguide Mesh driven by a Deep Neural Network},
pages={234--238},
year={2017},
author={Gully, Amelia J and Yoshimura, Takenori and Murphy, Damian T and Hashimoto, Kei and Nankaku, Yoshihiko and Tokuda, Keiichi},
}

@article{Nielsen2017,
author={Nielsen, Frank},
archivePrefix={arXiv},
title={DeepBach: a Steerable Model for Bach Chorales Generation},
year={2017},
arxivId={arXiv:1612.01010v2},
eprint={arXiv:1612.01010v2},
}

@article{Jaderberg,
archivePrefix={arXiv},
title={Decoupled Neural Interfaces using Synthetic},
volume={1},
author={Jaderberg, Max and Graves, Alex},
arxivId={arXiv:1608.05343v2},
eprint={arXiv:1608.05343v2},
}

@article{Louizos2017,
archivePrefix={arXiv},
eprint={arXiv:1712.01312v1},
author={Louizos, Christos and Welling, Max},
arxivId={arXiv:1712.01312v1},
pages={1--13},
title={L EARNING S PARSE N EURAL N ETWORKS THROUGH L 0 R EGULARIZATION},
year={2017},
}

@article{Gabrielli2017,
year={2017},
author={Gabrielli, Leonardo and Squartini, Stefano and Tomassetti, Stefano and Zinato, Carlo},
title={INTRODUCING DEEP MACHINE LEARNING FOR PARAMETER ESTIMATION IN PHYSICAL MODELLING},
pages={11--16},
}

@article{Bello2017,
archivePrefix={arXiv},
arxivId={arXiv:1709.07417v2},
author={Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V},
number={2002},
eprint={arXiv:1709.07417v2},
title={Neural Optimizer Search with Reinforcement Learning},
year={2017},
}

@article{Kalchbrenner2016,
year={2016},
author={Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen},
arxivId={arXiv:1610.10099v2},
archivePrefix={arXiv},
title={Neural Machine Translation in Linear Time},
eprint={arXiv:1610.10099v2},
}

@article{Peng2017,
author={Peng, X U E B I N and Abbeel, Pieter},
eprint={arXiv:1804.02717v1},
arxivId={arXiv:1804.02717v1},
archivePrefix={arXiv},
title={DeepMimic : Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
year={2017},
}

@article{Eisenach2017,
year={2017},
author={Eisenach, Carson and Wang, Zhaoran},
title={N ONPARAMETRICALLY L EARNING A CTIVATION F UNCTIONS IN D EEP N EURAL N ETS},
pages={1--23},
}

@article{For2018,
year={2018},
title={Earching for},
author={For, Earching},
pages={1--13},
}

@article{Ephrat2017,
title={Looking to Listen at the Cocktail Party : A Speaker-Independent Audio-Visual Model for Speech Separation},
arxivId={arXiv:1804.03619v1},
eprint={arXiv:1804.03619v1},
year={2017},
author={Ephrat, Ariel and Hebrew, The and Freeman, William T and Rubinstein, Michael and Jon, Only and Rory, Only},
archivePrefix={arXiv},
}

@article{Donoso2007,
keywords={acoustics,helmholtz,musical instruments,resonance,violin},
number={December},
title={A f ´ ısica do violino},
author={Donoso, Pedro and Tann, Alberto and Guimar, Francisco},
year={2007},
}

@article{Donahue2014,
title={Synthesizing Audio with Generative Adversarial Networks},
archivePrefix={arXiv},
eprint={arXiv:1802.04208v1},
year={2014},
author={Donahue, Chris and Mcauley, Julian and Puckette, Miller},
arxivId={arXiv:1802.04208v1},
}

@misc{,
title={1962 On Estimation of a Probability Density Function and Mode.pdf},
}

@article{,
title={Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. {\textregistered} www.jstor.org},
}

@article{Bernroider2013,
year={2013},
author={Bernroider, Edward and Schm{\"{o}}llerl, Patrick},
title={ePub WU Institutional Repository A technological , organisational , and environmental analysis},
}

@article{Zopounidis2002,
year={2002},
volume={138},
title={Multicriteria classification and sorting methods : A literature review},
author={Zopounidis, Constantin and Doumpos, Michael},
keywords={classification,decision rules,multiple criteria analysis,outranking relations,preference,sorting,utility functions},
pages={229--246},
}

@article{Ishizaka2017,
author={Ishizaka, Alessio and Siraj, Sajid},
year={2017},
issn={0377-2217},
keywords={AHP,Decision analysis,Experimental evaluation,MACBETH,SMART},
publisher={Elsevier B.V.},
title={PT},
journal={European Journal of Operational Research},
doi={10.1016/j.ejor.2017.05.041},
url={http://dx.doi.org/10.1016/j.ejor.2017.05.041},
}

@article{Zamani-sabzi2016,
year={2016},
doi={10.1016/j.orp.2016.11.001},
keywords={Statistical analysis of ranking methods,Fuzzy envi,methods,statistical analysis of ranking},
title={Statistical and analytical comparison of multi-criteria decision-making techniques under fuzzy environment},
issn={2214-7160},
url={http://dx.doi.org/10.1016/j.orp.2016.11.001},
author={Zamani-sabzi, Hamed and Phillip, James and Gard, Charlotte C and Abudu, Shalamu},
journal={Operations Research Perspectives},
publisher={Elsevier Ltd},
volume={3},
pages={92--117},
}

@article{Carayannis2018,
pages={0--1},
doi={10.1016/j.techfore.2018.01.028},
url={http://dx.doi.org/10.1016/j.techfore.2018.01.028},
author={Carayannis, Elias G and Ferreira, Jo{\~{a}}o J M and Jalali, Marjan S and Ferreira, Fernando A F},
number={xxxx},
publisher={Elsevier},
year={2018},
issn={0040-1625},
journal={Technological Forecasting {\&} Social Change},
title={Technological Forecasting {\&} Social Change MCDA in knowledge-based economies : Methodological developments and real world applications},
}

@article{Marttunen2017,
doi={10.1016/j.ejor.2017.04.041},
journal={European Journal of Operational Research},
keywords={Multi-methodology,Multi-stakeholder decision-making,Multiple Criteria Decision Analysis,Problem Structuring},
publisher={Elsevier B.V.},
year={2017},
author={Marttunen, Mika and Lienert, Judit and Belton, Valerie},
title={PT US CR},
url={http://dx.doi.org/10.1016/j.ejor.2017.04.041},
issn={0377-2217},
}

@article{Guitouni1998,
author={Guitouni, Adel and Martel, Jean-marc},
pages={501--521},
title={Tentative guidelines to help choosing an appropriate MCDA method},
keywords={aggregation procedure,behavioural considerations,comparative analysis,decision making situation,multicriteria analysis,multicriterion,multicriterion decision aid method,preferences modelling},
volume={109},
year={1998},
}

@article{Solomon1998,
volume={2217},
year={1998},
number={97},
title={Multi-attribute decision making : A simulation comparison of select methods},
author={Solomon, Anthony and Wishart, Nicole and Dublish, Sandipa},
keywords={multiple criteria analysis},
}

@article{Jacquet-lagr2001,
author={Jacquet-lagr, Eric},
keywords={1,criteria,general philosophy,goal programming,in decision-making involving multiple,introduction and background,multicriteria analysis,preference disaggregation,regression},
title={Preference disaggregation : 20 years of MCDA experience},
volume={130},
year={2001},
pages={233--245},
}

@article{Information2000,
author={Information, Background and Of, Description and Mcdm, Some},
title={Chapter 2 MULTI-CRITERIA DECISION MAKING METHODS 2.1},
year={2000},
}

@article{Dehe2015,
title={Expert Systems with Applications Development , test and comparison of two Multiple Criteria Decision Analysis ( MCDA ) models : A case of healthcare infrastructure location},
publisher={Elsevier Ltd},
number={19},
issn={0957-4174},
doi={10.1016/j.eswa.2015.04.059},
url={http://dx.doi.org/10.1016/j.eswa.2015.04.059},
author={Dehe, Benjamin and Bamford, David},
journal={Expert Systems With Applications},
keywords={ahp,analytical hierarchy process,er,evidential reasoning,mcda,multiple criteria decision analysis},
pages={6717--6727},
year={2015},
volume={42},
}

@article{Behzadian2010,
issn={0377-2217},
pages={198--215},
title={PROMETHEE : A comprehensive literature review on methodologies and applications},
author={Behzadian, Majid and Kazemzadeh, R B and Albadvi, A and Aghdasi, M},
publisher={Elsevier B.V.},
year={2010},
url={http://dx.doi.org/10.1016/j.ejor.2009.01.021},
doi={10.1016/j.ejor.2009.01.021},
number={1},
journal={European Journal of Operational Research},
volume={200},
}

@article{Leoneti2016,
url={http://dx.doi.org/10.1016/j.orp.2016.04.001},
doi={10.1016/j.orp.2016.04.001},
pages={21--26},
publisher={Elsevier Ltd},
author={Leoneti, Alexandre Bevilacqua},
issn={2214-7160},
keywords={Utility function,Group Multicriteria Decision,MAUT},
year={2016},
journal={Operations Research Perspectives},
volume={3},
title={Utility Function for modeling Group Multicriteria Decision Making problems as games},
}

@article{Govindan2015,
journal={European Journal of Operational Research},
title={PT US CR},
author={Govindan, Kannan and Jepsen, Martin Brandt},
issn={0377-2217},
keywords={ELECTRE,Literature review,Multiple criteria decision aiding (MCDA),Outranking},
publisher={Elsevier Ltd.},
doi={10.1016/j.ejor.2015.07.019},
url={http://dx.doi.org/10.1016/j.ejor.2015.07.019},
year={2015},
}

@article{Stefano2015,
author={Stefano, Nara Medianeira and Stefano, N M and Filho, N Casarotto and Vergara, L G L and Rocha, R U G},
year={2015},
doi={10.1109/TLA.2015.7404925},
number={January 2016},
title={State of the Art Research and its Applications COPRAS ( Complex Proportional Assessment ): State of the Art Research and its Applications},
}

@article{Costa2016,
title={Graphical interpretation of outranking principles Avoiding misinterpretation results from},
year={2016},
author={Costa, Helder Gomes},
doi={10.1108/JM2-08-2013-0037},
}

@article{Oliveira,
author={Oliveira, G A Q S M and Seleme, R and Zattar, I C},
keywords={multicriteria,project management,roadmap,smart grids},
title={Smart Grid Performance Assessment Via Approach Method},
}

@article{Mix2017,
doi={10.1109/TLA.2016.7459613},
number={March 2016},
title={Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Power Mix},
author={Mix, Power and Dester, Mauricio and Dester, M},
keywords={energy storage systems,intermittent sources,mix,multi-criteria decision analysis,power,renewable sources},
year={2017},
}

@book{Analysis,
title={No Title},
author={Analysis, Multi-criteria Decision},
isbn={9781119974079},
}

@article{Jesus,
keywords={management,requirements engineering,requirements traceability},
author={Jesus, T O De and Soares, M S},
title={A Multi-Criteria Analysis of Techniques and Tools for Tracing Software Requirements},
}

@article{Widrow1990,
author={Widrow, Bernard and Lehr, Michael A},
number={9},
pages={1415--1442},
volume={78},
year={1990},
title={30 Years of Adaptive Neural Networks : Perceptron , Madaline , and Backpropagation},
}

@misc{,
title={1988 A theoretical framework for back-propagation.pdf},
}

@article{Mizutani,
author={Mizutani, Eiji},
title={On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application},
}

@book{,
title={No Title},
isbn={080581258X},
}

@article{Makin2006,
title={Backpropagation},
year={2006},
pages={1--8},
author={Makin, J G},
}

@article{Introduction1996,
title={Neural Networks},
year={1996},
author={Introduction, A Systematic},
}

@article{,
number={Xxx},
title={t{\~{a}}o simples e compacta quanto poss{\'{i}}vel , adotando-se , para tanto , a nota{\c{c}}{\~{a}}o matricial . Embora matematicamente equivalente {\`{a}}s deriva{\c{c}}{\~{o}}es apresentadas em ( XXX ), optou-se por uma abordagem direta com a inten{\c{c}}{\~{a}}o de tornar mais intuitivo o entendimento do},
}

@article{Sathyanarayana2014,
pages={1--15},
year={2014},
title={A Gentle Introduction to Backpropagation What is so difficult about designing a neural},
author={Sathyanarayana, Shashi and Ph, D},
}

@book{Patterson,
title={Deep Learning},
isbn={9781491914250},
author={Patterson, Josh and Gibson, Adam},
}

@misc{,
title={c1992backpropagationand.pdf},
}

@article{Sabour2017,
eprint={arXiv:1710.09829v1},
number={Nips},
year={2017},
title={Dynamic Routing Between Capsules},
archivePrefix={arXiv},
author={Sabour, Sara and Oct, C V and Hinton, Geoffrey E},
arxivId={arXiv:1710.09829v1},
}

@book{Zocca,
author={Zocca, Valentino and Slater, Daniel},
isbn={9781786464453},
title={Python Deep Learning},
}

@misc{,
title={Matthieu Ricard, Trinh Xuan Thuan-The quantum and the lotus{\_} a journey to the frontiers where science and Buddhism meet-Three Rivers Press (2004).pdf},
}

@article{,
title={No Title},
}

@article{Bahrampour2016,
year={2016},
title={C OMPARATIVE S TUDY OF C AFFE , N EON , T HEANO , AND T ORCH FOR D EEP L EARNING},
author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
pages={1--11},
}

@article{Hinton2018,
year={2018},
author={Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
pages={1--15},
title={M ATRIX CAPSULES WITH EM ROUTING},
}

@article{Shi,
author={Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
archivePrefix={arXiv},
arxivId={arXiv:1608.07249v7},
eprint={arXiv:1608.07249v7},
title={Benchmarking State-of-the-Art Deep Learning Software Tools},
}

@article{Bahrampour,
arxivId={arXiv:1511.06435v3},
eprint={arXiv:1511.06435v3},
title={Comparative Study of Deep Learning Software Frameworks},
archivePrefix={arXiv},
author={Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
}

@article{Sarroff,
volume={1},
number={1},
title={MUSICAL AUDIO SYNTHESIS USING AUTOENCODING NEURAL NETS},
author={Sarroff, Andy M and Casey, Michael},
}

@article{Erickson2017,
publisher={Journal of Digital Imaging},
title={Toolkits and Libraries for Deep Learning},
year={2017},
keywords={Artificial intelligence,Machine learning,Deep lear,artificial intelligence,as the proper weighting,convolutional neural network,deep,deep learning approaches learn,learning,machine learning,of those features to,tant features as well,the algorithm learns,the impor-},
author={Erickson, Bradley J and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy and Philbrick, Kenneth},
doi={10.1007/s10278-017-9965-6},
pages={400--405},
}

@article{Grinstein,
archivePrefix={arXiv},
arxivId={arXiv:1710.11385v1},
author={Grinstein, Eric and Duong, Ngoc Q K and Ozerov, Alexey and Patrick, P},
title={No Title},
eprint={arXiv:1710.11385v1},
}

@article{Simonyan2016,
eprint={arXiv:1609.03499v2},
archivePrefix={arXiv},
title={W n : a g m r a},
pages={1--15},
arxivId={arXiv:1609.03499v2},
author={Simonyan, Karen and Dieleman, Sander and Senior, Andrew and Graves, Alex},
year={2016},
}

@article{Jing,
archivePrefix={arXiv},
title={Neural Style Transfer: A Review},
eprint={arXiv:1705.04058v1},
author={Jing, Yongcheng},
arxivId={arXiv:1705.04058v1},
}

@article{Engel2009,
arxivId={arXiv:1704.01279v1},
title={Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders},
archivePrefix={arXiv},
eprint={arXiv:1704.01279v1},
author={Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas},
year={2009},
}

@article{Abnisa2014,
pmid={15003161},
author={Abnisa, Faisal and {Wan Daud}, Wan Mohd Ashri and Athanassiou, M and Zabaniotou, A and Bridgwater, a V and Brown, Duncan and Rowe, Andrew and Wild, Peter and Chen, Dezhen and Yin, Lijie and Wang, Huan and He, Pinjing and Chen, Qun and Yang, Ruiming and Zhao, Bo and Li, Yan and Wang, Shujuan Shaobin and Wu, Hongwei and Zhuo, Yuqun and Chen, Changhe and Cross, Andrew and Sohi, Saran P. and Ding, Hong-Sheng and Jiang, Hong and Francescato and G{\'{o}}mez, Natalia and Rosas, Jos{\'{e}} Guillermo and Cara, Jorge and Mart{\'{i}}nez, Olegario and Alburquerque, Jos{\'{e}} Antonio and S{\'{a}}nchez, Marta Elena and Kaiser, E R and Kempegowda, Rajesh S. and Skreiberg, {\O}yvind and Tran, Khanh-quang Quang and Kuppens, Tom and {Van Dael}, Miet and Vanreppelen, Kenny and Thewys, Theo and Yperman, Jan and Carleer, Robert and Schreurs, Sonja and {Van Passel}, Steven and Lehmann, Johannes and S, Naprave Podjetja Paloma D D and Soediono, Budi and Total, Outcome Defective Non-defective and Uslu, Ayla and Faaij, Andr{\'{e}} P.C. and Bergman, P.C.A. and Yang, Hua and Kudo, Shinji and Kuo, Hsiu Po and Norinaga, Koyo and Mori, Aska and Ma{\v{s}}ek, Ondřej and Hayashi, Jun Ichiro and Singh, M and S{\'{e}}t{\'{a}}l{\'{o}}, G and Guan, X and Warren, M and Toran-Allerand, C D and Brownsort, Peter and Dickinson, Dane and Rogers, J.G. and Brammer, J.G. and Sohi, Saran P. and Crombie, Kyle and Ma{\v{s}}ek, Ondřej and S, Naprave Podjetja Paloma D D and Ma{\v{s}}ek, Ondřej and Konno, Miki and Hosokai, Sou and Sonoyama, Nozomu and Norinaga, Koyo and Hayashi, Jun Ichiro and Athanassiou, M and Zabaniotou, A and Input, Biomass and Gases, Uncondensed H O T and Pyrolysis, Slow and Prost, Katharina and Borchard, Nils and Siemens, Jan and Kautz, Timo and S{\'{e}}quaris, Jean-Marie and M{\"{o}}ller, Andreas and Amelung, Wulf and Ronsse, Frederik and van Hecke, Sven and Dickinson, Dane and Prins, Wolter and Abiven, S and Singh, N and Maestrini, B and Rogovska, Natalia and Laird, David A. and Cruse, Richard and Fleming, Pierce and Parkin, Tim and Meek, David and Spokas, Kurt a and Yoder, Jonathan and Galinato, Suzette and Granatstein, David and Garcia-P{\'{e}}rez, Manuel and Galgani, Pietro and van der Voet, Ester and Korevaar, Gijsbert and {San Miguel}, G. and Dom{\'{i}}nguez, M. P. and Hern{\'{a}}ndez, M. and Sanz-P{\'{e}}rez, F. and Results, Publishable and Bernal, M P and Alburquerque, Jos{\'{e}} Antonio and Moral, R and Bott, R and Lewandowski, Clare M. and Co-investigator, New and Lewandowski, Clare M. and {(UKBRC/The University of Edinburgh)}, Peter a Brownsort and {Biochar Farms} and Preto, Fernando and Klinglm, Michaela and Crombie, Kyle and Ma{\v{s}}ek, Ondřej and Shackley, Simon and Hammond, Jim and Gaunt, John and Ibarrola, Rodrigo and Daugaard, Daren E. and Brown, Robert C. and Krull, E and Singh, Bp and Downie, Ms a and D, Deliverable W P and Kemp, Ian C. and Fyhr, B. Christran and Laurent, Stephane and Roques, Michel a. and Groenewold, Carda E. and Tsotsas, Evangelos and Sereno, Alberto a. and Bonazzi, Cathenne B. and Bimbenet, Jean-Jacques and Kind, Mathhues and EBC and Certificate, European Biochar and Troy, Shane M. and Nolan, Tereza and Leahy, James J. and Lawlor, Peadar G. and Healy, Mark G. and Kwapinski, Witold and Organisation, The International and Iso, E N and Iso, E N and Iso, E N and Iso, E N and Kauffman, Nathan and Dumortier, Jerome and Hayes, Dermot J. and Brown, Robert C. and Laird, David A. and Yang, Hua and Kudo, Shinji and Kuo, Hsiu Po and Norinaga, Koyo and Mori, Aska and Ma{\v{s}}ek, Ondřej and Hayashi, Jun Ichiro and Rogers, J.G. and Brammer, J.G. and Coskun, C. and Oktay, Z. and Ilten, N. and Energy, Relative and $\Delta$e, Fwhm $\Gamma$ and Lehmann, Johannes and Hansen, Veronika and M{\"{u}}ller-St{\"{o}}ver, Dorette and Ahrenfeldt, Jesper and Holm, Jens Kai and Henriksen, Ulrik Birk and Hauggaard-Nielsen, Henrik and Rauch, Sidney J. and G{\'{o}}mez, Natalia and Rosas, Jos{\'{e}} Guillermo and Cara, Jorge and Mart{\'{i}}nez, Olegario and Alburquerque, Jos{\'{e}} Antonio and S{\'{a}}nchez, Marta Elena and Chen, Qun and Yang, Ruiming and Zhao, Bo and Li, Yan and Wang, Shujuan Shaobin and Wu, Hongwei and Zhuo, Yuqun and Chen, Changhe and Raveendran, K and Definition, Product and Standards, Specification and Tomlinson, Thayer and Initiative, International Biochar and Minerals, Bio Carbon and Required, Inorganic C and Class, Minimum and Declaration, Required and Declaration, Required and Method, Standard Test and Determination, Rapid and Content, Carbonate and Method, Standard Test and Analysis, Chemical and Charcoal, Wood and Iso, D I N E N and Faculty, Power Engineering and Faculty, Power Engineering and Faculty, Power Engineering and Faculty, Power Engineering and He, Fang and Yi, Weiming and Bai, Xueyuan and Contents, Table O F and Conversion, Energy and Table, O and Kuppens, Tom and Dael, Miet Van and Vanreppelen, Kenny and Carleer, Robert and Miller-robbie, Leslie and Ulrich, Bridget A and Ramey, Dotti F and Spencer, Kathryn S and Herzog, Skuyler P and Cath, Tzahi Y and Stokes, Jennifer R and Higgins, Christopher P and Prot{\'{a}}sio, Thiago De Paula and Trugilho, Paulo Fernando and Napoli, Alfredo and Wei, Quanyuan and Qu, Yongshui and Tan, Tianwei and Dhillon, R S and Wuehlisch, George Von and Huff, G A and Vasalos, I A and Sharma, Abhishek and Pareek, Vishnu and Wang, Shujuan Shaobin and Zhang, Zhezi and Shinde, Yogesh and Pareek, Vishnu and Zhang, Dongke and Cowdery, T B Reed C D and Building, Crew and Gustafsson, Mattias and Reckamp, Joseph M and Garrido, Rene A and Satrio, Justinus A and Crombie, Kyle and Chen, Dezhen and Yin, Lijie and Wang, Huan and He, Pinjing and Chen, Qun and Yang, Ruiming and Zhao, Bo and Li, Yan and Wang, Shujuan Shaobin and Wu, Hongwei and Zhuo, Yuqun and Chen, Changhe and Bridgwater, a V and Crombie, Kyle and Ma{\v{s}}ek, Ondřej and Rosas, Guillermo and Cara, Jorge and Mart{\'{i}}nez, Olegario and Sohi, Saran P. and Brownsort, Peter and Carter, Sarah and Cook, Jason and Cunningham, Colin and Gaunt, John and Ibarrola, Rodrigo and Ma{\v{s}}ek, Ondřej and Sims, Kirsten and Thornley, Patricia and D, Deliverable W P and Welfle, Andrew and Gilbert, Paul and Thornley, Patricia and Him, Tsz and Pleissner, Daniel and Yan, Kin and Venus, Joachim and Pommeret, Aude and Sze, Carol and Lin, Ki and Sommerfeldt, Nelson and Madani, Hatef and Kuppens, Tom and Dael, Miet Van and Vanreppelen, Kenny and Thewys, Theo and Yperman, Jan and Carleer, Robert and Schreurs, Sonja and Passel, Steven Van and Kempegowda, Rajesh S. and Skreiberg, {\O}yvind and Tran, Khanh-quang Quang and Stelt, M J C Van Der and Gerhauser, H and Kiel, J H A and Ptasinski, K J and Uslu, Ayla and Beach, Long and Park, Won Chan and Service, Forest and Simpson, William T and Ivb, Interreg},
journal={Biomass and Bioenergy},
arxivId={arXiv:1011.1669v3},
abstract={What distinguishes a good manuscript from a bad one?},
volume={5},
pages={1--10},
number={1},
year={2014},
archivePrefix={arXiv},
issn={09619534},
doi={10.1016/j.biortech.2014.03.134},
title={How to get published},
eprint={arXiv:1011.1669v3},
isbn={1757-1693},
}

@article{,
title={A literature review is a description of the literature relevant to a particular field or topic. It gives an overview of:},
abstract={literature review},
journal={2016},
}

@misc{Hastie2009,
url={http://www.springerlink.com/index/10.1007/b94608},
title={The Elements of Statistical Learning},
abstract={During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
year={2009},
doi={10.1007/b94608},
pages={1--694},
pmid={12377617},
volume={1},
booktitle={Bayesian Forecasting and Dynamic Models},
arxivId={arXiv:1011.1669v3},
author={Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
issn={0172-7397},
archivePrefix={arXiv},
isbn={978-0-387-84857-0},
eprint={arXiv:1011.1669v3},
}

@book{Thesis,
title={No Title},
author={Thesis, Better},
isbn={9783319042855},
}

@article{Doctoral,
title={No Title},
author={Doctoral, Helping and Write, Students},
}

@book{Petre2010,
doi={10.1049/em:20040508},
pages={320},
title={The unwritten rules of PhD research},
url={http://books.google.com/books?id={\_}DDwCqx6wpcC{\&}printsec=frontcover{\&}dq=unwritten+rules+of+phd+research{\&}hl={\&}cd=1{\&}source=gbs{\_}api{\%}255Cnpapers2://publication/uuid/48967E01-55F9-4397-B941-310D9C5405FA{\%}255Cnhttp://medcontent.metapress.com/index/A65RM03P4874243N.p},
author={Petre, M and Rugg, G},
isbn={9780335237029},
booktitle={Vasa},
pmid={1275585},
abstract={"I feel grateful to have found this book only a year into my PhD. It has opened my eyes to the world of academia. There is more to a PhD than just research in the sense of working on a problem, getting some results and publishing your findings. This book has allowed me to open my eyes and see all the other things I should be doing to fully succeed at my endeavour of becoming a researcher myself." Dominic Hosler, University of Sheffield This bestselling book on the process of PhD research provides readers with engaging discussion and comprehensive guidance on aspects that other books don't usually mention. Covering all the key topics of the previous edition, including what a PhD is really about, how to do one well, how to decipher what your supervisor actually means by terms like 'good referencing' and 'clean research question', and how to design, report and defend your research,the authors continue to offer an accessible, down-to-earth, and insightful account of the whole PhD process. Their advice addresses how to avoid some of the pitfalls en route to a successful submission. Updated throughout, the new edition includes new material on: Critical thinking Research skills The route to research independence Different models of study The Unwritten Rules of PhD Research is essential reading for anyone considering a PhD or embarking on one. It will tell you the things many students wish someone had told them before they started.},
issn={14724677},
year={2010},
}

@article{Tabei1996,
journal={IEEE Transactions on Signal Processing},
volume={44},
issn={1053587X},
author={Tabei, Makoto and Musicus, Bruce R.},
doi={10.1109/78.506615},
number={6},
year={1996},
pages={1504--1511},
title={A simple estimator for frequency and decay rate},
}

@article{Robel,
author={Robel, Axel},
title={Signals},
}

@article{Brown1991,
year={1991},
abstract={In two recent papers, a description is given of a means of obtaining an arbitrarily narrow peak in the calculation of the autocorrelation function [J. C. Brown and M. S. Puckette, "Calculation of a narrowed autocorrelation function," J. Acoust. Soc. Am. 85, 1595?1601 (1989)] or of a narrow valley in the calculation of an inverse autocorrelation [J. C. Brown and M. S. Puckette, "Musical information from a narrowed autocorrelation function," Proceedings of the 1987 International Conference on Computer Music, Urbana, Illinois, 84?88 (1987)]. These calculations are applied to the determination of the fundamental frequency of musical signals produced by keyboard, wind, and string instruments. These results are compared to frequency tracking results obtained on these sounds with conventional autocorrelation. In so doing it is determined first whether the method of autocorrelation is well-adapted to the problem of tracking the frequency of musical signals, and, second, under what conditions "narrowed" autocorrelation is advantageous. {\textcopyright}1991 Acoustical Society of America.},
title={Musical frequency tracking using the methods of conventional and “narrowed” autocorrelation},
pages={2346--2354},
number={5},
volume={89},
journal={J. Acoust. Soc. Am.},
author={Brown, Judith C and Zhang, Bin},
doi={10.1121/1.400923},
issn={00014966},
}

@article{Provencher1976,
isbn={0006-3495},
issn={00063495},
journal={Biophysical Journal},
title={A Fourier method for the analysis of exponential decay curves},
pages={27--41},
abstract={A method based on the Fourier convolution theorem is developed for the analysis of data composed of random noise, plus an unknown constant "base line," plus a sum of (or an integral over a continuous spectrum of) exponential decay functions. The Fourier method's usual serious practical limitation of needing high accuracy data over a very wide range is eliminated by the introduction of convergence parameters and a Gaussian taper window. A computer program is described for the analysis of discrete spectra, where the data involves only a sum of exponentials. The program is completely automatic in that the only necessary inputs are the raw data (not necessarily in equal intervals of time); no potentially biased initial guesses concerning either the number or the values of the components are needed. The outputs include the number of components, the amplitudes and time constants together with their estimated errors, and a spectral plot of the solution. The limiting resolving power of the method is studied by analyzing a wide range of simulated two-, three-, and four-component data. The results seem to indicate that the method is applicable over a considerably wider range of conditions than nonlinear least squares or the method of moments. {\textcopyright} 1976, The Biophysical Society. All rights reserved.},
number={1},
author={Provencher, S. W.},
pmid={1244888},
volume={16},
year={1976},
doi={10.1016/S0006-3495(76)85660-3},
}

@misc{Martucci1994,
author={Martucci, Stephen A.},
number={5},
volume={42},
abstract={This paper discusses the use of symmetric convolution and the$\backslash$ndiscrete sine and cosine transforms (DSTs and DCTs) for general digital$\backslash$nsignal processing. The operation of symmetric convolution is a$\backslash$nformalized approach to convolving symmetrically extended sequences. The$\backslash$nresult is the same as that obtained by taking an inverse discrete$\backslash$ntrigonometric transform (DTT) of the product of the forward DTTs of$\backslash$nthose two sequences. There are 16 members in the family of DTTs. Each$\backslash$nprovides a representation for a corresponding distinct type of$\backslash$nsymmetric-periodic sequence. The author defines symmetric convolution,$\backslash$nrelates the DSTs and DCTs to symmetric-periodic sequences, and then use$\backslash$nthese principles to develop simple but powerful$\backslash$nconvolution-multiplication properties for the entire family of DSTs and$\backslash$nDCTs. Symmetric convolution can be used for discrete linear filtering$\backslash$nwhen the filter is symmetric or antisymmetric. The filtering will be$\backslash$nefficient because fast algorithms exist for all versions of the DTTs.$\backslash$nConventional linear convolution is possible if one first zero-pad the$\backslash$ninput data. Symmetric convolution and its fast implementation using DTTs$\backslash$nare now an alternative to circular convolution and the DFT},
pages={1038--1051},
issn={19410476},
doi={10.1109/78.295213},
title={Symmetric Convolution and the Discrete Sine and Cosine Transforms},
booktitle={IEEE Transactions on Signal Processing},
year={1994},
}

@article{Smith1999,
author={Smith, Julius O.},
title={Bark and ERB Bilinear Transforms},
volume={7},
pages={697--708},
pmid={799695},
year={1999},
issn={10636676},
journal={IEEE Transactions on Speech and Audio Processing},
number={6},
abstract={Use of a bilinear conformai map to achieve a frequency warping nearly identical to that of the Bark frequency scale is described. Because the map takes the unit circle to itself, its form is that of the transfer function of a first-order allpass filter. Since it is a first-order map, it preserves the model order of rational systems, making it a valuable frequency warping technique for use in audio filter design. A closed-form weighted-equationor method is derived that computes the optimal mapping coefficient as a function of sampling rate, and the solution is shown to be generally indistinguishable from the optimal least-squares solution. The optimal Chebyshev mapping is also found to be essentially identical to the optimal least-squares solution. The expression 0.8517 [arctan(0.06583fs)]1/2-0.916 is shown to accurately approximate the optimal allpass coefficient as a function of sampling rate fs in kHz for sampling rates greater than 1 kHz. A filter design example is included that illustrates improvements due to carrying out the design over a Bark scale. Corresponding results are also given and compared for approximating the related "equivalent rectangular bandwidth (ERB) scale≤ of Moore and Glasberg using a first-order allpass transformation. Due to the higher frequency resolution called for by the ERB scale, particularly at low frequencies, the first-order conformal map is less able to follow the desired mapping, and the error is two to three times greater than the Bark-scale case, depending on the sampling rate. {\textcopyright} 1999 IEEE Publisher Item Identifier S 1063-6676(99)07979-1.},
doi={10.1109/89.799695},
keywords={Bark,Bilinear transform,ERB,Filter design,Frequency warping},
isbn={- 1063-6676},
}

@article{Silvescu1999,
pages={488--491},
author={Silvescu, A.},
journal={IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
volume={1},
issn={1098-7576},
year={1999},
abstract={A new kind of neuron model that has a Fourier-like in/out function$\backslash$nis introduced. The model is discussed in a general theoretical framework$\backslash$nand some completeness theorems are presented. Current experimental$\backslash$nresults show that the new model outperforms, by a large margin both in$\backslash$nrepresentational power and convergence speed, the classical mathematical$\backslash$nmodel of neuron based on weighted sum of inputs filtered by a nonlinear$\backslash$nfunction. The new model is also appealing from a neurophysiological$\backslash$npoint of view because it produces a more realistic representation by$\backslash$nconsidering the inputs as oscillations},
isbn={0-7803-5529-6},
doi={10.1109/IJCNN.1999.831544},
title={Fourier neural networks},
url={http://ieeexplore.ieee.org/document/831544/},
}

@article{Monti2000,
pages={7--10},
number={1},
title={No Title},
author={Monti, Giuliano and Sandler, Mark},
year={2000},
}

@article{Rowland1999,
number={5},
title={The missing wave momentum mystery},
volume={67},
url={http://aapt.scitation.org/doi/10.1119/1.19272},
year={1999},
pages={378--388},
abstract={The usual suggestion for the longitudinally propagating momentum carried by a transverse wave on a string is shown to lead to paradoxes. Numerical simulations provide clues for resolving these paradoxes. The usual formula for wave momentum should be changed by a factor of 2 and the involvement of the cogenerated longitudinal waves is shown to be of crucial importance.},
author={Rowland, David R. and Pask, Colin},
doi={10.1119/1.19272},
issn={0002-9505},
journal={American Journal of Physics},
}

@article{Cecotti2008,
abstract={In BCI (brain - computer interface) systems, brain signals must be processed to identify distinct activities that convey different mental states. We propose a new technique for the classification of electroencephalographic (EEG) steady-state visual e...},
isbn={1051-4651},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4761638{\%}5Cnpapers3://publication/doi/10.1109/ICPR.2008.4761638},
author={Cecotti, Hubert and Graeser, Axel},
pages={1--4},
year={2008},
journal={2008 19th International Conference on Pattern Recognition (ICPR)},
title={Convolutional Neural Network with embedded Fourier Transform for EEG classification},
}

@article{Uncini,
author={Uncini, Aurelio},
title={Sound Synthesis by Flexible Activation Function Recurrent Neural Networks},
}

@article{G-1999,
journal={Audio},
author={G-, Cost and Effects, Digital Audio},
number={1},
title={Envelope model of isolated musical sounds},
doi={10.1109/MMSP.2001.962718},
year={1999},
abstract={This paper presents a model of the envelope of the additive parameters of isolated musical sounds, along with a new method for the estimation of the important envelope split- point times. The model consists of start, attack, sustain, release, and end segments with variable split-point amplitude and time. The estimation of the times is done using smoothed derivatives of the envelopes. The estimated split-point values can be used together with a curve-form model introduced in this paper in the analysis/synthesis of musical sounds. The envelope model can recreate noise-less musical sounds with good fidelity, and the method for the estimation of the envelope times performs significantly better than the classical percentage- based method.},
pages={9--12},
}

@article{Graves2013b,
arxivId={1308.0850},
pmid={23459267},
title={Generating Sequences With Recurrent Neural Networks},
year={2013},
author={Graves, Alex},
isbn={2000201075},
archivePrefix={arXiv},
doi={10.1145/2661829.2661935},
issn={18792782},
pages={1--43},
abstract={This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
url={http://arxiv.org/abs/1308.0850},
eprint={1308.0850},
}

@article{Embrechts2009,
issn={14322994},
journal={Mathematical Methods of Operations Research},
abstract={Numerical evaluation of compound distributions is an important task in insurance mathematics and quantitative risk management. In practice, both recursive methods as well as transform based techniques are widely used.We give a survey of these tools, point out the respective merits and provide some numerical examples.},
pages={497--508},
title={Panjer recursion versus FFT for compound distributions},
author={Embrechts, Paul and Frei, Marco},
volume={69},
year={2009},
doi={10.1007/s00186-008-0249-2},
keywords={Compound distributions,Fast Fourier transform,Panjer recursion,Risk management},
number={3},
}

@article{Merri2014,
arxivId={arXiv:1409.1259v2},
year={2014},
archivePrefix={arXiv},
title={On the Properties of Neural Machine Translation: Encoder–Decoder Approaches},
author={Merri, Bart Van},
eprint={arXiv:1409.1259v2},
}

@article{Lillicrap,
author={Lillicrap, Timothy P and Cownden, Daniel and Tweed, Douglas B and Akerman, Colin J},
eprint={arXiv:1411.0247v1},
arxivId={arXiv:1411.0247v1},
pages={1--27},
title={Random feedback weights support learning in deep neural networks},
archivePrefix={arXiv},
}

@article{Kopparapu,
archivePrefix={arXiv},
arxivId={arXiv:1406.3172v1},
author={Kopparapu, Sunil and Satish, M},
eprint={arXiv:1406.3172v1},
pages={1--7},
number={2},
title={Optimal Gaussian Filter for Effective Noise Filtering},
}

@article{Sarroff2014,
journal={Proceedings of the International Computer Music Conference},
year={2014},
author={Sarroff, Andy M and Casey, Michael},
title={Musical Audio Synthesis Using Autoencoding Neural Nets},
isbn={9789604661374},
pages={14--20},
number={September},
abstract={With an optimal network topology and tuning of hyperpa-rameters, artificial neural networks (ANNs) may be trained to learn a mapping from low level audio features to one or more higher-level representations. Such artificial neu-ral networks are commonly used in classification and re-gression settings to perform arbitrary tasks. In this work we suggest repurposing autoencoding neural networks as musical audio synthesizers. We offer an interactive musi-cal audio synthesis system that uses feedforward artificial neural networks for musical audio synthesis, rather than discriminative or regression tasks. In our system an ANN is trained on frames of low-level features. A high level representation of the musical audio is learned though an autoencoding neural net. Our real-time synthesis system allows one to interact directly with the parameters of the model and generate musical audio in real time. This work therefore proposes the exploitation of neural networks for creative musical applications.},
}

@article{Courtney2015,
author={Courtney, Elya and Courtney, Michael},
archivePrefix={arXiv},
title={arXiv : 1507 . 01832v1 [ physics . data-an ] 6 Jul 2015},
year={2015},
arxivId={arXiv:1507.01832v1},
eprint={arXiv:1507.01832v1},
}

@article{Vincent,
archivePrefix={arXiv},
author={Vincent, Pascal and Bouthillier, Xavier},
eprint={arXiv:1412.7091v3},
arxivId={arXiv:1412.7091v3},
pages={1--15},
title={Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets [ Technical report ]},
}

@article{Baharev2015,
pages={1--34},
author={Baharev, Ali and Schichl, Hermann and Neumaier, Arnold},
year={2015},
title={An exact method for the minimum feedback arc set problem},
}

@article{Moon2014,
number={1},
title={What Is Temporal Fine Structure and Why Is It Important ?},
volume={18},
year={2014},
author={Moon, Il Joon and Hong, Sung Hwa},
pages={1--7},
keywords={hearing,speech perception,temporal envelope,temporal fine structure},
}

@article{Nayebi,
pages={1--6},
author={Nayebi, Aran and Vitelli, Matt},
title={GRUV : Algorithmic Music Generation using Recurrent Neural Networks},
}

@article{Ioffe,
arxivId={arXiv:1502.03167v3},
title={Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift},
eprint={arXiv:1502.03167v3},
author={Ioffe, Sergey and Szegedy, Christian},
archivePrefix={arXiv},
}

@article{Highlander2015,
year={2015},
pages={1--9},
arxivId={arXiv:1601.06815v1},
author={Highlander, Tyler},
eprint={arXiv:1601.06815v1},
title={arXiv : 1601 . 06815v1 [ cs . NE ] 25 Jan 2016 Very Efficient Training of Convolutional Neural Networks using Fast Fourier},
archivePrefix={arXiv},
}

@article{Reyes2015,
year={2015},
author={Reyes, Marco A and Arcos-olalla, Rafael},
number={2},
title={Supersymmetric features of the Error and Dawson ' s functions arXiv : 1510 . 03735v1 [ math-ph ] 13 Oct 2015},
keywords={02,05,1,10,11,30,dawson,error function,gp,ln,mv,pacs numbers,pb,s function,supersymmetry,the error function,the integral,which is defined by},
archivePrefix={arXiv},
arxivId={arXiv:1510.03735v1},
eprint={arXiv:1510.03735v1},
pages={1--13},
}

@article{He2016,
year={2016},
pages={530--534},
title={A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform},
author={He, Lei and Dellwo, Volker},
number={September},
}

@article{Esser,
author={Esser, Steven K and Merolla, Paul A and Arthur, John V and Cassidy, Andrew S and Appuswamy, Rathinakumar and Berg, David J and Mckinstry, Jeffrey L and Melano, Timothy and Barch, Davis R and Nolfo, Carmelo and Amir, Arnon and Taba, Brian and Flickner, Myron D and Modha, Dharmendra S},
archivePrefix={arXiv},
eprint={arXiv:1603.08270v2},
pages={1--7},
number={Figure 1},
title={Convolutional Networks for Fast , Energy-Efficient Neuromorphic Computing},
arxivId={arXiv:1603.08270v2},
}

@misc{Moscaritolo,
author={Moscaritolo, Autores and Angelamoscaritolopcmagcom, Angela},
title={Pebble Smartwatch Sells Out , Collects {\$} 10 Million on Kickstarter},
}

@article{Balle2016b,
pmid={16508805},
url={http://arxiv.org/abs/1611.01704},
year={2016},
abstract={We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
title={End-to-end Optimized Image Compression},
arxivId={1611.01704},
doi={10.1016/S0197-3975(03)00059-6},
archivePrefix={arXiv},
eprint={1611.01704},
isbn={0197-3975},
issn={01973975},
}

@misc{,
title={.{\_}Sawhney, Verona e Prandelli{\_}2005(JIM){\_}COLLABORATING TO CREATE- THE INTERNET AS A PLATFORM FOR CUSTOMER ENGAGEMENT IN PRODUCT INNOVATION.pdf},
}

@misc{Hoover2012a,
pages={111},
author={Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
title={Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding},
year={2012},
isbn={9781905254668},
booktitle={International Conference on Computational Creativity},
}

@article{Fini2010,
doi={10.1016/j.respol.2010.05.014},
journal={Research Policy},
keywords={Academic entrepreneurship,Business creation,Knowledge transfer},
isbn={0048-7333},
pages={1060--1069},
title={Inside or outside the IP system? Business creation in academia},
volume={39},
author={Fini, Riccardo and Lacetera, Nicola and Shane, Scott},
abstract={Research and public policy on academic entrepreneurship are largely based on the assumption that faculty members start businesses to commercialize inventions that have been disclosed to university administrators and have been patented. In this paper, we analyze a sample of 11,572 professors and find that much academic entrepreneurship occurs outside the university intellectual property system. Specifically, about 2/3 of businesses started by academics are not based on disclosed and patented inventions. Moreover, we show that individual characteristics, departmental and organizational affiliations, and time allocation of academics that have started business outside the IP system are different from those of academics that have started businesses to exploit disclosed and patented inventions. We discuss the implications for research on and the practice of academic entrepreneurship. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
number={8},
year={2010},
issn={00487333},
}

@misc{,
title={Unknown - Unknown - Leaps and bounds.pdf.pdf},
}

@misc{,
title={Poverty and profits in the information age.pdf},
}

@article{Toderici2016b,
pmid={21655600},
abstract={This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
year={2016},
isbn={9780761914402},
eprint={1608.05148},
archivePrefix={arXiv},
title={Full Resolution Image Compression with Recurrent Neural Networks},
url={http://arxiv.org/abs/1608.05148},
doi={10.4135/9781412985277},
arxivId={1608.05148},
author={Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
issn={08936080},
}

@misc{,
title={Unknown - Unknown - Democratizing Innovation.pdf.pdf},
}

@misc{Alves1992,
issn={1098-6596},
eprint={arXiv:1011.1669v3},
year={1992},
booktitle={Cadernos de Pesquisa},
pages={53--60},
title={A "revis{\~{a}}o da bibliografia"em teses e disserta{\c{c}}{\~{o}}s: meus tipos inesquec{\'{i}}veis},
isbn={9788578110796},
archivePrefix={arXiv},
doi={10.1017/CBO9781107415324.004},
author={Alves, Alda Judith},
url={http://www.fcc.org.br/pesquisa/publicacoes/cp/arquivos/916.pdf},
arxivId={arXiv:1011.1669v3},
number={81},
pmid={25246403},
abstract={O artigo analisa o papel da revis{\~{a}}o da bibliografia em trabalhos de pesquisa e aponta as principais defici{\^{e}}ncias observadas em teses de mestrado e doutorado, no que se refere a esse aspecto. A primeira se{\c{c}}{\~{a}}o destaca a import{\^{a}}ncia da an{\'{a}}lise cr{\'{i}}tica do estado atual do conhecimento na {\'{a}}reade interesse do pesquisador para a problematiza{\c{c}}{\~{a}}o do temaa ser investigado. A segunda trata do referencial te{\'{o}}rico e discute as dificuldades encontradas na constru{\c{c}}{\~{a}}o te{\'{o}}rica no campo da educa{\c{c}}{\~{a}}o. Finalmente, a terceira se{\c{c}}{\~{a}}o apresenta os equ{\'{i}}vocos mais freq{\"{u}}entes observados em revis{\~{o}}es de bibliografia, utilizando o recurso da caricatura para tornar mais vis{\'{i}}veis certos tra{\c{c}}os.},
}

@article{,
journal={Computer Engineering},
pages={3572--3576},
title={KERNEL RECURRENT SYSTEM TRAINED BY REAL-TIME RECURRENT LEARNING ALGORITHM Pingping Zhu , Jos ´ University of Florida Electrical and Computer Engineering},
year={2013},
isbn={9781479903566},
}

@misc{,
title={Wang et al. - 2011 - Rapid parametric design methods for shoe-last customization.pdf},
}

@article{Tu2017,
keywords={Early event prediction,NeuCube architecture,spatiotemporal data,spiking neural network (SNN)},
year={2017},
journal={IEEE Transactions on Neural Networks and Learning Systems},
pages={1305--1317},
number={6},
eprint={1603.05594},
issn={21622388},
archivePrefix={arXiv},
abstract={This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.},
volume={28},
arxivId={1603.05594},
pmid={26992179},
title={Mapping Temporal Variables Into the NeuCube for Improved Pattern Recognition, Predictive Modeling, and Understanding of Stream Data},
doi={10.1109/TNNLS.2016.2536742},
author={Tu, Enmei and Kasabov, Nikola and Yang, Jie},
}

@article{Theis2017a,
abstract={We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
pages={1--19},
archivePrefix={arXiv},
title={Lossy Image Compression with Compressive Autoencoders},
arxivId={1703.00395},
url={http://arxiv.org/abs/1703.00395},
year={2017},
eprint={1703.00395},
author={Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Husz{\'{a}}r, Ferenc},
}

@misc{Agerfalk2008,
keywords={crowdsourcing,global software development,multi-method research,offshoreing,open source,opensourcing,outsourcing},
author={\AA}gerfalk, P{\"{a}}r J. and Fitzgerald, Brian and Agerfalk, Par J and Fitzgerald, Brian},
issn={02767783},
abstract={This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy-opensourcing, as we term it here-whereby commerical companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product. We followed this with a large-scale survey involving additional exemplars of the phenomenon. The study identifies a number of symmetrical and complementary customer and community obligations that are associated with opensourcing success. We also identify a number of tension points on which customer and community perceptions tend to vary. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness: The customer and community need to establish a trusted partnership of shared responsibility in building an overall opensourcing ecosystem. The study reveals an ongoing shift from OSS as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises. It also reveals that opensourcing provides ample opportunity for companies to headhunt top developers, hence moving from outsourcing to a largely unknown OSS workforce toward recruitment of developers from a global open source community whose talents have become known as a result of the opensourcing experience.},
number={2},
doi={Article},
volume={32},
isbn={02767783},
pmid={31831011},
title={Outsourcing to an unknown workforce: exploring opensourcing as a global sourcing strategy},
year={2008},
booktitle={MIS Quarterly},
pages={385--409},
}

@article{Balle2015a,
pages={1--14},
eprint={1511.06281},
url={http://arxiv.org/abs/1511.06281},
abstract={We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
title={Density Modeling of Images using a Generalized Normalization Transformation},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
arxivId={1511.06281},
archivePrefix={arXiv},
year={2015},
}

@article{Oord2016a,
author={van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
url={http://arxiv.org/abs/1601.06759},
year={2016},
eprint={1601.06759},
title={Pixel Recurrent Neural Networks},
archivePrefix={arXiv},
arxivId={1601.06759},
volume={48},
isbn={9781510829008},
abstract={Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
}

@article{Egmont-Petersen2002a,
author={Egmont-Petersen, M. and {De Ridder}, D. and Handels, H.},
issn={00313203},
journal={Pattern Recognition},
title={Image processing with neural networks- A review},
pages={2279--2301},
volume={35},
abstract={We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments. {\textcopyright} 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
keywords={Digital image processing,Feature extraction,Image compression,Image understanding,Invariant pattern recognition,Neural networks,Object recognition,Optimization,Preprocessing,Segmentation},
year={2002},
doi={10.1016/S0031-3203(01)00178-9},
isbn={0031-3203},
number={10},
}

@article{Kulkarni2015c,
archivePrefix={arXiv},
eprint={1503.03167},
issn={10897550},
pages={1--10},
doi={10.1063/1.4914407},
author={Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
abstract={This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
title={Deep Convolutional Inverse Graphics Network},
url={http://arxiv.org/abs/1503.03167},
arxivId={1503.03167},
year={2015},
}

@article{Toderici2015b,
year={2015},
url={http://arxiv.org/abs/1511.06085},
eprint={1511.06085},
abstract={A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32{\$}\backslashtimes{\$}32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10{\%} or more.},
arxivId={1511.06085},
pages={1--12},
title={Variable Rate Image Compression with Recurrent Neural Networks},
archivePrefix={arXiv},
author={Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
}

@article{Khorrami2016b,
url={http://arxiv.org/abs/1602.07377},
title={How Deep Neural Networks Can Improve Emotion Recognition on Video Data},
archivePrefix={arXiv},
year={2016},
eprint={1602.07377},
pages={1--5},
arxivId={1602.07377},
doi={10.1017/pasa.2016.3},
abstract={We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
author={Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
}

@article{Sainath2015a,
isbn={0893-6080},
abstract={Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12{\%}-14{\%} relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.},
author={Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and rahman Mohamed, Abdel and Dahl, George and Ramabhadran, Bhuvana},
arxivId={1309.1501},
eprint={1309.1501},
doi={10.1016/j.neunet.2014.08.005},
url={http://dx.doi.org/10.1016/j.neunet.2014.08.005},
pages={39--48},
keywords={Deep learning,Neural networks,Speech recognition},
title={Deep Convolutional Neural Networks for Large-scale Speech Tasks},
pmid={25439765},
journal={Neural Networks},
publisher={Elsevier Ltd},
archivePrefix={arXiv},
issn={18792782},
volume={64},
year={2015},
}

@article{Frans2017c,
url={http://arxiv.org/abs/1704.08834},
eprint={1704.08834},
author={Frans, Kevin},
title={Outline Colorization through Tandem Adversarial Networks},
abstract={When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
archivePrefix={arXiv},
arxivId={1704.08834},
year={2017},
}

@article{Hoover2012b,
isbn={9781905254668},
journal={International Conference on Computational Creativity},
year={2012},
author={Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
pages={111},
title={Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding},
}

@article{Zen2015a,
year={2015},
doi={10.1109/ICASSP.2015.7178816},
isbn={9781467369978},
issn={15206149},
pages={4470--4474},
pmid={18557655},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
abstract={Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the con- cerns for applying them to text-to-speech applications is its effect on latency. To address this concern, this paper proposes a low-latency, streaming speech synthesis architecture using unidirectional LSTM- RNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of out- put acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch pro- cessing.},
title={Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis},
author={Zen, Heiga and Sak, Hasim},
keywords={Statistical parametric speech synthesis,long short-term memory,low-latency,recurrent neural networks},
volume={2015-Augus},
}

@article{Graves2013a,
archivePrefix={arXiv},
isbn={978-1-4799-0356-6},
doi={10.1109/ICASSP.2013.6638947},
author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
title={Speech Recognition with Deep Recurrent Neural Networks},
eprint={1303.5778},
number={3},
pmid={27295638},
url={http://arxiv.org/abs/1303.5778},
year={2013},
issn={1520-6149},
abstract={Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
arxivId={1303.5778},
}

@article{Raczynski2013a,
number={9},
author={Raczy{\'{n}}ski, Stanis{\l}aw A. and Vincent, Emmanuel and Sagayama, Shigeki},
doi={10.1109/TASL.2013.2258012},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
title={Dynamic Bayesian networks for symbolic polyhonic pitch modeling},
volume={21},
pages={1830 -- 1840},
issn={1558-7916},
year={2013},
}

@article{Zhu2016a,
arxivId={1609.03552},
abstract={Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
year={2016},
doi={10.1007/978-3-319-46454-1_36},
issn={16113349},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
archivePrefix={arXiv},
title={Generative visual manipulation on the natural image manifold},
pmid={4520227},
author={Zhu, Jun Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
isbn={9783319464534},
volume={9909 LNCS},
pages={597--613},
eprint={1609.03552},
}

@article{Choi2016b,
issn={15209210},
doi={10.1.1.302.7795},
pages={1--5},
title={Convolutional Recurrent Neural Networks for Music Classification},
author={Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
eprint={1609.04243},
url={http://arxiv.org/abs/1609.04243},
arxivId={1609.04243},
archivePrefix={arXiv},
isbn={9789881701282},
year={2016},
abstract={We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
}

@article{Bock2012a,
journal={Network},
pages={121--124},
isbn={9781467300469},
author={Bock, Sebastian and Schedl, Markus},
title={Polyphonic Piano Note Transcription With Recurrent Neural Networks},
year={2012},
}

@article{Boulanger-Lewandowski2013a,
year={2013},
abstract={We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.},
author={Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
keywords={Sequence transduction,polyphonic transcription,recurrent neural network,restricted Boltzmann machine},
issn={15206149},
eprint={1212.1936},
isbn={9781479903566},
number={5},
pages={3178--3182},
title={High-dimensional sequence transduction},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
arxivId={1212.1936},
doi={10.1109/ICASSP.2013.6638244},
archivePrefix={arXiv},
}

@article{Tuohy2006a,
author={Tuohy, D R and Potter, W D},
year={2006},
url={http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2006.119},
abstract={In this paper we describe a technique for creating guitar tablature using a neural network. Training data was parsed from an online repository of human-created tablatures. The contents of both the input layer and the set of training data have been optimized through genetic search in order to maximize the accuracy of the network. The output of the network is im- proved upon with a local heuristic hill-climber (HC). We implement this model in an existing system for generating guitar arrangements via genetic algorithm (GA). When compared to the original system for generating tablature, we note modest improvement in tablature quality and drastic improvements in execution time.},
isbn={\%}(},
number={January 2006},
journal={Procs. of the International Computer Music Conference (ICMC06)},
keywords={fingering{\_}prediction,guitar},
pages={576--579},
title={An Evolved Neural Network/HC Hybrid for Tablature Creation in GA- based Guitar Arranging},
}

@article{,
title={POLYPHONIC PIANO TRANSCRIPTION USING NON-NEGATIVE MATRIX FACTORISATION WITH GROUP SPARSITY Ken O ' Hanlon and Mark D . Plumbley Queen Mary University of London},
volume={1},
year={2014},
number={May},
pages={3136--3140},
}

@article{Hinton2012a,
pages={82--97},
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition},
abstract={Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
pmid={13057166},
arxivId={1207.0580},
year={2012},
author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Kingsbury, Brian},
isbn={1053-5888},
issn={1053-5888},
journal={Ieee Signal Processing Magazine},
archivePrefix={arXiv},
doi={10.1109/MSP.2012.2205597},
eprint={1207.0580},
number={November},
}

@article{Chen2017b,
year={2017},
pmid={19932002},
author={Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
title={Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN},
isbn={0925-2312},
doi={10.1016/j.eswa.2016.10.065},
eprint={1404.7828},
volume={72},
abstract={Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
keywords={Deep neural network,Natural language processing,Sentiment analysis},
archivePrefix={arXiv},
issn={09574174},
arxivId={1404.7828},
journal={Expert Systems with Applications},
pages={221--230},
}

@article{Stanley2002b,
archivePrefix={arXiv},
doi={10.1162/106365602320169811},
eprint={1407.0576},
number={2},
issn={1063-6560},
pages={99--127},
isbn={1063-6560},
arxivId={1407.0576},
journal={Evolutionary Computation},
pmid={12180173},
title={Evolving Neural Networks through Augmenting Topologies},
year={2002},
author={Stanley, Kenneth O. and Miikkulainen, Risto},
abstract={The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
volume={10},
url={http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
}

@article{He2015a,
doi={10.1145/2808196.2811641},
author={He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
abstract={This paper presents our system design for the Audio-Visual Emotion Challenge (AV +EC 2015). Besides the baseline features, we extract from audio the functionals on low-level descriptors (LLDs) obtained via the YAAFE toolbox, and from video the Local Phase Quantization from Three Or- thogonal Planes (LPQ-TOP) features. From the physiologi- cal signals, we extract 52 electro-cardiogram (ECG) features and 22 electro-dermal activity (EDA) features from various analysis domains. The extracted features along with the AV +EC 2015 baseline features of audio, ECG or EDA are concatenated for a further feature selection step, in which the concordance correlation coefficient (CCC), instead of the usual Pearson correlation coefficient (CC), has been used as objective function. In addition, offsets between the features and the arousal/valence labels are considered in both feature selection and modeling of the affective dimensions. For the fusion of multimodal features, we propose a Deep Bidirec- tional Long Short-Term Memory Recurrent Neural Network (DBLSTM-RNN) based multimodal affect prediction frame- work, in which the initial predictions from the single modali- ties via the DBLSTM-RNNs are firstly smoothed with Gaus- sian smoothing, then input into a second layer of DBLSTM- RNN for the final prediction of affective state. Experimental results show that our proposed features and the DBLSTM- RNN based fusion framework obtain very promising results. On the development set, the obtained CCC is up to 0.824 for arousal and 0.688 for valence, and on the test set, the CCC is 0.747 for arousal and 0.609 for valence},
year={2015},
pages={73--80},
title={Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks},
isbn={9781450337434},
url={http://dl.acm.org/citation.cfm?doid=2808196.2811641},
number={May},
journal={Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge - AVEC '15},
}

@article{DiPersio2016a,
abstract={We present an Artificial Neural Network (ANN) approach to predict stock market indices, particularly with respect to the forecast of their trend movements up or down. Exploiting different Neural Networks archi-tectures, we provide numerical analysis of concrete financial time series. In particular, after a brief r{\'{e}}sum{\'{e}} of the existing literature on the subject, we consider the Multi-layer Perceptron (MLP), the Convolutional Neural Net-works (CNN), and the Long Short-Term Memory (LSTM) recurrent neural networks techniques. We focus on the importance of choosing the correct input features, along with their preprocessing, for the specific learning algo-rithm one wants to use. Eventually, we consider the S{\&}P500 historical time series, predicting trend on the basis of data from the past days, and proposing a novel approach based on combination of wavelets and CNN, which outperforms the basic neural networks ones. We show, that neural networks are able to predict financial time series movements even trained only on plain time series data and propose more ways to improve results.},
pages={403--413},
keywords={Artificial neural networks,Convolutional neural network,Deep Learning,Financial forecasting,Long shortterm memory,Multi-layer neural network,Recurrent neural network,Stock markets analysis,Time series analysis},
title={Artificial neural networks architectures for stock price prediction: Comparisons and applications},
issn={19984464},
volume={10},
year={2016},
author={Di Persio}, Luca and Honchar, Oleksandr},
journal={International Journal of Circuits, Systems and Signal Processing},
}

@article{Zhao2016a,
url={http://arxiv.org/abs/1607.06997},
arxivId={1607.06997},
abstract={Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
archivePrefix={arXiv},
doi={10.1007/978-3-319-46475-6_27},
eprint={1607.06997},
pages={1--18},
title={Peak-Piloted Deep Network for Facial Expression Recognition},
author={Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
year={2016},
}

@article{Wu2015a,
title={Drum transcription using partially fixed non-negative matrix factorization},
isbn={9780992862633},
journal={2015 23rd European Signal Processing Conference, EUSIPCO 2015},
doi={10.1109/EUSIPCO.2015.7362590},
keywords={Automatic Music Transcription,Drum Transcription,MIR,NMF},
pages={1281--1285},
year={2015},
abstract={In this paper, a drum transcription algorithm using partially fixed non-negative matrix factorization is presented. The proposed method allows users to identify percussive events in complex mixtures with a minimal training set. The algorithm decomposes the music signal into two parts: percussive part with pre-defined drum templates and harmonic part with undefined entries. The harmonic part is able to adapt to the music content, allowing the algorithm to work in polyphonic mixtures. Drum event times can be simply picked from the percussive activation matrix with onset detection. The system is efficient and robust even with a minimal training set. The recognition rates for the ENST dataset vary from 56.7 to 78.9{\%} for three percussive instruments extracted from polyphonic music.},
author={Wu, Chih Wei and Lerch, Alexander},
}

@article{Sotelo2017b,
author={Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Kastner, Kyle},
title={C Har 2W Av : E Nd - To -E Nd S Peech S Ynthesis},
pages={1--6},
number={2015},
year={2017},
}

@article{Wu2016a,
title={Investigating gated recurrent networks for speech synthesis},
eprint={1601.02539},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
author={Wu, Zhizheng and King, Simon},
pages={5140--5144},
volume={2016-May},
archivePrefix={arXiv},
arxivId={1601.02539},
abstract={Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
doi={10.1109/ICASSP.2016.7472657},
isbn={9781479999880},
issn={15206149},
keywords={Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
year={2016},
}

@article{Ding2017,
arxivId={1609.06591},
author={Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
eprint={1609.06591},
year={2017},
pages={118--126},
title={FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition},
issn={2160-7508},
abstract={Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully- connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
journal={Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
doi={10.1109/FG.2017.23},
isbn={9781509040230},
archivePrefix={arXiv},
}

@article{Wang2016b,
issn={16875273},
journal={Computational Intelligence and Neuroscience},
year={2016},
volume={2016},
abstract={In recent years, financialmarket dynamics forecasting has been a focus of economic research. To predict the price indices of stock markets,we developed an architecturewhich combinedElman recurrent neural networkswith stochastic time effective function.By analyzing the proposedmodelwith the linear regression, complexity invariant distance (CID), andmultiscaleCID(MCID) analysis methods and taking themodel compared with differentmodels such as the backpropagation neural network (BPNN), the stochastic time effective neural network (STNN), and the Elman recurrent neural network (ERNN), the empirical results show that the proposed neural network displays the best performance among these neural networks in financial time series forecasting. Further, the empirical research is performed in testing the predictive effects of SSE, TWSE, KOSPI, and Nikkei225 with the established model, and the corresponding statistical comparisons of the above market indices are also exhibited. The experimental results show that this approach gives good performance in predicting the values fromthe stockmarket indices. 1.},
title={Financial Time Series Prediction Using Elman Recurrent Random Neural Networks},
isbn={1687-5265},
author={Wang, Jie and Wang, Jun and Fang, Wen and Niu, Hongli},
doi={10.1155/2016/4742515},
}

@article{Zweig2017,
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
archivePrefix={arXiv},
issn={15206149},
pages={4805--4809},
title={Advances in all-neural speech recognition},
keywords={CTC,end-to-end training,recurrent neural network,speech recognition},
year={2017},
arxivId={1609.05935},
author={Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
doi={10.1109/ICASSP.2017.7953069},
eprint={1609.05935},
abstract={This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.},
isbn={9781509041176},
}

@article{Krishna2016a,
title={Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks The MIT Faculty has made this article openly available . Please share Citation " Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Publisher Version},
author={Krishna, Tushar and Emer, Joel and Sze, Vivienne and Conference, International Solid-state Circuits and Francisco, San and Chen, Yu-hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
year={2016},
}

@article{Zhang2017a,
author={Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon and Zhang, Saizheng and Bengio, Cesar Laurent Yoshua and Courville, Aaron},
url={http://arxiv.org/abs/1701.02720},
arxivId={1701.02720},
abstract={Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.},
doi={10.21437/Interspeech.2016-1446},
archivePrefix={arXiv},
title={Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks},
year={2017},
eprint={1701.02720},
}

@article{Esteva2017a,
pages={115--118},
pmid={28117445},
publisher={Nature Publishing Group},
title={Dermatologist-level classification of skin cancer with deep neural networks},
number={7639},
doi={10.1038/nature21056},
url={http://dx.doi.org/10.1038/nature21056},
year={2017},
volume={542},
isbn={0028-0836},
author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
abstract={Skin cancer, the most common human malignancy1, 2, 3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4, 5 show potential for general and highly variable tasks across many fine-grained object categories6, 7, 8, 9, 10, 11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care. 最も多いヒト悪性腫瘍である皮膚がんは、主に視覚的に診断され、初期臨床スクリーニングが行われた後、ダーモスコピー解析、生検、および病理組織検査が行われることがある。画像を用いた皮膚病変の自動分類は、皮膚病変の外観には細かなばらつきがあるため、難しい課題である。深層畳み込みニューラルネットワーク（CNN）には、多くの細かな対象カテゴリー全体にわたり、一般的で変動性の高い課題をこなせる可能性がある。本研究で我々は、入力としてピクセルと疾病ラベルのみを用い、画像からエンドツーエンドで直接学習させた、単一のCNNを用いた皮膚病変の分類を実証する。我々は、2032の異なる疾病からなる12万9450の臨床画像（以前のデータセットよりも2桁多い）のデータセットを用いてCNNを学習させた。我々は生検で確認した臨床画像について、このCNNの成績を21人の皮膚科認定医と比較検証した。ここでは「角化細胞がん」対「良性脂漏性角化症」、「悪性黒色腫」対「良性母斑」という、2つの重要な二項分類使用症例を用いた。前者は最も多いがんの識別であり、後者は最も致命的な皮膚がんの識別である。CNNはどちらの課題においても、試験に参加した全ての専門家と同等の成績を達成したことから、人工知能は皮膚科医に相当する能力で皮膚がんを分類できることが示された。深層ニューラルネットワークを装備した携帯デバイスにより、皮膚科医の診察が診療所の外でも受けられる可能性がある。2021年までにスマートフォン契約者は63億人に達すると見込まれ、従って、重要な診断があらゆる場所で安価に受けられるようになるかもしれない。},
issn={14764687},
journal={Nature},
}

@article{Saxena2016b,
author={Saxena, Shreyas and Verbeek, Jakob},
arxivId={1606.02492},
eprint={1606.02492},
url={http://arxiv.org/abs/1606.02492},
year={2016},
archivePrefix={arXiv},
abstract={Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
issn={10495258},
title={Convolutional Neural Fabrics},
}

@article{Arik2017a,
arxivId={1702.07825},
author={Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
abstract={We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
title={Deep Voice: Real-time Neural Text-to-Speech},
url={http://arxiv.org/abs/1702.07825},
eprint={1702.07825},
issn={1938-7228},
year={2017},
number={Icml},
archivePrefix={arXiv},
}

@article{Wang2017,
issn={19909772},
eprint={1703.10135},
author={Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc},
abstract={A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix={arXiv},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords={Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.},
pages={4006--4010},
title={Tacotron: Towards end-To-end speech synthesis},
doi={10.21437/Interspeech.2017-1452},
volume={2017-Augus},
year={2017},
arxivId={1703.10135},
}

@article{Omar2017a,
abstract={This study explores the effectiveness of an Artificial Neural Network (ANN) in predicting fraudulent financial reporting in small market capitalization companies in Malaysia. Design/methodology/approach Based on the concepts of ANN, a mathematical model is developed to compare non-fraud and fraud companies selected from among small market capitalization companies in Malaysia; the fraud companies had already been charged by the Securities Commission for the falsification of financial statements. Ten financial ratios are used as fraud risk indicators to predict fraudulent financial reporting using ANN. Findings Indicate that the proposed ANN methodology outperforms other statistical techniques widely used for predicting fraudulent financial reporting. Originality/value The study is one of few to adopt the ANN approach to the prediction of financial reporting fraud.},
number={2},
title={Predicting fraudulent financial reporting using artificial neural network},
isbn={1359079051062},
url={http://dx.doi.org/10.1108/eb025814{\%}5Cnhttp://},
year={2017},
doi={10.1108/13590791011082797},
volume={24},
journal={Journal of Financial Crime Iss},
author={Omar, Normah and Johari, Zulaikha 'Amirah and Smith, Malcolm},
issn={1359-0790},
}

@article{Deng2015a,
year={2015},
number={2},
doi={10.1007/s11432-014-5269-3},
journal={Science China Information Sciences},
title={Extreme learning machines: new trends and applications},
url={http://link.springer.com/10.1007/s11432-014-5269-3},
author={Deng, ChenWei and Huang, GuangBin and Xu, Jia and Tang, JieXiong},
abstract={Extreme learning machine (ELM), as a new learning framework, draws increasing attractions in the areas of large-scale computing, high-speed signal processing, artificial intelligence, and so on. ELM aims to break the barriers between the conventional artificial learning techniques and biological learning mechanism and represents a suite of machine learning techniques in which hidden neurons need not to be tuned. ELM theories and algorithms argue that “random hidden neurons” capture the essence of some brain learning mechanisms as well as the intuitive sense that the efficiency of brain learning need not rely on computing power of neurons. Thus, compared with traditional neural networks and support vector machine, ELM offers significant advantages such as fast learning speed, ease of implementation, and minimal human intervention. Due to its remarkable generalization performance and implementation efficiency, ELM has been applied in various applications. In this paper, we first provide an overview of newly derived ELM theories and approaches. On the other hand, with the ongoing development of multilayer feature representation, some new trends on ELM-based hierarchical learning are discussed. Moreover, we also present several interesting ELM applications to showcase the practical advances on this subject.},
issn={1674-733X},
pages={1--16},
volume={58},
}

@article{Phumrattanaprapin2016a,
keywords={chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
title={Machine with Multilayer Perceptron},
volume={10},
year={2016},
pages={196--204},
author={Phumrattanaprapin, Khanittha},
number={2},
}

@article{Shi2016a,
issn={10450823},
doi={10.1109/TNNLS.2017.2705682},
journal={IJCAI International Joint Conference on Artificial Intelligence},
abstract={In this paper, we propose a novel method to im-prove object recognition accuracies of convolu-tional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The Min-Max objective explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin be-tween different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computa-tion cost. Experiments with shallow and deep mod-els on four benchmark datasets including CIFAR-10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max ob-jective achieve remarkable performance improve-ments compared to the corresponding baseline models.},
keywords={Machine Learning},
year={2016},
volume={2016-Janua},
pages={2004--2010},
title={Improving CNN performance with min-max objective},
author={Shi, Weiwei and Gong, Yihong and Wang, Jinjun},
}

@article{Zhang2016c,
issn={15206149},
archivePrefix={arXiv},
isbn={9781509041176},
eprint={1610.03022},
year={2016},
url={http://arxiv.org/abs/1610.03022},
pages={10--14},
abstract={Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
arxivId={1610.03022},
title={Very Deep Convolutional Networks for End-to-End Speech Recognition},
doi={10.1109/ICASSP.2017.7953077},
author={Zhang, Yu and Chan, William and Jaitly, Navdeep},
}

@article{Floreano2008b,
number={1},
keywords={Evolution,Learning,Neural networks},
volume={1},
title={Neuroevolution: From architectures to learning},
journal={Evolutionary Intelligence},
issn={18645909},
pages={47--62},
year={2008},
doi={10.1007/s12065-007-0002-4},
isbn={1206500700},
abstract={Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
author={Floreano, Dario and D{\"{u}}rr, Peter and Mattiussi, Claudio},
}

@article{Tang2015a,
pages={1--13},
journal={IEEE Transactions on Neural Networks and Learning Systems},
title={Extreme Learning Machine for Multilayer Perceptron},
abstract={— Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parame-ters are randomly generated and the output weights are analyti-cally computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme. Index Terms— Deep learning (DL), deep neural network (DNN), extreme learning machine (ELM), multilayer perceptron (MLP), random feature mapping.},
issn={2162-237X},
year={2015},
author={Tang, Jiexiong and ChenweiDeng and Huang, Guang-Bin},
doi={10.1109/TNNLS.2015.2424995},
pmid={25966483},
isbn={2162-2388 (Electronic) 2162-237X (Linking)},
}

@article{Schaffner2017,
author={Schaffner, Michael and Member, Student and Scheidegger, Florian and Cavigelli, Lukas and Member, Student and Kaeslin, Hubert and Member, Senior and Benini, Luca and Smolic, Aljosa},
title={Ro of Ro of},
volume={4},
year={2017},
}

@article{Shizhou2016b,
issn={10450823},
volume={2016-Janua},
journal={IJCAI International Joint Conference on Artificial Intelligence},
keywords={Machine Learning},
abstract={In this paper, we choose to learn useful cues from object recognition mechanisms of the human vi-sual cortex, and propose a DCNN performance im-provement method without the need for increasing the network complexity. Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As ex-perimental results show, when applying the pro-posed method to the " Quick " model and NIN models, image classification performances are re-markably improved on four widely used bench-mark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
author={Shizhou, Zhang and Gong, Yihong and Jinjun, Wang},
title={Improving DCNN performance with sparse category-selective objective function},
year={2016},
pages={2343--2349},
}

@article{Liao2016b,
abstract={Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
year={2016},
journal={2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
arxivId={1508.00330},
archivePrefix={arXiv},
doi={10.1109/WACV.2016.7477624},
eprint={1508.00330},
author={Liao, Zhibin and Carneiro, Gustavo},
isbn={9781509006410},
title={On the importance of normalisation layers in deep learning with piecewise linear activation units},
}

@article{Ludermir2006a,
volume={17},
doi={10.1109/TNN.2006.881047},
abstract={This paper introduces a methodology for neural network global optimization. The aim is the simultaneous optimization of multilayer perceptron (MLP) network weights and architectures, in order to generate topologies with few connections and high classification performance for any data sets. The approach combines the advantages of simulated annealing, tabu search and the backpropagation training algorithm in order to generate an automatic process for producing networks with high classification performance and low complexity. Experimental results obtained with four classification problems and one prediction problem has shown to be better than those obtained by the most commonly used optimization techniques.},
author={Ludermir, Teresa B. and Yamazaki, Akio and Zanchettin, Cleber},
issn={10459227},
pages={1452--1459},
number={6},
pmid={17131660},
title={An optimization methodology for neural network weights and architectures},
journal={IEEE Transactions on Neural Networks},
keywords={Multilayer perceptron (MLP),Optimization of weights and architectures,Simulating annealing,Tabu search},
year={2006},
}

@article{Tsai2006a,
doi={10.1109/TNN.2005.860885},
pmid={16526477},
pages={69--80},
abstract={In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.},
keywords={Genetic algorithm (GA),Neural networks (NN),Taguchi method},
number={1},
author={Tsai, Jinn Tsong and Chou, Jyh Horng and Liu, Tung Kuan},
year={2006},
journal={IEEE Transactions on Neural Networks},
issn={10459227},
volume={17},
title={Tuning the structure and parameters of a neural network by using hybrid Taguchi-genetic algorithm},
isbn={1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
}

@article{Leung2003a,
arxivId={arXiv:1403.7012v1},
author={Leung, F.H.F. and Lam, H.K. and Ling, S.H. and Tam, P.K.S.},
url={http://ieeexplore.ieee.org/document/1176129/},
pmid={18237992},
issn={1045-9227},
eprint={arXiv:1403.7012v1},
number={1},
isbn={1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
archivePrefix={arXiv},
volume={14},
year={2003},
pages={79--88},
title={Tuning of the structure and parameters of a neural network using an improved genetic algorithm},
journal={IEEE Transactions on Neural Networks},
doi={10.1109/TNN.2002.804317},
}

@article{McCulloch1943a,
isbn={0007-4985},
archivePrefix={arXiv},
author={McCulloch, Warren S. and Pitts, Walter},
abstract={Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
eprint={arXiv:1011.1669v3},
arxivId={arXiv:1011.1669v3},
journal={The Bulletin of Mathematical Biophysics},
number={4},
title={A logical calculus of the ideas immanent in nervous activity},
year={1943},
volume={5},
pmid={2185863},
doi={10.1007/BF02478259},
pages={115--133},
issn={00074985},
}

@article{Hopfield1982a,
eprint={arXiv:1411.3159v1},
pages={2554--2558},
isbn={0027-8424},
title={Neural networks and physical systems with emergent collective computational abilities.},
arxivId={arXiv:1411.3159v1},
author={Hopfield, J. J.},
number={8},
year={1982},
journal={Proceedings of the National Academy of Sciences},
issn={0027-8424},
url={http://www.pnas.org/cgi/doi/10.1073/pnas.79.8.2554},
abstract={Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
doi={10.1073/pnas.79.8.2554},
volume={79},
archivePrefix={arXiv},
pmid={6953413},
}

@article{Benardos2007a,
journal={Engineering Applications of Artificial Intelligence},
issn={09521976},
title={Optimizing feedforward artificial neural network architecture},
volume={20},
year={2007},
abstract={Despite the fact that feedforward artificial neural networks (ANNs) have been a hot topic of research for many years there still are certain issues regarding the development of an ANN model, resulting in a lack of absolute guarantee that the model will perform well for the problem at hand. The multitude of different approaches that have been adopted in order to deal with this problem have investigated all aspects of the ANN modelling procedure, from training data collection and pre/post-processing to elaborate training schemes and algorithms. Increased attention is especially directed to proposing a systematic way to establish an appropriate architecture in contrast to the current common practice that calls for a repetitive trial-and-error process, which is time-consuming and produces uncertain results. This paper proposes such a methodology for determining the best architecture and is based on the use of a genetic algorithm (GA) and the development of novel criteria that quantify an ANN's performance (both training and generalization) as well as its complexity. This approach is implemented in software and tested based on experimental data capturing workpiece elastic deflection in turning. The intention is to present simultaneously the approach's theoretical background and its practical application in real-life engineering problems. Results show that the approach performs better than a human expert, at the same time offering many advantages in comparison to similar approaches found in literature. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author={Benardos, P. G. and Vosniakos, G. C.},
pages={365--382},
doi={10.1016/j.engappai.2006.06.005},
keywords={ANN architecture,Engineering problems,Feedforward artificial neural networks,Generalization,Genetic algorithms},
isbn={0952-1976},
number={3},
}

@article{Deng2017,
pages={1164--1177},
journal={IEEE Transactions on Neural Networks and Learning Systems},
doi={10.1109/TNNLS.2016.2514368},
author={Deng, Shuiguang and Huang, Longtao and Xu, Guandong and Wu, Xindong and Wu, Zhaohui},
issn={21622388},
abstract={With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major benefit of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user's trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users' interests and their trusted friends' interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.},
isbn={2162-2388 (Electronic)$\backslash$r2162-237X (Linking)},
pmid={26915135},
title={On Deep Learning for Trust-Aware Recommendations in Social Networks},
keywords={Deep learning,Recommender systems (RSs),Social network,Trust},
volume={28},
number={5},
year={2017},
}

@article{Hornik1991a,
issn={08936080},
year={1991},
title={Approximation capabilities of multilayer feedforward networks},
author={Hornik, Kurt},
eprint={arXiv:1011.1669v3},
abstract={We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp($\mu$) performance criteria, for arbitrary finite input environment measures $\mu$, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. {\textcopyright} 1991.},
doi={10.1016/0893-6080(91)90009-T},
number={2},
pmid={25246403},
arxivId={arXiv:1011.1669v3},
journal={Neural Networks},
pages={251--257},
volume={4},
archivePrefix={arXiv},
keywords={Activation function,Input environment measure,Lp($\mu$) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
isbn={0893-6080},
}

@article{Elman1990,
author={Elman, J},
journal={Cognitive Science},
pages={179--211},
url={http://linkinghub.elsevier.com/retrieve/pii/036402139090002E{\%}0Apapers2://publication/uuid/BF1D3C8F-9A67-4350-ACE9-30BDA3C1FEC8},
abstract={Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report},
volume={14},
number={1 990},
year={1990},
isbn={0364-0213},
title={Finding structure in time* 1},
}

@article{DavidA.Cacchione1997,
isbn={2136240900},
issn={0003-0996},
number={2},
author={David A. Cacchione},
pages={108--112},
year={1997},
title={American Scientist},
doi={10.1511/2011.89.106},
volume={85},
}

@article{Boser1992a,
isbn={089791497X},
url={http://portal.acm.org/citation.cfm?doid=130385.130401},
abstract={A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
journal={Proceedings of the fifth annual workshop on Computational learning theory  - COLT '92},
pmid={25246403},
archivePrefix={arXiv},
eprint={arXiv:1011.1669v3},
title={A training algorithm for optimal margin classifiers},
issn={0-89791-497-X},
arxivId={arXiv:1011.1669v3},
author={Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
doi={10.1145/130385.130401},
pages={144--152},
year={1992},
}

@article{Graves2014a,
issn={2041-1723},
title={Neural Turing Machines},
eprint={1410.5401},
abstract={We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
pages={1--26},
doi={10.3389/neuro.12.006.2007},
url={http://arxiv.org/abs/1410.5401},
archivePrefix={arXiv},
isbn={0028-0836},
arxivId={1410.5401},
pmid={18958277},
year={2014},
}

@article{Paper2016,
number={July},
doi={10.1109/IJCNN.2016.7727308},
title={SAM : A Rethinking of prominent convolutional neural network architectures for visual object recognition SAM : A Rethinking of Prominent Convolutional Neural Network Architectures for},
author={Paper, Conference},
year={2016},
}

@article{Hornik1989a,
archivePrefix={arXiv},
isbn={08936080 (ISSN)},
title={Multilayer feedforward networks are universal approximators},
volume={2},
year={1989},
pmid={74},
eprint={arXiv:1011.1669v3},
doi={10.1016/0893-6080(89)90020-8},
issn={08936080},
journal={Neural Networks},
abstract={This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
arxivId={arXiv:1011.1669v3},
author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
keywords={Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number={5},
pages={359--366},
}

@article{,
archivePrefix={arXiv},
title={Lecun-98},
arxivId={1102.0183},
isbn={0018-9219},
doi={10.1109/5.726791},
pmid={15823584},
eprint={1102.0183},
issn={00189219},
}

@article{Guang-BinHuang2014a,
doi={10.1109/IJCNN.2004.1380068},
url={http://ieeexplore.ieee.org/document/1380068/},
volume={2},
year={2014},
number={February},
author={Guang-Bin Huang} and {Qin-Yu Zhu} and {Chee-Kheong Siew},
pages={985--990},
title={Extreme learning machine: a new learning scheme of feedforward neural networks},
isbn={0-7803-8359-1},
journal={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
}

@misc{Edmunds2000,
pmid={10272},
title={The problem of information overload in business organisations: a review of the literature},
url={http://www.sciencedirect.com/science/article/pii/S0268401299000511},
isbn={0268-4012},
abstract={This paper reviews the literature on the problem of information overload, with particular reference to business organisations. The literature reveals that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognised and experienced. Both perceptions and the actual effects of information overload have been exacerbated by the rapid advances made in information and communication technology, although it is not clear cut as to whether the Internet has worsened or improved the situation. A theme stressed in the literature is the paradoxical situation that, although there is an abundance of information available, it is often difficult to obtain useful, relevant information when it is needed. Some solutions put forward to reduce information overload are: a reduction in the duplication of information found in the professional literature; the adoption of personal information management strategies, together with the integration of software solutions such as push technology and intelligent agents; and the provision of value-added information (filtered by software or information specialists). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
doi={10.1016/S0268-4012(99)00051-1},
volume={20},
year={2000},
author={Edmunds, Angela and Morris, Anne},
booktitle={International Journal of Information Management},
number={1},
pages={17--28},
issn={02684012},
keywords={infoglut,information fatigue syndrome,information overload},
}

@article{Bengio2007a,
author={Bengio, Yoshua and Lecun, Yann},
pages={1--41},
year={2007},
number={1},
title={Scaling Learning Algorithms towards AI To appear in “ Large-Scale Kernel Machines ”},
}

@article{Kim2014a,
author={Kim, Yoon},
url={http://arxiv.org/abs/1408.5882},
arxivId={1408.5882},
doi={10.3115/v1/D14-1181},
archivePrefix={arXiv},
issn={10709908},
pmid={10463930},
year={2014},
eprint={1408.5882},
isbn={9781937284961},
abstract={We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
title={Convolutional Neural Networks for Sentence Classification},
}

@article{Paper2012,
year={2012},
keywords={associative memory,hebbian,hopfield model,learning,perceptron learning},
number={February 2014},
title={Storage Capacity of the Hopfield Network Associative Memory},
author={Paper, Conference},
doi={10.1109/ICICTA.2012.89},
}

@article{Tang2014a,
journal={Proceedings of the 2014 Conferenve on Empirical Methods in Natural Language Processing (EMNLP)},
title={A Joint Segmentation and Classification Framework for Sentiment Analysis},
year={2014},
issn={2329-9290},
isbn={2329-9290 VO  - 23},
pages={477--487},
volume={23},
number={2002},
abstract={In this paper, we propose a joint segmenta- tion and classification framework for sen- timent analysis. Existing sentiment clas- sification algorithms typically split a sen- tence as a word sequence, which does not effectively handle the inconsistent senti- ment polarity between a phrase and the words it contains, such as “not bad” and “a great deal of ”. We address this issue by developing a joint segmentation and classification framework (JSC), which si- multaneously conducts sentence segmen- tation and sentence-level sentiment classi- fication. Specifically, we use a log-linear model to score each segmentation candi- date, and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier. A marginal log-likelihood objective function is de- vised for the segmentation model, which is optimized for enhancing the sentiment classification performance. The joint mod- el is trained only based on the annotat- ed sentiment polarity of sentences, with- out any segmentation annotations. Experi- ments on a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that, our joint model performs com- parably with the state-of-the-art methods.},
author={Tang, Duyu and Wei, Furu and Qin, Bing and Dong, Li and Liu, Ting and Zhou, Ming},
doi={10.1109/TASLP.2015.2449071},
}

@article{Boulanger-lewandowski,
title={PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS Universit ´ e de Montr ´ Montr ´ Jasha Droppo Mike Seltzer Dong Yu One Microsoft Way},
author={Boulanger-lewandowski, Nicolas},
number={5},
}

@article{Bagozzi2006,
author={Bagozzi, Richard P. and Dholakia, Utpal M.},
number={7},
url={http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1060.0545},
abstract={We conceptualize participation in Linux user groups (LUGs) in terms of group-referent intentional actions and investigate cognitive (attitudes, perceived behavioral control, identification with the open source movement), affective (positive and negative anticipated emotions), and social (social identity) determinants of participation and its consequences on Linux-related behaviors of users. This survey-based study, conducted with 402 active LUG members representing 191 different LUGs from 23 countries and employing structural equation modeling methodology, supports the proposed model. Furthermore, we find that the Linux user's experience level moderates the extent of the LUG's social influence and its impact on the user's participation. We conclude with a consideration of the managerial and research implications of the study's findings.},
doi={10.1287/mnsc.1060.0545},
isbn={0025-1909},
issn={0025-1909},
pmid={21517591},
year={2006},
title={Open Source Software User Communities: A Study of Participation in Linux User Groups},
volume={52},
pages={1099--1115},
journal={Management Science},
keywords={2004,accepted by eric von,anticipated emotions,for 3 revisions,hippel and georg von,history,krogh,linux,model of goal-directed behavior,novice versus experienced users,open source software,received september 1,social identity,special issue editors,the authors 4 months,this paper was with,virtual communities,we-intentions},
}

@article{Balle2016c,
archivePrefix={arXiv},
arxivId={1611.01704},
isbn={0197-3975},
abstract={We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
title={End-to-end Optimized Image Compression},
year={2016},
issn={01973975},
doi={10.1016/S0197-3975(03)00059-6},
url={http://arxiv.org/abs/1611.01704},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
eprint={1611.01704},
pmid={16508805},
}

@article{Arik2017b,
eprint={1702.07825},
arxivId={1702.07825},
number={Icml},
archivePrefix={arXiv},
url={http://arxiv.org/abs/1702.07825},
year={2017},
author={Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
abstract={We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
issn={1938-7228},
title={Deep Voice: Real-time Neural Text-to-Speech},
}

@misc{,
title={Estelles-Arolas, Gonzalez-Ladron-de-Guevara - 2012 - Towards an integrated crowdsourcing definition.pdf},
}

@article{DIppolito2014,
isbn={0166-4972},
year={2014},
pages={716--730},
title={The importance of design for firmscompetitiveness: A review of the literature},
volume={34},
doi={10.1016/j.technovation.2014.01.007},
author={D'Ippolito, Beatrice},
abstract={Scholars dedicated increasing attention towards appreciating how design has changed individualsperception of new products, firmsunderstanding and formulation of strategy, or other relevant actorsapproach to innovation and technology management. By emphasising the importance of design for the definition of consumersneeds, the restructuring of firmsorganisational structures and strategies, and the evolution of firmsvalue creation processes, this review paper identifies relevant research gaps and questions that would benefit from future scholarly attention. In particular, it is suggested that such effort should address the analysis of how design consumption can help better comprehend consumersneeds; what are the implications of design thinking on the skill sets of design professionals; the organisational structure of firms, including the reconfiguration of other business functions, and their strategy; and whether and how design thinking can shape firmsvalue creation processes and contribute to the formalisation of design tasks.},
issn={01664972},
number={11},
keywords={Consumers' needs,Design,Firm competitiveness,Literature review,Research gaps,Strategy making,Value creation},
pmid={1629546633},
journal={Technovation},
}

@article{Arik2017,
archivePrefix={arXiv},
abstract={We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
issn={1938-7228},
keywords={speech synthesis,state of the art},
mendeley-tags={speech synthesis,state of the art},
number={Icml},
url={http://arxiv.org/abs/1702.07825},
year={2017},
title={Deep Voice: Real-time Neural Text-to-Speech},
arxivId={1702.07825},
author={Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad and Others and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad and Others},
eprint={1702.07825},
journal={arXiv preprint arXiv:1702.07825},
}

@article{Balle2016a,
issn={01973975},
keywords={image compression,image compression- lossy},
journal={arXiv preprint arXiv:1611.01704},
year={2016},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
title={End-to-end Optimized Image Compression},
url={http://arxiv.org/abs/1611.01704},
archivePrefix={arXiv},
pmid={16508805},
doi={10.1016/S0197-3975(03)00059-6},
isbn={0197-3975},
mendeley-tags={image compression,image compression- lossy},
arxivId={1611.01704},
eprint={1611.01704},
abstract={We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
}

@article{Kim2014,
doi={10.3115/v1/D14-1181},
journal={arXiv preprint arXiv:1408.5882},
abstract={We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
archivePrefix={arXiv},
author={Kim, Yoon},
arxivId={1408.5882},
url={http://arxiv.org/abs/1408.5882},
eprint={1408.5882},
mendeley-tags={sentence classification,sentiment analysis},
isbn={9781937284961},
title={Convolutional Neural Networks for Sentence Classification},
year={2014},
issn={10709908},
keywords={sentence classification,sentiment analysis},
}

@article{Hoover2012,
year={2012},
isbn={9781905254668},
journal={International Conference on Computational Creativity},
keywords={music generation},
pages={111},
author={Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
mendeley-tags={music generation},
title={Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding},
}

@article{Zweig2016,
title={Advances in all-neural speech recognition},
mendeley-tags={speech recognition},
author={Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
year={2017},
archivePrefix={arXiv},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
institution={IEEE},
keywords={CTC,end-to-end training,recurrent neural network,speech recognition},
issn={15206149},
abstract={This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.},
arxivId={1609.05935},
isbn={9781509041176},
pages={4805--4809},
url={http://arxiv.org/abs/1609.05935},
eprint={1609.05935},
doi={10.1109/ICASSP.2017.7953069},
}

@article{Risi2017,
journal={IEEE Transactions on Computational Intelligence and AI in Games},
doi={10.1109/TCIAIG.2015.2494596},
arxivId={1410.7326},
mendeley-tags={evolutive,games,review},
volume={9},
abstract={This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
issn={1943068X},
archivePrefix={arXiv},
publisher={IEEE},
eprint={1410.7326},
title={Neuroevolution in Games: State of the Art and Open Challenges},
number={1},
keywords={Evolutionary algorithms,evolutive,games,neural networks,neuroevolution,review},
year={2017},
isbn={1943-068X VO  - PP},
pages={25--41},
author={Risi, Sebastian and Togelius, Julian},
}

@article{Gatys2016,
abstract={Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic in-formation and, thus, allow to separate image content from style. Here we use image representations derived from Con-volutional Neural Networks optimised for object recogni-tion, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can sep-arate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an ar-bitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neu-ral Networks and demonstrate their potential for high level image synthesis and manipulation.},
pages={2414--2423},
author={Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
pmid={15430064963552939126},
year={2016},
eprint={1505.07376},
isbn={9781467388511},
arxivId={1505.07376},
journal={The IEEE conference on computer vision and pattern recognition},
issn={10636919},
mendeley-tags={image synthesis},
doi={10.1109/CVPR.2016.265},
archivePrefix={arXiv},
title={Image style transfer using convolutional neural networks},
keywords={image synthesis},
}

@article{wang2017tacotron,
mendeley-tags={speech synthesis,state of the art},
archivePrefix={arXiv},
doi={10.21437/Interspeech.2017-1452},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords={Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.,speech synthesis,state of the art},
volume={2017-Augus},
issn={19909772},
abstract={A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
url={http://arxiv.org/abs/1703.10135},
pages={1--10},
eprint={1703.10135},
year={2017},
author={Wang, Yuxuan and Skerry-Ryan, RJ J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
title={Tacotron: Towards End-to-End Speech Synthesis},
arxivId={1703.10135},
}

@article{Khorrami2016,
journal={Image Processing (ICIP), 2016 IEEE International Conference on},
doi={10.1017/pasa.2016.3},
pages={619--623},
institution={IEEE},
title={How Deep Neural Networks Can Improve Emotion Recognition on Video Data},
eprint={1602.07377},
author={Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S. and {Le Paine}, Tom and Brady, Kevin and Dagli, Charlie and Huang, Thomas S. and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S. and {Le Paine}, Tom and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
mendeley-tags={sentiment analysis,video classification},
url={http://arxiv.org/abs/1602.07377},
year={2016},
archivePrefix={arXiv},
arxivId={1602.07377},
keywords={sentiment analysis,video classification},
abstract={We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
}

@article{stanley2007compositional,
isbn={1389-2576},
issn={13892576},
number={2},
journal={Genetic programming and evolvable machines},
volume={8},
year={2007},
pages={131--162},
publisher={Springer},
title={Compositional pattern producing networks: A novel abstraction of development},
doi={10.1007/s10710-007-9028-8},
abstract={Natural DNA can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks (CPPNs), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, CPPNs map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with CPPNs through interactive evolution of two dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
author={Stanley, Kenneth O.},
keywords={Artificial embryogeny,Complexity,Developmental encoding,Evolutionary computation,Generative systems,Indirect encoding,Representation},
}

@article{Wang2016a,
mendeley-tags={image classification},
institution={IEEE},
number={July},
year={2016},
keywords={image classification},
title={SAM : A Rethinking of Prominent Convolutional Neural Network Architectures for Visual Object Recognition},
journal={Neural Networks (IJCNN), 2016 International Joint Conference on},
pages={1008--1014},
author={Wang, Zhenyang and Deng, Zhidong and Wang, Shiyao},
doi={10.1109/IJCNN.2016.7727308},
isbn={9781509006205},
}

@article{Lin2013,
issn={03029743},
url={http://arxiv.org/abs/1312.4400},
year={2013},
author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
doi={10.1109/ASRU.2015.7404828},
keywords={image classification},
isbn={9781479972913},
abstract={We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
journal={arXiv preprint arXiv:1312.4400},
pmid={24356345},
archivePrefix={arXiv},
mendeley-tags={image classification},
eprint={1312.4400},
pages={1--10},
title={Network In Network},
arxivId={1312.4400},
}

@article{Ojha2017,
publisher={Elsevier},
keywords={Ensemble,Feedforward neural network,Metaheuristics,Multiobjective,Nature-inspired algorithms,evolutive,review},
eprint={arXiv:1705.05584v1},
abstract={Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such different viewpoints mainly to improve the FNN's generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimize the FNNs. Its success is evident from the FNN's application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.},
arxivId={arXiv:1705.05584v1},
issn={09521976},
title={Metaheuristic design of feedforward neural networks: A review of two decades of research},
number={2017},
author={Ojha, Varun Kumar and Abraham, Ajith and Sn{\'{a}}{\v{s}}el, V{\'{a}}clav},
doi={10.1016/j.engappai.2017.01.013},
volume={60},
mendeley-tags={evolutive,review},
year={2017},
archivePrefix={arXiv},
journal={Engineering Applications of Artificial Intelligence},
pages={97--116},
}

@article{Choi2016a,
year={2016},
archivePrefix={arXiv},
arxivId={1606.00298},
author={Choi, Keunwoo and Fazekas, George and Sandler, Mark},
title={Automatic tagging using deep convolutional neural networks},
abstract={We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
keywords={music classification},
url={http://arxiv.org/abs/1606.00298},
journal={arXiv preprint arXiv:1606.00298},
eprint={1606.00298},
mendeley-tags={music classification},
}

@article{Sak2015,
abstract={We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.},
pmid={1000285842},
url={http://arxiv.org/abs/1507.06947},
year={2015},
arxivId={1507.06947},
author={Sak, Ha$\backslash$csim Haşim and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
journal={arXiv preprint arXiv:1507.06947},
mendeley-tags={speech recognition},
title={Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition},
eprint={1507.06947},
keywords={speech recognition},
archivePrefix={arXiv},
}

@article{Ding2016,
title={FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition},
abstract={Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully- connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
arxivId={1609.06591},
institution={IEEE},
archivePrefix={arXiv},
author={Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
keywords={facial expression,state of the art},
url={http://arxiv.org/abs/1609.06591},
isbn={9781509040230},
doi={10.1109/FG.2017.23},
eprint={1609.06591},
year={2016},
journal={Automatic Face {\&} Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on},
pages={118--126},
mendeley-tags={facial expression,state of the art},
}

@article{Risi2017,
author={Risi, Sebastian and Togelius, Julian},
archivePrefix={arXiv},
volume={9},
abstract={This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
arxivId={1410.7326},
journal={IEEE Transactions on Computational Intelligence and AI in Games},
pages={25--41},
issn={1943068X},
mendeley-tags={evolutive,games,review},
number={1},
isbn={1943-068X VO  - PP},
title={Neuroevolution in Games: State of the Art and Open Challenges},
publisher={IEEE},
year={2017},
eprint={1410.7326},
doi={10.1109/TCIAIG.2015.2494596},
keywords={Evolutionary algorithms,evolutive,games,neural networks,neuroevolution,review},
}

@article{Stanley2002,
archivePrefix={arXiv},
number={2},
journal={Evolutionary Computation},
volume={10},
title={Evolving Neural Networks through Augmenting Topologies},
eprint={1407.0576},
abstract={The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
doi={10.1162/106365602320169811},
pmid={12180173},
arxivId={1407.0576},
author={Stanley, Kenneth O. and Miikkulainen, Risto},
publisher={MIT Press},
year={2002},
pages={99--127},
keywords={evolutive},
mendeley-tags={evolutive},
url={http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
isbn={1063-6560},
issn={1063-6560},
}

@article{Hopfield1982,
journal={Proceedings of the National Academy of Sciences},
abstract={Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
year={1982},
doi={10.1073/pnas.79.8.2554},
pages={2554--2558},
publisher={National Acad Sciences},
url={http://www.pnas.org/cgi/doi/10.1073/pnas.79.8.2554},
issn={0027-8424},
isbn={0027-8424},
pmid={6953413},
keywords={seminal},
mendeley-tags={seminal},
title={Neural networks and physical systems with emergent collective computational abilities.},
volume={79},
author={Hopfield, John J.},
number={8},
}

@article{wang2017tacotron,
archivePrefix={arXiv},
author={Wang, Yuxuan and Skerry-Ryan, RJ J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
pages={4006--4010},
issn={19909772},
title={Tacotron: Towards end-To-end speech synthesis},
url={http://arxiv.org/abs/1703.10135},
volume={2017-Augus},
keywords={Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.,speech synthesis,state of the art},
journal={Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
arxivId={1703.10135},
doi={10.21437/Interspeech.2017-1452},
mendeley-tags={speech synthesis,state of the art},
year={2017},
abstract={A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
eprint={1703.10135},
}

@article{Larsson2016,
isbn={9783319464923},
eprint={1603.06668},
issn={16113349},
abstract={We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
pages={577--593},
volume={9908 LNCS},
arxivId={1603.06668},
author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
doi={10.1007/978-3-319-46493-0_35},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
mendeley-tags={image synthesis},
year={2016},
institution={Springer},
keywords={image synthesis},
archivePrefix={arXiv},
title={Learning representations for automatic colorization},
}

@article{LeCun1998,
doi={10.1109/5.726791},
author={LeCun, Yann and Bottou, L{\'{e}}on L??on L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
mendeley-tags={seminal},
journal={Proceedings of the IEEE},
isbn={0018-9219},
publisher={IEEE},
arxivId={1102.0183},
volume={86},
eprint={1102.0183},
title={Gradient-based learning applied to document recognition},
keywords={Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR),seminal},
archivePrefix={arXiv},
issn={00189219},
pages={2278--2323},
year={1998},
abstract={Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
number={11},
pmid={15823584},
}

@article{Xu2015,
author={Xu, Wenduan and Auli, Michael and Clark, Stephen},
isbn={9781941643730},
journal={Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
pages={250--255},
doi={10.3115/v1/p15-2041},
publisher={Association for Computational Linguistics},
year={2015},
title={CCG Supertagging with a Recurrent Neural Network},
url={https://doi.org/10.3115/v1/p15-2041},
mendeley-tags={sentence classification},
number={2014},
keywords={sentence classification},
}

@article{Boser1992,
year={1992},
isbn={089791497X},
institution={ACM},
keywords={seminal},
journal={Proceedings of the fifth annual workshop on Computational learning theory  - COLT '92},
eprint={arXiv:1011.1669v3},
title={A training algorithm for optimal margin classifiers},
author={Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
url={http://portal.acm.org/citation.cfm?doid=130385.130401},
abstract={A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
archivePrefix={arXiv},
pages={144--152},
pmid={25246403},
doi={10.1145/130385.130401},
issn={0-89791-497-X},
mendeley-tags={seminal},
arxivId={arXiv:1011.1669v3},
}

@article{Boulanger-lewandowski2014,
number={2},
mendeley-tags={speech recognition},
institution={IEEE},
year={2014},
author={Boulanger-Lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong},
pages={5454--5458},
keywords={speech recognition},
title={PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS},
isbn={9781479928934},
journal={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
}

@article{Veit2016,
issn={10495258},
arxivId={1605.06431},
author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
eprint={1605.06431},
journal={Advances in Neural Information Processing Systems},
mendeley-tags={theory},
pages={1--9},
title={Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
url={http://arxiv.org/abs/1605.06431},
year={2016},
abstract={In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
archivePrefix={arXiv},
keywords={theory},
}

@article{Leung2003,
volume={14},
abstract={This paper presents the tuning of the structure and parameters of a neural network using an improved genetic algorithm (GA). It is also shown that the improved GA performs better than the standard GA based on some benchmark test functions. A neural network with switches introduced to its links is proposed. By doing this, the proposed neural network can learn both the input-output relationships of an application and the network structure using the improved GA. The number of hidden nodes is chosen manually by increasing it from a small number until the learning performance in terms of fitness value is good enough. Application examples on sunspot forecasting and associative memory are given to show the merits of the improved GA and the proposed neural network.},
mendeley-tags={evolutive},
publisher={IEEE},
isbn={1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
pmid={18237992},
title={Tuning of the structure and parameters of a neural network using an improved genetic algorithm.},
year={2003},
pages={79--88},
keywords={evolutive},
author={Leung, Frank Hung-Fat F and Lam, Hak-Keung K and Ling, Sai-Ho H and Tam, Peter Kwong-Shun S},
issn={1045-9227},
journal={IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
number={1},
doi={10.1109/TNN.2002.804317},
}

@article{Theis2015,
url={http://arxiv.org/abs/1506.03478},
year={2015},
journal={Advances in Neural Information Processing Systems},
arxivId={1506.03478},
keywords={image synthesis},
pages={1--9},
issn={10495258},
author={Theis, Lucas and Bethge, Matthias},
title={Generative Image Modeling Using Spatial LSTMs},
abstract={Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
eprint={1506.03478},
archivePrefix={arXiv},
mendeley-tags={image synthesis},
}

@article{Sigtia2016b,
arxivId={1508.01774v2},
title={An end-to-end neural network for polyphonic piano music transcription},
keywords={Automatic music transcription,Deep learning,Music language models,Recurrent neural networks,music transcription,state of the art},
doi={10.1109/TASLP.2016.2533858},
mendeley-tags={music transcription,state of the art},
author={Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
archivePrefix={arXiv},
number={5},
publisher={IEEE Press},
eprint={1508.01774v2},
pages={927--939},
volume={24},
abstract={We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yields the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
issn={23299290},
url={https://arxiv.org/abs/1508.01774},
journal={IEEE/ACM Transactions on Audio Speech and Language Processing},
year={2016},
}

@article{Sainath2015,
volume={64},
url={http://dx.doi.org/10.1016/j.neunet.2014.08.005},
abstract={Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12{\%}-14{\%} relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.},
mendeley-tags={speech recognition},
title={Deep Convolutional Neural Networks for Large-scale Speech Tasks},
year={2015},
issn={18792782},
author={Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and rahman Mohamed, Abdel-rahman and Dahl, George and Ramabhadran, Bhuvana},
keywords={Deep learning,Neural networks,Speech recognition,speech recognition},
pages={39--48},
journal={Neural Networks},
isbn={0893-6080},
pmid={25439765},
publisher={Elsevier Ltd},
doi={10.1016/j.neunet.2014.08.005},
}

@article{Park2017,
keywords={optimization},
volume={10112 LNCS},
mendeley-tags={optimization},
year={2017},
issn={16113349},
pages={189--204},
author={Park, Sungheon and Kwak, Nojun},
institution={Springer},
isbn={9783319541839},
doi={10.1007/978-3-319-54184-6_12},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
title={Analysis on the dropout effect in convolutional neural networks},
}

@article{Hutchings2017,
arxivId={1706.09558},
url={http://arxiv.org/abs/1706.09558},
eprint={1706.09558},
journal={arXiv preprint arXiv:1706.09558},
mendeley-tags={music generation,state of the art},
abstract={Presented is a method of generating a full drum kit part for a provided kick-drum sequence. A sequence to sequence neural network model used in natural language translation was adopted to encode multiple musical styles and an online survey was developed to test different techniques for sampling the output of the softmax function. The strongest results were found using a sampling technique that drew from the three most probable outputs at each subdivision of the drum pattern but the consistency of output was found to be heavily dependent on style.},
author={Hutchings, P.},
keywords={generative music,music generation,percussion,rnn,state of the art,translation},
archivePrefix={arXiv},
pages={43--47},
volume={1},
title={Talking Drums: Generating drum grooves with neural networks},
year={2017},
number={1},
}

@article{Zhang2017,
doi={10.21437/Interspeech.2016-1446},
eprint={1701.02720},
mendeley-tags={speech recognition},
title={Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks},
year={2017},
abstract={Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.},
archivePrefix={arXiv},
author={Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon Phil{\'{e}}mon Philemon and Zhang, Saizheng and Bengio, Cesar Laurent Yoshua and Courville, Aaron},
url={http://arxiv.org/abs/1701.02720},
keywords={speech recognition},
journal={arXiv preprint arXiv:1701.02720},
arxivId={1701.02720},
}

@article{Toderici2015,
year={2015},
title={Variable Rate Image Compression with Recurrent Neural Networks},
keywords={image compression- lossy},
abstract={A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32{\$}\backslashtimes{\$}32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10{\%} or more.},
journal={arXiv preprint arXiv:1511.06085},
url={http://arxiv.org/abs/1511.06085},
author={Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
archivePrefix={arXiv},
pages={1--12},
arxivId={1511.06085},
eprint={1511.06085},
mendeley-tags={image compression- lossy},
}

@article{Wang2015,
number={1},
url={http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
keywords={fuzzy theory,image compression,image compression- lossy,neural network},
title={An Image Compression Scheme Based on Fuzzy Neural Network},
mendeley-tags={image compression- lossy},
journal={TELKOMNIKA (Telecommunication Computing Electronics and Control)},
pages={137},
issn={2302-9293},
abstract={Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, self- adaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
volume={13},
author={Wang, Bo and Gao, Yubin},
doi={10.12928/telkomnika.v13i1.1270},
year={2015},
}

@article{Benardos2007,
keywords={ANN architecture,Engineering problems,Feedforward artificial neural networks,Generalization,Genetic algorithms,evolutive},
abstract={Despite the fact that feedforward artificial neural networks (ANNs) have been a hot topic of research for many years there still are certain issues regarding the development of an ANN model, resulting in a lack of absolute guarantee that the model will perform well for the problem at hand. The multitude of different approaches that have been adopted in order to deal with this problem have investigated all aspects of the ANN modelling procedure, from training data collection and pre/post-processing to elaborate training schemes and algorithms. Increased attention is especially directed to proposing a systematic way to establish an appropriate architecture in contrast to the current common practice that calls for a repetitive trial-and-error process, which is time-consuming and produces uncertain results. This paper proposes such a methodology for determining the best architecture and is based on the use of a genetic algorithm (GA) and the development of novel criteria that quantify an ANN's performance (both training and generalization) as well as its complexity. This approach is implemented in software and tested based on experimental data capturing workpiece elastic deflection in turning. The intention is to present simultaneously the approach's theoretical background and its practical application in real-life engineering problems. Results show that the approach performs better than a human expert, at the same time offering many advantages in comparison to similar approaches found in literature. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author={Benardos, P. G. and Vosniakos, G-C C.},
pages={365--382},
year={2007},
isbn={0952-1976},
publisher={Elsevier},
title={Optimizing feedforward artificial neural network architecture},
volume={20},
number={3},
doi={10.1016/j.engappai.2006.06.005},
mendeley-tags={evolutive},
issn={09521976},
journal={Engineering Applications of Artificial Intelligence},
}

@article{Esteva2017,
volume={542},
year={2017},
isbn={0028-0836},
doi={10.1038/nature21056},
publisher={Nature Publishing Group},
author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
issn={0028-0836},
pmid={28117445},
abstract={Skin cancer, the most common human malignancy1–3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4,5 show potential for general and highly variable tasks across many fine-grained object categories6–11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
mendeley-tags={health,image classification,state of the art},
pages={115--118},
url={http://www.nature.com/doifinder/10.1038/nature21056},
number={7639},
keywords={health,image classification,state of the art},
title={Dermatologist-level classification of skin cancer with deep neural networks},
journal={Nature},
}

@article{Barrow2016,
issn={01692070},
abstract={This paper evaluates k-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to k-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
mendeley-tags={forecasting},
number={4},
publisher={Elsevier B.V.},
url={http://dx.doi.org/10.1016/j.ijforecast.2015.12.011},
journal={International Journal of Forecasting},
doi={10.1016/j.ijforecast.2015.12.011},
volume={32},
year={2016},
keywords={Bootstrapping,Cross-validation,Forecast combination,Monte Carlo,Time series,forecasting},
title={Cross-validation aggregation for combining autoregressive neural network forecasts},
author={Barrow, Devon K. and Crone, Sven F.},
pages={1120--1137},
}

@article{Yang2017,
mendeley-tags={video classification},
arxivId={1707.01786},
journal={arXiv preprint arXiv:1707.01786},
archivePrefix={arXiv},
abstract={The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
url={https://arxiv.org/pdf/1707.01786.pdf},
year={2017},
keywords={video classification},
eprint={1707.01786},
title={Tensor-Train Recurrent Neural Networks for Video Classification},
author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
}

@article{Balle2015,
journal={arXiv preprint arXiv:1511.06281},
url={http://arxiv.org/abs/1511.06281},
eprint={1511.06281},
abstract={We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
mendeley-tags={image classification},
keywords={image classification},
year={2015},
arxivId={1511.06281},
pages={1--14},
archivePrefix={arXiv},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
title={Density Modeling of Images using a Generalized Normalization Transformation},
}

@article{Frans2017,
archivePrefix={arXiv},
arxivId={1704.08834},
abstract={When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
title={Outline Colorization through Tandem Adversarial Networks},
url={http://arxiv.org/abs/1704.08834},
author={Frans, Kevin},
keywords={image synthesis},
eprint={1704.08834},
journal={arXiv preprint arXiv:1704.08834},
mendeley-tags={image synthesis},
year={2017},
}

@article{Sato2015,
eprint={1505.03229},
year={2015},
author={Sato, Ikuro and Nishimura, Hiroki and Yokoi, Kensuke},
url={http://arxiv.org/abs/1505.03229},
arxivId={1505.03229},
journal={arXiv preprint arXiv:1505.03229},
keywords={image classification,optimization},
archivePrefix={arXiv},
mendeley-tags={image classification,optimization},
abstract={Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classification problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classifiers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classification, which is a way of classification using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classifiers. Even our multilayer perceptron model beats some of the convolutional models with recently invented stochastic regularization techniques on the CIFAR-10 dataset.},
title={APAC: Augmented PAttern Classification with Neural Networks},
}

@article{Zhang2016,
mendeley-tags={speech recognition,state of the art},
keywords={speech recognition,state of the art},
title={Very Deep Convolutional Networks for End-to-End Speech Recognition},
url={http://arxiv.org/abs/1610.03022},
abstract={Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
isbn={9781509041176},
institution={IEEE},
archivePrefix={arXiv},
journal={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
author={Zhang, Yu and Chan, William and Jaitly, Navdeep},
eprint={1610.03022},
pages={10--14},
year={2016},
arxivId={1610.03022},
}

@article{Fritzke1994,
url={http://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
number={9},
author={Fritzke, Bernd},
isbn={0893-6080},
mendeley-tags={evolutive,seminal},
abstract={We present a new self-organizing neural network model that has two variants. The first variant performs unsupervised learning and can be used for data visualization, clustering, and vector quantization. The main advantage over existing approaches (e.g., the Kohonen feature map) is the ability of the model to automatically find a suitable network structure and size. This is achieved through a controlled growth process that also includes occasional removal of units. The second variant of the model is a supervised learning method that results from the combination of the above-mentioned self-organizing network with the radial basis function (RBF) approach. In this model it is possible—in contrast to earlier approaches—to perform the positioning of the RBF units and the supervised training of the weights in parallel. Therefore, the current classification error can be used to determine where to insert new RBF units. This leads to small networks that generalize very well. Results on the two-spirals benchmark and a vowel classification problem are presented that are better than any results previously published.},
publisher={Elsevier},
pages={1441--1460},
keywords={--self-organization,1,as proposed by,classification,clustering,data visualization,evolutive,feature map,i n t r,incremental learning,o d u c,pattern,radial basis function,self-organizing neural network models,seminal,t i o n,two-spiral problem},
title={Growing cell structures—A self-organizing network for unsupervised and supervised learning},
doi={10.1016/0893-6080(94)90091-4},
journal={Neural Networks},
issn={08936080},
year={1994},
volume={7},
}

@article{Liao2016,
arxivId={1508.00330},
year={2016},
journal={2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
institution={IEEE},
mendeley-tags={image classification,optimization,theory},
pages={1--8},
keywords={image classification,optimization,theory},
abstract={Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
author={Liao, Zhibin and Carneiro, Gustavo},
doi={10.1109/WACV.2016.7477624},
archivePrefix={arXiv},
title={On the importance of normalisation layers in deep learning with piecewise linear activation units},
isbn={9781509006410},
eprint={1508.00330},
}

@article{Pang2017,
journal={IEEE Transactions on Neural Networks and Learning Systems},
arxivId={1603.06759},
keywords={image classification},
mendeley-tags={image classification},
pages={1--11},
year={2017},
title={Convolution in Convolution for Network in Network},
doi={10.1109/TNNLS.2017.2676130},
publisher={IEEE},
abstract={Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and {\$} 1\backslashtimes 1 {\$} convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method.},
archivePrefix={arXiv},
author={Pang, Yanwei and Sun, Manli and Jiang, Xiaoheng and Li, Xuelong},
issn={21622388},
eprint={1603.06759},
}

@article{Hoover2012,
journal={International Conference on Computational Creativity},
title={Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding},
pages={111},
keywords={music generation},
mendeley-tags={music generation},
isbn={9781905254668},
year={2012},
author={Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
}

@article{Goldberg2015,
archivePrefix={arXiv},
issn={1076-9757},
author={Goldberg, Yoav},
arxivId={1510.00726},
url={http://arxiv.org/abs/1510.00726},
doi={10.1613/jair.4992},
abstract={Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
keywords={review,speech recognition,speech synthesis},
journal={J. Artif. Intell. Res.(JAIR)},
volume={57},
eprint={1510.00726},
title={A Primer on Neural Network Models for Natural Language Processing},
year={2015},
pages={1--76},
mendeley-tags={review,speech recognition,speech synthesis},
}

@article{Costa2017,
pages={28--38},
author={Costa, Yandre M.G. G and Oliveira, Luiz S. and Silla, Carlos N.},
doi={10.1016/j.asoc.2016.12.024},
journal={Applied Soft Computing},
mendeley-tags={music classification,state of the art},
abstract={Music genre recognition based on visual representation has been successfully explored over the last years. Classifiers trained with textural descriptors (e.g., Local Binary Patterns, Local Phase Quantization, and Gabor filters) extracted from the spectrograms have achieved state-of-the-art results on several music datasets. In this work, though, we argue that we can go further with the time-frequency analysis through the use of representation learning. To show that, we compare the results obtained with a Convolutional Neural Network (CNN) with the results obtained by using handcrafted features and SVM classifiers. In addition, we have performed experiments fusing the results obtained with learned features and handcrafted features to assess the complementarity between these representations for the music classification task. Experiments were conducted on three music databases with distinct characteristics, specifically a western music collection largely used in research benchmarks (ISMIR 2004 Database), a collection of Latin American music (LMD database), and a collection of field recordings of ethnic African music. Our experiments show that the CNN compares favorably to other classifiers in several scenarios, hence, it is a very interesting alternative for music genre recognition. Considering the African database, the CNN surpassed the handcrafted representations and also the state-of-the-art by a margin. In the case of the LMD database, the combination of CNN and Robust Local Binary Pattern achieved a recognition rate of 92{\%}, which to the best of our knowledge, is the best result (using an artist filter) on this dataset so far. On the ISMIR 2004 dataset, although the CNN did not improve the state of the art, it performed better than the classifiers based individually on other kind of features.},
keywords={music classification,music genre recognition,state of the art},
title={An evaluation of Convolutional Neural Networks for music classification using spectrograms},
url={http://linkinghub.elsevier.com/retrieve/pii/S1568494616306421},
year={2017},
issn={15684946},
volume={52},
publisher={Elsevier B.V.},
}

@article{Janai2017,
url={http://arxiv.org/abs/1704.05519},
author={Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
archivePrefix={arXiv},
year={2017},
abstract={Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
mendeley-tags={image classification,object detection,review},
keywords={and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,image classification,manner 1,object detection,overview,review,survey will become a,useful tool,we hope that our},
title={Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art},
eprint={1704.05519},
arxivId={1704.05519},
journal={arXiv preprint arXiv:1704.05519},
}

@article{Choi2016,
isbn={9789881701282},
author={Choi, Keunwoo and Fazekas, George Gy{\"{o}}rgy and Sandler, Mark and Cho, Kyunghyun},
title={Convolutional Recurrent Neural Networks for Music Classification},
url={http://arxiv.org/abs/1609.04243},
pages={1--5},
archivePrefix={arXiv},
institution={IEEE},
doi={10.1.1.302.7795},
issn={15209210},
mendeley-tags={music classification},
abstract={We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
eprint={1609.04243},
journal={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
arxivId={1609.04243},
year={2016},
keywords={music classification},
}

@article{Ludermir2006,
abstract={This paper introduces a methodology for neural network global optimization. The aim is the simultaneous optimization of multilayer perceptron (MLP) network weights and architectures, in order to generate topologies with few connections and high classification performance for any data sets. The approach combines the advantages of simulated annealing, tabu search and the backpropagation training algorithm in order to generate an automatic process for producing networks with high classification performance and low complexity. Experimental results obtained with four classification problems and one prediction problem has shown to be better than those obtained by the most commonly used optimization techniques.},
title={An optimization methodology for neural network weights and architectures},
journal={IEEE Transactions on Neural Networks},
pages={1452--1459},
keywords={Multilayer perceptron (MLP),Optimization of weights and architectures,Simulating annealing,Tabu search,evolutive},
mendeley-tags={evolutive},
author={Ludermir, Teresa B. and Yamazaki, Akio and Zanchettin, Cleber},
doi={10.1109/TNN.2006.881047},
publisher={IEEE},
volume={17},
number={6},
issn={10459227},
pmid={17131660},
year={2006},
}

@article{Collobert2004,
doi={10.1145/1015330.1015415},
isbn={1581138285},
journal={Twenty-first international conference on Machine learning - ICML '04},
mendeley-tags={theory},
title={Links between perceptrons, MLPs and SVMs},
abstract={We propose to study links between three important classification algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs). We first study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs. After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function. We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin. These ideas are extended afterward to the case of MLPs. Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space. Finally, we present a very simple MLP based on the previous findings, which yields better performances in generalization and speed than the other models.},
author={Collobert, Ronan and Bengio, Samy},
url={http://portal.acm.org/citation.cfm?doid=1015330.1015415},
issn={1581138385},
institution={ACM},
keywords={theory},
year={2004},
pages={23},
}

@article{Zagoruyko2016,
url={http://arxiv.org/abs/1605.07146},
abstract={Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
keywords={image classification,object detection,state of the art},
eprint={1605.07146},
mendeley-tags={image classification,object detection,state of the art},
year={2016},
journal={arXiv preprint arXiv:1605.07146},
author={Zagoruyko, Sergey and Komodakis, Nikos},
arxivId={1605.07146},
title={Wide Residual Networks},
archivePrefix={arXiv},
}

@article{Rehman2014,
title={Image compression: A survey},
abstract={Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction. {\textcopyright} Maxwell Scientific Organization, 2014.},
issn={20407459},
pages={656--672},
volume={7},
isbn={9233351788872},
number={4},
journal={Research Journal of Applied Sciences, Engineering and Technology},
author={Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
mendeley-tags={image compression- lossy},
keywords={Compression,Image,Lossless,Lossy,Review,image compression- lossy},
publisher={Maxwell Science Publishing},
year={2014},
}

@article{Hornik1991,
isbn={0893-6080},
publisher={Elsevier},
mendeley-tags={theory},
abstract={We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
journal={Neural Networks},
author={Hornik, Kurt},
keywords={Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities,theory},
issn={08936080},
title={Approximation capabilities of multilayer feedforward networks},
eprint={arXiv:1011.1669v3},
doi={10.1016/0893-6080(91)90009-T},
archivePrefix={arXiv},
pmid={25246403},
volume={4},
arxivId={arXiv:1011.1669v3},
number={2},
pages={251--257},
year={1991},
}

@article{Song2016,
author={Song, Qing and Zhao, Xu and Fan, Haijin and Wang, Danwei},
number={5},
title={Robust Recurrent Kernel Online Learning},
year={2016},
publisher={IEEE},
doi={10.1109/TNNLS.2016.2518223},
issn={21622388},
journal={IEEE Transactions on Neural Networks and Learning Systems},
volume={28},
mendeley-tags={forecasting},
pages={1068--1081},
keywords={forecasting},
}

@article{Saxena2016,
pages={4053--4061},
title={Convolutional Neural Fabrics},
archivePrefix={arXiv},
year={2016},
url={http://arxiv.org/abs/1606.02492},
eprint={1606.02492},
abstract={Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
author={Saxena, Shreyas and Verbeek, Jakob},
journal={Advances in Neural Information Processing Systems},
keywords={evolutive},
arxivId={1606.02492},
mendeley-tags={evolutive},
issn={10495258},
}

@article{Oord2016,
author={van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
title={Pixel Recurrent Neural Networks},
volume={48},
journal={arXiv preprint arXiv:1601.06759},
eprint={1601.06759},
abstract={Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
keywords={image synthesis},
arxivId={1601.06759},
mendeley-tags={image synthesis},
archivePrefix={arXiv},
url={http://arxiv.org/abs/1601.06759},
isbn={9781510829008},
year={2016},
}

@article{Santurkar2017,
arxivId={1703.01467},
title={Generative Compression},
keywords={image compression- lossy},
url={http://arxiv.org/abs/1703.01467},
eprint={1703.01467},
year={2017},
author={Santurkar, Shibani and Budden, David and Shavit, Nir},
mendeley-tags={image compression- lossy},
journal={arXiv preprint arXiv:1703.01467},
archivePrefix={arXiv},
abstract={Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes.},
}

@article{Palmes2005,
mendeley-tags={evolutive},
pmid={15940989},
title={Mutation-based genetic neural network},
journal={IEEE Transactions on Neural Networks},
publisher={IEEE},
issn={10459227},
year={2005},
volume={16},
keywords={Artificial neural networks (ANNs),Evolutionary algorithm (EA),Evolutionary programming (EP),Evolutionary strategies (ESs),Genetic algorithm (GA),Hybrid algorithm (HA),evolutive},
isbn={1045-9227},
pages={587--600},
doi={10.1109/TNN.2005.844858},
author={Palmes, Paulito P. and Hayasaka, Taichi and Usui, Shiro},
number={3},
abstract={Evolving gradient-learning artificial neural networks (ANNs) using an evolutionary algorithm (EA) is a popular approach to address the local optima and design problems of ANN. The typical approach is to combine the strength of backpropagation (BP) in weight learning and EA's capability of searching the architecture space. However, the BP's "gradient descent" approach requires a highly computer-intensive operation that relatively restricts the search coverage of EA by compelling it to use a small population size. To address this problem, we utilized mutation-based genetic neural network (MGNN) to replace BP by using the mutation strategy of local adaptation of evolutionary programming (EP) to effect weight learning. The MGNN's mutation enables the network to dynamically evolve its structure and adapt its weights at the same time. Moreover, MGNN's EP-based encoding scheme allows for a flexible and less restricted formulation of the fitness function and makes fitness computation fast and efficient. This makes it feasible to use larger population sizes and allows MGNN to have a relatively wide search coverage of the architecture space. MGNN implements a stopping criterion where overfitness occurrences are monitored through "sliding-windows" to avoid premature learning and overlearning. Statistical analysis of its performance to some well-known classification problems demonstrate its good generalization capability. It also reveals that locally adapting or scheduling the strategy parameters embedded in each individual network may provide a proper balance between the local and global searching capabilities of MGNN.},
}

@article{Graves2013,
pages={6645--6649},
archivePrefix={arXiv},
doi={10.1109/ICASSP.2013.6638947},
mendeley-tags={speech recognition},
institution={IEEE},
arxivId={1303.5778},
author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
journal={Acoustics, speech and signal processing (icassp), 2013 ieee international conference on},
abstract={Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
keywords={speech recognition},
year={2013},
url={http://arxiv.org/abs/1303.5778},
pmid={27295638},
title={Speech Recognition with Deep Recurrent Neural Networks},
issn={1520-6149},
eprint={1303.5778},
number={3},
isbn={978-1-4799-0356-6},
}

@article{Theis2017,
journal={arXiv preprint arXiv:1703.00395},
pages={1--19},
archivePrefix={arXiv},
author={Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Husz{\'{a}}r, Ferenc},
url={http://arxiv.org/abs/1703.00395},
year={2017},
arxivId={1703.00395},
keywords={image compression- lossy},
eprint={1703.00395},
mendeley-tags={image compression- lossy},
title={Lossy Image Compression with Compressive Autoencoders},
abstract={We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
}

@article{Deng2013,
author={Deng, Li and Hinton, Geoffrey E. and Kingsbury, Brian},
doi={10.1109/ICASSP.2013.6639344},
abstract={In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled “New Types of Deep Neural Network Learning for Speech Recognition and Related Applications,” as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.},
archivePrefix={arXiv},
institution={IEEE},
arxivId={arXiv:1303.5778v1},
year={2013},
isbn={978-1-4799-0356-6},
eprint={arXiv:1303.5778v1},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6639344},
pages={8599--8603},
pmid={23127789},
issn={1520-6149},
keywords={Acoustics,Hidden Markov models,ICASSP- 2013,Neural networks,Optimization,Speech,Speech recognition,Training,acoustic models,acoustic signal processing,convolutional neural network,deep neural network,deep neural network learning,dialects,learning (artificial intelligence),multilingual,multiple languages,multitask,music processing,myriad hyper-parameter determination,network architectures,neural activation function,neural net architecture,optimisation,parameter estimation,recurrent neural network,review,spectrogram features,speech preprocessing,speech recognition},
journal={2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
title={New types of deep neural network learning for speech recognition and related applications: An overview},
mendeley-tags={review,speech recognition},
}

@article{Hornik1989,
arxivId={arXiv:1011.1669v3},
title={Multilayer feedforward networks are universal approximators},
abstract={This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
mendeley-tags={seminal,theory},
year={1989},
author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
eprint={arXiv:1011.1669v3},
archivePrefix={arXiv},
keywords={Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation,seminal,theory},
pages={359--366},
issn={08936080},
publisher={Elsevier},
isbn={08936080 (ISSN)},
doi={10.1016/0893-6080(89)90020-8},
journal={Neural Networks},
pmid={74},
volume={2},
number={5},
}

@article{Gregor2016,
url={http://arxiv.org/abs/1604.08772},
year={2016},
abstract={We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
arxivId={1604.08772},
issn={10495258},
mendeley-tags={image compression - conceptual,state of the art},
pages={3549--3557},
journal={Advances In Neural Information Processing Systems},
keywords={image compression - conceptual,state of the art},
title={Towards Conceptual Compression},
archivePrefix={arXiv},
author={Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
eprint={1604.08772},
number={Nips},
}

@article{Mollahosseini2015,
pages={1--10},
author={Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},
arxivId={1511.04110},
abstract={Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.},
doi={10.1109/WACV.2016.7477450},
eprint={1511.04110},
keywords={facial expression},
archivePrefix={arXiv},
journal={Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
mendeley-tags={facial expression},
url={http://arxiv.org/abs/1511.04110{\%}0Ahttp://dx.doi.org/10.1109/WACV.2016.7477450},
year={2015},
institution={IEEE},
title={Going Deeper in Facial Expression Recognition using Deep Neural Networks},
}

@article{Hou2015,
mendeley-tags={image classification,sentiment analysis},
keywords={image classification,sentiment analysis},
journal={IEEE transactions on neural networks and learning systems},
pages={1275--1286},
volume={26},
number={6},
title={Blind Image Quality Assessment via Deep Learning},
year={2015},
author={Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong and Member, Senior and Tao, Dacheng and Member, Senior},
publisher={IEEE},
}

@article{Wu2016,
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
abstract={Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
institution={IEEE},
arxivId={1601.02539},
volume={2016-May},
mendeley-tags={speech synthesis},
year={2016},
doi={10.1109/ICASSP.2016.7472657},
isbn={9781479999880},
title={Investigating gated recurrent networks for speech synthesis},
keywords={Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network,speech synthesis},
eprint={1601.02539},
pages={5140--5144},
archivePrefix={arXiv},
author={Wu, Zhizheng and King, Simon},
issn={15206149},
}

@article{L.Elman1990,
keywords={seminal},
volume={14},
title={Finding structure in time},
pmid={19563812},
isbn={1551-6709},
number={2},
issn={03640213},
mendeley-tags={seminal},
pages={179--211},
url={http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=17969258453606390073related:OdlkvFuhX{\_}kJ{\%}5Cnpapers2://publication/uuid/3DEB06EE-B169-47A0-BD84-BFEE3386098E},
doi={10.1207/s15516709cog1402_1},
author={L.Elman, Jeffrey and Elman, Jeffrey L},
publisher={Wiley Online Library},
year={1990},
journal={Cognitive science},
}

@article{JeffHwang2016,
mendeley-tags={image synthesis},
keywords={image synthesis},
author={Hwang, Jeff and Zhou, You and {Jeff Hwang}, You Zhou},
abstract={We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
title={Image Colorization with Deep Convolutional Neural Networks},
journal={Cs231N.Stanford.Edu},
year={2016},
}

@article{Bell2015,
author={Bell, Sean and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross and {Lawrence Zitnick}, C and Bala, Kavita and Girshick, Ross and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross},
isbn={978-1-4673-8851-1},
issn={978-953-7619-08-4},
eprint={1512.04143},
journal={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
title={Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks},
pages={2874--2883},
abstract={It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9{\%} to 76.4{\%} mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7{\%} to 33.1{\%} mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
mendeley-tags={object detection},
doi={10.1109/CVPR.2016.314},
archivePrefix={arXiv},
keywords={object detection},
url={http://arxiv.org/abs/1512.04143},
year={2015},
arxivId={1512.04143},
pmid={21803542},
}

@article{Wang2016,
mendeley-tags={forecasting},
abstract={In recent years, financialmarket dynamics forecasting has been a focus of economic research. To predict the price indices of stock markets,we developed an architecturewhich combinedElman recurrent neural networkswith stochastic time effective function.By analyzing the proposedmodelwith the linear regression, complexity invariant distance (CID), andmultiscaleCID(MCID) analysis methods and taking themodel compared with differentmodels such as the backpropagation neural network (BPNN), the stochastic time effective neural network (STNN), and the Elman recurrent neural network (ERNN), the empirical results show that the proposed neural network displays the best performance among these neural networks in financial time series forecasting. Further, the empirical research is performed in testing the predictive effects of SSE, TWSE, KOSPI, and Nikkei225 with the established model, and the corresponding statistical comparisons of the above market indices are also exhibited. The experimental results show that this approach gives good performance in predicting the values fromthe stockmarket indices. 1.},
doi={10.1155/2016/4742515},
volume={2016},
author={Wang, Jie Jun Jie and Wang, Jie Jun Jie and Fang, Wen and Niu, Hongli},
year={2016},
keywords={forecasting},
issn={16875273},
journal={Computational Intelligence and Neuroscience},
publisher={Hindawi Publishing Corporation},
title={Financial Time Series Prediction Using Elman Recurrent Random Neural Networks},
}

@article{Tsai2006,
isbn={1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
journal={IEEE Transactions on Neural Networks},
mendeley-tags={evolutive},
number={1},
publisher={IEEE},
author={Tsai, Jinn-Tsong Tsong and Chou, Jyh-Horng Horng and Liu, Tung-Kuan Kuan},
volume={17},
issn={10459227},
pages={69--80},
year={2006},
doi={10.1109/TNN.2005.860885},
keywords={Genetic algorithm (GA),Neural networks (NN),Taguchi method,evolutive},
pmid={16526477},
abstract={In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.},
title={Tuning the structure and parameters of a neural network by using hybrid Taguchi-genetic algorithm},
}

@article{Bock2012,
keywords={music transcription},
pages={121--124},
year={2012},
mendeley-tags={music transcription},
title={Polyphonic Piano Note Transcription With Recurrent Neural Networks},
institution={IEEE},
author={Bock, Sebastian and Schedl, Markus and B{\"{o}}ck, Sebastian and Schedl, Markus},
isbn={9781467300469},
journal={Network},
}

@article{Guang-BinHuang2014,
year={2014},
keywords={seminal},
institution={IEEE},
isbn={0-7803-8359-1},
title={Extreme learning machine: a new learning scheme of feedforward neural networks},
mendeley-tags={seminal},
number={August 2004},
pages={985--990},
journal={2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
author={Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong and {Guang-Bin Huang} and {Qin-Yu Zhu} and {Chee-Kheong Siew},
doi={10.1109/IJCNN.2004.1380068},
url={http://ieeexplore.ieee.org/document/1380068/},
volume={2},
}

@article{Zhu2016,
abstract={Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
institution={Springer},
year={2016},
issn={16113349},
archivePrefix={arXiv},
arxivId={1609.03552},
eprint={1609.03552},
keywords={image synthesis},
volume={9909 LNCS},
title={Generative visual manipulation on the natural image manifold},
isbn={9783319464534},
pmid={4520227},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi={10.1007/978-3-319-46454-1_36},
mendeley-tags={image synthesis},
author={Zhu, Jun-Yan Yan and Kr??henb??hl, Philipp and Shechtman, Eli and Efros, Alexei A. and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
pages={597--613},
}

@article{Sotelo2017a,
number={2015},
year={2017},
author={Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Kastner, Kyle},
pages={1--6},
title={C Har 2W Av : E Nd - To -E Nd S Peech S Ynthesis},
}

@article{Khorrami2016c,
archivePrefix={arXiv},
title={How Deep Neural Networks Can Improve Emotion Recognition on Video Data},
eprint={1602.07377},
author={Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
abstract={We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
url={http://arxiv.org/abs/1602.07377},
pages={1--5},
year={2016},
doi={10.1017/pasa.2016.3},
arxivId={1602.07377},
}

@article{Isola2016,
archivePrefix={arXiv},
title={Image-to-Image Translation with Conditional Adversarial Networks},
doi={arXiv:1611.07004},
author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
url={http://arxiv.org/abs/1611.07004},
year={2016},
mendeley-tags={image synthesis},
abstract={We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
arxivId={1611.07004},
eprint={1611.07004},
keywords={image synthesis},
journal={arXiv preprint arXiv:1611.07004},
}

@article{DiPersio2016,
issn={19984464},
keywords={Artificial neural networks,Convolutional neural network,Deep Learning,Financial forecasting,Long shortterm memory,Multi-layer neural network,Recurrent neural network,Stock markets analysis,Time series analysis,forecasting},
volume={10},
author={Di Persio}, Luca and Honchar, Oleksandr},
title={Artificial neural networks architectures for stock price prediction: Comparisons and applications},
journal={International Journal of Circuits, Systems and Signal Processing},
mendeley-tags={forecasting},
pages={403--413},
year={2016},
abstract={? 2016, North Atlantic University Union. All rights reserved.We present an Artificial Neural Network (ANN) approach to predict stock market indices, particularly with respect to the forecast of their trend movements up or down. Exploiting different Neural Networks architectures, we provide numerical analysis of concrete financial time series. In particular, after a brief r?esum?e of the existing literature on the subject, we consider the Multi-layer Perceptron (MLP), the Convolutional Neural Networks (CNN), and the Long Short-Term Memory (LSTM) recurrent neural networks techniques. We focus on the importance of choosing the correct input features, along with their preprocessing, for the specific learning algorithm one wants to use. Eventually, we consider the S{\&}P500 historical time series, predicting trend on the basis of data from the past days, and proposing a novel approach based on combination of wavelets and CNN, which outperforms the basic neural networks ones. We show, that neural networks are able to predict financial time series movements even trained only on plain time series data and propose more ways to improve results.},
}

@article{He2015,
journal={Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge - AVEC '15},
keywords={audio and video features,dblstm-rnn,multimodal fusion,offset,physiological fea-,state of the art,ture,video classification},
isbn={9781450337434},
abstract={This paper presents our system design for the Audio-Visual Emotion Challenge (AV +EC 2015). Besides the baseline features, we extract from audio the functionals on low-level descriptors (LLDs) obtained via the YAAFE toolbox, and from video the Local Phase Quantization from Three Or- thogonal Planes (LPQ-TOP) features. From the physiologi- cal signals, we extract 52 electro-cardiogram (ECG) features and 22 electro-dermal activity (EDA) features from various analysis domains. The extracted features along with the AV +EC 2015 baseline features of audio, ECG or EDA are concatenated for a further feature selection step, in which the concordance correlation coefficient (CCC), instead of the usual Pearson correlation coefficient (CC), has been used as objective function. In addition, offsets between the features and the arousal/valence labels are considered in both feature selection and modeling of the affective dimensions. For the fusion of multimodal features, we propose a Deep Bidirec- tional Long Short-Term Memory Recurrent Neural Network (DBLSTM-RNN) based multimodal affect prediction frame- work, in which the initial predictions from the single modali- ties via the DBLSTM-RNNs are firstly smoothed with Gaus- sian smoothing, then input into a second layer of DBLSTM- RNN for the final prediction of affective state. Experimental results show that our proposed features and the DBLSTM- RNN based fusion framework obtain very promising results. On the development set, the obtained CCC is up to 0.824 for arousal and 0.688 for valence, and on the test set, the CCC is 0.747 for arousal and 0.609 for valence},
institution={ACM},
doi={10.1145/2808196.2811641},
pages={73--80},
author={He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
mendeley-tags={state of the art,video classification},
number={October 2015},
year={2015},
title={Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks},
}

@article{Yao1999,
volume={87},
abstract={Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
keywords={evolutionary computation,evolutive,intelligent systems,neu-,seminal},
isbn={9780470287194},
archivePrefix={arXiv},
journal={Proceedings of the IEEE},
issn={00189219},
publisher={IEEE},
author={Yao, Xin},
title={Evolving artificial neural networks},
mendeley-tags={evolutive,seminal},
pmid={9821520},
year={1999},
doi={10.1109/5.784219},
eprint={1108.1530},
pages={1423--1447},
arxivId={1108.1530},
number={9},
}

@article{Choi2016a,
mendeley-tags={music classification},
journal={arXiv preprint arXiv:1606.00298},
year={2016},
eprint={1606.00298},
archivePrefix={arXiv},
author={Choi, Keunwoo and Fazekas, George and Sandler, Mark},
abstract={We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
arxivId={1606.00298},
keywords={music classification},
title={Automatic tagging using deep convolutional neural networks},
url={http://arxiv.org/abs/1606.00298},
}

@article{Theis2015,
keywords={image synthesis},
abstract={Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
mendeley-tags={image synthesis},
pages={1--9},
title={Generative Image Modeling Using Spatial LSTMs},
author={Theis, Lucas and Bethge, Matthias},
eprint={1506.03478},
arxivId={1506.03478},
url={http://arxiv.org/abs/1506.03478},
year={2015},
journal={Advances in Neural Information Processing Systems},
issn={10495258},
archivePrefix={arXiv},
}

@article{Gatys2016,
journal={The IEEE conference on computer vision and pattern recognition},
pages={2414--2423},
arxivId={1505.07376},
title={Image style transfer using convolutional neural networks},
isbn={9781467388511},
keywords={image synthesis},
author={Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi={10.1109/CVPR.2016.265},
eprint={1505.07376},
abstract={Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic in-formation and, thus, allow to separate image content from style. Here we use image representations derived from Con-volutional Neural Networks optimised for object recogni-tion, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can sep-arate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an ar-bitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neu-ral Networks and demonstrate their potential for high level image synthesis and manipulation.},
issn={10636919},
mendeley-tags={image synthesis},
year={2016},
pmid={15430064963552939126},
archivePrefix={arXiv},
}

@article{Boulanger-Lewandowski2013,
abstract={We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.},
mendeley-tags={music transcription},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords={Sequence transduction,music transcription,polyphonic transcription,recurrent neural network,restricted Boltzmann machine},
title={High-dimensional sequence transduction},
isbn={9781479903566},
doi={10.1109/ICASSP.2013.6638244},
eprint={1212.1936},
institution={IEEE},
issn={15206149},
number={5},
pages={3178--3182},
archivePrefix={arXiv},
arxivId={1212.1936},
year={2013},
author={Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
}

@article{Sotelo2017,
mendeley-tags={speech synthesis},
pages={44--51},
url={https://openreview.net/pdf?id=B1VWyySKx},
year={2017},
author={Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Santos, Joao Felipe and Kastner, Kyle and Courville, Aaron and Bengio, Yoshua},
abstract={We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder . The reader is an encoder- decoder model with attention. The encoder is a bidirectional recurrent neural net- work that accepts text or phonemes as inputs, while the decoder is a recurrent neu- ral network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
keywords={speech synthesis},
number={October},
title={Char2Wav: End-to-End Speech Synthesis},
journal={Iclr},
}

@article{Esteva2017,
volume={542},
author={Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
pages={115--118},
pmid={28117445},
keywords={health,image classification,state of the art},
url={http://www.nature.com/doifinder/10.1038/nature21056},
publisher={Nature Publishing Group},
year={2017},
doi={10.1038/nature21056},
journal={Nature},
title={Dermatologist-level classification of skin cancer with deep neural networks},
number={7639},
mendeley-tags={health,image classification,state of the art},
issn={0028-0836},
abstract={Skin cancer, the most common human malignancy1–3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4,5 show potential for general and highly variable tasks across many fine-grained object categories6–11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
isbn={0028-0836},
}

@article{Wang2015,
title={An Image Compression Scheme Based on Fuzzy Neural Network},
volume={13},
year={2015},
url={http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
doi={10.12928/telkomnika.v13i1.1270},
author={Wang, Bo and Gao, Yubin},
journal={TELKOMNIKA (Telecommunication Computing Electronics and Control)},
keywords={fuzzy theory,image compression,image compression- lossy,neural network},
number={1},
issn={2302-9293},
mendeley-tags={image compression- lossy},
pages={137},
abstract={Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, self- adaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
}

@article{Shin2016,
pages={1285--1298},
archivePrefix={arXiv},
mendeley-tags={image classification,optimization},
number={5},
keywords={Biomedical imaging,computer aided diagnosis,image analysis,image classification,machine learning,neural networks,optimization},
pmid={26886976},
publisher={IEEE},
isbn={0278-0062 VO - 35},
eprint={1602.03409},
abstract={Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and the revival of deep CNN. CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, with 85{\%} sensitivity at 3 false positive per patient, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
author={Shin, Hoo-Chang Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
volume={35},
title={Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning},
issn={1558254X},
year={2016},
arxivId={1602.03409},
journal={IEEE Transactions on Medical Imaging},
doi={10.1109/TMI.2016.2528162},
}

@article{Salama2015,
abstract={Ant colony optimization (ACO) has been successfully applied to classification, where the aim is to build a model that captures the relationships between the input attributes and the target class in a given domain's dataset. The constructed classification model can then be used to predict the unknown class of a new pattern. While artificial neural networks are one of the most widely used models for pattern classification, their application is commonly restricted to fully connected three-layer topologies. In this paper, we present a new algorithm, ANN-Miner, which uses ACO to learn the structure of feed-forward neural networks. We report computational results on 40 benchmark datasets for several variations of the algorithm. Performance is compared to the standard three-layer structure trained with two different weight-learning algorithms (back propagation, and the ACOℝ algorithm), and also to a greedy algorithm for learning NN structures. A nonparametric Friedman test is used to determine statistical significance. In addition, we compare our proposed algorithm with NEAT, a prominent evolutionary algorithm for evolving neural networks, as well as three different well-known state-of-the-art classifiers, namely the C4.5 decision tree induction algorithm, the Ripper classification rule induction algorithm, and support vector machines.},
title={Learning neural network structures with ant colony algorithms},
number={4},
publisher={Springer US},
volume={9},
author={Salama, Khalid M. and Abdelbar, Ashraf M.},
mendeley-tags={evolutive},
journal={Swarm Intelligence},
doi={10.1007/s11721-015-0112-z},
keywords={Ant colony optimization (ACO),Machine learning,Neural networks,Pattern classification,evolutive},
pages={229--265},
year={2015},
issn={19353820},
}

@article{Chen2017,
doi={10.1016/j.eswa.2016.10.065},
year={2017},
mendeley-tags={sentence classification,state of the art},
keywords={Deep neural network,Natural language processing,Sentiment analysis,sentence classification,state of the art},
url={http://dx.doi.org/10.1016/j.eswa.2016.10.065},
journal={Expert Systems with Applications},
issn={09574174},
volume={72},
pages={221--230},
title={Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN},
publisher={Elsevier Ltd},
abstract={Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
author={Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
}

@article{Costa2017,
doi={10.1016/j.asoc.2016.12.024},
title={An evaluation of Convolutional Neural Networks for music classification using spectrograms},
url={http://linkinghub.elsevier.com/retrieve/pii/S1568494616306421},
volume={52},
abstract={Music genre recognition based on visual representation has been successfully explored over the last years. Classifiers trained with textural descriptors (e.g., Local Binary Patterns, Local Phase Quantization, and Gabor filters) extracted from the spectrograms have achieved state-of-the-art results on several music datasets. In this work, though, we argue that we can go further with the time-frequency analysis through the use of representation learning. To show that, we compare the results obtained with a Convolutional Neural Network (CNN) with the results obtained by using handcrafted features and SVM classifiers. In addition, we have performed experiments fusing the results obtained with learned features and handcrafted features to assess the complementarity between these representations for the music classification task. Experiments were conducted on three music databases with distinct characteristics, specifically a western music collection largely used in research benchmarks (ISMIR 2004 Database), a collection of Latin American music (LMD database), and a collection of field recordings of ethnic African music. Our experiments show that the CNN compares favorably to other classifiers in several scenarios, hence, it is a very interesting alternative for music genre recognition. Considering the African database, the CNN surpassed the handcrafted representations and also the state-of-the-art by a margin. In the case of the LMD database, the combination of CNN and Robust Local Binary Pattern achieved a recognition rate of 92{\%}, which to the best of our knowledge, is the best result (using an artist filter) on this dataset so far. On the ISMIR 2004 dataset, although the CNN did not improve the state of the art, it performed better than the classifiers based individually on other kind of features.},
keywords={music classification,music genre recognition,state of the art},
mendeley-tags={music classification,state of the art},
year={2017},
issn={15684946},
author={Costa, Yandre M.G. G and Oliveira, Luiz S. and Silla, Carlos N.},
pages={28--38},
journal={Applied Soft Computing},
publisher={Elsevier B.V.},
}

@article{Wu2012,
title={Storage capacity of the hopfield network associative memory},
number={January},
keywords={Associative memory,Hebbian learning,Hopfield model,Perceptron learning,theory},
isbn={9780769546377},
abstract={The Hop field model is a well-known dynamic associative-memory model. In this paper, we investigate various aspects of the Hop field model for associative memory. We conduct a systematic simulation investigation of several storage algorithms for Hop field networks, and conclude that the perceptron learning based storage algorithms can achieve much better storage capacity than the Hebbian learning based algorithms.},
journal={Proceedings - 2012 5th International Conference on Intelligent Computation Technology and Automation, ICICTA 2012},
mendeley-tags={theory},
institution={IEEE},
pages={330--336},
doi={10.1109/ICICTA.2012.89},
author={Wu, Yue and Hu, Jianqing and Wu, Wei and Zhou, Yong and Du, K. L.},
year={2012},
}

@article{Araque2017,
mendeley-tags={sentiment analysis},
volume={77},
issn={09574174},
title={Enhancing deep learning sentiment analysis with ensemble techniques in social applications},
publisher={Elsevier Ltd},
journal={Expert Systems with Applications},
author={Araque, Oscar and Corcuera-Platas, Ignacio and S{\'{a}}nchez-Rada, J. Fernando and Iglesias, Carlos A.},
year={2017},
pages={236--246},
abstract={Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-Score.},
keywords={Deep learning,Ensemble,Machine learning,Natural language processing,Sentiment analysis,sentiment analysis},
doi={10.1016/j.eswa.2017.02.002},
isbn={0957-4174},
}

@article{werbos74,
journal={Doctoral Dissertation, Applied Mathematics, Harvard University, MA},
author={Werbos, Paul John},
title={Beyond regression: New tools for prediction and analysis in the behavioral sciences},
year={1974},
}

@article{Iizuka2016,
issn={07300301},
abstract={We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
publisher={ACM},
volume={35},
year={2016},
number={4},
pages={1--11},
journal={ACM Transactions on Graphics},
author={Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
doi={10.1145/2897824.2925974},
url={http://dl.acm.org/citation.cfm?doid=2897824.2925974},
isbn={9781450342797},
title={Let there be color! :Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification},
mendeley-tags={image synthesis},
keywords={colorization,computing methodologies,convolutional neural network concepts,image processing,image synthesis,neural net-},
}

@article{lin2013network,
author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
journal={arXiv preprint arXiv:1312.4400},
title={Network in network},
year={2013},
}

@article{Hornik1991,
journal={Neural Networks},
issn={08936080},
author={Hornik, Kurt},
eprint={arXiv:1011.1669v3},
title={Approximation capabilities of multilayer feedforward networks},
arxivId={arXiv:1011.1669v3},
abstract={We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
archivePrefix={arXiv},
number={2},
pages={251--257},
isbn={0893-6080},
publisher={Elsevier},
year={1991},
pmid={25246403},
keywords={Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities,theory},
mendeley-tags={theory},
volume={4},
doi={10.1016/0893-6080(91)90009-T},
}

@article{Rehman2014,
author={Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
issn={20407459},
number={4},
year={2014},
pages={656--672},
keywords={Compression,Image,Lossless,Lossy,Review,image compression- lossy},
journal={Research Journal of Applied Sciences, Engineering and Technology},
mendeley-tags={image compression- lossy},
isbn={9233351788872},
title={Image compression: A survey},
volume={7},
publisher={Maxwell Science Publishing},
abstract={Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction. {\textcopyright} Maxwell Scientific Organization, 2014.},
}

@article{Johnston2017,
title={Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
url={http://arxiv.org/abs/1703.10114},
year={2017},
journal={arXiv preprint arXiv:1703.10114},
keywords={image compression- lossy},
archivePrefix={arXiv},
abstract={We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
eprint={1703.10114},
author={Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
mendeley-tags={image compression- lossy},
arxivId={1703.10114},
}

@article{Toderici2016,
doi={10.4135/9781412985277},
mendeley-tags={image compression- lossy},
isbn={9780761914402},
issn={08936080},
author={Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
url={http://arxiv.org/abs/1608.05148},
keywords={image compression- lossy},
year={2016},
arxivId={1608.05148},
abstract={This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
eprint={1608.05148},
title={Full Resolution Image Compression with Recurrent Neural Networks},
archivePrefix={arXiv},
journal={arXiv preprint arXiv:1608.05148},
pmid={21655600},
}

@article{Yang2017,
abstract={The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
archivePrefix={arXiv},
author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
keywords={video classification},
mendeley-tags={video classification},
url={https://arxiv.org/pdf/1707.01786.pdf},
year={2017},
eprint={1707.01786},
journal={arXiv preprint arXiv:1707.01786},
arxivId={1707.01786},
title={Tensor-Train Recurrent Neural Networks for Video Classification},
}

@article{Sangkloy2016,
arxivId={1612.00835},
url={http://arxiv.org/abs/1612.00835},
title={Scribbler: Controlling Deep Image Synthesis with Sketch and Color},
keywords={image synthesis,state of the art},
year={2016},
abstract={Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.},
author={Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
journal={arXiv preprint arXiv:1612.00835},
mendeley-tags={image synthesis,state of the art},
archivePrefix={arXiv},
eprint={1612.00835},
}

@article{Hornik1989,
pmid={74},
year={1989},
archivePrefix={arXiv},
journal={Neural Networks},
keywords={Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation,seminal,theory},
pages={359--366},
arxivId={arXiv:1011.1669v3},
abstract={This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
mendeley-tags={seminal,theory},
author={Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
eprint={arXiv:1011.1669v3},
issn={08936080},
publisher={Elsevier},
doi={10.1016/0893-6080(89)90020-8},
title={Multilayer feedforward networks are universal approximators},
number={5},
volume={2},
isbn={08936080 (ISSN)},
}

@article{Cortes2016,
arxivId={1607.01097},
author={Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
eprint={1607.01097},
mendeley-tags={evolutive},
url={http://arxiv.org/abs/1607.01097},
abstract={We present new algorithms for adaptively learning artificial neural networks. Our algorithms (AdaNet) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.},
year={2016},
journal={arXiv preprint arXiv:1607.01097},
keywords={evolutive},
title={AdaNet: Adaptive Structural Learning of Artificial Neural Networks},
archivePrefix={arXiv},
}

@article{Li2015,
pages={1--6},
year={2015},
title={Facial expression recognition using deep neural networks},
issn={1558-2809},
journal={Imaging Systems and Techniques (IST), 2015 IEEE International Conference on},
author={Li, Junnan and Lam, Edmund Y.},
institution={IEEE},
mendeley-tags={facial expression},
doi={10.1109/IST.2015.7294547},
isbn={9781479986330},
keywords={emo,facial expression,gabor filters,kernel principal component analysis,multi-layer neural network},
}

@article{Xu2017,
author={Xu, Lamei and Lin, Jin and Wang, Lina and Yin, Chunyong and Wang, Jin},
number={Ast},
title={Deep Convolutional Neural Network based Approach for Aspect-based Sentiment Analysis},
volume={143},
journal={Advanced Science and Technology Letters},
year={2017},
pages={199--204},
mendeley-tags={sentiment analysis},
keywords={aspect-based sentiment,convolution neural network,sentiment analysis,word2vec},
}

@article{Shizhou2016,
mendeley-tags={image synthesis},
issn={10450823},
abstract={In this paper, we choose to learn useful cues from object recognition mechanisms of the human vi-sual cortex, and propose a DCNN performance im-provement method without the need for increasing the network complexity. Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As ex-perimental results show, when applying the pro-posed method to the " Quick " model and NIN models, image classification performances are re-markably improved on four widely used bench-mark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
journal={IJCAI International Joint Conference on Artificial Intelligence},
pages={2343--2349},
keywords={Machine Learning,image synthesis},
title={Improving DCNN Performance with Sparse Category-Selective Objective Function.},
volume={2016-Janua},
year={2016},
author={Zhang, Shizhou and Gong, Yihong and Wang, Jinjun and Shizhou, Zhang and Gong, Yihong and Jinjun, Wang},
}

@article{Graves2013,
author={Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
issn={1520-6149},
journal={Acoustics, speech and signal processing (icassp), 2013 ieee international conference on},
url={http://arxiv.org/abs/1303.5778},
keywords={speech recognition},
arxivId={1303.5778},
institution={IEEE},
pages={6645--6649},
title={Speech Recognition with Deep Recurrent Neural Networks},
number={3},
year={2013},
archivePrefix={arXiv},
pmid={27295638},
eprint={1303.5778},
abstract={Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
doi={10.1109/ICASSP.2013.6638947},
mendeley-tags={speech recognition},
isbn={978-1-4799-0356-6},
}

@article{Hou2015,
author={Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong and Member, Senior and Tao, Dacheng and Member, Senior},
number={6},
volume={26},
mendeley-tags={image classification,sentiment analysis},
title={Blind Image Quality Assessment via Deep Learning},
pages={1275--1286},
year={2015},
keywords={image classification,sentiment analysis},
journal={IEEE transactions on neural networks and learning systems},
publisher={IEEE},
}

@article{Zhu2016,
volume={9909 LNCS},
keywords={image synthesis},
year={2016},
arxivId={1609.03552},
author={Zhu, Jun-Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A},
doi={10.1007/978-3-319-46454-1_36},
pages={597--613},
eprint={1609.03552},
mendeley-tags={image synthesis},
issn={16113349},
pmid={4520227},
title={Generative visual manipulation on the natural image manifold},
institution={Springer},
archivePrefix={arXiv},
abstract={Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
isbn={9783319464534},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
}

@article{Bas2016,
doi={10.1515/jaiscr-2016-0001},
volume={6},
year={2016},
mendeley-tags={evolutive,forecasting},
keywords={Artificial neural networks,Differential evolution algorithm,Forecasting,Multiplicative neuron model,evolutive,forecasting},
pages={5--11},
abstract={In recent years, artificial neural networks have been commonly used for time series forecasting by researchers from various fields. There are some types of artificial neural networks and feed forward artificial neural networks model is one of them. Although feed forward artificial neural networks gives successful forecasting results they have a basic problem. This problem is architecture selection problem. In order to eliminate this problem, Yadav et al. (2007) proposed multiplicative neuron model artificial neural network. In this study, differential evolution algorithm is proposed for the training of multiplicative neuron model for forecasting. The proposed method is applied to two well-known different real world time series data.},
author={Bas, Eren},
issn={24496499},
journal={Journal of Artificial Intelligence and Soft Computing Research},
number={1},
title={The training of multiplicative neuron model based artificial neural networks with differential evolution algorithm for forecasting},
}

@article{Boulanger-lewandowski2014,
author={Boulanger-lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong},
journal={Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
number={2},
pages={5454--5458},
title={PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS},
year={2014},
keywords={speech recognition},
mendeley-tags={speech recognition},
isbn={9781479928934},
institution={IEEE},
}

@article{L.Elman1990,
doi={10.1207/s15516709cog1402_1},
isbn={1551-6709},
author={L.Elman, Jeffrey and Elman, Jeffrey L},
journal={Cognitive science},
url={http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=17969258453606390073related:OdlkvFuhX{\_}kJ{\%}5Cnpapers2://publication/uuid/3DEB06EE-B169-47A0-BD84-BFEE3386098E},
issn={03640213},
volume={14},
publisher={Wiley Online Library},
pmid={19563812},
mendeley-tags={seminal},
keywords={seminal},
title={Finding structure in time},
number={2},
year={1990},
pages={179--211},
}

@article{JeffHwang2016,
journal={Cs231N.Stanford.Edu},
mendeley-tags={image synthesis},
title={Image Colorization with Deep Convolutional Neural Networks},
keywords={image synthesis},
author={Hwang, Jeff and Zhou, You and {Jeff Hwang}, You Zhou},
year={2016},
abstract={We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
}

@article{PingpingZhu2013,
author={Pingping Zhu}, Jos{\'{e}} and Pr{\'{i}}ncipe, e C. and Zhu, Pingping and Pr$\backslash$'$\backslash$incipe, Jos{\'{e}} C},
journal={Computer Engineering},
keywords={forecasting},
isbn={9781479903566},
mendeley-tags={forecasting},
year={2013},
title={KERNEL RECURRENT SYSTEM TRAINED BY REAL-TIME RECURRENT LEARNING ALGORITHM},
pages={3572--3576},
}

@article{Zhao2016,
archivePrefix={arXiv},
author={Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
keywords={Deep network,Facial expression recognition,Peak gradient suppression,Peak-piloted,facial expression,state of the art},
arxivId={1607.06997},
pages={425--442},
year={2016},
title={Peak-piloted deep network for facial expression recognition},
mendeley-tags={facial expression,state of the art},
volume={9906 LNCS},
pmid={4520227},
eprint={1607.06997},
institution={Springer},
doi={10.1007/978-3-319-46475-6_27},
abstract={Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
isbn={9783319464749},
issn={16113349},
}

@article{Sak2015,
url={http://arxiv.org/abs/1507.06947},
journal={arXiv preprint arXiv:1507.06947},
abstract={We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.},
eprint={1507.06947},
author={Sak, Ha$\backslash$csim Haşim and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
archivePrefix={arXiv},
pmid={1000285842},
mendeley-tags={speech recognition},
arxivId={1507.06947},
keywords={speech recognition},
title={Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition},
year={2015},
}

@article{Larsson2016,
volume={9908 LNCS},
archivePrefix={arXiv},
author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
isbn={9783319464923},
year={2016},
mendeley-tags={image synthesis},
arxivId={1603.06668},
institution={Springer},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi={10.1007/978-3-319-46493-0_35},
eprint={1603.06668},
keywords={image synthesis},
pages={577--593},
issn={16113349},
title={Learning representations for automatic colorization},
abstract={We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
}

@article{LeCun1998,
doi={10.1109/5.726791},
journal={Proceedings of the IEEE},
keywords={Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR),seminal},
abstract={Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
title={Gradient-based learning applied to document recognition},
isbn={0018-9219},
arxivId={1102.0183},
eprint={1102.0183},
issn={00189219},
volume={86},
number={11},
pmid={15823584},
mendeley-tags={seminal},
pages={2278--2323},
author={LeCun, Yann and Bottou, L??on L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
publisher={IEEE},
year={1998},
archivePrefix={arXiv},
}

@article{Veit2016,
url={http://arxiv.org/abs/1605.06431},
mendeley-tags={theory},
arxivId={1605.06431},
issn={10495258},
keywords={theory},
eprint={1605.06431},
author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
abstract={In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
archivePrefix={arXiv},
pages={1--9},
title={Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
year={2016},
journal={Advances in Neural Information Processing Systems},
}

@article{Sainath2015,
url={http://dx.doi.org/10.1016/j.neunet.2014.08.005},
pages={39--48},
abstract={Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12{\%}-14{\%} relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.},
issn={18792782},
pmid={25439765},
publisher={Elsevier Ltd},
journal={Neural Networks},
isbn={0893-6080},
author={Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and rahman Mohamed, Abdel-rahman and Dahl, George and Ramabhadran, Bhuvana},
doi={10.1016/j.neunet.2014.08.005},
title={Deep Convolutional Neural Networks for Large-scale Speech Tasks},
year={2015},
volume={64},
keywords={Deep learning,Neural networks,Speech recognition,speech recognition},
mendeley-tags={speech recognition},
}

@article{Wang2016,
mendeley-tags={forecasting},
keywords={forecasting},
doi={10.1155/2016/4742515},
title={Financial Time Series Prediction Using Elman Recurrent Random Neural Networks},
volume={2016},
year={2016},
issn={16875273},
abstract={In recent years, financialmarket dynamics forecasting has been a focus of economic research. To predict the price indices of stock markets,we developed an architecturewhich combinedElman recurrent neural networkswith stochastic time effective function.By analyzing the proposedmodelwith the linear regression, complexity invariant distance (CID), andmultiscaleCID(MCID) analysis methods and taking themodel compared with differentmodels such as the backpropagation neural network (BPNN), the stochastic time effective neural network (STNN), and the Elman recurrent neural network (ERNN), the empirical results show that the proposed neural network displays the best performance among these neural networks in financial time series forecasting. Further, the empirical research is performed in testing the predictive effects of SSE, TWSE, KOSPI, and Nikkei225 with the established model, and the corresponding statistical comparisons of the above market indices are also exhibited. The experimental results show that this approach gives good performance in predicting the values fromthe stockmarket indices. 1.},
author={Wang, Jun Jie and Wang, Jun Jie and Fang, Wen and Niu, Hongli},
publisher={Hindawi Publishing Corporation},
journal={Computational Intelligence and Neuroscience},
}

@article{Zhang2016a,
year={2016},
volume={9907 LNCS},
issn={16113349},
author={Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
mendeley-tags={image synthesis},
pages={649--666},
abstract={Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32{\%} of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
pmid={10463930},
institution={Springer},
archivePrefix={arXiv},
doi={10.1007/978-3-319-46487-9_40},
eprint={1603.08511},
arxivId={1603.08511},
isbn={9783319464862},
keywords={CNNs,Colorization,Self-supervised learning,Vision for graphics,image synthesis},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
title={Colorful image colorization},
}

@article{Shi2016,
issn={10450823},
title={Improving CNN performance with min-max objective},
year={2016},
keywords={Machine Learning,object detection},
journal={IJCAI International Joint Conference on Artificial Intelligence},
volume={2016-Janua},
abstract={In this paper, we propose a novel method to im-prove object recognition accuracies of convolu-tional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The Min-Max objective explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin be-tween different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computa-tion cost. Experiments with shallow and deep mod-els on four benchmark datasets including CIFAR-10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max ob-jective achieve remarkable performance improve-ments compared to the corresponding baseline models.},
mendeley-tags={object detection},
author={Shi, Weiwei and Gong, Yihong and Wang, Jinjun},
pages={2004--2010},
}

@article{Southall2016,
title={Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks},
isbn={978-0-692-75506-8},
pages={591--597},
journal={Proceedings of International Society for Music Information Retrieval Conference (ISMIR)},
author={Southall, Carl and Stables, Ryan and Hockman, Jason},
abstract={Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in-struments in audio recordings. Neural networks have al-ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro-pose the use of neural networks for ADT in order to ex-ploit their ability to capture a complex configuration of fea-tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu-ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit-able for online operation. In both systems, a separate net-work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis-ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec-tively. The results demonstrate the effectiveness of the pre-sented methods for solo percussion and a capacity for iden-tifying snare drums, which are historically the most diffi-cult drum class to detect.},
keywords={music transcription,tesis},
year={2016},
mendeley-tags={music transcription,tesis},
}

@article{wang2017tacotron,
keywords={speech synthesis,state of the art},
year={2017},
abstract={A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
arxivId={1703.10135},
url={http://arxiv.org/abs/1703.10135},
mendeley-tags={speech synthesis,state of the art},
eprint={1703.10135},
archivePrefix={arXiv},
pages={1--10},
title={Tacotron: Towards End-to-End Speech Synthesis},
author={Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
doi={10.21437/Interspeech.2017-1452},
}

@article{Zen2015,
journal={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
author={Zen, Heiga and Sak, Hasim Ha$\backslash$csim},
keywords={speech synthesis},
isbn={9781467369978},
issn={9781467369978},
pages={4470--4474},
title={Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis},
institution={IEEE},
mendeley-tags={speech synthesis},
year={2015},
}

@article{Karpathy2014,
abstract={Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new...},
arxivId={1412.0767},
eprint={1412.0767},
year={2014},
title={Large-Scale Video Classification with Convolutional Neural Networks},
doi={10.1109/CVPR.2014.223},
author={Karpathy, Andrej and Toderici, George and Shetty, Sanketh Sachin and Leung, Tommy Thomas and Sukthankar, Rahul and Fei-Fei, Li},
mendeley-tags={video classification},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909619{\%}0Apapers3://publication/doi/10.1109/CVPR.2014.223},
issn={978-1-4799-5118-5},
archivePrefix={arXiv},
pages={1725--1732},
isbn={978-1-4799-5118-5},
keywords={video classification},
journal={2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
}

@article{Lee2015,
author={Lee, Chen-Yu and Gallagher, Patrick W. and Tu, Zhuowen},
pages={464--472},
url={http://arxiv.org/abs/1509.08985},
year={2015},
archivePrefix={arXiv},
arxivId={1509.08985},
volume={51},
journal={Artificial Intelligence and Statistics},
abstract={We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.},
doi={10.1109/TPAMI.2017.2703082},
mendeley-tags={optimization},
title={Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree},
issn={0162-8828},
keywords={optimization},
pmid={67101},
eprint={1509.08985},
}

@article{Kulkarni2015,
archivePrefix={arXiv},
mendeley-tags={image synthesis,tesis},
year={2015},
pages={1--10},
title={Deep Convolutional Inverse Graphics Network},
eprint={1503.03167},
abstract={This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
issn={10897550},
doi={10.1063/1.4914407},
journal={Advances in Neural Information Processing Systems},
keywords={image synthesis,tesis},
arxivId={1503.03167},
author={Kulkarni, Tejas D. and Whitney, William F and Kohli, Pushmeet and Tenenbaum, Joshua B.},
url={http://arxiv.org/abs/1503.03167},
}

@article{Tu2016,
abstract={This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.},
pages={1--14},
arxivId={1603.05594},
author={Tu, Enmei and Kasabov, Nikola and Yang, Jie},
year={2016},
pmid={26992179},
number={6},
publisher={IEEE},
journal={IEEE Transactions on Neural Networks and Learning Systems},
issn={21622388},
doi={10.1109/TNNLS.2016.2536742},
volume={28},
archivePrefix={arXiv},
title={Mapping Temporal Variables Into the NeuCube for Improved Pattern Recognition, Predictive Modeling, and Understanding of Stream Data},
keywords={forecasting,state of the art},
eprint={1603.05594},
mendeley-tags={forecasting,state of the art},
}

@article{Goldberg2015,
title={A Primer on Neural Network Models for Natural Language Processing},
journal={J. Artif. Intell. Res.(JAIR)},
url={http://arxiv.org/abs/1510.00726},
year={2015},
mendeley-tags={review,speech recognition,speech synthesis},
archivePrefix={arXiv},
arxivId={1510.00726},
keywords={review,speech recognition,speech synthesis},
issn={1076-9757},
pages={1--76},
abstract={Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
volume={57},
eprint={1510.00726},
doi={10.1613/jair.4992},
author={Goldberg, Yoav},
}

@article{Maas2013,
pages={6},
abstract={Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2{\%} absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
author={Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
journal={Proceedings of the 30 th International Conference on Machine Learning},
keywords={speech recognition},
title={Rectifier Nonlinearities Improve Neural Network Acoustic Models},
year={2013},
url={https://web.stanford.edu/{~}awni/papers/relu{\_}hybrid{\_}icml2013{\_}final.pdf},
mendeley-tags={speech recognition},
volume={28},
number={1},
}

@article{Yadav2007,
issn={15684946},
journal={Applied Soft Computing Journal},
doi={10.1016/j.asoc.2006.01.003},
number={4},
title={Time series prediction with single multiplicative neuron model},
year={2007},
author={Yadav, Ram N. and Kalra, Prem Kumar and John, Joseph},
publisher={Elsevier},
volume={7},
mendeley-tags={forecasting},
abstract={Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network. ?? 2006 Elsevier B.V. All rights reserved.},
pages={1157--1163},
keywords={Capacity of single neuron,Financial time series prediction,Mackey-Glass time series,Multiplicative neuron model,Time series prediction,forecasting},
}

@inproceedings{he2016deep,
author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
title={Deep residual learning for image recognition},
year={2016},
booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
pages={770--778},
}

@article{Floreano2008,
isbn={1206500700},
pages={47--62},
author={Floreano, Dario and D{\"{u}}rr, Peter and Mattiussi, Claudio},
issn={18645909},
keywords={Evolution,Learning,Neural networks,evolutive,review},
abstract={Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
number={1},
title={Neuroevolution: From architectures to learning},
journal={Evolutionary Intelligence},
doi={10.1007/s12065-007-0002-4},
mendeley-tags={evolutive,review},
publisher={Springer},
volume={1},
year={2008},
}

@inproceedings{zhang2017very,
pages={4845--4849},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
title={Very deep convolutional networks for end-to-end speech recognition},
author={Zhang, Yu and Chan, William and Jaitly, Navdeep},
organization={IEEE},
year={2017},
}

@article{johnston2017improved,
title={Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
author={Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
year={2017},
journal={arXiv preprint arXiv:1703.10114},
}

@inproceedings{tuohy2006evolved,
year={2006},
author={Tuohy, Daniel R and Potter, Walter D and Center, Artificial Intelligence},
title={An Evolved Neural Network/HC Hybrid for Tablature Creation in GA-based Guitar Arranging.},
booktitle={ICMC},
}

@inproceedings{park2016analysis,
booktitle={Asian Conference on Computer Vision},
pages={189--204},
author={Park, Sungheon and Kwak, Nojun},
title={Analysis on the Dropout Effect in Convolutional Neural Networks},
organization={Springer},
year={2016},
}

@article{yadav2007time,
number={4},
publisher={Elsevier},
volume={7},
journal={Applied soft computing},
title={Time series prediction with single multiplicative neuron model},
author={Yadav, Ram N and Kalra, Prem Kumar and John, Joseph},
pages={1157--1163},
year={2007},
}

@article{stanley2007compositional,
title={Compositional pattern producing networks: A novel abstraction of development},
pages={131--162},
volume={8},
number={2},
author={Stanley, Kenneth O},
publisher={Springer},
journal={Genetic programming and evolvable machines},
year={2007},
}

@book{bishop06,
publisher={springer},
author={Bishop, Christopher M},
title={Pattern recognition and machine learning},
year={2006},
}

@inproceedings{wu2016investigating,
year={2016},
organization={IEEE},
title={Investigating gated recurrent networks for speech synthesis},
author={Wu, Zhizheng and King, Simon},
pages={5140--5144},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
}

@inproceedings{southall2016automatic,
pages={591--597},
title={Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks.},
author={Southall, Carl and Stables, Ryan and Hockman, Jason},
year={2016},
booktitle={ISMIR},
}

@inproceedings{zhang2016colorful,
pages={649--666},
title={Colorful image colorization},
year={2016},
author={Zhang, Richard and Isola, Phillip and Efros, Alexei A},
booktitle={European Conference on Computer Vision},
organization={Springer},
}

@article{leshno93,
publisher={Elsevier},
year={1993},
journal={Neural networks},
author={Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
title={Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
volume={6},
pages={861--867},
number={6},
}

@inproceedings{zhu2016generative,
author={Zhu, Jun-Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A},
booktitle={European Conference on Computer Vision},
pages={597--613},
title={Generative visual manipulation on the natural image manifold},
year={2016},
organization={Springer},
}

@article{yang2017tensor,
author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
title={Tensor-Train Recurrent Neural Networks for Video Classification},
journal={arXiv preprint arXiv:1707.01786},
year={2017},
}

@article{xu2017deep,
year={2017},
author={Xu, Lamei and Lin, Jin and Wang, Lina and Yin, Chunyong and Wang, Jin},
title={Deep Convolutional Neural Network based Approach for Aspect-based Sentiment Analysis},
}

@inproceedings{boulanger2013high,
author={Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
pages={3178--3182},
organization={IEEE},
year={2013},
title={High-dimensional sequence transduction},
}

@article{choi2016automatic,
author={Choi, Keunwoo and Fazekas, George and Sandler, Mark},
journal={arXiv preprint arXiv:1606.00298},
title={Automatic tagging using deep convolutional neural networks},
year={2016},
}

@inproceedings{mollahosseini2016going,
author={Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H},
organization={IEEE},
booktitle={Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
pages={1--10},
year={2016},
title={Going deeper in facial expression recognition using deep neural networks},
}

@book{Rosenblatt57,
title={The perceptron, a perceiving and recognizing automaton Project Para},
year={1957},
author={Rosenblatt, Frank},
publisher={Cornell Aeronautical Laboratory},
}

@article{zagoruyko2016wide,
title={Wide residual networks},
author={Zagoruyko, Sergey and Komodakis, Nikos},
year={2016},
journal={arXiv preprint arXiv:1605.07146},
}

@article{hou2015blind,
year={2015},
number={6},
publisher={IEEE},
volume={26},
author={Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong},
pages={1275--1286},
journal={IEEE transactions on neural networks and learning systems},
title={Blind image quality assessment via deep learning},
}

@article{sak2015fast,
author={Sak, Ha$\backslash$csim and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
year={2015},
journal={arXiv preprint arXiv:1507.06947},
title={Fast and accurate recurrent neural network acoustic models for speech recognition},
}

@inproceedings{veit2016residual,
title={Residual networks behave like ensembles of relatively shallow networks},
pages={550--558},
year={2016},
author={Veit, Andreas and Wilber, Michael J and Belongie, Serge},
booktitle={Advances in Neural Information Processing Systems},
}

@article{socher2014recursive,
year={2014},
publisher={Citeseer},
author={Socher, Richard},
title={Recursive deep learning for natural language processing and computer vision},
}

@inproceedings{lee2016generalizing,
title={Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree},
pages={464--472},
year={2016},
author={Lee, Chen-Yu and Gallagher, Patrick W and Tu, Zhuowen},
booktitle={Artificial Intelligence and Statistics},
}

@inproceedings{deng2013new,
pages={8599--8603},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
year={2013},
author={Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
title={New types of deep neural network learning for speech recognition and related applications: An overview},
organization={IEEE},
}

@article{janai2017computer,
author={Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
title={Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art},
journal={arXiv preprint arXiv:1704.05519},
year={2017},
}

@inproceedings{zen2015unidirectional,
year={2015},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
author={Zen, Heiga and Sak, Ha$\backslash$csim},
organization={IEEE},
pages={4470--4474},
title={Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis},
}

@article{ludermir2006optimization,
title={An optimization methodology for neural network weights and architectures},
author={Ludermir, Teresa B and Yamazaki, Akio and Zanchettin, Cleber},
journal={IEEE Transactions on Neural Networks},
year={2006},
publisher={IEEE},
pages={1452--1459},
number={6},
volume={17},
}

@inproceedings{choi2017convolutional,
author={Choi, Keunwoo and Fazekas, Gy{\"{o}}rgy and Sandler, Mark and Cho, Kyunghyun},
title={Convolutional recurrent neural networks for music classification},
booktitle={Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
organization={IEEE},
pages={2392--2396},
year={2017},
}

@article{sigtia2016end,
year={2016},
title={An end-to-end neural network for polyphonic piano music transcription},
journal={IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
number={5},
volume={24},
author={Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
pages={927--939},
publisher={IEEE Press},
}

@article{oord2016pixel,
journal={arXiv preprint arXiv:1601.06759},
year={2016},
author={van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
title={Pixel recurrent neural networks},
}

@inproceedings{bell2016inside,
author={Bell, Sean and {Lawrence Zitnick}, C and Bala, Kavita and Girshick, Ross},
pages={2874--2883},
title={Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks},
year={2016},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
}

@article{ojha2017metaheuristic,
pages={97--116},
title={Metaheuristic design of feedforward neural networks: A review of two decades of research},
journal={Engineering Applications of Artificial Intelligence},
author={Ojha, Varun Kumar and Abraham, Ajith and Sn{\'{a}}{\v{s}}el, V{\'{a}}clav},
publisher={Elsevier},
year={2017},
volume={60},
}

@article{palmes2005mutation,
pages={587--600},
publisher={IEEE},
volume={16},
author={Palmes, Paulito P and Hayasaka, Taichi and Usui, Shiro},
number={3},
title={Mutation-based genetic neural network},
journal={IEEE Transactions on Neural Networks},
year={2005},
}

@article{mao2000probabilistic,
number={4},
publisher={IEEE},
volume={11},
author={Mao, Ke Zhi and Tan, K-C and Ser, Wee},
title={Probabilistic neural-network structure determination for pattern classification},
pages={1009--1016},
year={2000},
journal={IEEE Transactions on neural networks},
}

@article{goldberg2016primer,
pages={345--420},
author={Goldberg, Yoav},
journal={J. Artif. Intell. Res.(JAIR)},
title={A Primer on Neural Network Models for Natural Language Processing.},
volume={57},
year={2016},
}

@article{tsai2006tuning,
journal={IEEE Transactions on Neural Networks},
year={2006},
volume={17},
author={Tsai, Jinn-Tsong and Chou, Jyh-Horng and Liu, Tung-Kuan},
publisher={IEEE},
title={Tuning the structure and parameters of a neural network by using hybrid Taguchi-genetic algorithm},
number={1},
pages={69--80},
}

@article{egmont2002image,
pages={2279--2301},
year={2002},
author={Egmont-Petersen, Michael and de Ridder, Dick and Handels, Heinz},
title={Image processing with neural networks—a review},
publisher={Elsevier},
number={10},
volume={35},
journal={Pattern recognition},
}

@article{arik2017deep,
author={Arik, Sercan O and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Raiman, Jonathan and Sengupta, Shubho and Others},
journal={arXiv preprint arXiv:1702.07825},
title={Deep Voice: Real-time neural text-to-speech},
year={2017},
}

@inproceedings{kasabov2012neucube,
organization={Springer},
pages={225--243},
author={Kasabov, Nikola},
title={NeuCube EvoSpike Architecture for Spatio-temporal Modelling and Pattern Recognition of Brain Signals.},
year={2012},
booktitle={ANNPR},
}

@inproceedings{he2015multimodal,
year={2015},
organization={ACM},
pages={73--80},
title={Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks},
author={He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
booktitle={Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge},
}

@article{jiang1999image,
publisher={Elsevier},
pages={737--760},
year={1999},
author={Jiang, J},
number={9},
title={Image compression with neural networks--a survey},
journal={Signal Processing: Image Communication},
volume={14},
}

@article{santurkar2017generative,
journal={arXiv preprint arXiv:1703.01467},
title={Generative compression},
author={Santurkar, Shibani and Budden, David and Shavit, Nir},
year={2017},
}

@article{araque2017enhancing,
year={2017},
author={Araque, Oscar and Corcuera-Platas, Ignacio and S{\'{a}}nchez-Rada, J Fernando and Iglesias, Carlos A},
journal={Expert Systems with Applications},
publisher={Elsevier},
volume={77},
title={Enhancing deep learning sentiment analysis with ensemble techniques in social applications},
pages={236--246},
}

@article{sato2015apac,
author={Sato, Ikuro and Nishimura, Hiroki and Yokoi, Kensuke},
journal={arXiv preprint arXiv:1505.03229},
year={2015},
title={Apac: Augmented pattern classification with neural networks},
}

@article{isola2016image,
author={Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
journal={arXiv preprint arXiv:1611.07004},
title={Image-to-image translation with conditional adversarial networks},
year={2016},
}

@article{huang2017learning,
title={Learning kernel extended dictionary for face recognition},
journal={IEEE transactions on neural networks and learning systems},
number={5},
year={2017},
volume={28},
publisher={IEEE},
pages={1082--1094},
author={Huang, Ke-Kun and Dai, Dao-Qing and Ren, Chuan-Xian and Lai, Zhao-Rong},
}

@inproceedings{collobert2004links,
organization={ACM},
pages={23},
year={2004},
booktitle={Proceedings of the twenty-first international conference on Machine learning},
author={Collobert, Ronan and Bengio, Samy},
title={Links between perceptrons, MLPs and SVMs},
}

@article{hinton2012deep,
pages={82--97},
volume={29},
number={6},
author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Others},
journal={IEEE Signal Processing Magazine},
title={Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups},
publisher={IEEE},
year={2012},
}

@article{costa2017evaluation,
journal={Applied Soft Computing},
publisher={Elsevier},
title={An evaluation of Convolutional Neural Networks for music classification using spectrograms},
year={2017},
author={Costa, Yandre M G and Oliveira, Luiz S and Silla, Carlos N},
volume={52},
pages={28--38},
}

@article{yao1999evolving,
journal={Proceedings of the IEEE},
pages={1423--1447},
number={9},
year={1999},
volume={87},
publisher={IEEE},
title={Evolving artificial neural networks},
author={Yao, Xin},
}

@article{hwangimage,
year={2016},
author={Hwang, Jeff and Zhou, You},
title={Image Colorization with Deep Convolutional Neural Networks},
}

@inproceedings{gregor2016towards,
author={Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
booktitle={Advances In Neural Information Processing Systems},
year={2016},
pages={3549--3557},
title={Towards conceptual compression},
}

@article{salama2015learning,
volume={9},
journal={Swarm Intelligence},
author={Salama, Khalid M and Abdelbar, Ashraf M},
title={Learning neural network structures with ant colony algorithms},
year={2015},
publisher={Springer},
number={4},
pages={229--265},
}

@article{balle2015density,
journal={arXiv preprint arXiv:1511.06281},
title={Density modeling of images using a generalized normalization transformation},
year={2015},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P},
}

@article{hutchings2017talking,
title={Talking Drums: Generating drum grooves with neural networks},
author={Hutchings, P},
journal={arXiv preprint arXiv:1706.09558},
year={2017},
}

@article{ritchie2003optimizationof,
publisher={BioMed Central},
pages={28},
volume={4},
author={Ritchie, Marylyn D and White, Bill C and Parker, Joel S and Hahn, Lance W and Moore, Jason H},
title={Optimizationof neural network architecture using genetic programming improvesdetection and modeling of gene-gene interactions in studies of humandiseases},
journal={BMC bioinformatics},
number={1},
year={2003},
}

@inproceedings{goodfellow2014generative,
year={2014},
pages={2672--2680},
booktitle={Advances in neural information processing systems},
author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
title={Generative adversarial nets},
}

@inproceedings{gatys2016image,
title={Image style transfer using convolutional neural networks},
year={2016},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
author={Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
pages={2414--2423},
}

@article{Wu2015,
doi={10.1109/EUSIPCO.2015.7362590},
journal={2015 23rd European Signal Processing Conference, EUSIPCO 2015},
title={Drum transcription using partially fixed non-negative matrix factorization},
abstract={In this paper, a drum transcription algorithm using partially fixed non-negative matrix factorization is presented. The proposed method allows users to identify percussive events in complex mixtures with a minimal training set. The algorithm decomposes the music signal into two parts: percussive part with pre-defined drum templates and harmonic part with undefined entries. The harmonic part is able to adapt to the music content, allowing the algorithm to work in polyphonic mixtures. Drum event times can be simply picked from the percussive activation matrix with onset detection. The system is efficient and robust even with a minimal training set. The recognition rates for the ENST dataset vary from 56.7 to 78.9{\%} for three percussive instruments extracted from polyphonic music.},
isbn={9780992862633},
keywords={Automatic Music Transcription,Drum Transcription,MIR,NMF,music transcription,tesis},
author={Wu, Chih Wei and Lerch, Alexander},
year={2015},
pages={1281--1285},
mendeley-tags={music transcription,tesis},
}

@article{Stanley2002a,
mendeley-tags={evolutive},
issn={1063-6560},
archivePrefix={arXiv},
isbn={1063-6560},
arxivId={1407.0576},
pmid={12180173},
title={Evolving Neural Networks through Augmenting Topologies},
number={2},
year={2002},
eprint={1407.0576},
author={Stanley, Kenneth O. and Miikkulainen, Risto},
abstract={The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
journal={Evolutionary Computation},
url={http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
pages={99--127},
keywords={evolutive},
doi={10.1162/106365602320169811},
volume={10},
}

@article{Yadav2007a,
year={2007},
doi={10.1016/j.asoc.2006.01.003},
volume={7},
keywords={Capacity of single neuron,Financial time series prediction,Mackey-Glass time series,Multiplicative neuron model,Time series prediction,forecasting},
abstract={Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network. ?? 2006 Elsevier B.V. All rights reserved.},
journal={Applied Soft Computing Journal},
pages={1157--1163},
issn={15684946},
number={4},
mendeley-tags={forecasting},
author={Yadav, R. N. and Kalra, P. K. and John, J.},
title={Time series prediction with single multiplicative neuron model},
}

@article{Zweig2016,
pages={4805--4809},
url={http://arxiv.org/abs/1609.05935},
author={Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
archivePrefix={arXiv},
isbn={9781509041176},
year={2017},
mendeley-tags={speech recognition},
keywords={CTC,end-to-end training,recurrent neural network,speech recognition},
doi={10.1109/ICASSP.2017.7953069},
eprint={1609.05935},
abstract={This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.},
arxivId={1609.05935},
issn={15206149},
journal={ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
title={Advances in all-neural speech recognition},
}

@article{Zhang2016b,
mendeley-tags={speech recognition,state of the art},
isbn={9781509041176},
arxivId={1610.03022},
keywords={speech recognition,state of the art},
url={http://arxiv.org/abs/1610.03022},
pages={10--14},
year={2016},
archivePrefix={arXiv},
author={Zhang, Yu and Chan, William and Jaitly, Navdeep},
title={Very Deep Convolutional Networks for End-to-End Speech Recognition},
eprint={1610.03022},
abstract={Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
}

@article{Hinton2012,
journal={IEEE Signal Processing Magazine},
keywords={speech recognition},
issn={1053-5888},
doi={10.1109/MSP.2012.2205597},
title={Deep Neural Networks for Acoustic Modeling in Speech Recognition},
mendeley-tags={speech recognition},
arxivId={1207.0580},
abstract={Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
author={Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Kingsbury, Brian},
pmid={13057166},
isbn={1053-5888},
eprint={1207.0580},
year={2012},
pages={82--97},
archivePrefix={arXiv},
number={November},
}

@article{Wang2017a,
pages={1--10},
arxivId={1703.10135},
keywords={speech synthesis,state of the art},
abstract={A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
eprint={1703.10135},
title={Tacotron: Towards End-to-End Speech Synthesis},
url={http://arxiv.org/abs/1703.10135},
year={2017},
mendeley-tags={speech synthesis,state of the art},
author={Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
archivePrefix={arXiv},
}

@article{Stanley2007,
abstract={Natural DNA can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks (CPPNs), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, CPPNs map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with CPPNs through interactive evolution of two dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
journal={Genetic Programming and Evolvable Machines},
doi={10.1007/s10710-007-9028-8},
keywords={Artificial embryogeny,Complexity,Developmental encoding,Evolutionary computation,Generative systems,Indirect encoding,Representation},
isbn={1389-2576},
issn={13892576},
pages={131--162},
volume={8},
number={2},
author={Stanley, Kenneth O.},
year={2007},
title={Compositional pattern producing networks: A novel abstraction of development},
}

@article{Mollahosseini2015a,
author={Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},
year={2015},
eprint={1511.04110},
abstract={Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.},
mendeley-tags={facial expression},
keywords={facial expression},
doi={10.1109/WACV.2016.7477450},
url={http://arxiv.org/abs/1511.04110{\%}0Ahttp://dx.doi.org/10.1109/WACV.2016.7477450},
arxivId={1511.04110},
archivePrefix={arXiv},
title={Going Deeper in Facial Expression Recognition using Deep Neural Networks},
}

@article{Lin2013b,
keywords={image classification},
year={2013},
isbn={9781479972913},
doi={10.1109/ASRU.2015.7404828},
url={http://arxiv.org/abs/1312.4400},
mendeley-tags={image classification},
pmid={24356345},
title={Network In Network},
eprint={1312.4400},
abstract={We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
author={Lin, Min and Chen, Qiang and Yan, Shuicheng},
archivePrefix={arXiv},
pages={1--10},
arxivId={1312.4400},
issn={03029743},
}

@article{Saxena2016a,
issn={10495258},
eprint={1606.02492},
archivePrefix={arXiv},
url={http://arxiv.org/abs/1606.02492},
author={Saxena, Shreyas and Verbeek, Jakob},
title={Convolutional Neural Fabrics},
mendeley-tags={evolutive},
year={2016},
arxivId={1606.02492},
abstract={Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
keywords={evolutive},
}

@article{Barrow2016a,
keywords={Bootstrapping,Cross-validation,Forecast combination,Monte Carlo,Time series,forecasting},
title={Cross-validation aggregation for combining autoregressive neural network forecasts},
issn={01692070},
number={4},
url={http://dx.doi.org/10.1016/j.ijforecast.2015.12.011},
year={2016},
mendeley-tags={forecasting},
abstract={This paper evaluates k-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to k-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
journal={International Journal of Forecasting},
pages={1120--1137},
publisher={Elsevier B.V.},
author={Barrow, Devon K. and Crone, Sven F.},
volume={32},
doi={10.1016/j.ijforecast.2015.12.011},
}

@article{Omar2017,
keywords={security},
isbn={1359079051062},
number={2},
url={http://dx.doi.org/10.1108/eb025814{\%}5Cnhttp://},
abstract={This study explores the effectiveness of an Artificial Neural Network (ANN) in predicting fraudulent financial reporting in small market capitalization companies in Malaysia. Design/methodology/approach Based on the concepts of ANN, a mathematical model is developed to compare non-fraud and fraud companies selected from among small market capitalization companies in Malaysia; the fraud companies had already been charged by the Securities Commission for the falsification of financial statements. Ten financial ratios are used as fraud risk indicators to predict fraudulent financial reporting using ANN. Findings Indicate that the proposed ANN methodology outperforms other statistical techniques widely used for predicting fraudulent financial reporting. Originality/value The study is one of few to adopt the ANN approach to the prediction of financial reporting fraud.},
journal={Journal of Financial Crime Iss},
volume={24},
year={2017},
doi={10.1108/13590791011082797},
issn={1359-0790},
title={Predicting fraudulent financial reporting using artificial neural network},
author={Omar, Normah and Johari, Zulaikha 'Amirah and Smith, Malcolm},
mendeley-tags={security},
}

@article{Janai2017a,
archivePrefix={arXiv},
mendeley-tags={image classification,object detection,review},
eprint={1704.05519},
author={Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
title={Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art},
abstract={Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
url={http://arxiv.org/abs/1704.05519},
arxivId={1704.05519},
keywords={and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,image classification,manner 1,object detection,overview,review,survey will become a,useful tool,we hope that our},
year={2017},
}

@article{Aljumah2016,
title={A Novel Approach for Detecting DDoS using Artificial Neural},
pages={132--138},
author={Aljumah, Abdullah and Ahamad, Tariq},
mendeley-tags={security},
volume={16},
number={12},
year={2016},
keywords={security},
}

@article{Raczynski2013,
number={9},
author={Raczy{\'{n}}ski, Stanis{\l}aw A. and Vincent, Emmanuel and Sagayama, Shigeki},
year={2013},
mendeley-tags={music transcription},
pages={1830 -- 1840},
volume={21},
doi={10.1109/TASL.2013.2258012},
keywords={music transcription},
title={Dynamic Bayesian networks for symbolic polyphonic pitch modeling},
journal={IEEE Transactions on Audio, Speech, and Language Processing},
}

@article{Bengio2007,
year={2007},
keywords={theory},
doi={10.1.1.72.4580},
number={1},
pages={321--360},
isbn={1002620262},
mendeley-tags={theory},
pmid={11359439},
author={Bengio, Yoshua and {\{}LeCun{\}}, Yann and Lecun, Yann},
abstract={One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
issn={00099104},
journal={Large Scale Kernel Machines},
title={Scaling Learning Algorithms towards AI},
}

@article{Clune2013,
abstract={A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks--their organization as functional, sparsely connected subunits--but there is no consensus regarding why modularity itself evolved. Although most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Computational evolution experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyse research in numerous disciplines, such as neuroscience and genetics, and enhance our ability to harness evolution for engineering purposes.},
number={1755},
pages={20122863--20122863},
year={2013},
url={http://rspb.royalsocietypublishing.org/cgi/doi/10.1098/rspb.2012.2863},
eprint={1207.2743v1},
issn={0962-8452},
volume={280},
archivePrefix={arXiv},
keywords={computational biology,evolution},
arxivId={1207.2743v1},
journal={Proceedings of the Royal Society B: Biological Sciences},
title={The evolutionary origins of modularity},
isbn={1471-2954 (Electronic)$\backslash$n0962-8452 (Linking)},
doi={10.1098/rspb.2012.2863},
pmid={23363632},
author={Clune, J. and Mouret, J.-B. and Lipson, H.},
}

@misc{Rosenblatt1957,
booktitle={Report 85, Cornell Aeronautical Laboratory},
author={Rosenblatt, F.},
title={The Perceptron - A Perceiving and Recognizing Automaton},
pages={460--1},
mendeley-tags={seminal},
keywords={seminal},
year={1957},
abstract={First publication about the perceptron},
doi={85-460-1},
}

@article{Yang2017,
archivePrefix={arXiv},
arxivId={1707.01786},
author={Yang, Yinchong and Krompass, Denis and Tresp, Volker},
url={https://arxiv.org/pdf/1707.01786.pdf},
mendeley-tags={video classification},
year={2017},
eprint={1707.01786},
title={Tensor-Train Recurrent Neural Networks for Video Classification},
abstract={The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
keywords={video classification},
}

@article{McCulloch1943,
number={4},
abstract={Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
pages={115--133},
pmid={2185863},
arxivId={arXiv:1011.1669v3},
volume={5},
doi={10.1007/BF02478259},
issn={00074985},
keywords={seminal},
mendeley-tags={seminal},
author={McCulloch, Warren S. and Pitts, Walter},
journal={The Bulletin of Mathematical Biophysics},
title={A logical calculus of the ideas immanent in nervous activity},
isbn={0007-4985},
archivePrefix={arXiv},
year={1943},
eprint={arXiv:1011.1669v3},
}

@article{Rosenblatt1958,
doi={10.1037/h0042519},
issn={0033-295X},
journal={Psychological Review},
url={http://content.apa.org/journals/rev/65/6/386},
isbn={0033-295X},
number={6},
keywords={seminal},
pages={386--408},
abstract={To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
year={1958},
volume={65},
mendeley-tags={seminal},
pmid={13602029},
author={Rosenblatt, F.},
title={The perceptron: A probabilistic model for information storage and organization in the brain.},
}

@article{Xu2015a,
isbn={9781941643730},
journal={Acl-2015},
pages={250--255},
title={CCG Supertagging with a Recurrent Neural Network},
keywords={sentence classification},
year={2015},
mendeley-tags={sentence classification},
number={2014},
author={Xu, Wenduan and Auli, Michael and Clark, Stephen},
}

@article{Araque2017a,
publisher={Elsevier Ltd},
year={2017},
isbn={0957-4174},
keywords={Deep learning,Ensemble,Machine learning,Natural language processing,Sentiment analysis,sentiment analysis},
volume={77},
mendeley-tags={sentiment analysis},
author={Araque, Oscar and Corcuera-Platas, Ignacio and S{\'{a}}nchez-Rada, J. Fernando and Iglesias, Carlos A.},
pages={236--246},
doi={10.1016/j.eswa.2017.02.002},
abstract={Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-Score.},
journal={Expert Systems with Applications},
title={Enhancing deep learning sentiment analysis with ensemble techniques in social applications},
issn={09574174},
}

@article{Leshno1991,
pages={1--16},
title={Multilayer Feedforward Networks with Non-Polynomial Activation Functions Can Approximate Any Function},
mendeley-tags={theory},
author={Leshno, Moshe and Schocken, Shimon},
year={1991},
keywords={theory},
url={https://archive.nyu.edu/bitstream/2451/14384/1/IS-91-26.pdf},
number={21},
}

@article{Widrob1990,
mendeley-tags={review},
number={9},
issn={15582256},
year={1990},
pages={1415--1442},
title={30 years of adaptive neural networks: perceptron, Madaline, andbackpropagation},
isbn={0018-9219},
volume={78},
keywords={review},
journal={Proceedings of the IEEE},
abstract={Fundamental developments in feedforward artificial neural networks from the past thirty years are reviewed. The history, origination, operating characteristics, and basic theory of several supervised neural-network training algorithms (including the perceptron rule, the least-mean-square algorithm, three Madaline rules, and the backpropagation technique) are described. The concept underlying these iterative adaptation algorithms is the minimal disturbance principle, which suggests that during training it is advisable to inject new information into a network in a manner that disturbs stored information to the smallest extent possible. The two principal kinds of online rules that have developed for altering the weights of a network are examined for both single-threshold elements and multielement networks. They are error-correction rules, which alter the weights of a network to correct error in the output response to the present input pattern, and gradient rules, which alter the weights of a network during each pattern presentation by gradient descent with the objective of reducing mean-square error (averaged over all training patterns)},
author={Widrob, B and Widrob, B and Lehr, Michael A. and Lehr, Michael A. and Widrow, Bernard and Lehr, Michael A.},
doi={10.1109/5.58323},
}

@article{Fritzke1994a,
url={http://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
year={1994},
isbn={0893-6080},
number={9},
pages={1441--1460},
mendeley-tags={evolutive,seminal},
title={Growing cell structures—A self-organizing network for unsupervised and supervised learning},
keywords={--self-organization,1,as proposed by,classification,clustering,data visualization,evolutive,feature map,i n t r,incremental learning,o d u c,pattern,radial basis function,self-organizing neural network models,seminal,t i o n,two-spiral problem},
author={Fritzke, Bernd},
issn={08936080},
volume={7},
doi={10.1016/0893-6080(94)90091-4},
journal={Neural Networks},
abstract={We present a new self-organizing neural network model that has two variants. The first variant performs unsupervised learning and can be used for data visualization, clustering, and vector quantization. The main advantage over existing approaches (e.g., the Kohonen feature map) is the ability of the model to automatically find a suitable network structure and size. This is achieved through a controlled growth process that also includes occasional removal of units. The second variant of the model is a supervised learning method that results from the combination of the above-mentioned self-organizing network with the radial basis function (RBF) approach. In this model it is possible—in contrast to earlier approaches—to perform the positioning of the RBF units and the supervised training of the weights in parallel. Therefore, the current classification error can be used to determine where to insert new RBF units. This leads to small networks that generalize very well. Results on the two-spirals benchmark and a vowel classification problem are presented that are better than any results previously published.},
}

@article{M??ller2001,
issn={10459227},
abstract={This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.},
pages={181--201},
author={M??ller, Klaus Robert and Mika, Sebastian and R??tsch, Gunnar and Tsuda, Koji and Sch??lkopf, Bernhard},
pmid={18244377},
keywords={Boosting,Fisher's discriminant,Kernel PCA,Kernel methods,Mathematical programming machines,Mercer kernels,Principal component analysis (PCA),Single-class classification,Support vector machines (SVMs),seminal},
mendeley-tags={seminal},
title={An introduction to kernel-based learning algorithms},
doi={10.1109/72.914517},
number={2},
volume={12},
isbn={1045-9227},
journal={IEEE Transactions on Neural Networks},
year={2001},
}

@article{Deng2016,
mendeley-tags={social recommendations},
abstract={With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major benefit of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user's trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users' interests and their trusted friends' interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.},
issn={21622388},
year={2016},
journal={IEEE Transactions on Neural Networks and Learning Systems},
author={Deng, Shuiguang and Huang, Longtao and Xu, Guandong and Wu, Xindong and Wu, Zhaohui},
doi={10.1109/TNNLS.2016.2514368},
title={On Deep Learning for Trust-Aware Recommendations in Social Networks},
number={February},
isbn={2162-2388 (Electronic)$\backslash$r2162-237X (Linking)},
pmid={26915135},
keywords={social recommendations},
}

@article{Tang2014,
issn={2329-9290},
volume={23},
title={A Joint Segmentation and Classification Framework for Sentiment Analysis},
year={2014},
number={2002},
journal={Proceedings of the 2014 Conferenve on Empirical Methods in Natural Language Processing (EMNLP)},
pages={477--487},
author={Tang, Duyu and Wei, Furu and Qin, Bing and Dong, Li and Liu, Ting and Zhou, Ming},
mendeley-tags={sentiment analysis},
isbn={2329-9290 VO - 23},
keywords={sentiment analysis},
abstract={In this paper, we propose a joint segmenta- tion and classification framework for sen- timent analysis. Existing sentiment clas- sification algorithms typically split a sen- tence as a word sequence, which does not effectively handle the inconsistent senti- ment polarity between a phrase and the words it contains, such as “not bad” and “a great deal of ”. We address this issue by developing a joint segmentation and classification framework (JSC), which si- multaneously conducts sentence segmen- tation and sentence-level sentiment classi- fication. Specifically, we use a log-linear model to score each segmentation candi- date, and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier. A marginal log-likelihood objective function is de- vised for the segmentation model, which is optimized for enhancing the sentiment classification performance. The joint mod- el is trained only based on the annotat- ed sentiment polarity of sentences, with- out any segmentation annotations. Experi- ments on a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that, our joint model performs com- parably with the state-of-the-art methods.},
doi={10.1109/TASLP.2015.2449071},
}

@article{Chen2017a,
abstract={Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
author={Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
journal={Expert Systems with Applications},
keywords={Deep neural network,Natural language processing,Sentiment analysis,sentence classification,state of the art},
pages={221--230},
doi={10.1016/j.eswa.2016.10.065},
publisher={Elsevier Ltd},
title={Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN},
url={http://dx.doi.org/10.1016/j.eswa.2016.10.065},
volume={72},
year={2017},
mendeley-tags={sentence classification,state of the art},
issn={09574174},
}

@article{Tang2015,
isbn={2162-2388 (Electronic) 2162-237X (Linking)},
mendeley-tags={ELM},
pmid={25966483},
keywords={ELM},
journal={IEEE Transactions on Neural Networks and Learning Systems},
abstract={— Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parame-ters are randomly generated and the output weights are analyti-cally computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme. Index Terms— Deep learning (DL), deep neural network (DNN), extreme learning machine (ELM), multilayer perceptron (MLP), random feature mapping.},
year={2015},
author={Tang, Jiexiong and ChenweiDeng and Huang, Guang-Bin},
pages={1--13},
title={Extreme Learning Machine for Multilayer Perceptron},
doi={10.1109/TNNLS.2015.2424995},
issn={2162-237X},
}

@article{Mao2000,
pages={1009--1016},
mendeley-tags={evolutive,seminal},
year={2000},
author={Mao, K Z and Tan, K C and Ser, W},
volume={11},
journal={IEEE Transactions on Neural Networks},
keywords={evolutive,seminal},
title={Probabilistic neural network structure determination for pattern classification},
number={4},
}

@article{Karpathy2014a,
archivePrefix={arXiv},
abstract={Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new...},
journal={2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
author={Karpathy, Andrej and Toderici, George and Shetty, Sachin and Leung, Tommy and Sukthankar, Rahul and Fei-Fei, Li},
issn={978-1-4799-5118-5},
pages={1725--1732},
eprint={1412.0767},
year={2014},
isbn={978-1-4799-5118-5},
url={http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909619{\%}0Apapers3://publication/doi/10.1109/CVPR.2014.223},
title={Large-Scale Video Classification with Convolutional Neural Networks},
doi={10.1109/CVPR.2014.223},
keywords={video classification},
arxivId={1412.0767},
mendeley-tags={video classification},
}

@article{Schmidhuber2015,
isbn={0893-6080},
volume={61},
archivePrefix={arXiv},
journal={Neural Networks},
title={Deep Learning in neural networks: An overview},
doi={10.1016/j.neunet.2014.09.003},
arxivId={1404.7828},
keywords={Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning,review},
issn={18792782},
mendeley-tags={review},
author={Schmidhuber, J??rgen},
pmid={25462637},
pages={85--117},
abstract={In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
year={2015},
eprint={1404.7828},
}

@article{Ritchie2003,
keywords={*Epistasis,*Models,*Neural Networks (Computer),*Software,Algorithms,Artificial Intelligence,Gene Expression Regulation/*genetics,Genetic,Humans,Molecular Epidemiology/*methods/*trends,Polymorphism,Predictive Value of Tests,Research Design,Single Nucleotide/genetics,Software Validation,evolutive},
journal={BMC bioinformatics},
abstract={BACKGROUND: Appropriate definition of neural network architecture prior to data analysis is crucial for successful data mining. This can be challenging when the underlying model of the data is unknown. The goal of this study was to determine whether optimizing neural network architecture using genetic programming as a machine learning strategy would improve the ability of neural networks to model and detect nonlinear interactions among genes in studies of common human diseases. RESULTS: Using simulated data, we show that a genetic programming optimized neural network approach is able to model gene-gene interactions as well as a traditional back propagation neural network. Furthermore, the genetic programming optimized neural network is better than the traditional back propagation neural network approach in terms of predictive ability and power to detect gene-gene interactions when non-functional polymorphisms are present. CONCLUSION: This study suggests that a machine learning strategy for optimizing neural network architecture may be preferable to traditional trial-and-error approaches for the identification and characterization of gene-gene interactions in common, complex human diseases.},
mendeley-tags={evolutive},
isbn={1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
pages={28},
url={http://www.ncbi.nlm.nih.gov/pubmed/12846935},
volume={4},
author={Ritchie, M D and White, B C and Parker, J S and Hahn, L W and Moore, J H},
pmid={12846935},
doi={10.1186/1471-2105-4-28},
year={2003},
issn={1471-2105},
title={Optimization of neural network architecture using genetic programming improves detection and modeling of gene-gene interactions in studies of human diseases},
}

@article{Shin2016,
arxivId={1602.03409},
title={Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning},
journal={IEEE Transactions on Medical Imaging},
volume={35},
isbn={0278-0062 VO - 35},
mendeley-tags={image classification,optimization},
year={2016},
number={5},
eprint={1602.03409},
abstract={Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and the revival of deep CNN. CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, with 85{\%} sensitivity at 3 false positive per patient, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
keywords={Biomedical imaging,computer aided diagnosis,image analysis,image classification,machine learning,neural networks,optimization},
author={Shin, Hoo Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
pmid={26886976},
archivePrefix={arXiv},
doi={10.1109/TMI.2016.2528162},
pages={1285--1298},
issn={1558254X},
}

@article{Bell2015a,
author={Bell, Sean and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross},
url={http://arxiv.org/abs/1512.04143},
keywords={object detection},
isbn={978-1-4673-8851-1},
mendeley-tags={object detection},
pages={2874--2883},
year={2015},
doi={10.1109/CVPR.2016.314},
arxivId={1512.04143},
eprint={1512.04143},
title={Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks},
issn={978-953-7619-08-4},
archivePrefix={arXiv},
pmid={21803542},
abstract={It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9{\%} to 76.4{\%} mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7{\%} to 33.1{\%} mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
}

@article{Srivastava2014,
abstract={Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
mendeley-tags={seminal},
author={Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi={10.1214/12-AOS1000},
volume={15},
journal={Journal of Machine Learning Research},
issn={15337928},
title={Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
keywords={deep learning,model combination,neural networks,regularization,seminal},
pages={1929--1958},
isbn={1532-4435},
archivePrefix={arXiv},
year={2014},
arxivId={1102.4807},
eprint={1102.4807},
}

@article{Goodfellow2014,
isbn={1406.2661},
pages={2672--2680},
issn={10495258},
eprint={arXiv:1406.2661v1},
journal={Advances in Neural Information Processing Systems 27},
abstract={We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
keywords={seminal},
title={Generative Adversarial Nets (NIPS version)},
url={http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year={2014},
doi={10.1001/jamainternmed.2016.8245},
archivePrefix={arXiv},
arxivId={arXiv:1406.2661v1},
mendeley-tags={seminal},
author={Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
}

@article{Hayes2013,
year={2013},
number={April},
author={Hayes, Brian},
title={First Links in the Markov Chain Brian},
}

@article{Yosinski2014,
arxivId={1411.1792},
archivePrefix={arXiv},
url={http://arxiv.org/abs/1411.1792},
keywords={theory},
eprint={1411.1792},
year={2014},
title={How transferable are features in deep neural networks?},
pages={1--9},
issn={10495258},
author={Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
mendeley-tags={theory},
abstract={Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
}

@article{Graves2014,
arxivId={1410.5401},
title={Neural Turing Machines},
url={http://arxiv.org/abs/1410.5401},
author={Graves, Alex and Wayne, Greg and Danihelka, Ivo},
pmid={18958277},
mendeley-tags={other},
doi={10.3389/neuro.12.006.2007},
issn={2041-1723},
keywords={other},
eprint={1410.5401},
year={2014},
pages={1--26},
archivePrefix={arXiv},
isbn={0028-0836},
abstract={We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
}

@article{Wu2017,
doi={10.1007/s11042-017-4440-4},
issn={15737721},
pmid={23554596},
mendeley-tags={object detection},
author={Wu, Songtao and Zhong, Shenghua and Liu, Yan},
eprint={1512.03385},
arxivId={1512.03385},
pages={1--17},
archivePrefix={arXiv},
keywords={Convolutional neural networks,Image steganalysis,Residual learning,object detection},
title={Deep residual learning for image Recognition},
journal={Multimedia Tools and Applications},
year={2017},
abstract={Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
isbn={978-1-4673-6964-0},
}

@article{LeCun1998a,
author={LeCun, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
doi={10.1109/5.726791},
pmid={15823584},
abstract={Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
isbn={0018-9219},
journal={Proceedings of the IEEE},
keywords={Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR),seminal},
title={Gradient-based learning applied to document recognition},
archivePrefix={arXiv},
arxivId={1102.0183},
eprint={1102.0183},
mendeley-tags={seminal},
volume={86},
number={11},
issn={00189219},
pages={2278--2323},
year={1998},
}

@article{Santurkar2017a,
url={http://arxiv.org/abs/1703.01467},
archivePrefix={arXiv},
eprint={1703.01467},
year={2017},
keywords={image compression- lossy},
author={Santurkar, Shibani and Budden, David and Shavit, Nir},
title={Generative Compression},
arxivId={1703.01467},
abstract={Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes.},
mendeley-tags={image compression- lossy},
}

@article{Johnston2017a,
mendeley-tags={image compression- lossy},
year={2017},
author={Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
eprint={1703.10114},
title={Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
arxivId={1703.10114},
abstract={We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
keywords={image compression- lossy},
archivePrefix={arXiv},
url={http://arxiv.org/abs/1703.10114},
}

@article{Iizuka2016a,
doi={10.1145/2897824.2925974},
issn={07300301},
title={Let there be color! :Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification},
url={http://dl.acm.org/citation.cfm?doid=2897824.2925974},
journal={ACM Transactions on Graphics},
volume={35},
year={2016},
mendeley-tags={image synthesis},
author={Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
abstract={We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
number={4},
keywords={colorization,computing methodologies,convolutional neural network concepts,image processing,image synthesis,neural net-},
isbn={9781450342797},
pages={1--11},
}

@article{Pang2017a,
arxivId={1603.06759},
author={Pang, Yanwei and Sun, Manli and Jiang, Xiaoheng and Li, Xuelong},
year={2017},
abstract={Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and {\$} 1\backslashtimes 1 {\$} convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method.},
eprint={1603.06759},
issn={21622388},
pages={1--11},
keywords={image classification},
mendeley-tags={image classification},
journal={IEEE Transactions on Neural Networks and Learning Systems},
archivePrefix={arXiv},
doi={10.1109/TNNLS.2017.2676130},
title={Convolution in Convolution for Network in Network},
}

@article{Theis2015a,
issn={10495258},
year={2015},
author={Theis, Lucas and Bethge, Matthias},
arxivId={1506.03478},
abstract={Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
mendeley-tags={image synthesis},
title={Generative Image Modeling Using Spatial LSTMs},
pages={1--9},
keywords={image synthesis},
url={http://arxiv.org/abs/1506.03478},
archivePrefix={arXiv},
eprint={1506.03478},
}

@article{Jiang1999,
number={9},
pages={737--760},
title={Image compression with neural networks - a survey},
year={1999},
issn={09235965},
volume={14},
mendeley-tags={image compression- lossy},
abstract={Apart from the existing technology on image compression represented by series of JPEG, MPEG and H.26x standards, new technology such as neural networks and genetic algorithms are being developed to explore the future of image coding. Successful applications of neural networks to vector quantization have now become well established, and other aspects of neural network involvement in this area are stepping up to play significant roles in assisting with those traditional technologies. This paper presents an extensive survey on the development of neural networks for image compression which covers three categories: direct image compression by neural networks; neural network implementation of existing techniques, and neural network based technology which provide improvement over traditional algorithms.},
keywords={image compression and coding,image compression- lossy,neural network},
isbn={0923-5965},
author={Jiang, J.},
doi={10.1016/S0923-5965(98)00041-1},
journal={Signal Processing: Image Communication},
}

@article{Wang2015a,
title={An Image Compression Scheme Based on Fuzzy Neural Network},
volume={13},
author={Wang, Bo and Gao, Yubin},
pages={137},
year={2015},
issn={2302-9293},
url={http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
doi={10.12928/telkomnika.v13i1.1270},
journal={TELKOMNIKA (Telecommunication Computing Electronics and Control)},
keywords={fuzzy theory,image compression,image compression- lossy,neural network},
number={1},
abstract={Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, self- adaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
mendeley-tags={image compression- lossy},
}

@article{Gregor2016a,
number={Nips},
abstract={We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
author={Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
eprint={1604.08772},
title={Towards Conceptual Compression},
mendeley-tags={image compression - conceptual,state of the art},
arxivId={1604.08772},
url={http://arxiv.org/abs/1604.08772},
issn={10495258},
year={2016},
archivePrefix={arXiv},
keywords={image compression - conceptual,state of the art},
}

@article{Toderici2016a,
url={http://arxiv.org/abs/1608.05148},
doi={10.4135/9781412985277},
archivePrefix={arXiv},
mendeley-tags={image compression- lossy},
eprint={1608.05148},
arxivId={1608.05148},
isbn={9780761914402},
issn={08936080},
author={Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
keywords={image compression- lossy},
pmid={21655600},
title={Full Resolution Image Compression with Recurrent Neural Networks},
year={2016},
abstract={This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
}

@article{Risi2017,
volume={9},
journal={IEEE Transactions on Computational Intelligence and AI in Games},
keywords={Evolutionary algorithms,evolutive,games,neural networks,neuroevolution,review},
mendeley-tags={evolutive,games,review},
title={Neuroevolution in Games: State of the Art and Open Challenges},
year={2017},
pages={25--41},
number={1},
archivePrefix={arXiv},
eprint={1410.7326},
abstract={This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
isbn={1943-068X VO  - PP},
doi={10.1109/TCIAIG.2015.2494596},
issn={1943068X},
arxivId={1410.7326},
author={Risi, Sebastian and Togelius, Julian},
}

@article{Toderici2015a,
arxivId={1511.06085},
mendeley-tags={image compression- lossy},
title={Variable Rate Image Compression with Recurrent Neural Networks},
keywords={image compression- lossy},
author={Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
eprint={1511.06085},
year={2015},
abstract={A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32{\$}\backslashtimes{\$}32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10{\%} or more.},
archivePrefix={arXiv},
pages={1--12},
url={http://arxiv.org/abs/1511.06085},
}

@article{Balle2016,
eprint={1611.01704},
keywords={image compression},
archivePrefix={arXiv},
abstract={We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
issn={01973975},
pmid={16508805},
arxivId={1611.01704},
doi={10.1016/S0197-3975(03)00059-6},
author={Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
url={http://arxiv.org/abs/1611.01704},
title={End-to-end Optimized Image Compression},
year={2016},
isbn={0197-3975},
mendeley-tags={image compression},
}

@article{Egmont-Petersen2002,
volume={35},
year={2002},
abstract={We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments. ?? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
issn={00313203},
pages={2279--2301},
title={Image processing with neural networks- A review},
journal={Pattern Recognition},
isbn={0031-3203},
doi={10.1016/S0031-3203(01)00178-9},
keywords={Digital image processing,Feature extraction,Image compression,Image understanding,Invariant pattern recognition,Neural networks,Object recognition,Optimization,Preprocessing,Segmentation,image classification,object detection,review},
author={Egmont-Petersen, M. and {De Ridder}, D. and Handels, H.},
mendeley-tags={image classification,object detection,review},
number={10},
}

@article{Huang2016,
mendeley-tags={image classification},
doi={10.1109/TNNLS.2016.2522431},
year={2016},
author={Huang, Ke Kun and Dai, Dao Qing and Ren, Chuan Xian and Lai, Zhao Rong},
title={Learning Kernel Extended Dictionary for Face Recognition},
keywords={image classification},
number={March},
journal={IEEE Transactions on Neural Networks and Learning Systems},
issn={21622388},
}

@article{Kulkarni2015a,
year={2015},
title={Deep Convolutional Inverse Graphics Network},
author={Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
doi={10.1063/1.4914407},
arxivId={1503.03167},
mendeley-tags={image synthesis,tesis},
keywords={image synthesis,tesis},
eprint={1503.03167},
archivePrefix={arXiv},
pages={1--10},
abstract={This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
url={http://arxiv.org/abs/1503.03167},
issn={10897550},
}

@article{Krishna2016,
author={Krishna, Tushar and Emer, Joel and Sze, Vivienne and Conference, International Solid-state Circuits and Francisco, San and Chen, Yu-hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
title={Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks The MIT Faculty has made this article openly available . Please share Citation " Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Publisher Version},
mendeley-tags={hardware},
keywords={hardware},
year={2016},
}

@article{Silver2016,
year={2016},
author={Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
isbn={1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
pmid={26819042},
keywords={games},
eprint={1610.00633},
publisher={Nature Publishing Group},
doi={10.1038/nature16961},
title={Mastering the game of Go with deep neural networks and tree search},
journal={Nature},
pages={484--489},
arxivId={1610.00633},
issn={0028-0836},
abstract={The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
volume={529},
url={http://www.nature.com/doifinder/10.1038/nature16961},
archivePrefix={arXiv},
mendeley-tags={games},
number={7587},
}

@article{Hayashi2017,
title={A new design principle of robust onion-like networks self-organized in growth},
url={http://arxiv.org/abs/1706.03910},
year={2017},
arxivId={1706.03910},
abstract={Today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading efficiency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures.},
author={Hayashi, Yukio},
eprint={1706.03910},
keywords={coexistence of efficiency and,evolutive,interwoven,long-distance relations,loops,onion-like structure,robustness,unselfish self-organization},
mendeley-tags={evolutive},
archivePrefix={arXiv},
}

@article{Khorrami2016a,
url={http://arxiv.org/abs/1602.07377},
mendeley-tags={sentiment analysis,video classification},
title={How Deep Neural Networks Can Improve Emotion Recognition on Video Data},
abstract={We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
eprint={1602.07377},
arxivId={1602.07377},
keywords={sentiment analysis,video classification},
year={2016},
author={Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
archivePrefix={arXiv},
}

@article{Hausknecht2017,
abstract={This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional-integral-derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator's inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.},
isbn={8750141007},
pmid={26829807},
year={2017},
journal={IEEE Transactions on Neural Networks and Learning Systems},
keywords={Cerebellar pattern recognition,MNIST handwritten digit recognition,Robot balance,cerebellum,health,inverted pendulum balancing (cart-pole),proportional-integral-derivative (PID) control},
volume={28},
author={Hausknecht, Matthew and Li, Wen Ke and Mauk, Michael and Stone, Peter},
title={Machine Learning Capabilities of a Simulated Cerebellum},
pages={510--522},
doi={10.1109/TNNLS.2015.2512838},
issn={21622388},
mendeley-tags={health},
number={3},
}

@article{Bennasar2015,
url={http://dx.doi.org/10.1016/j.eswa.2015.07.007},
number={22},
volume={42},
author={Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
title={Feature selection using Joint Mutual Information Maximisation},
mendeley-tags={other},
doi={10.1016/j.eswa.2015.07.007},
abstract={Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the 'maximum of the minimum' criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the JMIM method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6{\%} in comparison to the next best performing method. The statistical significance of the results is confirmed by the ANOVA test. Moreover, this method produces the best trade-off between accuracy and stability.},
isbn={0957-4174},
issn={09574174},
journal={Expert Systems with Applications},
pages={8520--8532},
publisher={Elsevier Ltd.},
keywords={Classification,Conditional mutual information,Dimensionality reduction,Feature selection,Feature selection stability,Joint mutual information,Mutual information,Subset feature selection,other},
year={2015},
}

@article{Shizhou2016a,
title={Improving DCNN performance with sparse category-selective objective function},
volume={2016-Janua},
mendeley-tags={image synthesis},
author={Shizhou, Zhang and Gong, Yihong and Jinjun, Wang},
issn={10450823},
pages={2343--2349},
year={2016},
keywords={Machine Learning,image synthesis},
journal={IJCAI International Joint Conference on Artificial Intelligence},
abstract={In this paper, we choose to learn useful cues from object recognition mechanisms of the human vi-sual cortex, and propose a DCNN performance im-provement method without the need for increasing the network complexity. Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As ex-perimental results show, when applying the pro-posed method to the " Quick " model and NIN models, image classification performances are re-markably improved on four widely used bench-mark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
}

@article{Park2017a,
year={2017},
doi={10.1007/978-3-319-54184-6_12},
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
issn={16113349},
mendeley-tags={optimization},
author={Park, Sungheon and Kwak, Nojun},
title={Analysis on the dropout effect in convolutional neural networks},
pages={189--204},
isbn={9783319541839},
keywords={optimization},
volume={10112 LNCS},
}

@article{Phumrattanaprapin2016,
number={2},
keywords={ELM,chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
year={2016},
author={Phumrattanaprapin, Khanittha;Punyaphol Horata},
title={Extended Hierarchical Extreme Learning Machine with Multilayer Perceptron},
mendeley-tags={ELM},
volume={10},
pages={196--204},
}

@article{Liao2016a,
title={On the importance of normalisation layers in deep learning with piecewise linear activation units},
doi={10.1109/WACV.2016.7477624},
archivePrefix={arXiv},
journal={2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
eprint={1508.00330},
mendeley-tags={image classification,optimization,theory},
year={2016},
arxivId={1508.00330},
keywords={image classification,optimization,theory},
abstract={Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
isbn={9781509006410},
author={Liao, Zhibin and Carneiro, Gustavo},
}

@article{Huang2015,
doi={10.1007/s12559-015-9333-0},
year={2015},
issn={18669964},
keywords={ELM,Extreme learning machine,Feedforward neural network,QuickNet,Radial basis function network,Random vector functional link,Randomness},
number={3},
pages={263--278},
publisher={Springer US},
author={Huang, Guang Bin},
title={What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt's Dream and John von Neumann's Puzzle},
journal={Cognitive Computation},
mendeley-tags={ELM},
abstract={Abstract—The emergent machine learning technique - Ex- treme Learning Machines (ELMs) - has become a hot area of research over the past years, which is attributed to the growing research activities and significant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that i) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes / neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes / neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; ii) ELM is proposed for both generalized single hidden layer feedfoward network and multi hidden layers feedforward networks; iii) Homogeneous architecture based ELM is proposed for feature learning, clustering, regression and (binary / multi- class) classification. iv) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi layers of networks either.},
isbn={1866-9956},
volume={7},
}

@article{Sigtia2016,
year={2016},
abstract={We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yields the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
journal={IEEE/ACM Transactions on Audio Speech and Language Processing},
pages={927--939},
arxivId={1508.01774},
issn={23299290},
doi={10.1109/TASLP.2016.2533858},
archivePrefix={arXiv},
title={An end-to-end neural network for polyphonic piano music transcription},
keywords={Automatic music transcription,Deep learning,Music language models,Recurrent neural networks},
number={5},
volume={24},
author={Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
eprint={1508.01774},
}

@article{Southall2016a,
journal={Proceedings of International Society for Music Information Retrieval Conference (ISMIR)},
abstract={Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in-struments in audio recordings. Neural networks have al-ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro-pose the use of neural networks for ADT in order to ex-ploit their ability to capture a complex configuration of fea-tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu-ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit-able for online operation. In both systems, a separate net-work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis-ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec-tively. The results demonstrate the effectiveness of the pre-sented methods for solo percussion and a capacity for iden-tifying snare drums, which are historically the most diffi-cult drum class to detect.},
title={Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks},
keywords={music transcription,tesis},
isbn={978-0-692-75506-8},
mendeley-tags={music transcription,tesis},
year={2016},
author={Southall, Carl and Stables, Ryan and Hockman, Jason},
}

@article{Floreano2008a,
year={2008},
abstract={Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
issn={18645909},
pages={47--62},
doi={10.1007/s12065-007-0002-4},
mendeley-tags={evolutive,review},
number={1},
title={Neuroevolution: From architectures to learning},
isbn={1206500700},
keywords={Evolution,Learning,Neural networks,evolutive,review},
author={Floreano, Dario and D??rr, Peter and Mattiussi, Claudio},
journal={Evolutionary Intelligence},
volume={1},
}

@article{Deng2015,
author={Deng, ChenWei and Huang, GuangBin and Xu, Jia and Tang, JieXiong},
journal={Science China Information Sciences},
title={Extreme learning machines: new trends and applications},
year={2015},
mendeley-tags={ELM,review},
doi={10.1007/s11432-014-5269-3},
keywords={ELM,review},
number={2},
pages={1--16},
url={http://link.springer.com/10.1007/s11432-014-5269-3},
issn={1674-733X},
volume={58},
abstract={Extreme learning machine (ELM), as a new learning framework, draws increasing attractions in the areas of large-scale computing, high-speed signal processing, artificial intelligence, and so on. ELM aims to break the barriers between the conventional artificial learning techniques and biological learning mechanism and represents a suite of machine learning techniques in which hidden neurons need not to be tuned. ELM theories and algorithms argue that “random hidden neurons” capture the essence of some brain learning mechanisms as well as the intuitive sense that the efficiency of brain learning need not rely on computing power of neurons. Thus, compared with traditional neural networks and support vector machine, ELM offers significant advantages such as fast learning speed, ease of implementation, and minimal human intervention. Due to its remarkable generalization performance and implementation efficiency, ELM has been applied in various applications. In this paper, we first provide an overview of newly derived ELM theories and approaches. On the other hand, with the ongoing development of multilayer feature representation, some new trends on ELM-based hierarchical learning are discussed. Moreover, we also present several interesting ELM applications to showcase the practical advances on this subject.},
}

@article{KenOHanlonandMarkD.PlumbleyQueenMaryUniversityofLondon2014,
year={2014},
number={May},
author={Ken O ' Hanlon and Mark D . Plumbley Queen Mary University of London},
title={POLYPHONIC PIANO TRANSCRIPTION USING NON-NEGATIVE MATRIX FACTORISATION WITH GROUP SPARSITY},
volume={1},
pages={3136--3140},
}

@article{Tuohy2006,
isbn={\%}(},
pages={576--579},
title={An Evolved Neural Network/HC Hybrid for Tablature Creation in GA- based Guitar Arranging},
year={2006},
mendeley-tags={GA,music transcription},
keywords={GA,fingering{\_}prediction,guitar,music transcription},
abstract={In this paper we describe a technique for creating guitar tablature using a neural network. Training data was parsed from an online repository of human-created tablatures. The contents of both the input layer and the set of training data have been optimized through genetic search in order to maximize the accuracy of the network. The output of the network is im- proved upon with a local heuristic hill-climber (HC). We implement this model in an existing system for generating guitar arrangements via genetic algorithm (GA). When compared to the original system for generating tablature, we note modest improvement in tablature quality and drastic improvements in execution time.},
journal={Procs. of the International Computer Music Conference (ICMC06)},
url={http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2006.119},
number={January 2006},
author={Tuohy, D R and Potter, W D},
}

@article{Frans2017a,
eprint={1704.08834},
archivePrefix={arXiv},
year={2017},
abstract={When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
mendeley-tags={image synthesis},
title={Outline Colorization through Tandem Adversarial Networks},
author={Frans, Kevin},
arxivId={1704.08834},
keywords={image synthesis},
url={http://arxiv.org/abs/1704.08834},
}

@article{Larsson2016b,
journal={Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
title={Learning representations for automatic colorization},
isbn={9783319464923},
keywords={image synthesis},
abstract={We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
pages={577--593},
volume={9908 LNCS},
eprint={1603.06668},
archivePrefix={arXiv},
arxivId={1603.06668},
author={Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
year={2016},
issn={16113349},
mendeley-tags={image synthesis},
doi={10.1007/978-3-319-46493-0_35},
}

@article{Leite2016,
issn={07981015},
journal={Espacios},
title={The process of product development for startups based on creative innovation},
volume={37},
number={7},
author={Leite, M.L.G. and Purcidonio, P.M. and Tarjano, C.},
keywords={Innovation,Product development,Startups},
year={2016},
abstract={The systematic innovation has been considered one of the most important practices in the business environment, increasingly uncertain and changeable. Understanding how changes they are occurring in society motivated by advances in information technology have impacted the innovation process, through the lens of startups. it is essential for the economic growth of a country, since most new products come these processes. The aim of this study is to develop a new model of development products in startups and small organizations seeking to develop physical products from innovations based on creativity.},
}

@article{Weinman2014,
pages={261277},
year={2014},
number={May 2012},
author={Weinman, Jaime J},
title={Top of page},
}

@article{Estelles-Arolas2012,
journal={Journal of Information Science},
isbn={0165551500000},
abstract={Crowdsourcing is a relatively recent concept that encompasses many practices. This diversity leads to the blurring of the limits of crowdsourcing that may be identified virtually with any type of Internet-based collaborative activity, such as co-creation or user innovation. Varying definitions of crowdsourcing exist and therefore, some authors present certain specific examples of crowdsourcing as paradigmatic, while others present the same examples as the opposite. In this paper, existing definitions of crowdsourcing are analyzed to extract common elements and to establish the basic characteristics of any crowdsourcing initiative. Based on these existing definitions, an exhaustive and consistent definition for crowdsourcing is presented and contrasted in eleven cases.},
volume={38},
keywords={crowdsourcing,definition,innovation},
number={2},
pages={189--200},
year={2012},
author={Estelles-Arolas, E. and Gonzalez-Ladron-de-Guevara, F.},
doi={10.1177/016555150000000},
title={Towards an integrated crowdsourcing definition},
}

@article{Berthon2007,
abstract={Creative consumers (defined as customers who adapt, modify, or transform a proprietary offering) represent an intriguing paradox for business. On one hand, they can signify a black hole for future revenue, with breach of copyright and intellectual property. On the other hand, they represent a gold mine of ideas and business opportunities. Central to business is the need to create and capture value, and creative consumers demand a shift in the mindsets and business models of how firms accomplish both. Based upon their attitude and action toward customer innovation, we develop a typology of firms' stances toward creative consumers. We then consider the implications of the stances model for corporate strategy and examine a three-step approach to dealing with creative consumers: awareness, analysis, and response. ?? 2006 Kelley School of Business, Indiana University.},
keywords={Creative customers,Diagnostics,Firm stance,Strategic response},
pages={39--47},
issn={00076813},
doi={10.1016/j.bushor.2006.05.005},
author={Berthon, Pierre R. and Pitt, Leyland F. and McCarthy, Ian and Kates, Steven M.},
title={When customers get clever: Managerial approaches to dealing with creative consumers},
url={http://linkinghub.elsevier.com/retrieve/pii/S0007681306000796},
volume={50},
number={1},
year={2007},
journal={Business Horizons},
month={jan},
}

@article{Angell,
title={Winners and Losers in the Information Age},
author={Angell, Ian},
}

@article{Index2013,
title={Technology Is Wiping Out Companies Faster than Ever},
pages={2013--2014},
author={Index, Industrial},
year={2013},
}

@article{Pisano2008,
journal={Harvard Business Review},
number={12},
title={Which Kind of Collaboration Is Right for You?},
url={http://search.ebscohost.com/login.aspx?direct=true{\&}db=buh{\&}AN=35387060{\&}site=ehost-live{\%}5Cnhttp://content.ebscohost.com/ContentServer.asp?T=P{\&}P=AN{\&}K=35387060{\&}S=R{\&}D=buh{\&}EbscoContent=dGJyMMvl7ESep7Q4wtvhOLCmr0uep65Ssqu4TLGWxWXS{\&}ContentCustomer=dGJyMPGotk{\%}2BxrLZQuePfgeyx44Dt6fIA},
volume={86},
year={2008},
abstract={Nowadays, virtually no companies innovate alone. Firms team up with a variety of partners, in a wide number of ways, to create new technologies, products, and services. But what is the best way to leverage the power of outsiders? To help executives answer that question, Pisano, of Harvard Business School, and Verganti, of Politecnico di Milano, developed a simple framework focused on two questions: Given your strategy, how open or closed should your network of collaborators be? And who should decide which problems to tackle and which solutions to adopt? There are four basic modes of collaboration, say the authors. An elite circle is a closed network with a hierarchical governance: One company selects the participants, defines the problem, and chooses the solution. For instance, Alessi, an Italian home-products company, invited 200 outside experts in postmodern architecture to contribute ideas for new home-product designs. An innovation mall is hierarchical but open: Anyone can post a problem or propose solutions in it, but the company posting the problem chooses the solution. An example is InnoCentive.com, an eBay-like site where companies post scientific challenges. An innovation community is open and decentralized: Anyone can propose problems, offer solutions, and decide which ideas to use -- as happens in the Linux open-source software community. A consortium is a private group of participants that operate as equals and jointly select problems, decide how to conduct work, and choose solutions. IBM has set up a number of consortia with other companies to develop next-generation semiconductor technologies. No one approach is superior; each involves strategic trade-offs. When choosing among modes, firms must weigh their advantages and challenges, and assess which will work best with their strategy, capabilities, structure, and assets.  INSETS: The Four Ways to Collaborate;How to Choose the Best Mode of Collaboration.},
pages={78--86},
author={Pisano, Gary P. and Verganti, Roberto},
}

@article{Piller2006,
year={2006},
pages={307--318},
url={http://doi.wiley.com/10.1111/j.1467-9310.2006.00432.x},
number={3},
title={Toolkits for idea competitions: a novel method to integrate users in new product development},
volume={36},
author={Piller, Frank T. and Walcher, Dominik},
journal={R{\&}D Management},
abstract={Research has shown that many innovations originate not in the manufacturer but the user domain. Internet-based toolkits for idea competitions (TIC) are a novel way for manufacturers to access innovative ideas and solutions from users. Idea competitions build on the nature of competition as a means to encourage users to participate at an open innovation process, to inspire their creativity, and to increase the quality of the submissions. When the contest ends, submissions are evaluated by an expert panel. Users whose submissions score highest receive an award from the manufacturer, which is often granted in exchange for the right to exploit the solution in its domain. Following the idea of evolutionary prototyping, we developed a TIC in cooperation with a manufacturer of sports goods. The TIC was launched as a pilot in one of the company's markets. Submissions were evaluated using the consensual assessment technique. The evaluation of this study provides suggestions for further research, but also implications for managers willing to explore TIC in their organization.},
}

@article{Poetz2012,
abstract={Generating ideas for new products used to be the exclusive domain of marketers, engineers, and/or designers. Users have only recently been recognized as an alternative source of new product ideas. Whereas some have attributed great potential to outsourcing idea generation to the “crowd” of users (“crowdsourcing”), others have clearly been more skeptical. The authors join this debate by presenting a real-world comparison of ideas actually generated by a firm's professionals with those generated by users in the course of an idea generation contest. Both professionals and users provided ideas to solve an effective and relevant problem in the consumer goods market for baby products. Executives from the underlying company evaluated all ideas (blind to their source) in terms of key quality dimensions including novelty, customer benefit, and feasibility. The study reveals that the crowdsourcing process generated user ideas that score significantly higher in terms of novelty and customer benefit, and somewhat lower in terms of feasibility. However, the average values for feasibility—in sharp contrast to novelty and customer benefit—tended to be relatively high overall, meaning that feasibility did not constitute a narrow bottleneck in this study. Even more interestingly, it is found that user ideas are placed more frequently than expected among the very best in terms of novelty and customer benefit. These findings, which are quite counterintuitive from the perspective of classic new product development (NPD) literature, suggest that, at least under certain conditions, crowdsourcing might constitute a promising method to gather user ideas that can complement those of a firm's professionals at the idea generation stage in NPD.},
title={The Value of Crowdsourcing: Can Users Really Compete with Professionals in Generating New Product Ideas?},
number={2},
doi={10.1111/j.1540-5885.2011.00893.x},
journal={Journal of Product Innovation Management},
author={Poetz, Marion K. and Schreier, Martin},
url={http://doi.wiley.com/10.1111/j.1540-5885.2011.00893.x},
issn={07376782},
month={mar},
pages={245--256},
volume={29},
year={2012},
}

@article{Ledford2013,
author={Ledford, B Y Heidi},
title={START-UP},
year={2013},
}

@article{Dodgson2006,
year={2006},
author={Dodgson, Mark and Gann, David and Salter, Ammon},
pages={333--346},
title={The role of technology in the shift towards open innovation : the case},
}

@article{Maeda,
author={Maeda, John},
title={l aw s o f},
}

@article{Edmunds2000a,
abstract={This paper reviews the literature on the problem of information overload, with particular reference to business organisations. The literature reveals that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognised and experienced. Both perceptions and the actual effects of information overload have been exacerbated by the rapid advances made in information and communication technology, although it is not clear cut as to whether the Internet has worsened or improved the situation. A theme stressed in the literature is the paradoxical situation that, although there is an abundance of information available, it is often difficult to obtain useful, relevant information when it is needed. Some solutions put forward to reduce information overload are: a reduction in the duplication of information found in the professional literature; the adoption of personal information management strategies, together with the integration of software solutions such as push technology and intelligent agents; and the provision of value-added information (filtered by software or information specialists). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
author={Edmunds, Angela and Morris, Anne},
journal={International Journal of Information Management},
year={2000},
number={1},
pages={17--28},
title={The problem of information overload in business organisations: a review of the literature},
volume={20},
keywords={infoglut,information fatigue syndrome,information overload},
}

@article{Alberts1997,
title={The Information Age : An Anthology on Its Impact and Consequences Table of Contents},
author={Alberts, David S and Papp, Daniel S},
year={1997},
}

@article{DIppolito2014a,
author={D'Ippolito, Beatrice},
publisher={Elsevier},
title={The importance of design for firms׳ competitiveness: A review of the literature},
url={http://www.sciencedirect.com/science/article/pii/S016649721400008X},
abstract={Scholars dedicated increasing attention towards appreciating how design has changed individuals׳ perception of new products, firms׳ understanding and formulation of strategy, or other relevant actors׳ approach to innovation and technology management. By emphasising the importance of design for the definition of consumers׳ needs, the restructuring of firms׳ organisational structures and strategies, and the evolution of firms׳ value creation processes, this review paper identifies relevant research gaps and questions that would benefit from future scholarly attention. In particular, it is suggested that such effort should address the analysis of how design consumption can help better comprehend consumers׳ needs; what are the implications of design thinking on the skill sets of design professionals; the organisational structure of firms, including the reconfiguration of other business functions, and their strategy; and whether and how design thinking can shape firms׳ value creation processes and contribute to the formalisation of design tasks.},
year={2014},
pages={1--15},
doi={10.1016/j.technovation.2014.01.007},
issn={01664972},
month={feb},
journal={Technovation},
keywords={Literature Review Process},
}

@book{Norman,
title={of EVERYDAY THINGS THE DESIGN OF EVERYDAY},
author={Norman, Don},
isbn={9780465050659},
}

@article{Fini2010a,
year={2010},
pages={1060--1069},
month={oct},
issn={00487333},
journal={Research Policy},
keywords={Academic entrepreneurship,Business creation,Knowledge transfer},
abstract={Research and public policy on academic entrepreneurship are largely based on the assumption that faculty members start businesses to commercialize inventions that have been disclosed to university administrators and have been patented. In this paper, we analyze a sample of 11,572 professors and find that much academic entrepreneurship occurs outside the university intellectual property system. Specifically, about 2/3 of businesses started by academics are not based on disclosed and patented inventions. Moreover, we show that individual characteristics, departmental and organizational affiliations, and time allocation of academics that have started business outside the IP system are different from those of academics that have started businesses to exploit disclosed and patented inventions. We discuss the implications for research on and the practice of academic entrepreneurship. ?? 2010 Elsevier B.V. All rights reserved.},
author={Fini, Riccardo and Lacetera, Nicola and Shane, Scott},
doi={10.1016/j.respol.2010.05.014},
number={8},
volume={39},
title={Inside or outside the IP system? Business creation in academia},
url={http://linkinghub.elsevier.com/retrieve/pii/S0048733310001381},
}

@article{Lee-mortimer1994,
year={1994},
pages={31--34},
title={Strategic Design},
volume={1},
number={2},
author={Lee-mortimer, Andrew},
}

@article{Dewick2002a,
number={January},
title={Tyndall ˚ Centre and the Environment Technological Change , Industry Structure and the Environment},
year={2002},
author={Dewick, Paul and Green, Ken and Miozzo, Marcela},
}

@article{Hutchins2013,
author={Hutchins, Aaron},
pages={649206},
title={Top of page},
year={2013},
}

@article{,
title={No Title},
}

@article{Schumpeter1994,
author={Schumpeter, Joseph A and Mccraw, Thomas and Mirowski, Phillip},
title={Thomas K . McCraw , Cambridge : Harvard University Press , 719 pages },
year={1994},
pages={1--8},
}

@article{Sampler1998,
journal={Strategic Management Journal},
volume={19},
year={1998},
abstract={We are entering a new competitive age in which the basis of competition is being fundamentally altered through the introduction of advanced information technologies and public communication infrastructures, such as the Internet. In these environments, the nature and locus of competition will radically alter, as information becomes an increasingly important resource. This paper develops ideas around the strategic characteristics of information-information separability and industry concentration, related diversification, and innovation for firms competing in the Information Age.},
number={4},
url={http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0266(199804)19:4{\%}3C343::AID-SMJ975{\%}3E3.0.CO;2-G/abstract},
pages={343--355},
author={Sampler, J.},
title={Redefining industry structure for the information age},
publisher={Wiley Online Library},
keywords={industry boundary,industry structure,information,information age,information separability},
}

@article{Wang2011,
title={Rapid parametric design methods for shoe-last customization},
url={http://link.springer.com/10.1007/s00170-010-3144-y},
year={2011},
issn={0268-3768},
number={1-4},
volume={54},
keywords={deformation,form reuse,interactive,parametric design,shoe-last},
journal={The International Journal of Advanced Manufacturing Technology},
abstract={With the development of computer-aided design (CAD) technology and increasing demands of customized footwear, shoe-lasts are requested to be designed rapidly so as to speed-up the process of footwear manufacturing. Thus, this study presents a CAD system for shoe-last rapid customized design based on the piecewise reconstruction to realize the interactive deformation and separate/global shoe-last form reuse. First, piecewise remodeling method is proposed based on the multi-layer parametric definition and contour curves are extracted from the mesh. Then, five types of proper constraints to support surface manipulation are proposed, and the draft-driven deformation by the contour curve bending can realize the interactive local surface design in free angle of view. Finally, shoe-last styles can be saved and reused globally or separately to share design results between different shoe-lasts. Experimental examples show that customized shoe-lasts can be easily and rapidly generated by adopting the parametric design methods.},
author={Wang, Jin and Zhang, Haining and Lu, Guodong and Liu, Zheng},
month={jan},
doi={10.1007/s00170-010-3144-y},
pages={173--186},
}

@article{Moscaritoloa,
author={Moscaritolo, Autores and Angelamoscaritolopcmagcom, Angela},
title={Pebble Smartwatch Sells Out , Collects {\$} 10 Million on Kickstarter},
}

@article{Agerfalk2008a,
abstract={This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy -- opensourcing, as the authors termed it -- whereby commercial companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness. The study reveals an ongoing shift from open source software (OSS) as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises.},
shorttitle={OUTSOURCING TO AN UNKNOWN WORKFORCE},
title={Outsourcing to an Unknown Workforce: Exploring Opensourcing as a Global Sourcing Strategy},
journal={MIS Quarterly},
author={\AA}gerfalk, P. and Fitzgerald, B.},
volume={32},
year={2008},
pages={385},
number={2},
keywords={crowdsourcing,global software development,multi-,offshoring,open source,opensourcing,out-,sourcing},
}

@misc{,
title={Poverty and profits in the information age.pdf},
}

@book{,
title={Licensed to : CengageBrain User Licensed to : CengageBrain User},
isbn={9781111221294},
}

@article{The,
title={DEAD ?},
author={The, I S},
number={March 2002},
}

@article{Noble2011,
url={http://dx.doi.org/10.1111/j.1540-5885.2011.00808.x},
volume={28},
pages={389--393},
publisher={Blackwell Publishing Inc},
title={On Elevating Strategic Design Research*},
author={Noble, Charles H},
year={2011},
abstract={While the popular understanding of the influence of design is growing, academic research has largely been restricted to considering consumer-level responses to design elements. This paper reviews this past work and proposes a more strategic research agenda for the field, with the potential to explicate linkages between design elements and strategies and outcomes related to innovation and corporate performance.},
number={3},
journal={Journal of Product Innovation Management},
}

@misc{,
title={Leaps and bounds.pdf},
}

@article{Bagozzi2006a,
issn={0025-1909},
month={jul},
title={Open Source Software User Communities: A Study of Participation in Linux User Groups},
doi={10.1287/mnsc.1060.0545},
url={http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1060.0545},
abstract={We conceptualize participation in Linux user groups (LUGs) in terms of group-referent intentional actions and investigate cognitive (attitudes, perceived behavioral control, identification with the open source movement), affective (positive and negative anticipated emotions), and social (social identity) determinants of participation and its consequences on Linux-related behaviors of users. This survey-based study, conducted with 402 active LUG members representing 191 different LUGs from 23 countries and employing structural equation modeling methodology, supports the proposed model. Furthermore, we find that the Linux user's experience level moderates the extent of the LUG's social influence and its impact on the user's participation. We conclude with a consideration of the managerial and research implications of the study's findings.},
author={Bagozzi, Richard P. and Dholakia, Utpal M.},
journal={Management Science},
pages={1099--1115},
number={7},
volume={52},
year={2006},
keywords={2004,accepted by eric von,anticipated emotions,for 3 revisions,hippel and georg von,history,krogh,linux,model of goal-directed behavior,novice versus experienced users,open source software,received september 1,social identity,special issue editors,the authors 4 months,this paper was with,virtual communities,we-intentions},
}

@article{,
year={2000},
title={Copyright {\textcopyright}2000. All Rights Reserved.},
}

@article{Arthur1875,
author={Arthur, By W Brian},
year={1875},
title={Is the Information Revolution Dead ?},
}

@article{Stilgoe2013,
journal={Research Policy},
pages={1568--1580},
doi={10.1016/j.respol.2013.05.008},
publisher={Elsevier B.V.},
number={9},
title={Developing a framework for responsible innovation},
url={http://linkinghub.elsevier.com/retrieve/pii/S0048733313000930},
volume={42},
abstract={The governance of emerging science and innovation is a major challenge for contemporary democracies. In this paper we present a framework for understanding and supporting efforts aimed at 'responsible innovation'. The framework was developed in part through work with one of the first major research projects in the controversial area of geoengineering, funded by the UK Research Councils. We describe this case study, and how this became a location to articulate and explore four integrated dimensions of responsible innovation: anticipation, reflexivity, inclusion and responsiveness. Although the framework for responsible innovation was designed for use by the UK Research Councils and the scientific communities they support, we argue that it has more general application and relevance. ?? 2013 Elsevier B.V. All rights reserved.},
issn={00487333},
year={2013},
author={Stilgoe, Jack and Owen, Richard and Macnaghten, Phil},
keywords={Emerging technologies,Ethics,Geoengineering,Governance,Responsible innovation},
month={nov},
}

@article{Scott1987,
year={1987},
author={Scott, Mel and Bruce, Richard},
title={Five Stages of Growth Business in Small},
number={3},
volume={20},
}

@misc{,
title={Democratizing innovation.pdf},
}

@article{Brabham2008,
keywords={about human ingenuity,challenge,collective intelligence,crowdsourcing,designer,distributed problem solving,goldcorp,innocentive,is going on,istockphoto,its unfolding is to,of client,open source,reject the binary notion,right now,story to be told,the first step to,the next step is,there is an incredible,threadless,to look to what,wisdom of crowds},
pages={75--90},
number={1},
month={feb},
volume={14},
isbn={1354856507084},
journal={Convergence: The International Journal of Research into New Media Technologies},
issn={1354-8565},
author={Brabham, D. C.},
doi={10.1177/1354856507084420},
title={Crowdsourcing as a Model for Problem Solving: An Introduction and Cases},
url={http://con.sagepub.com/cgi/doi/10.1177/1354856507084420},
year={2008},
abstract={Crowdsourcing is an online, distributed problem-solving and production model that has emerged in recent years. Notable examples of the model include Threadless, iStockphoto, Inno- Centive, the Goldcorp Challenge, and user-generated advertising contests. This article provides an introduction to crowdsourcing, both its theoretical grounding and exemplar cases, taking care to distinguish crowdsourcing from open source production. This article also explores the possibilities for the model, its potential to exploit a crowd of innovators, and its potential for use beyond for- profit sectors. Finally, this article proposes an agenda for research into crowdsourcing.},
}

@article{Ordanini2011,
url={http://www.emeraldinsight.com/10.1108/09564231111155079},
journal={Journal of Service Management},
volume={22},
doi={10.1108/09564231111155079},
year={2011},
keywords={crowd-funding,customer-investors,customers,investments,paper type research paper,service innovation},
title={Crowd-funding: transforming customers into investors through innovative service platforms},
number={4},
pages={443--470},
author={Ordanini, Andrea and Miceli, Lucia and Pizzetti, Marta and Parasuraman, a.},
abstract={Purpose – The purpose of this paper is to analyze the emerging crowd-funding phenomenon, that is a collective effort by consumers who network and pool their money together, usually via the internet, in order to invest in and support efforts initiated by other people or organizations. Successful service businesses that organize crowd-funding and act as intermediaries are emerging, attesting to the viability of this means of attracting investment. Design/methodology/approach – The research employs a “grounded theory” approach, performing an in-depth qualitative analysis of three cases involving crowd-funding initiatives: SellaBand in the music business, Trampoline in financial services, and Kapipal in non-profit services. These cases were selected to represent a diverse set of crowd-funding operations that vary in terms of risk/return for the investor and the type of payoff associated to the investment. Findings – The research addresses two research questions: how and why do consumers turn into crowd-funding participants? and how and why do service providers set up a crowd-funding initiative? Concerning the first research question, the authors' findings reveal purposes, characteristics, roles and tasks, and investment size of crowd-funding activity from the consumer's point of view. Regarding the second research question, the authors' analysis reveals purposes, service roles, and network effects of crowd-funding activity investigated from the point of view of the service organization that set up the initiative. Practical implications – The findings also have implications for service managers interested in launching and/or managing crowd-funding initiatives. Originality/value – The paper addresses an emerging phenomenon and contributes to service theory in terms of extending the consumer's role from co-production and co-creation to investment.},
issn={1757-5818},
}

@article{Bettis-Outland2012,
url={http://www.sciencedirect.com/science/article/pii/S0148296310002845},
issn={01482963},
keywords={Decision making,Incrementalism,Information overload,Organizational learning},
abstract={Although an abundance of academic literature positions organizational information processing as antecedent to decision making, little attention is paid to the possibility that decision making can be antecedent to certain elements of organizational information processing. Specifically, does the decision making process impact the type of organizational learning that takes place? Do different approaches to decision making alter the amount and variety of information made available to the organization, that is, the level of information overload? This paper examines incremental and comprehensive decision making to understand the effects of different decision making types on organizational learning and information overload. Incrementalism suggests that decision making should take place in small steps or increments. This approach analyzes only a few scenarios to make decisions resulting in few, if any, major organizational changes. However, comprehensive decision making requires the consideration of all possible scenarios and potential outcomes, resulting in a major overhaul of traditions and procedures within the organization. Consequently, each decision making approach has a different impact on organizational learning and information overload. (C) 2011 Elsevier Inc. All rights reserved.},
doi={10.1016/j.jbusres.2010.12.021},
author={Bettis-Outland, H},
number={6},
pages={814--820},
publisher={Elsevier Inc.},
title={Decision-making's impact on organizational learning and information overload},
volume={65},
journal={Journal of Business Research},
year={2012},
month={jun},
}

@book{Thea,
isbn={0060189878},
title={No Title},
author={The, Rossing},
}

@article{Elliot2013,
issn={07376782},
author={Elliot, E.a. and Nakata, C.},
abstract={In today's global business environment, where multinational companies are pressed to increase revenues in order to survive, creativity may hold the key to ensuring their new product development (NPD) efforts lead to innovations with worldwide appeal, such as Apple's iPad and Gillette's Fusion Razor. To leverage creativity for effective global NPD, businesses want to know how cultures differ in their concepts of creativity and the impact of those differences on approaches to developing new products. Because global new products are increasingly developed in, by, and for multiple cultures, a particular need is for a culturally reflective understanding, or conceptualization, of creativity. While creativity is believed to be culturally tied, the dominant framework of creativity used in business and management assumes that creativity is culturally indifferent or insensitive. This knowledge gap is addressed by studying the role of creativity in NPD practices in a cross-cultural or global context. The study begins by first developing a culturally anchored conceptualization of creativity. Called cross-cultural creativity, the concept draws on creativity insights from the field of art and aesthetics. The concept specifies two modes of creativity, neither of which is superior to the other, called the spontaneous or S route and the divergent or D route. The S route emphasizes adaptiveness, processes, intuitiveness, and metamorphism, while the D route focuses on disruptiveness, results, rationality, and literalism. Next, this new concept is applied to NPD by positing how creativity in distinct cultures may shape NPD practices, as illustrated by Japanese and U.S. firms. Research propositions are formulated to capture these patterns, and thereafter, theoretical and practical implications of the framework and propositions are discussed. The implications center on global NPD, which is a complex enterprise involving typically more than one culture to design and develop new products for several geographic markets. The study is of interest to researchers needing a globally situated, culturally attached framework of creativity for international NPD studies, and managers seeking to exploit creativity in multinational and multicultural innovation projects.},
month={dec},
url={http://doi.wiley.com/10.1111/jpim.12066},
year={2013},
volume={30},
pages={110--125},
title={Cross-Cultural Creativity: Conceptualization and propositions for global new product development},
doi={10.1111/jpim.12066},
journal={Journal of Product Innovation Management},
}

@book{,
isbn={9780983648703},
title={No Title},
}

@article{Foster2012,
author={Foster, Richard N},
title={Creative Destruction Whips through Corporate America},
keywords={Kodak, Radio Shack, Bear Stearns},
year={2012},
}

@article{,
pages={1--9},
title={No Title},
year={1942},
}

@book{Adler,
author={Adler, Isabel K},
isbn={9788565424004},
title={Design Thinking Design Thinking Inova{\c{c}}{\~{a}}o em neg{\'{o}}cios},
}

@misc{,
title={A revis{\~{a}}o da bibliografia em teses e disserta{\c{c}}{\~{o}}es.pdf},
}

@article{Anderson2012,
volume={43},
author={Anderson, Simon P. and de Palma, Andr{\'{e},
journal={The RAND Journal of Economics},
abstract={The Information Age has a surfeit of information received relative to what is processed. We model multiple sectors competing for consumer attention, with competition in price within each sector. Sector advertising levels follow a constant elasticity of substitution (CES) form, and within-sector prices are dispersed with a truncated Pareto distribution. The “information hump” shows highest ad levels for intermediate attention levels. Overall, advertising is excessive, although the allocation across sectors is optimal. The blame for information overload falls most on product categories with low information transmission costs and low profits.},
number={1},
title={Competition for attention in the Information (overload) Age},
pages={1--25},
url={http://doi.wiley.com/10.1111/j.1756-2171.2011.00155.x},
year={2012},
}

@article{Sawhney2005,
title={Collaborating to create: The Internet as a platform for customer engagement in product innovation},
year={2005},
url={http://linkinghub.elsevier.com/retrieve/pii/S1094996805700785},
pages={4--17},
abstract={In the networked world, firms are recognizing the power of the Internet as a platform for co-creating value with customers.We focus on how the Internet has impacted the process of collaborative innovation—a key process in value co-creation.We outline the distinctive capabilities of the Internet as a platform for customer engagement, including interactivity, enhanced reach, persistence, speed, and flexibility, and suggest that firms can use these capabilities to engage customers in collaborative product innovation through a variety of Internet-based mechanisms.We discuss how these mechanisms can facilitate collaborative innovation at different stages of the New Product Development process (back end vs. front end stages) and for differing levels of customer involvement (high reach vs. high richness).We present two detailed exploratory case studies to illustrate the integrated and systematic usage of Internetbased collaborative innovation mechanisms—Ducati from the motorbike industry and Eli Lilly from the pharmaceutical industry.We derive implications for managerial practice and academic research on collaborative innovation.},
author={Sawhney, Mohanbir and Verona, Gianmario and Prandelli, Emanuela},
month={jan},
doi={10.1002/dir.20046},
number={4},
publisher={Elsevier},
volume={19},
issn={10949968},
journal={Journal of Interactive Marketing},
}

@article{Dewick2006,
year={2006},
issn={00401625},
title={Modelling creative destruction: Technological diffusion and industrial structure change to 2050},
volume={73},
month={nov},
number={9},
url={http://linkinghub.elsevier.com/retrieve/pii/S0040162506000862},
keywords={Biotechnologies,Energy,Industrial structure,Nanotechnologies,Technological diffusion},
pages={1084--1106},
author={Dewick, Paul and Green, Ken and Fleetwood, Toby and Miozzo, Marcela},
abstract={Future disruptive, pervasive technologies will have important consequences for industrial structure, economic growth and the environment. Drawing on theories of technological diffusion, industrial evolution and long-term technological change this paper explores the effect of the development and diffusion of two future pervasive technologies on five industrial sectors in three regions during the 21st century in terms of their effect on economic structural change. Through semi-structured interviews with over 100 experts in the two technologies, the paper quantifies the effects of future biotechnologies and nanotechnologies on the industrial structure of the EU, USA and China in 2020 and 2050. The paper finds that as a result of the development and diffusion of future biotechnologies and nanotechnologies, some industries grow whilst others decline and some new ones emerge. The evidence suggests that the effect is different across countries and time; whereas the experts commonly believe that effect of the technologies on the industrial structure of the EU and US is likely to be similar, the effect in China is considered to be less by 2020 but the same as in the EU and US by 2050. This finding has important implications for the location of production, economic growth and energy demand in the future. ?? 2006 Elsevier Inc. All rights reserved.},
journal={Technological Forecasting and Social Change},
doi={10.1016/j.techfore.2006.04.002},
}

@inproceedings{zen_deep_2014,
date={2014-05},
langid={english},
eventtitle={ICASSP} 2014 - 2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
urldate={2021-01-15},
location={Florence, Italy},
pages={3844--3848},
booktitle={2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
author={Zen, Heiga and Senior, Andrew},
isbn={978-1-4799-2893-4},
title={Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis},
url={http://ieeexplore.ieee.org/document/6854321/},
doi={10.1109/ICASSP.2014.6854321},
abstract={Statistical parametric speech synthesis ({SPSS}) using deep neural networks ({DNNs}) has shown its potential to produce naturally-sounding synthesized speech. However, there are limitations in the current implementation of {DNN}-based acoustic modeling for speech synthesis, such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations, this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech.},
publisher={IEEE},
}

@article{lyubimov_mathematical_2016,
journaltitle={Acoustical Physics},
volume={62},
title={Mathematical model of acoustic speech production with mobile walls of the vocal tract},
pages={225--234},
doi={10.1134/S1063771016020093},
url={http://link.springer.com/10.1134/S1063771016020093},
issn={1063-7710, 1562-6865},
number={2},
shortjournal={Acoust. Phys.},
urldate={2021-01-15},
langid={english},
author={Lyubimov, N. A. and Zakharov, E. V.},
abstract={A mathematical speech production model is considered that describes acoustic oscillation prop agation in a vocal tract with mobile walls. The wave field function satisfies the Helmholtz equation with boundary conditions of the third kind (impedance type). The impedance mode corresponds to a three parameter pendulum oscillation model. The experimental research demonstrates the nonlinear character of how the mobility of the vocal tract walls influence the spectral envelope of a speech signal.},
date={2016-03},
}

@inproceedings{li_acoustic_2017,
date={2017-08-20},
author={Li, Bo and Sainath, Tara N. and Narayanan, Arun and Caroselli, Joe and Bacchiani, Michiel and Misra, Ananya and Shafran, Izhak and Sak, Haşim and Pundak, Golan and Chin, Kean and Sim, Khe Chai and Weiss, Ron J. and Wilson, Kevin W. and Variani, Ehsan and Kim, Chanwoo and Siohan, Olivier and Weintraub, Mitchel and {McDermott}, Erik and Rose, Richard and Shannon, Matt},
url={http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0234.html},
eventtitle={Interspeech 2017},
urldate={2021-01-15},
title={Acoustic Modeling for Google Home},
abstract={This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-{LSTMs} to model frequency variations. On the system level, improvements include adapting the model using Google Home speciﬁc data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of {WER} of 8-28\% relative compared to the current production system.},
pages={399--403},
publisher={ISCA},
booktitle={Interspeech 2017},
langid={english},
doi={10.21437/Interspeech.2017-234},
}

@article{maestre_creating_2019,
abstract={We provide an overview of a current research project on measuring, modeling, and virtually recreating the sound radiation characteristics of real acoustic violins. Our general approach is based on measuring the directivity of an acoustic violin, and designing a digital ﬁlter structure that mimics the observed directivity while allowing interactive operation. The digital ﬁlter structure is fed by the electrical signal coming from a silent electric violin as played by a musician. In a hemi-anechoic chamber, we use a microphone array to characterize the frequency-dependent directivity transfer function of a real violin by exciting the bridge with an impact hammer and measuring the acoustic pressure at 4320 points on a sphere surrounding the instrument. From the input force and output pressure signals obtained from the real violin measurements, we use deconvolution to estimate 4320 impulse responses each corresponding to a radiation direction. With such impulse responses, we use State Wave Synthesis to model the observed directivity in time-varying conditions and efﬁciently render directional wavefronts in a virtual environment. We characterize the silent violin transfer function by exciting the bridge with an impact hammer and measuring the electrical signal at its output, leading to an impulse response that we use to design an inverse ﬁlter to recover the force excitation at the bridge.},
author={Maestre, Esteban and Scavone, Gary},
langid={english},
pages={7},
title={Creating Virtual Acoustic Replicas of Real Violins},
date={2019},
}

@article{kapralova_big_nodate,
pages={5},
title={A big data approach to acoustic model training corpus selection},
author={Kapralova, Olga and Alex, John and Weinstein, Eugene and Moreno, Pedro and Siohan, Olivier},
abstract={Deep neural networks ({DNNs}) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train {DNN} models for large vocabulary speech recognition. The core of our technique consists of two steps. We ﬁrst redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using conﬁdence scores, transcript length and transcript ﬂattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10\% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.},
langid={english},
}

@inproceedings{regnier_singing_2009,
isbn={978-1-4244-2353-8},
url={http://ieeexplore.ieee.org/document/4959926/},
title={Singing voice detection in music tracks using direct voice vibrato detection},
doi={10.1109/ICASSP.2009.4959926},
eventtitle={ICASSP} 2009 - 2009 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
publisher={IEEE},
author={Regnier, L. and Peeters, G.},
abstract={In this paper we investigate the problem of locating singing voice in music tracks. As opposed to most existing methods for this task, we rely on the extraction of the characteristics speciﬁc to singing voice. In our approach we suppose that the singing voice is characterized by harmonicity, formants, vibrato and tremolo. In the present study we deal only with the vibrato and tremolo characteristics. For this, we ﬁrst extract sinusoidal partials from the musical audio signal . The frequency modulation (vibrato) and amplitude modulation (tremolo) of each partial are then studied to determine if the partial corresponds to singing voice and hence the corresponding segment is supposed to contain singing voice. For this we estimate for each partial the rate (frequency of the modulations) and the extent (amplitude of modulation) of both vibrato and tremolo. A partial selection is then operated based on these values. A second criteria based on harmonicity is also introduced. Based on this, each segment can be labelled as singing or non-singing. Post-processing of the segmentation is then applied in order to remove short-duration segments. The proposed method is then evaluated on a large manually annotated test-set. The results of this evaluation are compared to the one obtained with a usual machine learning approach ({MFCC} and {SFM} modeling with {GMM}). The proposed method achieves very close results to the machine learning approach : 76.8\% compared to 77.4\% F-measure (frame classiﬁcation). This result is very promising, since both approaches are orthogonal and can then be combined.},
location={Taipei, Taiwan},
urldate={2021-01-15},
langid={english},
pages={1685--1688},
booktitle={2009 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
date={2009-04},
}

@inproceedings{huang_singing-voice_2012,
title={Singing-voice separation from monaural recordings using robust principal component analysis},
eventtitle={ICASSP} 2012 - 2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
doi={10.1109/ICASSP.2012.6287816},
location={Kyoto, Japan},
url={http://ieeexplore.ieee.org/document/6287816/},
isbn={978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
pages={57--60},
booktitle={2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
publisher={IEEE},
author={Huang, Po-Sen and Chen, Scott Deeann and Smaragdis, Paris and Hasegawa-Johnson, Mark},
urldate={2021-01-15},
date={2012-03},
langid={english},
abstract={Separating singing voices from music accompaniment is an important task in many applications, such as music information retrieval, lyric recognition and alignment. Music accompaniment can be assumed to be in a low-rank subspace, because of its repetition structure; on the other hand, singing voices can be regarded as relatively sparse within songs. In this paper, based on this assumption, we propose using robust principal component analysis for singing-voice separation from music accompaniment. Moreover, we examine the separation result by using a binary time-frequency masking method. Evaluations on the {MIR}-1K dataset show that this method can achieve around 1∼1.4 {dB} higher {GNSDR} compared with two state-of-the-art approaches without using prior training or requiring particular features.},
}

@article{jarvelainen_audibility_2001,
date={2001-07},
issn={1529-7853},
doi={10.1121/1.1374756},
url={http://asa.scitation.org/doi/10.1121/1.1374756},
title={Audibility of the timbral effects of inharmonicity in stringed instrument tones},
journaltitle={Acoustics Research Letters Online},
shortjournal={Acoustics Research Letters Online},
author={Järveläinen, Hanna and Välimäki, Vesa and Karjalainen, Matti},
volume={2},
number={3},
pages={79--84},
urldate={2021-01-15},
langid={english},
}

@thesis{davidsson_structure-acoustic_2004,
institution={Univ.},
type={phdthesis},
langid={english},
location={Lund},
title={Structure-acoustic analysis: finite element modelling and reduction methods},
note={ISBN}: 9789162861766{OCLC}: 186476406},
keywords={test},
shorttitle={Structure-acoustic analysis},
date={2004},
author={Davidsson, Peter},
}

@article{kartofelev_modeling_nodate,
pages={7},
langid={english},
author={Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-Maria and Va, Vesa},
title={MODELING} A {VIBRATING} {STRING} {TERMINATED} {AGAINST} A {BRIDGE} {WITH} {ARBITRARY} {GEOMETRY},
}

@inproceedings{kimoto1990stock,
title={Stock market prediction system with modular neural networks},
year={1990},
booktitle={1990 IJCNN international joint conference on neural networks},
organization={IEEE},
__markedentry={[tesse:1]},
author={Kimoto, Takashi and Asakawa, Kazuo and Yoda, Morio and Takeoka, Masakazu},
pages={1--6},
}

@article{saad1998comparative,
volume={9},
number={6},
__markedentry={[tesse:1]},
journal={IEEE Transactions on neural networks},
publisher={IEEE},
year={1998},
author={Saad, Emad W and Prokhorov, Danil V and Wunsch, Donald C},
pages={1456--1470},
title={Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks},
}

@article{chen2003application,
publisher={Elsevier},
author={Chen, An-Sing and Leung, Mark T and Daouk, Hazem},
journal={Computers \& Operations Research},
title={Application of neural networks to an emerging financial market: forecasting and trading the Taiwan Stock Index},
pages={901--923},
__markedentry={[tesse:1]},
number={6},
volume={30},
year={2003},
}

@article{kim2003financial,
volume={55},
year={2003},
publisher={Elsevier},
author={Kim, Kyoung-jae},
__markedentry={[tesse:1]},
number={1-2},
title={Financial time series forecasting using support vector machines},
journal={Neurocomputing},
pages={307--319},
}

@article{2005HuangForecasting,
journal={Computers {\&} Operations Research},
number={10},
author={Wei Huang and Yoshiteru Nakamori and Shou-Yang Wang},
month={oct},
title={Forecasting stock market movement direction with support vector machine},
doi={10.1016/j.cor.2004.03.016},
volume={32},
publisher={Elsevier {BV},
pages={2513--2522},
__markedentry={[tesse:1]},
year={2005},
}

@article{enke2005use,
pages={927--940},
number={4},
volume={29},
journal={Expert Systems with applications},
publisher={Elsevier},
author={Enke, David and Thawornwong, Suraphan},
title={The use of data mining and neural networks for forecasting stock market returns},
__markedentry={[tesse:1]},
year={2005},
}

@article{zhang2009stock,
journal={Expert systems with applications},
publisher={Elsevier},
pages={8849--8854},
year={2009},
author={Zhang, Yudong and Wu, Lenan},
volume={36},
number={5},
__markedentry={[tesse:1]},
title={Stock market prediction of S\&P 500 via combination of improved BCO approach and BP neural network},
}

@article{atsalakis2009surveying,
journal={Expert Systems with Applications},
author={Atsalakis, George S and Valavanis, Kimon P},
volume={36},
__markedentry={[tesse:1]},
publisher={Elsevier},
number={3},
pages={5932--5941},
year={2009},
title={Surveying stock market forecasting techniques--Part II: Soft computing methods},
}

@article{atsalakis2010surveying,
title={Surveying stock market forecasting techniques-Part I: Conventional methods},
author={Atsalakis, George S and Valavanis, Kimon P},
year={2010},
number={1},
__markedentry={[tesse:1]},
volume={2},
journal={Journal of Computational Optimization in Economics and Finance},
pages={45--92},
}

@article{kara2011predicting,
year={2011},
number={5},
publisher={Elsevier},
author={Kara, Yakup and Boyacioglu, Melek Acar and Baykan, {\"O}mer Kaan},
pages={5311--5319},
__markedentry={[tesse:1]},
journal={Expert systems with Applications},
title={Predicting direction of stock price index movement using artificial neural networks and support vector machines: The sample of the Istanbul Stock Exchange},
volume={38},
}

@article{guresen2011using,
author={Guresen, Erkam and Kayakutlu, Gulgun and Daim, Tugrul U},
year={2011},
number={8},
title={Using artificial neural network models in stock market index prediction},
publisher={Elsevier},
volume={38},
__markedentry={[tesse:1]},
journal={Expert Systems with Applications},
pages={10389--10397},
}

@article{mingers2015review,
year={2015},
volume={246},
author={Mingers, John and Leydesdorff, Loet},
title={A review of theory and practice in scientometrics},
number={1},
pages={1--19},
journal={European journal of operational research},
publisher={Elsevier},
}

@inproceedings{tan1995probabilistic,
title={Probabilistic and time-delay neural-network techniques for conservative short-term stock trend prediction},
booktitle={Proc. World Congr. Neural Networks},
author={Tan, Hong and Prokhorov, D and Wunsch, D},
year={1995},
}

@article{kreesuradej1994time,
title={Time delay neural network for small time series data sets},
publisher={Lawrence Erlbaum Associates},
year={1994},
author={Kreesuradej, W and Wunsch, Donald C and Lane, M},
}

@inproceedings{saad1996advanced,
volume={4},
year={1996},
pages={2021--2026},
title={Advanced neural network training methods for low false alarm stock trend prediction},
organization={IEEE},
booktitle={Proceedings of International Conference on Neural Networks (ICNN'96)},
author={Saad, Emad W and Prokhorov, Danil V and Wunsch, Donald C},
}

@inproceedings{han2015learning,
year={2015},
__markedentry={[tesse:1]},
author={Han, Song and Pool, Jeff and Tran, John and Dally, William},
title={Learning both weights and connections for efficient neural network},
booktitle={Advances in neural information processing systems},
pages={1135--1143},
}

@article{han2015deep,
title={Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
journal={arXiv preprint arXiv:1510.00149},
author={Han, Song and Mao, Huizi and Dally, William J},
year={2015},
__markedentry={[tesse:1]},
}

@inproceedings{hanson1989comparing,
pages={177--185},
booktitle={Advances in neural information processing systems},
__markedentry={[tesse:1]},
year={1989},
title={Comparing biases for minimal network construction with back-propagation},
author={Hanson, Stephen Jos{\'e} and Pratt, Lorien Y},
}

@inproceedings{mozer1989skeletonization,
booktitle={Advances in neural information processing systems},
year={1989},
__markedentry={[tesse:1]},
title={Skeletonization: A technique for trimming the fat from a network via relevance assessment},
author={Mozer, Michael C and Smolensky, Paul},
pages={107--115},
}

@inproceedings{lecun1990optimal,
pages={598--605},
title={Optimal brain damage},
year={1990},
author={LeCun, Yann and Denker, John S and Solla, Sara A},
__markedentry={[tesse:1]},
booktitle={Advances in neural information processing systems},
}

@article{reed1993pruning,
volume={4},
title={Pruning algorithms-a survey},
year={1993},
journal={IEEE transactions on Neural Networks},
number={5},
pages={740--747},
__markedentry={[tesse:1]},
author={Reed, Russell},
publisher={IEEE},
}

@inproceedings{blum1989training,
year={1989},
title={Training a 3-node neural network is NP-complete},
pages={494--501},
author={Blum, Avrim and Rivest, Ronald L},
booktitle={Advances in neural information processing systems},
}

@inproceedings{livni2014computational,
booktitle={Advances in neural information processing systems},
pages={855--863},
author={Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
title={On the computational efficiency of training neural networks},
year={2014},
}

@article{2018SilverGeneral,
volume={362},
journal={Science},
author={David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
month={dec},
publisher={American Association for the Advancement of Science ({AAAS})},
year={2018},
title={A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
doi={10.1126/science.aar6404},
number={6419},
pages={1140--1144},
}

@inproceedings{2018TobingEvaluation,
author={Patrick Lumban Tobing and Tomoki Hayashi and Yi-Chiao Wu and Kazuhiro Kobayashi and Tomoki Toda},
publisher={IEEE},
year={2018},
month={dec},
doi={10.1109/slt.2018.8639608},
title={An Evaluation of Deep Spectral Mappings and {WaveNet} Vocoder for Voice Conversion},
booktitle={2018 {IEEE} Spoken Language Technology Workshop ({SLT})},
}


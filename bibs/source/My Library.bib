% Encoding: UTF-8

@inproceedings{zen_deep_2014,
	location = {Florence, Italy},
	title = {Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis},
	isbn = {978-1-4799-2893-4},
	url = {http://ieeexplore.ieee.org/document/6854321/},
	doi = {10.1109/ICASSP.2014.6854321},
	abstract = {Statistical parametric speech synthesis ({SPSS}) using deep neural networks ({DNNs}) has shown its potential to produce naturally-sounding synthesized speech. However, there are limitations in the current implementation of {DNN}-based acoustic modeling for speech synthesis, such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations, this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech.},
	eventtitle = {{ICASSP} 2014 - 2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	pages = {3844--3848},
	booktitle = {2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Zen, Heiga and Senior, Andrew},
	urldate = {2021-01-15},
	date = {2014-05},
	langid = {english},
	file = {2014 Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis - Zen and Senior.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2014 Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis - Zen and Senior.pdf:application/pdf}
}

@article{lyubimov_mathematical_2016,
	title = {Mathematical model of acoustic speech production with mobile walls of the vocal tract},
	volume = {62},
	issn = {1063-7710, 1562-6865},
	url = {http://link.springer.com/10.1134/S1063771016020093},
	doi = {10.1134/S1063771016020093},
	abstract = {A mathematical speech production model is considered that describes acoustic oscillation prop agation in a vocal tract with mobile walls. The wave field function satisfies the Helmholtz equation with boundary conditions of the third kind (impedance type). The impedance mode corresponds to a three parameter pendulum oscillation model. The experimental research demonstrates the nonlinear character of how the mobility of the vocal tract walls influence the spectral envelope of a speech signal.},
	pages = {225--234},
	number = {2},
	journaltitle = {Acoustical Physics},
	shortjournal = {Acoust. Phys.},
	author = {Lyubimov, N. A. and Zakharov, E. V.},
	urldate = {2021-01-15},
	date = {2016-03},
	langid = {english},
	file = {2016 Mathematical model of acoustic speech production with mobile walls of the vocal tract - Lyubimov and Zakharov.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2016 Mathematical model of acoustic speech production with mobile walls of the vocal tract - Lyubimov and Zakharov.pdf:application/pdf}
}

@inproceedings{li_acoustic_2017,
	title = {Acoustic Modeling for Google Home},
	url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0234.html},
	doi = {10.21437/Interspeech.2017-234},
	abstract = {This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-{LSTMs} to model frequency variations. On the system level, improvements include adapting the model using Google Home speciﬁc data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of {WER} of 8-28\% relative compared to the current production system.},
	eventtitle = {Interspeech 2017},
	pages = {399--403},
	booktitle = {Interspeech 2017},
	publisher = {{ISCA}},
	author = {Li, Bo and Sainath, Tara N. and Narayanan, Arun and Caroselli, Joe and Bacchiani, Michiel and Misra, Ananya and Shafran, Izhak and Sak, Haşim and Pundak, Golan and Chin, Kean and Sim, Khe Chai and Weiss, Ron J. and Wilson, Kevin W. and Variani, Ehsan and Kim, Chanwoo and Siohan, Olivier and Weintraub, Mitchel and {McDermott}, Erik and Rose, Richard and Shannon, Matt},
	urldate = {2021-01-15},
	date = {2017-08-20},
	langid = {english},
	file = {2017 Acoustic Modeling for Google Home - Li et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2017 Acoustic Modeling for Google Home - Li et al..pdf:application/pdf}
}

@article{maestre_creating_2019,
	title = {Creating Virtual Acoustic Replicas of Real Violins},
	abstract = {We provide an overview of a current research project on measuring, modeling, and virtually recreating the sound radiation characteristics of real acoustic violins. Our general approach is based on measuring the directivity of an acoustic violin, and designing a digital ﬁlter structure that mimics the observed directivity while allowing interactive operation. The digital ﬁlter structure is fed by the electrical signal coming from a silent electric violin as played by a musician. In a hemi-anechoic chamber, we use a microphone array to characterize the frequency-dependent directivity transfer function of a real violin by exciting the bridge with an impact hammer and measuring the acoustic pressure at 4320 points on a sphere surrounding the instrument. From the input force and output pressure signals obtained from the real violin measurements, we use deconvolution to estimate 4320 impulse responses each corresponding to a radiation direction. With such impulse responses, we use State Wave Synthesis to model the observed directivity in time-varying conditions and efﬁciently render directional wavefronts in a virtual environment. We characterize the silent violin transfer function by exciting the bridge with an impact hammer and measuring the electrical signal at its output, leading to an impulse response that we use to design an inverse ﬁlter to recover the force excitation at the bridge.},
	pages = {7},
	author = {Maestre, Esteban and Scavone, Gary},
	date = {2019},
	langid = {english},
	file = {2019 Creating Virtual Acoustic Replicas of Real Violins - Maestre and Scavone.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2019 Creating Virtual Acoustic Replicas of Real Violins - Maestre and Scavone.pdf:application/pdf}
}

@article{kapralova_big_nodate,
	title = {A big data approach to acoustic model training corpus selection},
	abstract = {Deep neural networks ({DNNs}) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train {DNN} models for large vocabulary speech recognition. The core of our technique consists of two steps. We ﬁrst redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using conﬁdence scores, transcript length and transcript ﬂattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10\% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.},
	pages = {5},
	author = {Kapralova, Olga and Alex, John and Weinstein, Eugene and Moreno, Pedro and Siohan, Olivier},
	langid = {english},
	file = {A big data approach to acoustic model training corpus selection - Kapralova et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\A big data approach to acoustic model training corpus selection - Kapralova et al..pdf:application/pdf}
}

@inproceedings{regnier_singing_2009,
	location = {Taipei, Taiwan},
	title = {Singing voice detection in music tracks using direct voice vibrato detection},
	isbn = {978-1-4244-2353-8},
	url = {http://ieeexplore.ieee.org/document/4959926/},
	doi = {10.1109/ICASSP.2009.4959926},
	abstract = {In this paper we investigate the problem of locating singing voice in music tracks. As opposed to most existing methods for this task, we rely on the extraction of the characteristics speciﬁc to singing voice. In our approach we suppose that the singing voice is characterized by harmonicity, formants, vibrato and tremolo. In the present study we deal only with the vibrato and tremolo characteristics. For this, we ﬁrst extract sinusoidal partials from the musical audio signal . The frequency modulation (vibrato) and amplitude modulation (tremolo) of each partial are then studied to determine if the partial corresponds to singing voice and hence the corresponding segment is supposed to contain singing voice. For this we estimate for each partial the rate (frequency of the modulations) and the extent (amplitude of modulation) of both vibrato and tremolo. A partial selection is then operated based on these values. A second criteria based on harmonicity is also introduced. Based on this, each segment can be labelled as singing or non-singing. Post-processing of the segmentation is then applied in order to remove short-duration segments. The proposed method is then evaluated on a large manually annotated test-set. The results of this evaluation are compared to the one obtained with a usual machine learning approach ({MFCC} and {SFM} modeling with {GMM}). The proposed method achieves very close results to the machine learning approach : 76.8\% compared to 77.4\% F-measure (frame classiﬁcation). This result is very promising, since both approaches are orthogonal and can then be combined.},
	eventtitle = {{ICASSP} 2009 - 2009 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	pages = {1685--1688},
	booktitle = {2009 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	publisher = {{IEEE}},
	author = {Regnier, L. and Peeters, G.},
	urldate = {2021-01-15},
	date = {2009-04},
	langid = {english},
	file = {2009 Singing voice detection in music tracks using direct voice vibrato detection - Regnier and Peeters.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2009 Singing voice detection in music tracks using direct voice vibrato detection - Regnier and Peeters.pdf:application/pdf}
}

@inproceedings{huang_singing-voice_2012,
	location = {Kyoto, Japan},
	title = {Singing-voice separation from monaural recordings using robust principal component analysis},
	isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
	url = {http://ieeexplore.ieee.org/document/6287816/},
	doi = {10.1109/ICASSP.2012.6287816},
	abstract = {Separating singing voices from music accompaniment is an important task in many applications, such as music information retrieval, lyric recognition and alignment. Music accompaniment can be assumed to be in a low-rank subspace, because of its repetition structure; on the other hand, singing voices can be regarded as relatively sparse within songs. In this paper, based on this assumption, we propose using robust principal component analysis for singing-voice separation from music accompaniment. Moreover, we examine the separation result by using a binary time-frequency masking method. Evaluations on the {MIR}-1K dataset show that this method can achieve around 1∼1.4 {dB} higher {GNSDR} compared with two state-of-the-art approaches without using prior training or requiring particular features.},
	eventtitle = {{ICASSP} 2012 - 2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
	pages = {57--60},
	booktitle = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
	publisher = {{IEEE}},
	author = {Huang, Po-Sen and Chen, Scott Deeann and Smaragdis, Paris and Hasegawa-Johnson, Mark},
	urldate = {2021-01-15},
	date = {2012-03},
	langid = {english},
	file = {2012 Singing-voice separation from monaural recordings using robust principal component analysis - Huang et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2012 Singing-voice separation from monaural recordings using robust principal component analysis - Huang et al..pdf:application/pdf}
}

@article{jarvelainen_audibility_2001,
	title = {Audibility of the timbral effects of inharmonicity in stringed instrument tones},
	volume = {2},
	issn = {1529-7853},
	url = {http://asa.scitation.org/doi/10.1121/1.1374756},
	doi = {10.1121/1.1374756},
	pages = {79--84},
	number = {3},
	journaltitle = {Acoustics Research Letters Online},
	shortjournal = {Acoustics Research Letters Online},
	author = {Järveläinen, Hanna and Välimäki, Vesa and Karjalainen, Matti},
	urldate = {2021-01-15},
	date = {2001-07},
	langid = {english},
	file = {2001 Audibility of the timbral effects of inharmonicity in stringed instrument tones - Järveläinen et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2001 Audibility of the timbral effects of inharmonicity in stringed instrument tones - Järveläinen et al..pdf:application/pdf}
}

@thesis{davidsson_structure-acoustic_2004,
	location = {Lund},
	title = {Structure-acoustic analysis: finite element modelling and reduction methods},
	shorttitle = {Structure-acoustic analysis},
	institution = {Univ.},
	type = {phdthesis},
	author = {Davidsson, Peter},
	date = {2004},
	langid = {english},
	note = {{ISBN}: 9789162861766
{OCLC}: 186476406},
	keywords = {test},
	file = {2004 Structure-acoustic analysis finite element modelling and reduction methods - Davidsson.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2004 Structure-acoustic analysis finite element modelling and reduction methods - Davidsson.pdf:application/pdf}
}

@article{kartofelev_modeling_nodate,
	title = {{MODELING} A {VIBRATING} {STRING} {TERMINATED} {AGAINST} A {BRIDGE} {WITH} {ARBITRARY} {GEOMETRY}},
	pages = {7},
	author = {Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-Maria and Va, Vesa},
	langid = {english},
	file = {MODELING A VIBRATING STRING TERMINATED AGAINST A BRIDGE WITH ARBITRARY GEOMETRY - Kartofelev et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\MODELING A VIBRATING STRING TERMINATED AGAINST A BRIDGE WITH ARBITRARY GEOMETRY - Kartofelev et al..pdf:application/pdf}
}
@TechReport{1957RosenblattPerceptron,
author        = {Rosenblatt, Frank},
date          = {1957},
institution   = {Cornell Aeronautical Laboratory},
title         = {The Perceptron: A Perceiving and Recognizing Automaton},
note          = {(Project PARA)},
number        = {85-460-1},
pages         = {460--1},
type          = {techreport},
url           = {https://blogs.umass.edu/brain-wars/files/2016/03/rosenblatt-1957.pdf},
abstract      = {First publication about the perceptron},
booktitle     = {Report 85, Cornell Aeronautical Laboratory},
file          = {:bibliography/ Seminal/1957 The Perceptron_ a Perceiving and Recognizing Automaton_1957RosenblattPerceptron.pdf:PDF},
groups        = {Seminal},
keywords      = {seminal},
mendeley-tags = {seminal},
publisher     = {Cornell Aeronautical Laboratory},
volume        = {1957},
id = {1},
}
@Article{1999YaoEvolving,
author       = {Yao, Xin},
date         = {1999},
journaltitle = {PROCEEDINGS OF THE IEEE},
title        = {Evolving Artiﬁcial Neural Networks},
doi          = {10.1109/5.784219},
number       = {9},
pages        = {25},
volume       = {87},
file         = {:bibliography/ Seminal/1999 Evolving Artiﬁcial Neural Networks_1999YaoEvolving.pdf:PDF},
groups       = {Seminal},
journal      = {Proceedings of the IEEE},
langid       = {english},
publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
year         = {1999},
}
@Article{1956RosenblattRemarks,
author       = {Rosenblatt, Murray},
date         = {1956},
journaltitle = {The Annals of Mathematical Statistics},
title        = {Remarks on Some Nonparametric Estimates of a Density Function},
issn         = {0003-4851},
number       = {3},
pages        = {832--837},
url          = {http://www.jstor.org/stable/2237390},
volume       = {27},
abstract     = {This note discusses some aspects of the estimation of the density function of a univariate probability distribution. All estimates of the density function satisfying relatively mild conditions are shown to be biased. The asymptotic mean square error of a particular class of estimates is evaluated.},
file         = {:bibliography/ Seminal/1956 Remarks on Some Nonparametric Estimates of a Density Function_1956RosenblattRemarks.pdf:PDF},
groups       = {Seminal},
publisher    = {Institute of Mathematical Statistics},
}
@Article{1943McCullochlogical,
author       = {McCulloch, Warren S. and Pitts, Walter},
date         = {1943-12},
journaltitle = {The Bulletin of Mathematical Biophysics},
title        = {A logical calculus of the ideas immanent in nervous activity},
doi          = {10.1007/bf02478259},
number       = {4},
pages        = {115--133},
volume       = {5},
file         = {:bibliography/ Seminal/1943 A Logical Calculus of the Ideas Immanent in Nervous Activity_1943McCullochlogical.pdf:PDF},
groups       = {Seminal},
}
@Article{1962ParzenEstimation,
author       = {Parzen, Emanuel},
date         = {1962},
journaltitle = {The Annals of Mathematical Statistics},
title        = {On Estimation of a Probability Density Function and Mode},
issn         = {0003-4851},
number       = {3},
pages        = {1065--1076},
url          = {https://www.jstor.org/stable/2237880},
urldate      = {2019-03-28},
volume       = {33},
file         = {:bibliography/ Seminal/1962 On Estimation of a Probability Density Function and Mode_1962ParzenEstimation.pdf:PDF},
groups       = {Seminal},
}
@Article{2012HoogenboomHow,
author       = {Hoogenboom, Barbara J. and Manske, Robert C.},
date         = {2012},
journaltitle = {The International Journal of Sports Physical Therapy},
title        = {How To Write A Scientific Article},
pages        = {6},
abstract     = {Successful production of a written product for submission to a peer-reviewed scientific journal requires substantial effort. Such an effort can be maximized by following a few simple suggestions when composing/ creating the product for submission. By following some suggested guidelines and avoiding common errors, the process can be streamlined and success realized for even beginning/novice authors as they negotiate the publication process. The purpose of this invited commentary is to offer practical suggestions for achieving success when writing and submitting manuscripts to The International Journal of Sports Physical Therapy and other professional journals.},
file         = {:bibliography/ Writing/2012 How to Write a Scientific Article_2012HoogenboomHow.pdf:PDF},
groups       = {Writing},
langid       = {english},
}
@Unpublished{2018Elsevier4,
author    = {Elsevier},
date      = {2018},
title     = {4 step guide to writing a literature review},
url       = {http://www.emeraldgrouppublishing.com/authors/guides/write/literature.htm},
file      = {:bibliography/ Writing/2018 4 Step Guide to Writing a Literature Review_2018Elsevier4.pdf:PDF},
groups    = {Writing},
publisher = {Emerald Publishing},
}
@InProceedings{2013KartofelevModeling,
author    = {Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-Maria and Va, Vesa},
booktitle = {4th Stockholm Music Acoustics Conference},
date      = {2013},
title     = {Modeling a vibrating string terminated against a bridge with arbitrary geometry},
pages     = {7},
url       = {http://homes.ioc.ee/stulov/smac13.pdf},
file      = {:bibliography/ Acoustics/2013 Modeling a Vibrating String Terminated against a Bridge with Arbitrary Geometry_2013KartofelevModeling.pdf:application/pdf},
groups    = {Acoustics},
langid    = {english},
}
@Article{2009TorresInfluence,
author       = {Torres, Jesús Alejandro and Boullosa, Ricardo R.},
date         = {2009-12},
journaltitle = {Applied Acoustics},
title        = {Influence of the bridge on the vibrations of the top plate of a classical guitar},
doi          = {10.1016/j.apacoust.2009.07.002},
issn         = {0003-682X},
number       = {11},
pages        = {1371--1377},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X09001613},
urldate      = {2019-04-05},
volume       = {70},
abstract     = {In this work the effect of two different bridge conﬁgurations on the vibrations of the top plate of a classical guitar is presented. Experimental harmonic analysis and visualization techniques, in addition to detailed damped simulations using the ﬁnite element method, were used to obtain mobility functions and operating deﬂection shapes of a top plate. The mobility functions were obtained in the following sequence. First the mobility functions were obtained on the top plate without a bridge attached. Then a bridge was glued to the top plate and new measurements were made. Finally the already attached bridge was cross-cut in situ without detaching it from the top plate, and the measurements were repeated. Those speciﬁc designs were chosen on the grounds of simplicity of construction both experimentally and in FEM simulations (no particular preference for those designs is implied). The assembly and the speciﬁc design of the bridge have shown to have considerable inﬂuence on the response of the top plate regarding the mode shapes above 300 Hz. Depending on the geometry of the bridge, its deﬂections can either be comparable to the maximum deﬂections of the top plate or can have amplitudes so small that the bridge effectively creates a nodal zone on the plate vibrations. This suggests that the shape, the stiffness, and the mass properties of the bridge may play a role in the sound quality of the instrument. Ó 2009 Elsevier Ltd. All rights reserved.},
file         = {:bibliography/ Acoustics/2009 Influence of the Bridge on the Vibrations of the Top Plate of a Classical Guitar_2009TorresInfluence.pdf:application/pdf},
groups       = {Acoustics},
langid       = {english},
}
@Collection{2009BruneauMaterials,
date      = {2009},
editor    = {Bruneau, Michel and Potel, Catherine},
title     = {Materials and acoustics handbook},
isbn      = {978-1-84821-074-5},
location  = {London, UK},
pagetotal = {919},
publisher = {ISTE ; Hoboken, NJ : J. Wiley},
file      = {:bibliography/ Acoustics/2009 Materials and Acoustics Handbook_2009BruneauMaterials.pdf:application/pdf},
groups    = {Acoustics},
keywords  = {Handbooks, manuals, etc, Acoustical engineering, Acoustical materials, Materials, Testing},
langid    = {english},
}
@Article{2006WagnerLarge,
author = {Wagner, Claus and Huttl, Thomas and Sagaut, Pierre},
date   = {2006},
title  = {Large-Eddy Simulation for Acoustics},
pages  = {471},
file   = {:bibliography/ Acoustics/2006 Large Eddy Simulation for Acoustics_2006WagnerLarge.pdf:application/pdf},
groups = {Acoustics},
langid = {english},
}
@InProceedings{2003KarjalainenCompilation,
author     = {Karjalainen, M. and Erkut, C. and Savioja, L.},
booktitle  = {2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP '03).},
date       = {2003},
title      = {Compilation of unified physical models for efficient sound synthesis},
doi        = {10.1109/ICASSP.2003.1199999},
eventtitle = {International Conference on Acoustics, Speech and Signal Processing (ICASSP'03)},
isbn       = {978-0-7803-7663-2},
location   = {Hong Kong, China},
pages      = {V–433–6},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/1199999/},
urldate    = {2019-03-31},
volume     = {5},
abstract   = {This paper describes a systematic approach to specification and compilation of different physical modeling schemes particularly for sound synthesis studies. First we formulate theoretically a unified way of constructing physical interaction models which include elements that use both wave variables and Kirchhoff variables. These elements can be applied to build 1-D and multidimensional structures as well as lumped element models. In addition, typical signal processing algorithms are supported. A software environment (Block Compiler) has been developed, allowing for high-level object-based specification of physical models and their compilation to efficient code for execution.},
file       = {:bibliography/ Acoustics/2003 Compilation of Unified Physical Models for Efficient Sound Synthesis_2003KarjalainenCompilation.pdf:application/pdf},
groups     = {Acoustics},
langid     = {english},
}
@Article{2002BoullosaVibration,
author       = {Boullosa, Ricardo R.},
date         = {2002-03},
journaltitle = {Applied Acoustics},
title        = {Vibration measurements in the classical guitar},
doi          = {10.1016/S0003-682X(01)00037-8},
issn         = {0003-682X},
number       = {3},
pages        = {311--322},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0003682X01000378},
urldate      = {2019-04-05},
volume       = {63},
abstract     = {Some measurements are described that pertain to the high frequency behaviour of the vibrations of the string and soundboard of the guitar.Three isolated soundboards of diﬀerent woods were compared with respect to their early decay times (by means of measurements of their vibrational impulse responses), in order to assess its possible use as an objective measure of the reverberation time of the plates. The results of all these measurements show that high frequency vibrations and radiation occurs signiﬁcantly from 1 to 14 kHz, but that the sound radiation has harmonics at frequencies up to 20 kHz. This report does not produce any experimental evidence as to the importance of the high frequency region from the subjective point of view, or to the correlation of the subjective impression of reverberation of the plates with measured data of early decay times, as these aspects were not addressed at this point.},
file         = {:bibliography/ Acoustics/2002 Vibration Measurements in the Classical Guitar_2002BoullosaVibration.pdf:application/pdf},
groups       = {Acoustics},
langid       = {english},
}
@Article{2001JaervelaeinenAudibility,
author       = {Järveläinen, Hanna and Välimäki, Vesa and Karjalainen, Matti},
date         = {2001-07},
journaltitle = {Acoustics Research Letters Online},
title        = {Audibility of the timbral effects of inharmonicity in stringed instrument tones},
doi          = {10.1121/1.1374756},
issn         = {1529-7853},
number       = {3},
pages        = {79--84},
urldate      = {2019-03-28},
volume       = {2},
file         = {:bibliography/ Acoustics/2001 Audibility of the Timbral Effects of Inharmonicity in Stringed Instrument Tones_2001JaervelaeinenAudibility.pdf:application/pdf},
groups       = {Acoustics},
langid       = {english},
}
@Article{1995CaldersmithDesigning,
author       = {Caldersmith, Graham},
date         = {1995},
journaltitle = {Applied Acoustics},
title        = {Designing a guitar family},
doi          = {10.1016/0003-682X(95)93949-I},
issn         = {0003-682X},
number       = {1},
pages        = {3--17},
url          = {http://linkinghub.elsevier.com/retrieve/pii/0003682X9593949I},
urldate      = {2019-04-05},
volume       = {46},
abstract     = {When the standard classical guitar is scaled up a musical fourth to a treble guitar, down a musical fifth to a baritone guitar, and down an octave to a bass guitar, design compromises are necessary to maintain playability and favourable tone qulality. The resulting instruments exhibit interesting natural vibration mode and sound radiation physics. The tone qualities of the new instruments suggest relations between the guitar response envelope and human sound perception. Translating the principle natural modes of the guitar up or down with string frequencies does not necessarily produce pleasing tone qualities nor optimal playing dynamics. However, designing bracing configurations for both classical and folk baritone and bass guitars to maximise low frequency radiation eficiency does seem to produce new instruments of musical appeal, Frequency response records of standard and guitar family variants illustrate the physical behaviour of the difierent designs. Experience of musicians with the guitar family instruments indicates that creative new guitar territory is available.},
file         = {:bibliography/ Acoustics/1995 Designing a Guitar Family_1995CaldersmithDesigning.pdf:application/pdf},
groups       = {Acoustics},
langid       = {english},
}
@Collection{2007BeauchampAnalysis,
date       = {2007},
editor     = {Beauchamp, James W.},
title      = {Analysis, synthesis, and perception of musical sounds: the sound of music},
isbn       = {978-0-387-32496-8},
location   = {New York, NY},
note       = {OCLC: 237018036},
pagetotal  = {325},
publisher  = {Springer},
series     = {Modern acoustics and signal processing},
file       = {:bibliography/ Acoustics/2007 Analysis, Synthesis, and Perception of Musical Sounds_ the Sound of Music_2007BeauchampAnalysis.pdf:PDF},
groups     = {Acoustics},
langid     = {english},
shorttitle = {Analysis, synthesis, and perception of musical sounds},
}
@Article{2018MartinMartinGoogle,
author       = {Martín-Martín, Alberto and Orduna-Malea, Enrique and Thelwall, Mike and Delgado López-Cózar, Emilio},
date         = {2018-11},
journaltitle = {Journal of Informetrics},
title        = {Google Scholar, Web of Science, and Scopus: A systematic comparison of citations in 252 subject categories},
doi          = {10.1016/j.joi.2018.09.002},
issn         = {1751-1577},
number       = {4},
pages        = {1160--1177},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1751157718303249},
urldate      = {2019-04-07},
volume       = {12},
abstract     = {Despite citation counts from Google Scholar (GS), Web of Science (WoS), and Scopus being widely consulted by researchers and sometimes used in research evaluations, there is no recent or systematic evidence about the differences between them. In response, this paper investigates 2,448,055 citations to 2299 English-language highly-cited documents from 252 GS subject categories published in 2006, comparing GS, the WoS Core Collection, and Scopus. GS consistently found the largest percentage of citations across all areas (93/////// %–96/////// %), far ahead of Scopus (35/////// %–77/////// %) and WoS (27/////// %–73/////// %). GS found nearly all the WoS (95/////// %) and Scopus (92/////// %) citations. Most citations found only by GS were from non-journal sources (48/////// %–65/////// %), including theses, books, conference papers, and unpublished materials. Many were non-English (19/////// %–38/////// %), and they tended to be much less cited than citing sources that were also in Scopus or WoS. Despite the many unique GS citing sources, Spearman correlations between citation counts in GS and WoS or Scopus are high (0.78-0.99). They are lower in the Humanities, and lower between GS and WoS than between GS and Scopus. The results suggest that in all areas GS citation data is essentially a superset of WoS and Scopus, with substantial extra coverage.},
file         = {:bibliography/ Bibliometry/2018 Google Scholar, Web of Science, and Scopus_ a Systematic Comparison of Citations in 252 Subject Categories_2018MartinMartinGoogle.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Google Scholar, Web of Science, and Scopus},
}
@Article{2017ZatorskiYoung,
author       = {Zatorski, Hubert and Fichna, Jakub},
date         = {2017-12},
journaltitle = {United European Gastroenterology Journal},
title        = {Young GI angle: The role of bibliometrics in scientist’s career development},
doi          = {10.1177/2050640617744497},
issn         = {2050-6406},
number       = {8},
pages        = {1151--1152},
urldate      = {2019-04-07},
volume       = {5},
file         = {:bibliography/ Bibliometry/2017 Young GI Angle_ the Role of Bibliometrics in Scientist’s Career Development_2017ZatorskiYoung.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Young GI angle},
}
@Article{2016PerianesRodriguezConstructing,
author       = {Perianes-Rodriguez, Antonio and Waltman, Ludo and van Eck, Nees Jan},
date         = {2016-11},
journaltitle = {Journal of Informetrics},
title        = {Constructing bibliometric networks: A comparison between full and fractional counting},
doi          = {10.1016/j.joi.2016.10.006},
issn         = {1751-1577},
number       = {4},
pages        = {1178--1195},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1751157716302036},
urldate      = {2019-04-07},
volume       = {10},
file         = {:bibliography/ Bibliometry/2016 Constructing Bibliometric Networks_ a Comparison between Full and Fractional Counting_2016PerianesRodriguezConstructing.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Constructing bibliometric networks},
}
@Article{2016Mongeonjournal,
author       = {Mongeon, Philippe and Paul-Hus, Adèle},
date         = {2016-01},
journaltitle = {Scientometrics},
title        = {The journal coverage of Web of Science and Scopus: a comparative analysis},
doi          = {10.1007/s11192-015-1765-5},
issn         = {0138-9130},
number       = {1},
pages        = {213--228},
urldate      = {2019-04-07},
volume       = {106},
abstract     = {Bibliometric methods are used in multiple ﬁelds for a variety of purposes, namely for research evaluation. Most bibliometric analyses have in common their data sources: Thomson Reuters’ Web of Science (WoS) and Elsevier’s Scopus. The objective of this research is to describe the journal coverage of those two databases and to assess whether some ﬁeld, publishing country and language are over or underrepresented. To do this we compared the coverage of active scholarly journals in WoS (13,605 journals) and Scopus (20,346 journals) with Ulrich’s extensive periodical directory (63,013 journals). Results indicate that the use of either WoS or Scopus for research evaluation may introduce biases that favor Natural Sciences and Engineering as well as Biomedical Research to the detriment of Social Sciences and Arts and Humanities. Similarly, English-language journals are overrepresented to the detriment of other languages. While both databases share these biases, their coverage differs substantially. As a consequence, the results of bibliometric analyses may vary depending on the database used. These results imply that in the context of comparative research evaluation, WoS and Scopus should be used with caution, especially when comparing different ﬁelds, institutions, countries or languages. The bibliometric community should continue its efforts to develop methods and indicators that include scientiﬁc output that are not covered in WoS or Scopus, such as ﬁeld-speciﬁc and national citation indexes.},
file         = {:bibliography/ Bibliometry/2016 The Journal Coverage of Web of Science and Scopus_ a Comparative Analysis_2016Mongeonjournal.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {The journal coverage of Web of Science and Scopus},
}
@Article{2016HarzingGoogle,
author       = {Harzing, Anne-Wil and Alakangas, Satu},
date         = {2016-02},
journaltitle = {Scientometrics},
title        = {Google Scholar, Scopus and the Web of Science: a longitudinal and cross-disciplinary comparison},
doi          = {10.1007/s11192-015-1798-9},
issn         = {0138-9130},
number       = {2},
pages        = {787--804},
urldate      = {2019-04-07},
volume       = {106},
abstract     = {This article aims to provide a systematic and comprehensive comparison of the coverage of the three major bibliometric databases: Google Scholar, Scopus and the Web of Science. Based on a sample of 146 senior academics in ﬁve broad disciplinary areas, we therefore provide both a longitudinal and a cross-disciplinary comparison of the three databases. Our longitudinal comparison of eight data points between 2013 and 2015 shows a consistent and reasonably stable quarterly growth for both publications and citations across the three databases. This suggests that all three databases provide sufﬁcient stability of coverage to be used for more detailed cross-disciplinary comparisons. Our cross-disciplinary comparison of the three databases includes four key research metrics (publications, citations, h-index, and hI, annual, an annualised individual h-index) and ﬁve major disciplines (Humanities, Social Sciences, Engineering, Sciences and Life Sciences). We show that both the data source and the speciﬁc metrics used change the conclusions that can be drawn from cross-disciplinary comparisons.},
file         = {:bibliography/ Bibliometry/2016 Google Scholar, Scopus and the Web of Science_ a Longitudinal and Cross Disciplinary Comparison_2016HarzingGoogle.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Google Scholar, Scopus and the Web of Science},
}
@Article{2014ZahediHow,
author       = {Zahedi, Zohreh and Costas, Rodrigo and Wouters, Paul},
date         = {2014-11},
journaltitle = {Scientometrics},
title        = {How well developed are altmetrics? A cross-disciplinary analysis of the presence of ‘alternative metrics’ in scientific publications},
doi          = {10.1007/s11192-014-1264-0},
issn         = {0138-9130},
number       = {2},
pages        = {1491--1513},
urldate      = {2019-04-07},
volume       = {101},
abstract     = {In this paper an analysis of the presence and possibilities of altmetrics for bibliometric and performance analysis is carried out. Using the web based tool Impact Story, we collected metrics for 20,000 random publications from the Web of Science. We studied both the presence and distribution of altmetrics in the set of publications, across ﬁelds, document types and over publication years, as well as the extent to which altmetrics correlate with citation indicators. The main result of the study is that the altmetrics source that provides the most metrics is Mendeley, with metrics on readerships for 62.6 /////// % of all the publications studied, other sources only provide marginal information. In terms of relation with citations, a moderate spearman correlation (r = 0.49) has been found between Mendeley readership counts and citation indicators. Other possibilities and limitations of these indicators are discussed and future research lines are outlined.},
file         = {:bibliography/ Bibliometry/2014 How Well Developed Are Altmetrics_ a Cross Disciplinary Analysis of the Presence of ‘alternative Metrics’ in Scientific Publications_2014ZahediHow.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {How well developed are altmetrics?},
}
@Article{2014Winterexpansion,
author       = {de Winter, Joost C. F. and Zadpoor, Amir A. and Dodou, Dimitra},
date         = {2014-02},
journaltitle = {Scientometrics},
title        = {The expansion of Google Scholar versus Web of Science: a longitudinal study},
doi          = {10.1007/s11192-013-1089-2},
issn         = {0138-9130},
number       = {2},
pages        = {1547--1565},
urldate      = {2019-04-07},
volume       = {98},
abstract     = {Web of Science (WoS) and Google Scholar (GS) are prominent citation services with distinct indexing mechanisms. Comprehensive knowledge about the growth patterns of these two citation services is lacking. We analyzed the development of citation counts in WoS and GS for two classic articles and 56 articles from diverse research ﬁelds, making a distinction between retroactive growth (i.e., the relative difference between citation counts up to mid-2005 measured in mid-2005 and citation counts up to mid-2005 measured in April 2013) and actual growth (i.e., the relative difference between citation counts up to mid-2005 measured in April 2013 and citation counts up to April 2013 measured in April 2013). One of the classic articles was used for a citation-by-citation analysis. Results showed that GS has substantially grown in a retroactive manner (median of 170 /////// % across articles), especially for articles that initially had low citations counts in GS as compared to WoS. Retroactive growth of WoS was small, with a median of 2 /////// % across articles. Actual growth percentages were moderately higher for GS than for WoS (medians of 54 vs. 41 /////// %). The citation-by-citation analysis showed that the percentage of citations being unique in WoS was lower for more recent citations (6.8 /////// % for citations from 1995 and later vs. 41 /////// % for citations from before 1995), whereas the opposite was noted for GS (57 vs. 33 /////// %). It is concluded that, since its inception, GS has shown substantial expansion, and that the majority of recent works indexed in WoS are now also retrievable via GS. A discussion is provided on quantity versus quality of citations, threats for WoS, weaknesses of GS, and implications for literature research and research evaluation.},
file         = {:bibliography/ Bibliometry/2014 The Expansion of Google Scholar Versus Web of Science_ a Longitudinal Study_2014Winterexpansion.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {The expansion of Google Scholar versus Web of Science},
}
@Article{2014HausteinCoverage,
author       = {Haustein, Stefanie and Peters, Isabella and Bar-Ilan, Judit and Priem, Jason and Shema, Hadas and Terliesner, Jens},
date         = {2014-11},
journaltitle = {Scientometrics},
title        = {Coverage and adoption of altmetrics sources in the bibliometric community},
doi          = {10.1007/s11192-013-1221-3},
issn         = {0138-9130},
number       = {2},
pages        = {1145--1163},
urldate      = {2019-04-07},
volume       = {101},
file         = {:bibliography/ Bibliometry/2014 Coverage and Adoption of Altmetrics Sources in the Bibliometric Community_2014HausteinCoverage.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
}
@Misc{2014HarinarayanaData,
author = {Harinarayana, N.},
date   = {2014},
title  = {Data sources and software tools for bibliometric studies},
url    = {https://epgp.inflibnet.ac.in/epgpdata/uploads/epgp///////\\_content/library///////\\_and///////\\_information///////\\_science/informetrics///////\\_///////\\&///////\\_scientometrics/data///////\\_sources///////\\_and///////\\_software///////\\_tools///////\\_for///////\\_bibliometric///////\\_studies/et/333///////\\_et///////\\_m2.pdf},
file   = {:bibliography/ Bibliometry/2014 Data Sources and Software Tools for Bibliometric Studies_2014HarinarayanaData.pdf:PDF},
groups = {Bibliometry},
}
@Article{2014BornmannValidity,
author       = {Bornmann, Lutz},
date         = {2014-10},
journaltitle = {Journal of Informetrics},
title        = {Validity of altmetrics data for measuring societal impact: A study using data from Altmetric and F1000Prime},
doi          = {10.1016/j.joi.2014.09.007},
issn         = {1751-1577},
number       = {4},
pages        = {935--950},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1751157714000881},
urldate      = {2019-04-07},
volume       = {8},
abstract     = {Can altmetric data be validly used for the measurement of societal impact? The current study seeks to answer this question with a comprehensive dataset (about 100,000 records) from very disparate sources (F1000, Altmetric, and an in-house database based on Web of Science). In the F1000 peer review system, experts attach particular tags to scientiﬁc papers which indicate whether a paper could be of interest for science or rather for other segments of society. The results show that papers with the tag "good for teaching" do achieve higher altmetric counts than papers without this tag – if the quality of the papers is controlled. At the same time, a higher citation count is shown especially by papers with a tag that is speciﬁcally scientiﬁcally oriented ("new ﬁnding"). The ﬁndings indicate that papers tailored for a readership outside the area of research should lead to societal impact.},
file         = {:bibliography/ Bibliometry/2014 Validity of Altmetrics Data for Measuring Societal Impact_ a Study Using Data from Altmetric and F1000Prime_2014BornmannValidity.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Validity of altmetrics data for measuring societal impact},
}
@Article{2014BornmannDo,
author       = {Bornmann, Lutz},
date         = {2014-10},
journaltitle = {Journal of Informetrics},
title        = {Do altmetrics point to the broader impact of research? An overview of benefits and disadvantages of altmetrics},
doi          = {10.1016/j.joi.2014.09.005},
issn         = {1751-1577},
number       = {4},
pages        = {895--903},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1751157714000868},
urldate      = {2019-04-07},
volume       = {8},
file         = {:bibliography/ Bibliometry/2014 Do Altmetrics Point to the Broader Impact of Research_ an Overview of Benefits and Disadvantages of Altmetrics_2014BornmannDo.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Do altmetrics point to the broader impact of research?},
}
@Article{2013ThelwallDo,
author       = {Thelwall, Mike and Haustein, Stefanie and Larivière, Vincent and Sugimoto, Cassidy R.},
date         = {2013-05-28},
journaltitle = {PLoS ONE},
title        = {Do Altmetrics Work? Twitter and Ten Other Social Web Services},
doi          = {10.1371/journal.pone.0064841},
editor       = {Bornmann, Lutz},
issn         = {1932-6203},
number       = {5},
pages        = {e64841},
urldate      = {2019-04-07},
volume       = {8},
abstract     = {Altmetric measurements derived from the social web are increasingly advocated and used as early indicators of article impact and usefulness. Nevertheless, there is a lack of systematic scientific evidence that altmetrics are valid proxies of either impact or utility although a few case studies have reported medium correlations between specific altmetrics and citation rates for individual journals or fields. To fill this gap, this study compares 11 altmetrics with Web of Science citations for 76 to 208,739 PubMed articles with at least one altmetric mention in each case and up to 1,891 journals per metric. It also introduces a simple sign test to overcome biases caused by different citation and usage windows. Statistically significant associations were found between higher metric scores and higher citations for articles with positive altmetric scores in all cases with sufficient evidence (Twitter, Facebook wall posts, research highlights, blogs, mainstream media and forums) except perhaps for Google+ posts. Evidence was insufficient for LinkedIn, Pinterest, question and answer sites, and Reddit, and no conclusions should be drawn about articles with zero altmetric scores or the strength of any correlation between altmetrics and citations. Nevertheless, comparisons between citations and metric values for articles published at different times, even within the same year, can remove or reverse this association and so publishers and scientometricians should consider the effect of time when using altmetrics to rank articles. Finally, the coverage of all the altmetrics except for Twitter seems to be low and so it is not clear if they are prevalent enough to be useful in practice.},
file         = {:bibliography/ Bibliometry/2013 Do Altmetrics Work_ Twitter and Ten Other Social Web Services_2013ThelwallDo.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Do Altmetrics Work?},
}
@Article{2013S.AdriaanseWeb,
author       = {S. Adriaanse, Leslie and Rensleigh, Chris},
date         = {2013-11-18},
journaltitle = {The Electronic Library},
title        = {Web of Science, Scopus and Google Scholar: A content comprehensiveness comparison},
doi          = {10.1108/EL-12-2011-0174},
issn         = {0264-0473},
number       = {6},
pages        = {727--744},
urldate      = {2019-04-07},
volume       = {31},
abstract     = {Purpose – The research aim for this study was to compare three citation resources with one another to identify the citation resource with the most representative South African scholarly environmental sciences citation coverage. This paper focuses on the results of the content veriﬁcation process which measured amongst others the citation counts, multiple copies and inconsistencies encountered across the three citation resources ISI Web of Science, Scopus and Google Scholar.},
file         = {:bibliography/ Bibliometry/2013 Web of Science, Scopus and Google Scholar_ a Content Comprehensiveness Comparison_2013S.AdriaanseWeb.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Web of Science, Scopus and Google Scholar},
}
@Article{2013GalliganAltmetrics,
author       = {Galligan, Finbar and Dyas-Correia, Sharon},
date         = {2013-03},
journaltitle = {Serials Review},
title        = {Altmetrics: Rethinking the Way We Measure},
doi          = {10.1016/j.serrev.2013.01.003},
issn         = {0098-7913},
number       = {1},
pages        = {56--61},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S009879131300004X},
urldate      = {2019-04-07},
volume       = {39},
abstract     = {Altmetrics is the focus for this edition of "Balance Point." The column editor invited Finbar Galligan who has gained considerable knowledge of altmetrics to co-author the column. Altmetrics, their relationship to traditional metrics, their importance, uses, potential impacts, and possible future directions are examined. The authors conclude that altmetrics have an important future role to play and that they offer the potential to revolutionize the analysis of the value and impact of scholarly work.},
file         = {:bibliography/ Bibliometry/2013 Altmetrics_ Rethinking the Way We Measure_2013GalliganAltmetrics.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Altmetrics},
}
@Article{2012VanclayImpact,
author       = {Vanclay, Jerome K.},
date         = {2012-08},
journaltitle = {Scientometrics},
title        = {Impact factor: outdated artefact or stepping-stone to journal certification?},
doi          = {10.1007/s11192-011-0561-0},
issn         = {0138-9130},
number       = {2},
pages        = {211--238},
urldate      = {2019-04-07},
volume       = {92},
abstract     = {A review of Garﬁeld’s journal impact factor and its speciﬁc implementation as the Thomson Reuters impact factor reveals several weaknesses in this commonly-used indicator of journal standing. Key limitations include the mismatch between citing and cited documents, the deceptive display of three decimals that belies the real precision, and the absence of conﬁdence intervals. These are minor issues that are easily amended and should be corrected, but more substantive improvements are needed. There are indications that the scientiﬁc community seeks and needs better certiﬁcation of journal procedures to improve the quality of published science. Comprehensive certiﬁcation of editorial and review procedures could help ensure adequate procedures to detect duplicate and fraudulent submissions.},
file         = {:bibliography/ Bibliometry/2012 Impact Factor_ Outdated Artefact or Stepping Stone to Journal Certification__2012VanclayImpact.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Impact factor},
}
@Article{2006BakkalbasiThree,
author       = {Bakkalbasi, Nisa and Bauer, Kathleen and Glover, Janis and Wang, Lei},
date         = {2006-12},
journaltitle = {Biomedical Digital Libraries},
title        = {Three options for citation tracking: Google Scholar, Scopus and Web of Science},
doi          = {10.1186/1742-5581-3-7},
issn         = {1742-5581},
number       = {1},
urldate      = {2019-04-07},
volume       = {3},
abstract     = {Background: Researchers turn to citation tracking to find the most influential articles for a particular topic and to see how often their own published papers are cited. For years researchers looking for this type of information had only one resource to consult: the Web of Science from Thomson Scientific. In 2004 two competitors emerged – Scopus from Elsevier and Google Scholar from Google. The research reported here uses citation analysis in an observational study examining these three databases; comparing citation counts for articles from two disciplines (oncology and condensed matter physics) and two years (1993 and 2003) to test the hypothesis that the different scholarly publication coverage provided by the three search tools will lead to different citation counts from each. Methods: Eleven journal titles with varying impact factors were selected from each discipline (oncology and condensed matter physics) using the Journal Citation Reports (JCR). All articles published in the selected titles were retrieved for the years 1993 and 2003, and a stratified random sample of articles was chosen, resulting in four sets of articles. During the week of November 7–12, 2005, the citation counts for each research article were extracted from the three sources. The actual citing references for a subset of the articles published in 2003 were also gathered from each of the three sources. Results: For oncology 1993 Web of Science returned the highest average number of citations, 45.3. Scopus returned the highest average number of citations (8.9) for oncology 2003. Web of Science returned the highest number of citations for condensed matter physics 1993 and 2003 (22.5 and 3.9 respectively). The data showed a significant difference in the mean citation rates between all pairs of resources except between Google Scholar and Scopus for condensed matter physics 2003. For articles published in 2003 Google Scholar returned the largest amount of unique citing material for oncology and Web of Science returned the most for condensed matter physics. Conclusion: This study did not identify any one of these three resources as the answer to all citation tracking needs. Scopus showed strength in providing citing literature for current (2003) oncology articles, while Web of Science produced more citing material for 2003 and 1993 condensed matter physics, and 1993 oncology articles. All three tools returned some unique material. Our data indicate that the question of which tool provides the most complete set of citing literature may depend on the subject and publication year of a given article.},
file         = {:bibliography/ Bibliometry/2006 Three Options for Citation Tracking_ Google Scholar, Scopus and Web of Science_2006BakkalbasiThree.pdf:PDF},
groups       = {Bibliometry},
langid       = {english},
shorttitle   = {Three options for citation tracking},
}
@Collection{2018ErdtAltmetrics,
date      = {2018},
editor    = {Erdt, Mojisola and Sesagiri Raamkumar, Aravind and Rasmussen, Edie and Theng, Yin-Leng},
title     = {Altmetrics for Research Outputs Measurement and Scholarly Information Management},
doi       = {10.1007/978-981-13-1053-9},
isbn      = {9789811310522},
location  = {Singapore},
publisher = {Springer Singapore},
series    = {Communications in Computer and Information Science},
urldate   = {2019-04-07},
volume    = {856},
file      = {:bibliography/ Bibliometry/2018 Altmetrics for Research Outputs Measurement and Scholarly Information Management_2018ErdtAltmetrics.pdf:PDF},
groups    = {Bibliometry},
langid    = {english},
}
@Misc{2017EbrahimHow,
author    = {Ebrahim, Nader Ale},
date      = {2017},
title     = {How to Write a Bibliometric Paper},
doi       = {10.6084/m9.figshare.5374615.v1},
url       = {https://figshare.com/articles/How///////\\_to///////\\_Write///////\\_a///////\\_Bibliometric///////\\_Paper/5374615/1},
urldate   = {2019-04-07},
abstract  = {Bibliometrics can be defined as the statistical analysis of publications. Bibliometrics has focused on the quantitative analysis of citations and citation counts which is complex. It is so complex and specialized that personal knowledge and experience are insufficient for understanding trends and then making decisions. We need tools for analysis of bibliometrics information to recognize the research trends and evaluate scientific/institution/country’s research productivity. This presentation will provide procedure to write a Bibliometrics paper.},
file      = {:bibliography/ Writing/2017 How to Write a Bibliometric Paper_2017EbrahimHow.pdf:PDF},
groups    = {Writing},
langid    = {english},
publisher = {Figshare},
}
@Article{HendryInharmonicity,
author   = {Hendry, Simon},
title    = {Inharmonicity of Piano Strings},
pages    = {32},
abstract = {Piano partials deviate from integer harmonics due to the stiffness and linear density of the strings. Values for the inharmonicity coefficient of six strings were determined experimentally and compared with calculated values. An investigation into stretched tuning was also performed with detailed readings taken from a well tuned Steinway Model D grand piano. These results are compared with Railsback‟s predictions of 1938.},
file     = {:bibliography/ Piano/Inharmonicity of Piano Strings_HendryInharmonicity.pdf:PDF},
groups   = {Piano},
langid   = {english},
}
@Article{2016Graciawave,
author       = {Gracia, X. and Sanz-perela, T.},
date         = {2016},
journaltitle = {Reports@SCM},
title        = {The wave equation for stiff strings and piano tuning.},
eprint       = {1603.05516},
eprinttype   = {arXiv},
number       = {1},
pages        = {1--16},
url          = {http://revistes.iec.cat/index.php/reports/article/view/142136/141068},
volume       = {3},
annotation   = {preprint},
arxivid      = {1603.05516},
file         = {:bibliography/ Piano/2016 The Wave Equation for Stiff Strings and Piano Tuning._2016Graciawave.pdf:PDF},
groups       = {Piano},
keywords     = {inharmonic spectrum,musical,stiffness,vibrating string,wave equation},
}
@Article{2015GiordanoExplaining,
author       = {Giordano, N.},
date         = {2015-10},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Explaining the Railsback stretch in terms of the inharmonicity of piano tones and sensory dissonance},
doi          = {10.1121/1.4931439},
issn         = {0001-4966},
number       = {4},
pages        = {2359--2366},
urldate      = {2019-03-26},
volume       = {138},
file         = {:bibliography/ Piano/2015 Explaining the Railsback Stretch in Terms of the Inharmonicity of Piano Tones and Sensory Dissonance_2015GiordanoExplaining.pdf:PDF},
groups       = {Piano},
langid       = {english},
}
@InProceedings{2015ChengModelling,
author     = {Cheng, Tian and Dixon, Simon and Mauch, Matthias},
booktitle  = {2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date       = {2015-04},
title      = {Modelling the decay of piano sounds},
doi        = {10.1109/ICASSP.2015.7178038},
eventtitle = {ICASSP 2015 - 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn       = {978-1-4673-6997-8},
location   = {South Brisbane, Queensland, Australia},
pages      = {594--598},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/7178038/},
urldate    = {2019-03-28},
abstract   = {We investigate piano acoustics and compare the theoretical temporal decay of individual partials to recordings of real-world piano notes from the RWC Music Database. We ﬁrst describe the theory behind double decay and beats, known phenomena caused by the interaction between strings and soundboard. Then we ﬁt the decay of the ﬁrst 30 partials to a standard linear model and two physically-motivated non-linear models that take into account the coupling of strings and soundboard. We show that the use of non-linear models provides a better ﬁt to the data. We use these estimated decay rates to parameterise the characteristic decay response (decay rates along frequencies) of the piano under investigation. The results also show that dynamics have no signiﬁcant effect on the decay rate.},
file       = {:bibliography/ Piano/2015 Modelling the Decay of Piano Sounds_2015ChengModelling.pdf:PDF},
groups     = {Piano},
langid     = {english},
}
@Report{2004BurredAcoustics,
author      = {Burred, Juan Jose},
date        = {2004},
institution = {Professional Conservatory of Music Arturo Soria},
title       = {The Acoustics of the Piano},
type        = {resreport},
pages       = {43},
file        = {:bibliography/ Piano/2004 The Acoustics of the Piano_2004BurredAcoustics.pdf:PDF},
groups      = {Piano},
langid      = {english},
year        = {2004},
}
@Report{1992ChaigneNumerical,
author      = {Chaigne, A. and Askenfelt, A.},
date        = {1992},
institution = {KTH},
title       = {Numerical simulations of piano strings},
type        = {resreport},
pages       = {24},
abstract    = {Thefirst attempt to generate musical sounds by solving the equations of vibrating strings by means of Finite Difference Methods (FDM)was made by Hiller /
& Ruiz (].AudioEng.Soc. 19, pp. 462-472, 19711. It is shown here how their numerical approach and the underlying physical model can be improved in order to simulate the motion of the piano string with a high d e g e e of realism. Starting from the fundamental equations of a damped, stlff string interacting with a nonlinear hammer, a numericalfinite difference scheme is derived,from which the time and spatial dependence of string displacement, velocity, and interactingforce between hammer and string, as well as theforce acting on the bridge, are computed in the time-domain. The strength of the model is illustrated by comparisons between measured and simulated piano tones. After this verification of the accuracy of the method, the model is used as a tool for systematically exploring the influence of string stiffness, relative strikin, position, and hammer-string mass ratio on string waveforms and spectra.},
file        = {:bibliography/ Piano/1992 Numerical Simulations of Piano Strings_1992ChaigneNumerical.pdf:PDF},
groups      = {Piano},
langid      = {english},
year        = {1992},
}
@Article{1964FletcherNormal,
author       = {Fletcher, Harvey},
date         = {1964-01},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Normal Vibration Frequencies of a Stiff Piano String},
doi          = {10.1121/1.1918933},
issn         = {0001-4966},
number       = {1},
pages        = {203--209},
urldate      = {2019-03-28},
volume       = {36},
file         = {:bibliography/ Piano/1964 Normal Vibration Frequencies of a Stiff Piano String_1964FletcherNormal.pdf:PDF},
groups       = {Piano},
journal      = {The Journal Of The Acoustical Society Of America},
langid       = {english},
month        = jan,
publisher    = {Acoustical Society of America (ASA)},
year         = {1964},
}
@Article{1961MartinSubjective,
author       = {Martin, Daniel W. and Ward, W. D.},
date         = {1961},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Subjective evaluation of musical scale temperament in pianos},
number       = {5},
pages        = {582--585},
volume       = {33},
file         = {:bibliography/ Piano/1961 Subjective Evaluation of Musical Scale Temperament in Pianos_1961MartinSubjective.pdf:PDF},
groups       = {Piano},
publisher    = {ASA},
}
@Article{2016ChabassierTime,
author       = {Chabassier, Juliette and Duruflé, Marc and Joly, Patrick},
date         = {2016-01},
journaltitle = {ESAIM: Mathematical Modelling and Numerical Analysis},
title        = {Time domain simulation of a piano. Part 2: numerical aspects},
doi          = {10.1051/m2an/2015007},
issn         = {0764-583X},
number       = {1},
pages        = {93--133},
urldate      = {2019-03-26},
volume       = {50},
abstract     = {This article is the second of a series of two papers devoted to the numerical simulation of the piano. It concerns the numerical aspects of the work, the implementation of a piano code and the presentation of corresponding simulations. The main diﬃculty is time discretisation and stability is achieved via energy methods. Numerical illustrations are provided for a realistic piano and compared to experimental recordings.},
file         = {:bibliography/ Piano/2016 Time Domain Simulation of a Piano. Part 2_ Numerical Aspects_2016ChabassierTime.pdf:PDF},
groups       = {Piano},
langid       = {english},
shorttitle   = {Time domain simulation of a piano. Part 2},
}
@Article{2014ChabassierTime,
author       = {Chabassier, J. and Chaigne, A. and Joly, P.},
date         = {2014-09},
journaltitle = {ESAIM: Mathematical Modelling and Numerical Analysis},
title        = {Time domain simulation of a piano. Part 1: model description},
doi          = {10.1051/m2an/2013136},
issn         = {0764-583X},
number       = {5},
pages        = {1241--1278},
urldate      = {2019-03-26},
volume       = {48},
abstract     = {The purpose of this study is the time domain modeling of a piano. We aim at explaining the vibratory and acoustical behavior of the piano, by taking into account the main elements that contribute to sound production. The soundboard is modeled as a bidimensional thick, orthotropic, heterogeneous, frequency dependant damped plate, using Reissner Mindlin equations. The vibroacoustics equations allow the soundboard to radiate into the surrounding air, in which we wish to compute the complete acoustical ﬁeld around the perfectly rigid rim. The soundboard is also coupled to the strings at the bridge, where they form a slight angle from the horizontal plane. Each string is modeled by a one dimensional damped system of equations, taking into account not only the transversal waves excited by the hammer, but also the stiﬀness thanks to shear waves, as well as the longitudinal waves arising from geometric nonlinearities. The hammer is given an initial velocity that projects it towards a choir of strings, before being repelled. The interacting force is a nonlinear function of the hammer compression. The ﬁnal piano model is a coupled system of partial diﬀerential equations, each of them exhibiting speciﬁc diﬃculties (nonlinear nature of the string system of equations, frequency dependant damping of the soundboard, great number of unknowns required for the acoustic propagation), in addition to couplings’ inherent diﬃculties.},
file         = {:bibliography/ Piano/2014 Time Domain Simulation of a Piano. Part 1_ Model Description_2014ChabassierTime.pdf:PDF},
groups       = {Piano},
langid       = {english},
shorttitle   = {Time domain simulation of a piano. Part 1},
}
@Article{2009BrynerStiff,
author       = {Bryner, John C.},
date         = {2009-12},
journaltitle = {Physics Today},
title        = {Stiff-string theory: Richard Feynman on piano tuning},
doi          = {10.1063/1.3273016},
issn         = {0031-9228},
number       = {12},
pages        = {46--49},
urldate      = {2019-03-26},
volume       = {62},
file         = {:bibliography/ Piano/2009 Stiff String Theory_ Richard Feynman on Piano Tuning_2009BrynerStiff.pdf:PDF},
groups       = {Piano},
langid       = {english},
shorttitle   = {Stiff-string theory},
}
@Unpublished{2009TaoFourier,
author = {Tao, Terence},
date   = {2009},
title  = {Fourier Transform},
url    = {https://www.math.ucla.edu//textasciitilde tao/preprints/fourier.pdf},
file   = {:bibliography/ Harmonic Analysis/2009 Fourier Transform_2009TaoFourier.pdf:PDF},
groups = {Harmonic Analysis},
}
@Misc{2004FesslerDiscrete,
author = {Fessler, Jeffrey},
date   = {2004},
title  = {The Discrete Fourier Transform},
note   = {chapter 5},
url    = {https://web.eecs.umich.edu//textasciitilde fessler/course/451/l/pdf/c5.pdf},
file   = {:bibliography/ Harmonic Analysis/2004 The Discrete Fourier Transform_2004FesslerDiscrete.pdf:PDF},
groups = {Harmonic Analysis},
}
@Unpublished{2009MorinFourier,
author = {Morin, David},
date   = {2009},
title  = {Fourier analysis.pdf},
note   = {Chapter 3},
url    = {http://www.people.fas.harvard.edu//textasciitilde djmorin/waves/Fourier.pdf},
file   = {:bibliography/ Harmonic Analysis/2009 Fourier Analysis.pdf_2009MorinFourier.pdf:PDF},
groups = {Harmonic Analysis},
year   = {2009},
}
@Article{2015CourtneyMore,
author      = {Courtney, Elya and Courtney, Michael},
date        = {2015-07-06},
title       = {A More Accurate Fourier Transform},
eprint      = {1507.01832},
eprintclass = {physics},
eprinttype  = {arxiv},
abstract    = {Fourier transform methods are used to analyze functions and data sets to provide frequencies, amplitudes, and phases of underlying oscillatory components. Fast Fourier transform (FFT) methods oﬀer speed advantages over evaluation of explicit integrals (EI) that deﬁne Fourier transforms. This paper compares frequency, amplitude, and phase accuracy of the two methods for well resolved peaks over a wide array of data sets including cosine series with and without random noise and a variety of physical data sets, including atmospheric CO2 concentrations, tides, temperatures, sound waveforms, and atomic spectra. The FFT uses MIT’s FFTW3 library. The EI method uses the rectangle method to compute the areas under the curve via complex math. Results support the hypothesis that EI methods are more accurate than FFT methods. Errors range from 5 to 10 times higher when determining peak frequency by FFT, 1.4 to 60 times higher for peak amplitude, and 6 to 10 times higher for phase under a peak. The ability to compute more accurate Fourier transforms has promise for improved data analysis in many ﬁelds, including more sensitive assessment of hypotheses in the environmental sciences related to CO2 concentrations and temperature. Other methods are available to address diﬀerent weaknesses in FFTs; however, the EI method always produces the most accurate output possible for a given data set. On the 2011 Lenovo ThinkPad used in this study, an EI transform on a 10,000 point data set took 31 seconds to complete. Source code (C) and Windows executable for the EI method are available at https://sourceforge.net/projects/amoreaccuratefouriertransform/.},
file        = {:bibliography/ Harmonic Analysis/2015 A More Accurate Fourier Transform_2015CourtneyMore.pdf:PDF},
groups      = {Harmonic Analysis},
langid      = {english},
}
@Article{1987SorensenReal,
author       = {Sorensen, H. and Jones, D. and Heideman, M. and Burrus, C.},
date         = {1987-06},
journaltitle = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
title        = {Real-valued fast Fourier transform algorithms},
doi          = {10.1109/TASSP.1987.1165220},
issn         = {0096-3518},
number       = {6},
pages        = {849--863},
volume       = {35},
file         = {:bibliography/ Harmonic Analysis/1987 Real Valued Fast Fourier Transform Algorithms_1987SorensenReal.pdf:PDF},
groups       = {Harmonic Analysis},
keywords     = {Signal processing algorithms, Algorithm design and analysis, Application software, Convolutional codes, Digital images, Discrete Fourier transforms, Discrete transforms, Fast Fourier transforms, NASA},
}
@InProceedings{2015LiaoAnalytical,
author     = {Liao, Jan-Ray},
booktitle  = {2015 15th International Symposium on Communications and Information Technologies (ISCIT)},
date       = {2015-10},
title      = {Analytical solution of DFT interpolated frequency estimator for Hanning windowed signal},
doi        = {10.1109/ISCIT.2015.7458336},
eventtitle = {2015 15th International Symposium on Communications and Information Technologies (ISCIT)},
isbn       = {978-1-4673-6820-9},
location   = {Nara, Japan},
pages      = {177--180},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/7458336/},
urldate    = {2019-03-26},
abstract   = {Frequency estimation from discrete Fourier transform (DFT) coefﬁcients of a rectangular windowed signal under the inﬂuence of additive white noise is a well studied problem in signal processing. In its simplest form, the process involves ﬁnding the spectral peaks. When higher frequency resolution is required, a frequency offset can be found from the interpolation of DFT coefﬁcients. However, most of the past researches focus on monotonic cisoid signals. In practical situations where multiple harmonics are present, the sidelobes from other harmonics interfere with the estimation of the harmonic being considered. In this case, windows with smaller sidelobes such as Hanning window are preferred over rectangular window. Given the increased mathematical complexity of Hanning window, analytical solution has not yet been available for DFT interpolation. In this paper, we derive an exact analytical solution of the estimated frequency from DFT interpolation of Hanning windowed signal. In experiments, we show that the new analytical solution is accurate for monotonic cisoid signal and can considerably reduce the effect of interharmonic interference as compared to previous rectangular windowed methods.},
file       = {:bibliography/ Harmonic Analysis/2015 Analytical Solution of DFT Interpolated Frequency Estimator for Hanning Windowed Signal_2015LiaoAnalytical.pdf:PDF},
groups     = {Harmonic Analysis},
langid     = {english},
}
@Unpublished{2012MorseFourier,
author = {Morse, Bryan},
date   = {2012},
title  = {The Fourier Transform: Examples, Properties, Common Pairs},
file   = {:bibliography/ Harmonic Analysis/2012 The Fourier Transform_ Examples, Properties, Common Pairs_2012MorseFourier.pdf:PDF},
groups = {Harmonic Analysis},
langid = {english},
pages  = {4},
}
@Collection{1979CarmonaNon,
date       = {1979},
editor     = {Carmona, Jacques and Vergne, Michèle},
title      = {Non-commutative harmonic analysis: proceedings, Marseille-Luminy, France, June 26 to 30, 1978: actes du Colloque d'analyse harmonique non commutative},
isbn       = {978-0-387-09516-5},
location   = {Berlin ; New York},
pagetotal  = {244},
publisher  = {Springer-Verlag},
series     = {Lecture notes in mathematics ; 728},
file       = {:bibliography/ Harmonic Analysis/1979 Non Commutative Harmonic Analysis_ Proceedings, Marseille Luminy, France, June 26 to 30, 1978_ Actes Du Colloque D'analyse Harmonique Non Commutative_1979CarmonaNon.pdf:PDF},
groups     = {Harmonic Analysis},
keywords   = {Congresses, Harmonic analysis, Lie algebras, Lie groups},
shorttitle = {Non-commutative harmonic analysis},
}
@Collection{2010SzoekefalviNagyHarmonic,
date      = {2010},
editor    = {Szőkefalvi-Nagy, Béla},
title     = {Harmonic analysis of operators on Hilbert space},
edition   = {{2}},
isbn      = {978-1-4419-6093-1},
location  = {New York, NY},
note      = {OCLC: 845818617},
pagetotal = {474},
publisher = {Springer},
series    = {Universitext},
file      = {:bibliography/ Harmonic Analysis/2010 Harmonic Analysis of Operators on Hilbert Space_2010SzoekefalviNagyHarmonic.pdf:PDF},
groups    = {Harmonic Analysis},
langid    = {english},
}
@Unpublished{2007AnandBrief,
author   = {Anand, Aashirwad Viswanathan},
date     = {2007},
title    = {A Brief Study Of Discrete And Fast Fourier Transforms},
abstract = {This paper studies the mathematical machinery underlying the Discrete and Fast Fourier Transforms, algorithmic processes widely used in quantum mechanics, signal analysis, options pricing, and other diverse ﬁelds. Beginning with the basic properties of Fourier Transform, we proceed to study the derivation of the Discrete Fourier Transform, as well as computational considerations that necessitate the development of a faster way to calculate the DFT. With these considerations in mind, we study the construction of the Fast Fourier Transform, as proposed by Cooley and Tukey [7].},
file     = {:bibliography/ Harmonic Analysis/2007 A Brief Study of Discrete and Fast Fourier Transforms_2007AnandBrief.pdf:PDF},
groups   = {Harmonic Analysis},
langid   = {english},
pages    = {11},
year     = {2007},
}
@Unpublished{2003KornerFirst,
author = {Korner, T. W.},
date   = {2003},
title  = {A First Look at Fourier Analysis},
file   = {:bibliography/ Harmonic Analysis/2003 A First Look at Fourier Analysis_2003KornerFirst.pdf:PDF},
groups = {Harmonic Analysis},
langid = {english},
pages  = {60},
}
@Misc{2017BattyDiscrete,
author = {Batty, C.},
date   = {2017},
title  = {Discrete Fourier Transform Derivation},
file   = {:bibliography/ Harmonic Analysis/2017 Discrete Fourier Transform Derivation_2017BattyDiscrete.pdf:PDF},
groups = {Harmonic Analysis},
}
@Unpublished{2017DeSerioAddendum,
author = {DeSerio, Robert},
date   = {2017},
title  = {Addendum : The Fourier transform of decaying oscillations},
file   = {:bibliography/ Harmonic Analysis/2017 Addendum _ the Fourier Transform of Decaying Oscillations_2017DeSerioAddendum.pdf:PDF},
groups = {Harmonic Analysis},
langid = {english},
number = {6},
pages  = {12},
}
@Article{2014SerraSpectral,
author = {Serra, Xavier},
date   = {2014},
title  = {Spectral Modeling Synthesis: Past and Present},
number = {November},
pages  = {26},
file   = {:bibliography/ Harmonic Analysis/2014 Spectral Modeling Synthesis_ Past and Present_2014SerraSpectral.pdf:PDF},
groups = {Harmonic Analysis},
langid = {english},
}
@Misc{2017OliehoekGANGs,
author       = {Oliehoek, Frans A. and Savani, Rahul and Gallego-Posada, Jose and van der Pol, Elise and de Jong, Edwin D. and Gross, Roderich},
date         = {2017-12-02},
title        = {GANGs: Generative Adversarial Network Games},
eprint       = {1712.00679},
eprinttype   = {arXiv},
abstract     = {Generative Adversarial Networks (GAN) have become one of the most successful frameworks for unsupervised generative modeling. As GANs are difficult to train much research has focused on this. However, very little of this research has directly exploited gametheoretic techniques. We introduce Generative Adversarial Network Games (GANGs), which explicitly model a finite zero-sum game between a generator (G) and classifier (C) that use mixed strategies. The size of these games precludes exact solution methods, therefore we define resource-bounded best responses (RBBRs), and a resourcebounded Nash Equilibrium (RB-NE) as a pair of mixed strategies such that neither G or C can find a better RBBR. The RB-NE solution concept is richer than the notion of ‘local Nash equilibria’ in that it captures not only failures of escaping local optima of gradient descent, but applies to any approximate best response computations, including methods with random restarts. To validate our approach, we solve GANGs with the Parallel Nash Memory algorithm, which provably monotonically converges to an RB-NE. We compare our results to standard GAN setups, and demonstrate that our method deals well with typical GAN problems such as mode collapse, partial mode coverage and forgetting.},
file         = {:bibliography/ Game Theory/2017 GANGs_ Generative Adversarial Network Games_2017OliehoekGANGs.pdf:PDF},
groups       = {Game Theory},
journaltitle = {arXiv},
langid       = {english},
shorttitle   = {GANGs},
}
@InCollection{2017AnStackelberg,
author    = {An, Bo and Tambe, Milind},
booktitle = {Improving Homeland Security Decisions},
date      = {2017-11-02},
title     = {Stackelberg Security Games (SSG) Basics and Application Overview},
doi       = {10.1017/9781316676714.021},
edition   = {{1}},
editor    = {Abbas, Ali E. and Tambe, Milind and von Winterfeldt, Detlof},
isbn      = {978-1-316-67671-4},
pages     = {485--507},
publisher = {Cambridge University Press},
url       = {https://www.cambridge.org/core/product/identifier/9781316676714///////\\%23CN-bp-21/type/book///////\\_part},
urldate   = {2019-03-28},
abstract  = {Security is a critical concern around the world, whether it is the challenge of protecting ports, airports and other critical infrastructure, interdicting the illegal ﬂow of drugs, weapons and money, protecting endangered species, forests and ﬁsheries, suppressing urban crime or security in cyberspace. Unfortunately, limited security resources prevent full security coverage at all times; instead, we must optimize the use of limited security resources. To that end, we founded the "security games" framework to build decision-aids for security agencies. Security games is a novel area of research that is based on computational and behavioral game theory, while also incorporating elements of AI planning under uncertainty and machine learning. We have deployed securitygames based decision aids for infrastructure security such as at the ports and ferry trafﬁc with the US coast guard (in the ports of New York, Boston, Los Angeles/Long Beach, Houston and others), for security of airports and air trafﬁc with the US Federal Air Marshals and the Los Angeles World Airport (LAX) police, and tested this framework for security of metro trains with the Los Angeles Sheriff’s Department. Moreover, recent work on "green security games" has led to testing our decision aids for protection of ﬁsheries with the US Coast Guard and protection of wildlife at sites in multiple countries, and opportunistic crime security games have focused on suppressing urban crime. This chapter will discuss applications of security games, and outline research challenges in security games including algorithms for scaling up security games as well as for handling signiﬁcant adversarial uncertainty and learning models of human adversary behaviors.},
file      = {:bibliography/ Game Theory/2017 Stackelberg Security Games (SSG) Basics and Application Overview_2017AnStackelberg.pdf:PDF},
groups    = {Game Theory},
langid    = {english},
}
@Collection{2016RotheEconomics,
date       = {2016},
editor     = {Rothe, Jörg},
title      = {Economics and computation: an introduction to algorithmic game theory, computational social choice, and fair division},
isbn       = {978-3-662-47903-2},
location   = {Berlin},
note       = {OCLC: 923541416},
pagetotal  = {612},
publisher  = {Springer},
series     = {Springer texts in business and economics},
file       = {:bibliography/ Game Theory/2016 Economics and Computation_ an Introduction to Algorithmic Game Theory, Computational Social Choice, and Fair Division_2016RotheEconomics.pdf:PDF},
groups     = {Game Theory},
langid     = {english},
shorttitle = {Economics and computation},
}
@Article{2016PerolatLearning,
author      = {Pérolat, Julien and Strub, Florian and Piot, Bilal and Pietquin, Olivier},
date        = {2016-06-28},
title       = {Learning Nash Equilibrium for General-Sum Markov Games from Batch Data},
eprint      = {1606.08718},
eprintclass = {cs},
eprinttype  = {arxiv},
abstract    = {This paper addresses the problem of learning a Nash equilibrium in γ-discounted multiplayer general-sum Markov Games (MGs) in a batch setting. As the number of players increases in MG, the agents may either collaborate or team apart to increase their ﬁnal rewards. One solution to address this problem is to look for a Nash equilibrium. Although, several techniques were found for the subcase of two-player zero-sum MGs, those techniques fail to ﬁnd a Nash equilibrium in general-sum Markov Games. In this paper, we introduce a new deﬁnition of ǫ-Nash equilibrium in MGs which grasps the strategy’s quality for multiplayer games. We prove that minimizing the norm of two Bellmanlike residuals implies to learn such an ǫ-Nash equilibrium. Then, we show that minimizing an empirical estimate of the Lp norm of these Bellman-like residuals allows learning for general-sum games within the batch setting. Finally, we introduce a neural network architecture that successfully learns a Nash equilibrium in generic multiplayer generalsum turn-based MGs.},
file        = {:bibliography/ Game Theory/2016 Learning Nash Equilibrium for General Sum Markov Games from Batch Data_2016PerolatLearning.pdf:PDF},
groups      = {Game Theory},
langid      = {english},
}
@InCollection{2016KrawczykMultistage,
author    = {Krawczyk, Jacek B. and Petkov, Vladimir},
booktitle = {Handbook of Dynamic Game Theory},
date      = {2016},
title     = {Multistage Games},
doi       = {10.1007/978-3-319-27335-8///////\\_3-1},
editor    = {Basar, Tamer and Zaccour, Georges},
isbn      = {978-3-319-27335-8},
location  = {Cham},
pages     = {1--57},
publisher = {Springer International Publishing},
urldate   = {2019-03-28},
abstract  = {In this chapter, we build on the concept of a repeated game discussed in Chap. Repeated Games and introduce the notion of a multistage game. In both types of games, several antagonistic agents interact with each other over time. The difference is that, in a multistage game, there is a dynamic system whose state keeps changing: the controls chosen by the agents in the current period affect the system’s future. In contrast with repeated games, the agents’ payoffs in multistage games depend directly on the state of this system. Examples of such settings range from a microeconomic dynamic model of a ﬁsh biomass exploited by several agents to a macroeconomic interaction between the government and the business sector. In some multistage games, physically different decisionmakers engage in simultaneous-move competition. In others, agents execute their actions sequentially rather than simultaneously. We also study hierarchical games, where a leader moves ahead of a follower. This chapter concludes with an example of memory-based strategies that can support Pareto-efﬁcient outcomes.},
file      = {:bibliography/ Game Theory/2016 Multistage Games_2016KrawczykMultistage.pdf:PDF},
groups    = {Game Theory},
langid    = {english},
}
@InProceedings{2015LiuComplex,
author     = {Liu, Xingfeng and Zhou, Tiansong and Zheng, Zhongxia},
booktitle  = {Proceedings of the International Conference on Logistics, Engineering, Management and Computer Science},
date       = {2015},
title      = {Complex Information Game Problem Based on Artificial Neural Network},
doi        = {10.2991/lemcs-15.2015.34},
eventtitle = {International Conference on Logistics Engineering, Management and Computer Science (LEMCS 2015)},
isbn       = {978-94-6252-102-5},
location   = {Shenyang, China},
publisher  = {Atlantis Press},
url        = {http://www.atlantis-press.com/php/paper-details.php?id=25838084},
urldate    = {2019-03-28},
abstract   = {The assumption of game theory is that the players in game must be rational. In the game of incomplete information, participants are not completely clear about the game. Therefore, usually there is a probability distribution of strategy selection in game. It is very complicated to know the real game information of the social and economic problems. In fact, the actual situation for many problems is that game players are irrational, or the probability distribution of game players’ strategies cannot be gotten, even the strategy sets are not complete (infinite strategy sets).There are many limitations in application of the traditional game theory. In this paper, the concept of complex information game and its Nash equilibrium are presented. It is proved that the complex information game problem can be solved by artificial neural network. An example on how to solve the complex information game problem with artificial neural network is given as well. Researchers hope that more and more scholars can use artificial intelligence theory to analyze the game theory problem. Therefore, the complex information game problems can be dealt more efficiently.},
file       = {:bibliography/ Game Theory/2015 Complex Information Game Problem Based on Artificial Neural Network_2015LiuComplex.pdf:PDF},
groups     = {Game Theory},
langid     = {english},
}
@InProceedings{2014BhatiaRecurrent,
author    = {Bhatia, Sudeep and Golman, Russell},
booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
date      = {2014},
title     = {A Recurrent Neural Network for Game Theoretic Decision Making},
pages     = {6},
volume    = {36},
abstract  = {We describe the properties of a connectionist network that is able to make decisions in strategic games. We use the structure of Bidirectional Associative Memory (BAM), a minimal two-layer recurrent neural network with binary activation functions and binary connection weights. We apply BAM to finite-strategy two-player games, and show that network activation in the long run is restricted to the set of rationalizable strategies. The network is not guaranteed to reach a stable activation state, but any pure strategy profile that constitutes a stable state in the network must be a pure strategy Nash equilibrium.},
file      = {:bibliography/ Game Theory/2014 A Recurrent Neural Network for Game Theoretic Decision Making_2014BhatiaRecurrent.pdf:PDF},
groups    = {Game Theory},
langid    = {english},
}
@InCollection{2012NoweGame,
author    = {Nowé, Ann and Vrancx, Peter and De Hauwere, Yann-Michaël},
booktitle = {Reinforcement Learning},
date      = {2012},
title     = {Game Theory and Multi-agent Reinforcement Learning},
doi       = {10.1007/978-3-642-27645-3///////\\_14},
editor    = {Wiering, Marco and van Otterlo, Martijn},
isbn      = {978-3-642-27644-6},
location  = {Berlin, Heidelberg},
pages     = {441--470},
publisher = {Springer Berlin Heidelberg},
urldate   = {2019-04-16},
volume    = {12},
file      = {:bibliography/ Game Theory/2012 Game Theory and Multi Agent Reinforcement Learning_2012NoweGame.pdf:PDF},
groups    = {Game Theory},
}
@Article{2012FriedrichSpike,
author       = {Friedrich, Johannes and Senn, Walter},
date         = {2012-09-27},
journaltitle = {PLoS Computational Biology},
title        = {Spike-based Decision Learning of Nash Equilibria in Two-Player Games},
doi          = {10.1371/journal.pcbi.1002691},
editor       = {Sporns, Olaf},
issn         = {1553-7358},
number       = {9},
pages        = {e1002691},
urldate      = {2019-03-28},
volume       = {8},
abstract     = {Humans and animals face decision tasks in an uncertain multi-agent environment where an agent’s strategy may change in time due to the co-adaptation of others strategies. The neuronal substrate and the computational algorithms underlying such adaptive decision making, however, is largely unknown. We propose a population coding model of spiking neurons with a policy gradient procedure that successfully acquires optimal strategies for classical game-theoretical tasks. The suggested population reinforcement learning reproduces data from human behavioral experiments for the blackjack and the inspector game. It performs optimally according to a pure (deterministic) and mixed (stochastic) Nash equilibrium, respectively. In contrast, temporal-difference(TD)-learning, covariance-learning, and basic reinforcement learning fail to perform optimally for the stochastic strategy. Spike-based population reinforcement learning, shown to follow the stochastic reward gradient, is therefore a viable candidate to explain automated decision learning of a Nash equilibrium in two-player games.},
file         = {:bibliography/ Game Theory/2012 Spike Based Decision Learning of Nash Equilibria in Two Player Games_2012FriedrichSpike.pdf:PDF},
groups       = {Game Theory},
langid       = {english},
}
@Article{2010SchusterApplication,
author       = {Schuster, Alfons and Yamaguchi, Yoko},
date         = {2010},
journaltitle = {Advances in Artificial Intelligence},
title        = {Application of Game Theory to Neuronal Networks},
doi          = {10.1155/2010/521606},
issn         = {1687-7470},
pages        = {1--12},
urldate      = {2019-03-28},
volume       = {2010},
file         = {:bibliography/ Game Theory/2010 Application of Game Theory to Neuronal Networks_2010SchusterApplication.pdf:PDF},
groups       = {Game Theory},
langid       = {english},
}
@Article{2010ElkindAlgorithmic,
author       = {Elkind, Edith and Leyton-Brown, Kevin},
date         = {2010-09-20},
journaltitle = {AI Magazine},
title        = {Algorithmic Game Theory and Artificial Intelligence},
doi          = {10.1609/aimag.v31i4.2310},
issn         = {0738-4602},
number       = {4},
pages        = {9},
url          = {https://aaai.org/ojs/index.php/aimagazine/article/view/2310},
urldate      = {2019-03-26},
volume       = {31},
abstract     = {We brieﬂy survey the rise of game theory as a topic of study in artiﬁcial intelligence, and explain the term algorithmic game theory. We then describe three broad areas of current inquiry by AI researchers in algorithmic game theory: game playing, social choice, and mechanism design. Finally, we give short summaries of each of the six articles appearing in this issue.},
file         = {:bibliography/ Game Theory/2010 Algorithmic Game Theory and Artificial Intelligence_2010ElkindAlgorithmic.pdf:PDF},
groups       = {Game Theory},
langid       = {english},
}
@Article{2009SgroiLearning,
author       = {Sgroi, Daniel and Zizzo, Daniel John},
date         = {2009-01},
journaltitle = {Journal of Economic Behavior & Organization},
title        = {Learning to play games: Neural networks as bounded-rational players},
doi          = {10.1016/j.jebo.2008.09.008},
issn         = {0167-2681},
number       = {1},
pages        = {27--38},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0167268108001959},
urldate      = {2019-03-28},
volume       = {69},
abstract     = {We present a neural network methodology for learning game-playing rules in general. Existing research suggests learning to …nd a Nash equilibrium in a new game is too di¢ cult a task for a neural network, but says little about what it will do instead. We observe that a neural network trained to …nd Nash equilibria in a known subset of games, will use self-taught rules developed endogenously when facing new games. These rules are close to payo¤ dominance and its best response. Our …ndings are consistent with existing experimental results, both in terms of subject’s methodology and success rates.},
file         = {:bibliography/ Game Theory/2009 Learning to Play Games_ Neural Networks As Bounded Rational Players_2009SgroiLearning.pdf:PDF},
groups       = {Game Theory},
langid       = {english},
shorttitle   = {Learning to play games},
}
@Collection{2007NisanAlgorithmic,
date      = {2007},
editor    = {Nisan, Noam},
title     = {Algorithmic game theory},
isbn      = {978-0-521-87282-9},
location  = {Cambridge ; New York},
note      = {OCLC: ocn122526907},
pagetotal = {754},
publisher = {Cambridge University Press},
file      = {:bibliography/ Game Theory/2007 Algorithmic Game Theory_2007NisanAlgorithmic.pdf:PDF},
groups    = {Game Theory},
keywords  = {Algorithms, Game theory},
langid    = {english},
}
@Misc{2006BlumMachine,
author = {Blum, A. and Blum, M. and Kearns, M. and Sandholm, T. and Hajiaghayi, M. T.},
date   = {2006},
title  = {Machine Learning, Game Theory, and Mechanism Design for a Networked World},
file   = {:bibliography/ Game Theory/2006 Machine Learning, Game Theory, and Mechanism Design for a Networked World_2006BlumMachine.pdf:PDF},
groups = {Game Theory},
langid = {english},
pages  = {18},
}
@Article{2005DarmonConvergence,
author       = {Darmon, Eric and Waldeck, Roger},
date         = {2005-09},
journaltitle = {Physica A: Statistical Mechanics and its Applications},
title        = {Convergence of reinforcement learning to Nash equilibrium: A search-market experiment},
doi          = {10.1016/j.physa.2005.02.074},
issn         = {0378-4371},
number       = {1},
pages        = {119--130},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0378437105002839},
urldate      = {2019-03-28},
volume       = {355},
abstract     = {Since the introduction of Reinforcement Learning (RL) in Game Theory, a growing literature is concerned with the theoretical convergence of RL-driven outcomes towards Nash equilibrium. In this paper, we apply this issue to a search-theoretic framework (posted-price market) where sellers are confronted with a population of imperfectly informed buyers and take one decision per period (posted prices) with no direct interactions between sellers. We focus on three different scenarios with varying buyers’ characteristics. For each of these scenarios, we quantitatively and qualitatively test whether the learned variable (price strategy) converges to the Nash equilibrium. We also study the impact of the temperature parameter (deﬁning the exploitation/exploration trade off) on these results.},
file         = {:bibliography/ Game Theory/2005 Convergence of Reinforcement Learning to Nash Equilibrium_ a Search Market Experiment_2005DarmonConvergence.pdf:PDF},
groups       = {Game Theory},
langid       = {english},
shorttitle   = {Convergence of reinforcement learning to Nash equilibrium},
}
@Unpublished{2005AgrawalWhen,
author   = {Agrawal, Anurag and Jaiswal, Deepak},
date     = {2005},
title    = {When Machine Learning Meets AI and Game Theory},
abstract = {We study the problem of development of intelligent machine learning applications to exploit the problems of adaptation that arise in multi-agent systems, for expected-long-termproﬁt maximization. We present two results. First, we propose a learning algorithm for the Iterated Prisoners Dilemma (IPD) problem. Using numerical analysis we show that it performs strictly better than the tit-for-tat algorithm and many other adaptive and non-adaptive strategies. Second, we study the same problem from the aspect of zero-sum games. We discuss how AI and Machine Learning techniques work closely to give our agent a ’mind-reading’ capability.},
file     = {:bibliography/ Game Theory/2005 When Machine Learning Meets AI and Game Theory_2005AgrawalWhen.pdf:PDF},
groups   = {Game Theory},
langid   = {english},
pages    = {5},
}
@InProceedings{2003WangReinforcement,
author    = {Wang, Xiaofeng and Sandholm, Tuomas},
booktitle = {Advances in neural information processing systems},
date      = {2003},
title     = {Reinforcement learning to play an optimal Nash equilibrium in team Markov games},
pages     = {1603--1610},
abstract  = {Multiagent learning is a key problem in AI. In the presence of multiple Nash equilibria, even agents with non-conﬂicting interests may not be able to learn an optimal coordination policy. The problem is exaccerbated if the agents do not know the game and independently receive noisy payoffs. So, multiagent reinforfcement learning involves two interrelated problems: identifying the game and learning to play. In this paper, we present optimal adaptive learning, the ﬁrst algorithm that converges to an optimal Nash equilibrium with probability 1 in any team Markov game. We provide a convergence proof, and show that the algorithm’s parameters are easy to set to meet the convergence conditions.},
file      = {:bibliography/ Game Theory/2003 Reinforcement Learning to Play an Optimal Nash Equilibrium in Team Markov Games_2003WangReinforcement.pdf:PDF},
groups    = {Game Theory},
}
@InCollection{2002TennenholtzGame,
author      = {Tennenholtz, Moshe},
booktitle   = {Foundations and Applications of Multi-Agent Systems},
date        = {2002},
title       = {Game Theory and Artificial Intelligence},
doi         = {10.1007/3-540-45634-1///////\\_4},
editor      = {d’Inverno, Mark and Luck, Michael and Fisher, Michael and Preist, Chris},
editorb     = {Goos, G. and Hartmanis, J. and van Leeuwen, J.},
isbn        = {978-3-540-43962-2},
location    = {Berlin, Heidelberg},
pages       = {49--58},
publisher   = {Springer Berlin Heidelberg},
urldate     = {2019-03-26},
volume      = {2403},
abstract    = {Game Theory and Artiﬁcial Intelligence are two mature areas of research, originating from similar roots, which have taken diﬀerent research directions in the last 50 years. Recent research however shows that the connections between these areas are deep, and that the time had come for bridging the gap between these research disciplines. In this paper we concentrate on basic issues in representation, reasoning, and learning, and discuss work that lies in the intersection of Artiﬁcial Intelligence and Game Theory, for each of these subjects.},
editorbtype = {redactor},
file        = {:bibliography/ Game Theory/2002 Game Theory and Artificial Intelligence_2002TennenholtzGame.pdf:PDF},
groups      = {Game Theory},
langid      = {english},
}
@InProceedings{2014GoodfellowGenerative,
author        = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle     = {Procedings},
date          = {2014},
title         = {Generative Adversarial Nets},
doi           = {10.1001/jamainternmed.2016.8245},
eprint        = {arXiv:1406.2661v1},
eprinttype    = {arXiv},
pages         = {9},
url           = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
abstract      = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
arxivid       = {arXiv:1406.2661v1},
file          = {:done/2014GoodfellowGenerative Generative Adversarial Nets.pdf:application/pdf},
groups        = {Seminal},
issn          = {1049-5258},
journaltitle  = {Advances in Neural Information Processing Systems},
keywords      = {seminal},
langid        = {english},
mendeley-tags = {seminal},
}
@Thesis{2011EroecalAlgebraic,
author      = {Eröcal, Burçin},
date        = {2011},
institution = {Johannes Kepler University Linz},
title       = {Algebraic Extensions for Symbolic Summation},
type        = {phdthesis},
abstract    = {The main result of this thesis is an eﬀective method to extend Karr’s symbolic summation framework to algebraic extensions. These arise, for example, when working with expressions involving (−1)n. An implementation of this method, including a modernised version of Karr’s algorithm is also presented. Karr’s algorithm is the summation analogue of the Risch algorithm for indeﬁnite integration. In the summation case, towers of specialized diﬀerence ﬁelds called ΠΣ-ﬁelds are used to model nested sums and products. This is similar to the way elementary functions involving nested logarithms and exponentials are represented in diﬀerential ﬁelds in the integration case. In contrast to the integration framework, only transcendental extensions are allowed in Karr’s construction. Algebraic extensions of ΠΣ-ﬁelds can even be rings with zero divisors. Karr’s methods rely heavily on the ability to solve ﬁrst-order linear diﬀerence equations and they are no longer applicable over these rings. Based on Bronstein’s formulation of a method used by Singer for the solution of diﬀerential equations over algebraic extensions, we transform a ﬁrst-order linear equation over an algebraic extension to a system of ﬁrst-order equations over a purely transcendental extension ﬁeld. However, this domain is not necessarily a ΠΣ-ﬁeld. Using a structure theorem by Singer and van der Put, we reduce this system to a single ﬁrst-order equation over a ΠΣ-ﬁeld, which can be solved by Karr’s algorithm. We also describe how to construct towers of diﬀerence ring extensions on an algebraic extension, where the same reduction methods can be used. A common bottleneck for symbolic summation algorithms is the computation of nullspaces of matrices over rational function ﬁelds. We present a fast algorithm for matrices over Q(x) which uses fast arithmetic at the hardware level with calls to BLAS subroutines after modular reduction. This part is joint work with Arne Storjohann.},
file        = {:bibliography/ Mathematics/2011 Algebraic Extensions for Symbolic Summation_2011EroecalAlgebraic.pdf:application/pdf},
groups      = {Mathematics},
langid      = {english},
}
@Misc{Kotlin,
title  = {Kotlin Language Documentation},
file   = {:bibliography/ Programming/Kotlin Language Documentation_Kotlin.pdf:application/pdf},
groups = {Programming},
}
@Article{LowtherTeaching,
author   = {Lowther, John and Shene, Ching-Kuang},
title    = {Teaching B-splines Is Not Difficult!},
pages    = {5},
abstract = {This paper describes the authors’ approach of introducing important concepts and algorithms of B-splines to junior computer science students with the help of a pedagogical tool DesignMentor. This approach is non-mathematical and intuitive, and has been used and reﬁned in the past six years.},
file     = {:bibliography/ Computer Graphics/Teaching B Splines Is Not Difficult!_LowtherTeaching.pdf:application/pdf},
groups   = {Computer Graphics},
langid   = {english},
}
@Unpublished{2008LycheSpline,
author = {Lyche, Tom and Mørken, Knut},
date   = {2008},
title  = {Spline Methods.pdf},
note   = {draft},
doi    = {10/undervisningsmateriale},
file   = {:bibliography/ Computer Graphics/2008 Spline Methods.pdf_2008LycheSpline.pdf:application/pdf},
groups = {Computer Graphics},
year   = {2008},
}
@InCollection{1999PollockSmoothing,
author    = {Pollock, D. S. G.},
booktitle = {Handbook of Time Series Analysis, Signal Processing, and Dynamics},
date      = {1999},
title     = {Smoothing with Cubic Splines},
doi       = {10.1016/B978-012560990-6/50013-0},
isbn      = {978-0-12-560990-6},
pages     = {293--322},
publisher = {Elsevier},
url       = {https://linkinghub.elsevier.com/retrieve/pii/B9780125609906500130},
urldate   = {2019-03-28},
file      = {:bibliography/ Computer Graphics/1999 Smoothing with Cubic Splines_1999PollockSmoothing.pdf:application/pdf},
groups    = {Computer Graphics},
langid    = {english},
}
@Article{1996Loesinusoidal,
author       = {Loe, K. F.},
date         = {1996-07},
journaltitle = {Journal of Computational and Applied Mathematics},
title        = {A sinusoidal polynomial spline and its Bezier blended interpolant},
doi          = {10.1016/0377-0427(95)00241-3},
issn         = {0377-0427},
number       = {2},
pages        = {383--393},
url          = {http://linkinghub.elsevier.com/retrieve/pii/0377042795002413},
urldate      = {2019-03-28},
volume       = {71},
abstract     = {Functional polynomials composed of sinusoidal functions are introduced as basis functions to construct an interpolatory spline. An interpolant constructed in this way does not require solving a system of linear equations as many approaches do. However there are vanishing tangent vectors at the interpolating points. By blending with a Bezier curve using the data points as the control points, the blended curve is a proper smooth interpolant. The blending factor has the effect similar to the "tension" control of tension splines. Piecewise interpolants can be constructed in an analogous way as a connection of Bezier curve segments to achieve C 1 continuity at the connecting points. Smooth interpolating surface patches can also be defined by blending sinusoidal polynomial tensor surfaces and Bezier tensor surfaces. The interpolant can very efficiently be evaluated by tabulating the sinusoidal function.},
file         = {:bibliography/ Computer Graphics/1996 A Sinusoidal Polynomial Spline and Its Bezier Blended Interpolant_1996Loesinusoidal.pdf:application/pdf},
groups       = {Computer Graphics},
langid       = {english},
}
@Article{1986HobbySmooth,
author       = {Hobby, John D.},
date         = {1986},
journaltitle = {Discrete & Computational Geometry},
title        = {Smooth, easy to compute interpolating splines},
doi          = {10.1007/BF02187690},
issn         = {0179-5376},
number       = {2},
pages        = {123--140},
urldate      = {2019-03-31},
volume       = {1},
abstract     = {We present a system of interpolating splines with first and approximate second order geometric continuity. The curves are easily computed in linear time by solving a system of linear equations without the need to resort to any kind of successive approximation scheme. Emphasis is placed on the need to find aesthetically pleasing curves in a wide range of circumstances; favorable results are obtained even when the knots are very unequally spaced or widely separated. The curves are invariant under scaling, rotation, and reflection, and the effects of a local change fall off exponentially as one moves away from the disturbed knot.},
file         = {:bibliography/ Computer Graphics/1986 Smooth, Easy to Compute Interpolating Splines_1986HobbySmooth.pdf:application/pdf},
groups       = {Computer Graphics},
journal      = {Discrete & Computational Geometry},
langid       = {english},
month        = {6},
publisher    = {Springer Nature},
year         = {1986},
}
@PhdThesis{2009LevienSpiral,
author      = {Raph Levien},
date        = {2009},
institution = {Berkeley},
title       = {From Spiral to Spline: Optimal Techniques in Interactive Curve Design},
file        = {:bibliography/ Computer Graphics/From Spiral to Spline_ Optimal Techniques in Interactive Curve Design_LevienSpirala.pdf:PDF},
groups      = {Computer Graphics},
keywords    = {splines, elastica, euler spirals, curves},
}
@Report{1974Smithsmoothing,
author   = {R. E. Smith},
date     = {1974-02-01},
title    = {A smoothing algorithm using cubic spline functions},
url      = {https://ntrs.nasa.gov/search.jsp?R=19740008165},
urldate  = {2019-03-31},
abstract = {Two algorithms are presented for smoothing arbitrary sets of data. They are the explicit variable algorithm and the parametric variable algorithm. The former would be used where large gradients are not encountered because of the smaller amount of calculation required. The latter would be used if the data being smoothed were double valued or experienced large gradients. Both algorithms use a least-squares technique to obtain a cubic spline fit to the data. The advantage of the spline fit is that the first and second derivatives are continuous. This method is best used in an interactive graphics environment so that the junction values for the spline curve can be manipulated to improve the fit.},
file     = {:bibliography/ Computer Graphics/1974 A Smoothing Algorithm Using Cubic Spline Functions_1974Smithsmoothing.pdf:PDF},
groups   = {Computer Graphics, tesse:1},
month    = feb,
year     = {1974},
}
@InProceedings{2009Brunettereview,
author    = {Brunette, E. S. and Flemmer, R. C. and Flemmer, C. L.},
booktitle = {Proceedings of the 4th International Conference on Autonomous Robots and Agents},
date      = {2009-02},
title     = {A review of artificial intelligence},
doi       = {10.1109/icara.2000.4804025},
pages     = {385--392},
publisher = {IEEE},
file      = {:done/2009BrunetteReview A Review of Artificial Intelligence.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {agent-based artificial intelligence, artificial consciousness, artificial intelligence, Artificial intelligence, consciouness, embodied artificial intelligence, embodied intelligence, Geometry, Intelligent agent, Intelligent robots, Intelligent structures, Logic, machine intelligence, Machine intelligence, Natural language processing, Neurons, Strips},
}
@Article{2017Costaevaluation,
author       = {Costa, Yandre M. G. and Oliveira, Luiz S. and Silla, Carlos N.},
date         = {2017-03},
journaltitle = {Applied Soft Computing},
title        = {An evaluation of Convolutional Neural Networks for music classification using spectrograms},
doi          = {10.1016/j.asoc.2016.12.024},
pages        = {28--38},
volume       = {52},
file         = {:done/2017CostaEvaluation An Evaluation of Convolutional Neural Networks for Music Classification Using Spectrograms.pdf:application/pdf},
groups       = {tesse:5},
}
@Book{2014HaganNeural,
author    = {Hagan, Martin T. and Demuth, Howard B. and Beale, Mark H. and Jesús, Orlando De},
date      = {2014},
title     = {Neural Network Design},
edition   = {2},
isbn      = {0-9717321-1-6},
publisher = {Martin Hagan},
url       = {http://hagan.okstate.edu/nnd.html},
groups    = {tesse:4},
}
@Article{1989HornikMultilayer,
author       = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
date         = {1989-01},
journaltitle = {Neural Networks},
title        = {Multilayer feedforward networks are universal approximators},
doi          = {10.1016/0893-6080(89)90020-8},
number       = {5},
pages        = {359--366},
volume       = {2},
file         = {:done/1989HornikMultilayer Multilayer Feedforward Networks Are Universal Approximators.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{2017HutchingsTalkinga,
author    = {Hutchings, P.},
booktitle = {Proceedings of the First International Workshop on Deep Learning and Music joint with IJCNN},
date      = {2017},
title     = {Talking Drums: Generating drum grooves with neural networks.},
pages     = {43--47},
url       = {https://arxiv.org/abs/1706.09558},
file      = {:done/ tesse_5/2017HutchingsTalkinga Talking Drums_ Generating Drum Grooves with Neural Networks..pdf:PDF},
groups    = {tesse:5},
}
@Article{1998LeCunGradient,
author       = {LeCun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
date         = {1998-11},
journaltitle = {Proceedings of the IEEE},
title        = {Gradient-based learning applied to document recognition},
doi          = {10.1109/5.726791},
issn         = {0018-9219},
number       = {11},
pages        = {2278--2324},
volume       = {86},
file         = {:done/1998LecunGradient Gradient Based Learning Applied to Document Recognition.pdf:application/pdf},
groups       = {tesse:5},
keywords     = {Hidden Markov models, Feature extraction, Neural networks, 2D shape variability, back-propagation, backpropagation, Character recognition, cheque reading, complex decision surface synthesis, convolution, convolutional neural network character recognizers, document recognition, document recognition systems, field extraction, gradient based learning technique, gradient-based learning, graph transformer networks, GTN, handwritten character recognition, handwritten digit recognition task, high-dimensional patterns, language modeling, Machine learning, Multi-layer neural network, multilayer neural networks, multilayer perceptrons, multimodule systems, optical character recognition, Optical character recognition software, Optical computing, Pattern recognition, performance measure minimization, Principal component analysis, segmentation recognition},
}
@Book{2017MinskyPerceptrons,
author    = {Minsky, Marvin and Papert, Seymour A.},
date      = {2017-09-22},
title     = {Perceptrons: An Introduction to Computational Geometry},
isbn      = {0-262-53477-0},
location  = {MA},
pagetotal = {316},
publisher = {MIT Press},
url       = {https://www.ebook.de/de/product/28875910/marvin///////\\_minsky///////\\_seymour///////\\_a///////\\_papert///////\\_perceptrons///////\\_an///////\\_introduction///////\\_to///////\\_computational///////\\_geometry.html},
keywords  = {linear-classification neural-networks seminal,},
}
@Article{2006ReiffensteinCodification,
author       = {Reiffenstein, Tim},
date         = {2006-09},
journaltitle = {The Canadian Geographer / Le Géographe canadien},
title        = {Codification, patents and the geography of knowledge transfer in the electronic musical instrument industry},
doi          = {10.1111/j.1541-0064.2006.00143.x},
number       = {3},
pages        = {298--318},
volume       = {50},
abstract     = {Recent research in economic geography has emphasized tacit knowledge as the basis of industrial learning. In contrast, codification and the practices of industrial writing have received little attention for the roles they play in mobilizing knowledge across space. This paper offers insight into the geographies of codification through an examination of technology transfer in the electronic musical instrument industry between 1965 and 1995. The research draws on a variety of primary and secondary data that include interviews with inventors, biographical accounts and patent analysis. These sources offer perspective on the career trajectories of three U.S. inventors who transferred knowledge from various contexts in California's high-tech industry to the Japanese firm, Yamaha. Conceptually, the paper draws on the actor–network theory and Latour's idea of translation to highlight the detours inventors must take to register novelty. The analysis reveals the problematic nature of codified knowledge and its transfer; in this case codified knowledge was mobile internationally but not locally, at least until it reached Japan. The paper argues for the need to understand how texts such as patents are produced—the context of their authorship, the geographies of their circulation and their efficacy for shaping further innovative practice.},
file         = {:done/2006ReiffensteinCodification Codification, Patents and the Geography of Knowledge Transfer in the Electronic Musical Instrument Industry.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{2015SchmidhuberDeep,
author       = {Schmidhuber, Jürgen},
date         = {2015-01},
journaltitle = {Neural Networks},
title        = {Deep learning in neural networks: An overview},
doi          = {10.1016/j.neunet.2014.09.003},
pages        = {85--117},
volume       = {61},
}
@Article{2016SigtiaEnd,
author       = {Sigtia, S. and Benetos, E. and Dixon, S.},
date         = {2016-05},
journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title        = {An End-to-End Neural Network for Polyphonic Piano Music Transcription},
doi          = {10.1109/TASLP.2016.2533858},
issn         = {2329-9290},
number       = {5},
pages        = {927--939},
volume       = {24},
file         = {:done/2016SigtiaEnd An End to End Neural Network for Polyphonic Piano Music Transcription.pdf:application/pdf},
groups       = {tesse:5},
keywords     = {Hidden Markov models, recurrent neural network, Recurrent neural networks, Acoustics, recurrent neural networks, music information retrieval, Spectrogram, Feature extraction, Computational modeling, deep learning, Deep Learning, Recurrent Neural Networks, acoustic model, acoustic models, audio frame, Automatic music transcription, Automatic Music Transcription, beam search algorithm, end-to-end neural network, evaluation metrics, language model predictions, MIR, music language model, music language model predictions, music language models, Music Language Models, polyphonic music, polyphonic piano music transcription, probabilistic graphical model, speech recognition systems, supervised neural network model, unsupervised acoustic models},
}
@Article{2007StanleyCompositional,
author       = {Stanley, Kenneth O.},
date         = {2007-05},
journaltitle = {Genetic Programming and Evolvable Machines},
title        = {Compositional pattern producing networks: A novel abstraction of development},
doi          = {10.1007/s10710-007-9028-8},
number       = {2},
pages        = {131--162},
volume       = {8},
file         = {:done/2007StanleyCompositional Compositional Pattern Producing Networks/\\_ a Novel Abstraction of Development.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{2019Wang3D,
author       = {Wang, Keze and Lin, Liang and Jiang, Chenhan and Qian, Chen and Wei, Pengxu},
date         = {2019-01-11},
journaltitle = {arXiv:1901.03798 [cs]},
title        = {3D Human Pose Machines with Self-supervised Learning},
eprint       = {1901.03798},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1901.03798},
urldate      = {2019-03-26},
abstract     = {Driven by recent computer vision and robotic applications, recovering 3D human poses has become increasingly important and attracted growing interests. In fact, completing this task is quite challenging due to the diverse appearances, viewpoints, occlusions and inherently geometric ambiguities inside monocular images. Most of the existing methods focus on designing some elaborate priors /constraints to directly regress 3D human poses based on the corresponding 2D human pose-aware features or 2D pose predictions.},
file         = {:done/2019Wang3d 3D Human Pose Machines with Self Supervised Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@PhdThesis{2004TraubeInterdisciplinary,
author      = {Traube, Caroline},
date        = {2004},
institution = {McGill University},
title       = {An Interdisciplinary Study of the Timbre of the Classical Guitar.},
pages       = {237},
abstract    = {This dissertation proposes an interdisciplinary approach for the study of the timbre of the classical guitar. We start by identifying the static control parameters of timbre, relating to the structural components of the guitar and the dynamic control parameters of timbre, relating to the gestures applied by the performer on the instrument. From the plucked string physical model (obtained from the tranverse wave equation), we derive a digital signal interpretation of the plucking eﬀect which is a comb ﬁltering. Then we investigate how subjective characteristics of sound, like timbre, are related to gesture parameters. The starting point for exploration is an inventory of verbal descriptors commonly used by professional musicians to describe the brightness, the colour, the shape and the texture of the sounds they produce on their instruments. An explanation for the voice-like nature of guitar tones is proposed based on the observation that the maxima of the comb-ﬁltershaped magnitude spectrum of guitar tones are located at frequencies similar to the formant frequencies of a subset of identiﬁable vowels. These analogies at the spectral level might account for the origin of some timbre descriptors such as open, oval, round, thin, closed, nasal and hollow, that seem to refer to phonetic gestures. In a experiment conducted to conﬁrm these analogies, participants were asked to associate a consonant to the attack and a vowel to the decay of guitar tones. The results of this study support the idea that some perceptual dimensions of the guitar timbre space can be borrowed from phonetics. Finally, we address the problem of the indirect acquisition of instrumental gesture parameters. Pursuing previous research on the estimation of the plucking position from a recording, we propose a new estimation method based on an iterative weighted least-square algorithm, starting from a ﬁrst approximation derived from a variant of the autocorrelation function of the signal.},
file        = {:done/2004TraubeInterdisciplinary An Interdisciplinary Study of the Timbre of the Classical Guitar..pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
}
@InProceedings{2005Florianreinforcement,
author     = {Florian, R. V.},
booktitle  = {Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)},
date       = {2005},
title      = {A reinforcement learning algorithm for spiking neural networks},
doi        = {10.1109/SYNASC.2005.13},
eventtitle = {Seventh International Symposium on Symbolic and Numeric Algorithms for Scientific Computing (SYNASC'05)},
isbn       = {978-0-7695-2453-5},
location   = {Timisoara, Romania},
pages      = {8},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/1595864/},
urldate    = {2019-03-26},
abstract   = {The paper presents a new reinforcement learning mechanism for spiking neural networks. The algorithm is derived for networks of stochastic integrate-and-ﬁre neurons, but it can be also applied to generic spiking neural networks. Learning is achieved by synaptic changes that depend on the ﬁring of pre- and postsynaptic neurons, and that are modulated with a global reinforcement signal. The efﬁcacy of the algorithm is veriﬁed in a biologically-inspired experiment, featuring a simulated worm that searches for food. Our model recovers a form of neural plasticity experimentally observed in animals, combining spiketiming-dependent synaptic changes of one sign with nonassociative synaptic changes of the opposite sign determined by presynaptic spikes. The model also predicts that the time constant of spike-timing-dependent synaptic changes is equal to the membrane time constant of the neuron, in agreement with experimental observations in the brain. This study also led to the discovery of a biologicallyplausible reinforcement learning mechanism that works by modulating spike-timing-dependent plasticity (STDP) with a global reward signal.},
file       = {:done/2005FlorianReinforcement A Reinforcement Learning Algorithm for Spiking Neural Networks.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Article{2015MnihHuman,
author       = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
date         = {2015-02},
journaltitle = {Nature},
title        = {Human-level control through deep reinforcement learning},
doi          = {10.1038/nature14236},
issn         = {0028-0836, 1476-4687},
number       = {7540},
pages        = {529--533},
url          = {http://www.nature.com/articles/nature14236},
urldate      = {2019-03-26},
volume       = {518},
file         = {:done/2015MnihHuman Human Level Control through Deep Reinforcement Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Misc{2013MnihPlaying,
author      = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
date        = {2013},
title       = {Playing Atari with Deep Reinforcement Learning},
eprint      = {1312.5602},
eprintclass = {cs.LG},
eprinttype  = {arXiv},
file        = {:done/2013MnihPlaying Playing Atari with Deep Reinforcement Learning.pdf:application/pdf},
groups      = {tesse:5},
}
@Article{1999ChellapillaEvolution,
author       = {Chellapilla, K. and Fogel, D. B.},
date         = {1999-09},
journaltitle = {Proceedings of the IEEE},
title        = {Evolution, neural networks, games, and intelligence},
doi          = {10.1109/5.784222},
issn         = {0018-9219},
number       = {9},
pages        = {1471--1496},
url          = {http://ieeexplore.ieee.org/document/784222/},
urldate      = {2019-03-26},
volume       = {87},
abstract     = {Intelligence pertains to the ability to make appropriate decisions in light of specific goals and to adapt behavior to meet those goals in a range of environments. Mathematical games provide a framework for studying intelligent behavior in models of real-world settings or restricted domains. The behavior of alternative strategies in these games is defined by each individual’s stimulus-response mapping. Limiting these behaviors to linear functions of the environmental conditions renders the results to be little more than a façade: Effective decision making in any complex environment almost always requires nonlinear stimulus-response mappings. The obstacle then comes in choosing the appropriate representation and learning algorithm. Neural networks and evolutionary algorithms provide useful means for addressing these issues. This paper describes efforts to hybridize neural and evolutionary computation to learn appropriate strategies in zeroand nonzero-sum games, including the iterated prisoner’s dilemma, tic-tac-toe, and checkers. With respect to checkers, the evolutionary algorithm was able to discover a neural network that can be used to play at a near-expert level without injecting expert knowledge about how to play the game. The implications of evolutionary learning with respect to machine intelligence are also discussed. It is argued that evolution provides the framework for explaining naturally occurring intelligent entities and can be used to design machines that are also capable of intelligent behavior.},
file         = {:done/1999ChellapillaEvolution Evolution, Neural Networks, Games, and Intelligence.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014Sigtiarnn,
author   = {Sigtia, Siddharth and Benetos, Emmanouil and Cherla, Srikanth and Weyde, Tillman},
date     = {2014},
title    = {An rnn-based music language model for improving automatic music transcription},
pages    = {6},
abstract = {In this paper, we investigate the use of Music Language Models (MLMs) for improving Automatic Music Transcription performance. The MLMs are trained on sequences of symbolic polyphonic music from the Nottingham dataset. We train Recurrent Neural Network (RNN)-based models, as they are capable of capturing complex temporal structure present in symbolic music data. Similar to the function of language models in automatic speech recognition, we use the MLMs to generate a prior probability for the occurrence of a sequence. The acoustic AMT model is based on probabilistic latent component analysis, and prior information from the MLM is incorporated into the transcription framework using Dirichlet priors. We test our hybrid models on a dataset of multiple-instrument polyphonic music and report a signiﬁcant 3 /
file     = {:done/2014SigtiaRnn An Rnn Based Music Language Model for Improving Automatic Music Transcription.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2006BagozziOpen,
author       = {Bagozzi, Richard P. and Dholakia, Utpal M.},
date         = {2006-07},
journaltitle = {Management Science},
title        = {Open Source Software User Communities: A Study of Participation in Linux User Groups},
doi          = {10.1287/mnsc.1060.0545},
issn         = {0025-1909, 1526-5501},
number       = {7},
pages        = {1099--1115},
urldate      = {2019-03-26},
volume       = {52},
langid       = {english},
shorttitle   = {Open Source Software User Communities},
}
@InCollection{2015Daskinp,
author    = {Daskin, Mark S. and Maass, Kayse Lee},
booktitle = {Location Science},
date      = {2015},
title     = {The p-Median Problem},
doi       = {10.1007/978-3-319-13111-5///////\\_2},
editor    = {Laporte, Gilbert and Nickel, Stefan and Saldanha da Gama, Francisco},
isbn      = {978-3-319-13110-8 978-3-319-13111-5},
location  = {Cham},
pages     = {21--45},
publisher = {Springer International Publishing},
urldate   = {2019-03-26},
abstract  = {The p-median problem is central to much of discrete location modeling and theory. While the p-median problem is NP-hard on a general graph, it can be solved in polynomial time on a tree. A linear time algorithm for the 1-median problem on a tree is described. We also present a classical formulation of the problem. Basic construction and improvement algorithms are outlined. Results from the literature using various metaheuristics including tabu search, heuristic concentration, genetic algorithms, and simulated annealing are summarized. A Lagrangian relaxation approach is presented and used for computational results on 40 classical test instances as well as a 500-node instance derived from the most populous counties in the contiguous United States. We conclude with a discussion of multi-objective extensions of the p-median problem.},
file      = {:done/2015DaskinP The P Median Problem.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2014GanderScientific,
author    = {Gander, Walter and Gander, Martin J. and Kwok, Felix},
date      = {2014},
title     = {Scientific Computing - An Introduction using Maple and MATLAB},
doi       = {10.1007/978-3-319-04325-8},
isbn      = {978-3-319-04324-1 978-3-319-04325-8},
location  = {Cham},
publisher = {Springer International Publishing},
series    = {Texts in Computational Science and Engineering},
urldate   = {2019-03-26},
volume    = {11},
file      = {:done/2014GanderScientific Scientific Computing an Introduction Using Maple and MATLAB.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@InCollection{2011VerterUncapacitated,
author    = {Verter, Vedat},
booktitle = {Foundations of Location Analysis},
date      = {2011},
title     = {Uncapacitated and Capacitated Facility Location Problems},
doi       = {10.1007/978-1-4419-7572-0///////\\_2},
editor    = {Eiselt, H. A. and Marianov, Vladimir},
isbn      = {978-1-4419-7571-3 978-1-4419-7572-0},
location  = {Boston, MA},
pages     = {25--37},
publisher = {Springer US},
urldate   = {2019-03-26},
volume    = {155},
file      = {:done/2011VerterUncapacitated Uncapacitated and Capacitated Facility Location Problems.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@InProceedings{2018FukumotoGeneration,
author     = {Fukumoto, Yuuya and Shimizu, Daiki and Shibata, Chihiro},
booktitle  = {2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},
date       = {2018-07},
title      = {Generation of Character Illustrations from Stick Figures Using a Modification of Generative Adversarial Network},
doi        = {10.1109/COMPSAC.2018.10225},
eventtitle = {2018 IEEE 42nd Annual Computer Software and Applications Conference (COMPSAC)},
isbn       = {978-1-5386-2666-5},
location   = {Tokyo, Japan},
pages      = {183--186},
publisher  = {IEEE},
url        = {https://ieeexplore.ieee.org/document/8377853/},
urldate    = {2019-03-26},
abstract   = {We propose a modiﬁcation of generative adversarial networks (GANs) that generate illustrations of human ﬁgures from given poses represented by stick ﬁgures. In recent years, while various methods that generate images of characters using GANs have been proposed, it is not yet possible for users to freely designate poses of human ﬁgures. When generating an image of a character, the pose of the character takes is an important component of its composition. Thus it is necessary fora user who wants to create an illustration to be able to specify the pose easily. We collected a set of illustrations of human ﬁgures from the internet, and for each illustration, a simple line drawing that speciﬁes the pose was drawn manually. We constructed a GAN that takes a line drawing as its input and creates an illustration of a person in a pose that matches the line drawing. These networks are learned using the data set we prepared. In this paper, we propose a new network architecture. After constructing two networks both of which have almost the same structure as pix2pix, which is a variant model of GANs, we stack up those networks based on the idea of stack GAN. The experimental results show that, from stick ﬁgures representing common poses such as a standing pose, our methods was able to successfully generate images of characters. However, in the case of stick ﬁgures having rare poses that were not in the dataset, such as ﬁgures raising a hand or lying down, the generated images were blurred and not of a high-quality but still had the desired shapes. By expanding the dataset to include various poses, it is possible to generate diverse poses more precisely.},
langid     = {english},
}
@Article{2018ChanEverybody,
author       = {Chan, Caroline and Ginosar, Shiry and Zhou, Tinghui and Efros, Alexei A.},
date         = {2018-08-22},
journaltitle = {arXiv:1808.07371 [cs]},
title        = {Everybody Dance Now},
eprint       = {1808.07371},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1808.07371},
urldate      = {2019-03-26},
abstract     = {This paper presents a simple method for "do as I do" motion transfer: given a source video of a person dancing we can transfer that performance to a novel (amateur) target after only a few minutes of the target subject performing standard moves. We pose this problem as a per-frame image-to-image translation with spatio-temporal smoothing. Using pose detections as an intermediate representation between source and target, we learn a mapping from pose images to a target subject’s appearance. We adapt this setup for temporally coherent video generation including realistic face synthesis. Our video demo can be found at https://youtu.be/PCBTZh41Ris.},
file         = {:done/2018ChanEverybody Everybody Dance Now.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2018CaoReview,
author       = {Cao, Weipeng and Wang, Xizhao and Ming, Zhong and Gao, Jinzhu},
date         = {2018-01},
journaltitle = {Neurocomputing},
title        = {A review on neural networks with random weights},
doi          = {10.1016/j.neucom.2017.08.040},
issn         = {0925-2312},
pages        = {278--287},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0925231217314613},
urldate      = {2019-03-26},
volume       = {275},
abstract     = {In big data ﬁelds, with increasing computing capability, artiﬁcial neural networks have shown great strength in solving data classiﬁcation and regression problems. The traditional training of neural networks depends generally on the error back propagation method to iteratively tune all the parameters. When the number of hidden layers increases, this kind of training has many problems such as slow convergence, time consuming, and local minima. To avoid these problems, neural networks with random weights (NNRW) are proposed in which the weights between the hidden layer and input layer are randomly selected and the weights between the output layer and hidden layer are obtained analytically. Researchers have shown that NNRW has much lower training complexity in comparison with the traditional training of feed-forward neural networks. This paper objectively reviews the advantages and disadvantages of NNRW model, tries to reveal the essence of NNRW, gives our comments and remarks on NNRW, and provides some useful guidelines for users to choose a mechanism to train a feed-forward neural network.},
file         = {:done/2018CaoReview A Review on Neural Networks with Random Weights.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017LinRecurrent,
author       = {Lin, Mude and Lin, Liang and Liang, Xiaodan and Wang, Keze and Cheng, Hui},
date         = {2017-07-30},
journaltitle = {arXiv:1707.09695 [cs]},
title        = {Recurrent 3D Pose Sequence Machines},
eprint       = {1707.09695},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1707.09695},
urldate      = {2019-03-26},
abstract     = {3D human articulated pose recovery from monocular image sequences is very challenging due to the diverse appearances, viewpoints, occlusions, and also the human 3D pose is inherently ambiguous from the monocular imagery. It is thus critical to exploit rich spatial and temporal long-range dependencies among body joints for accurate 3D pose sequence prediction. Existing approaches usually manually design some elaborate prior terms and human body kinematic constraints for capturing structures, which are often insufficient to exploit all intrinsic structures and not scalable for all scenarios. In contrast, this paper presents a Recurrent 3D Pose Sequence Machine(RPSM) to automatically learn the image-dependent structural constraint and sequence-dependent temporal context by using a multi-stage sequential refinement. At each stage, our RPSM is composed of three modules to predict the 3D pose sequences based on the previously learned 2D pose representations and 3D poses: (i) a 2D pose module extracting the image-dependent pose representations, (ii) a 3D pose recurrent module regressing 3D poses and (iii) a feature adaption module serving as a bridge between module (i) and (ii) to enable the representation transformation from 2D to 3D domain. These three modules are then assembled into a sequential prediction framework to refine the predicted poses with multiple recurrent stages. Extensive evaluations on the Human3.6M dataset and HumanEva-I dataset show that our RPSM outperforms all state-of-the-art approaches for 3D pose estimation.},
file         = {:done/2017LinRecurrent Recurrent 3D Pose Sequence Machines.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016GiryesDeep,
author       = {Giryes, Raja and Sapiro, Guillermo and Bronstein, Alex M.},
date         = {2016-07-01},
journaltitle = {IEEE Transactions on Signal Processing},
title        = {Deep Neural Networks with Random Gaussian Weights: A Universal Classification Strategy?},
doi          = {10.1109/TSP.2016.2546221},
eprint       = {1504.08291},
eprinttype   = {arxiv},
issn         = {1053-587X, 1941-0476},
number       = {13},
pages        = {3444--3457},
url          = {http://arxiv.org/abs/1504.08291},
urldate      = {2019-03-26},
volume       = {64},
abstract     = {Three important properties of a classiﬁcation machinery are: (i) the system preserves the core information of the input data; (ii) the training examples convey information about unseen data; and (iii) the system is able to treat differently points from different classes. In this work we show that these fundamental properties are satisﬁed by the architecture of deep neural networks. We formally prove that these networks with random Gaussian weights perform a distance-preserving embedding of the data, with a special treatment for in-class and out-of-class data. Similar points at the input of the network are likely to have a similar output. The theoretical analysis of deep networks here presented exploits tools used in the compressed sensing and dictionary learning literature, thereby making a formal connection between these important topics. The derived results allow drawing conclusions on the metric learning properties of the network and their relation to its structure, as well as providing bounds on the required size of the training set such that the training examples would represent faithfully the unseen data. The results are validated with state-of-the-art trained networks.},
langid       = {english},
shorttitle   = {Deep Neural Networks with Random Gaussian Weights},
}
@Article{2014RingbauerExperimental,
author       = {Ringbauer, Martin and Broome, Matthew A. and Myers, Casey R. and White, Andrew G. and Ralph, Timothy C.},
date         = {2014-12},
journaltitle = {Nature Communications},
title        = {Experimental simulation of closed timelike curves},
doi          = {10.1038/ncomms5145},
issn         = {2041-1723},
number       = {1},
url          = {http://www.nature.com/articles/ncomms5145},
urldate      = {2019-03-26},
volume       = {5},
file         = {:done/2014RingbauerExperimental Experimental Simulation of Closed Timelike Curves.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2014ToshevDeeppose,
author     = {Toshev, Alexander and Szegedy, Christian},
booktitle  = {2014 IEEE Conference on Computer Vision and Pattern Recognition},
date       = {2014-06},
title      = {DeepPose: Human Pose Estimation via Deep Neural Networks},
doi        = {10.1109/CVPR.2014.214},
eventtitle = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
isbn       = {978-1-4799-5118-5},
location   = {Columbus, OH, USA},
pages      = {1653--1660},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909610},
urldate    = {2019-03-26},
abstract   = {We propose a method for human pose estimation based on Deep Neural Networks (DNNs). The pose estimation is formulated as a DNN-based regression problem towards body joints. We present a cascade of such DNN regressors which results in high precision pose estimates. The approach has the advantage of reasoning about pose in a holistic fashion and has a simple but yet powerful formulation which capitalizes on recent advances in Deep Learning. We present a detailed empirical analysis with state-ofart or better performance on four academic benchmarks of diverse real-world images.},
file       = {:done/2014ToshevDeeppose DeepPose/\\_ Human Pose Estimation Via Deep Neural Networks.pdf:application/pdf;:todo/done/2014ToshevDeeppose DeepPose/\\_ Human Pose Estimation Via Deep Neural Networks.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {DeepPose},
}
@Article{2017JanochaLoss,
author       = {Janocha, Katarzyna and Czarnecki, Wojciech Marian},
date         = {2017-02-18},
journaltitle = {arXiv:1702.05659 [cs]},
title        = {On Loss Functions for Deep Neural Networks in Classification},
eprint       = {1702.05659},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1702.05659},
urldate      = {2019-03-26},
abstract     = {Deep neural networks are currently among the most commonly used classiﬁers. Despite easily achieving very good performance, one of the best selling points of these models is their modular design – one can conveniently adapt their architecture to speciﬁc needs, change connectivity patterns, attach specialised layers, experiment with a large amount of activation functions, normalisation schemes and many others. While one can ﬁnd impressively wide spread of various conﬁgurations of almost every aspect of the deep nets, one element is, in authors’ opinion, underrepresented – while solving classiﬁcation problems, vast majority of papers and applications simply use log loss. In this paper we try to investigate how particular choices of loss functions aﬀect deep models and their learning dynamics, as well as resulting classiﬁers robustness to various eﬀects. We perform experiments on classical datasets, as well as provide some additional, theoretical insights into the problem. In particular we show that L1 and L2 losses are, quite surprisingly, justiﬁed classiﬁcation objectives for deep nets, by providing probabilistic interpretation in terms of expected misclassiﬁcation. We also introduce two losses which are not typically used as deep nets objectives and show that they are viable alternatives to the existing ones.},
file         = {:done/2017JanochaLoss On Loss Functions for Deep Neural Networks in Classification.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016RoszkowskaApplication,
author       = {Roszkowska, Ewa},
date         = {2016},
journaltitle = {Optimum. Studia Ekonomiczne},
title        = {The Application of UTA Method for Support Evaluation Negotiation Offers},
doi          = {10.15290/ose.2016.02.80.11},
issn         = {1506-7637},
number       = {2},
pages        = {144--162},
url          = {http://repozytorium.uwb.edu.pl/jspui/handle/11320/4640},
urldate      = {2019-03-26},
abstract     = {The MCDA technique has been extensively and successfully applied for supporting decision making in negotiation processes. The mostly used techniques SAW, AHP or TOPSIS are based on direct preference information which requires from negotiator a clear and precise definition all the parameters of the preference model (e.g. issue weights, option rates, aspiration and reservation values etc.), so those techniques can be successfully applied in well-structured negotiation problems. But, many real negotiation problems are illstructured, that means that the negotiation space is imprecisely defined, and the negotiator’s preferences the vagueness or imperfect.},
file         = {:done/2016RoszkowskaApplication The Application of UTA Method for Support Evaluation Negotiation Offers.pdf:application/pdf},
groups       = {tesse:5},
journal      = {Optimum. Studia Ekonomiczne},
langid       = {english},
publisher    = {University of Bialystok},
year         = {2016},
}
@Article{2018StierAnalysing,
author       = {Stier, Julian and Gianini, Gabriele and Granitzer, Michael and Ziegler, Konstantin},
date         = {2018},
journaltitle = {Procedia Computer Science},
title        = {Analysing Neural Network Topologies: a Game Theoretic Approach},
doi          = {10.1016/j.procs.2018.07.257},
issn         = {1877-0509},
pages        = {234--243},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S187705091831233X},
urldate      = {2019-03-26},
volume       = {126},
abstract     = {Abstract Artiﬁcial Neural Networks have shown impressive success in very diﬀerent application cases. Choosing a proper network architectAurrteiﬁisciaacl rNiteicuarladl eNceistwioonrkfosrhaavneetswhoorwkn’sismupccreessssi,vuesusuacllcyedssoninevineraymdiaﬀneuraelnmt aapnpnleirc.aAtisona cstarsaeisg.hCtfhoorwosairndgsatrpatreogpye,rlnaregtwe,omrkosatrlcyhfiutellcytcuorneniescatecdriatirccahlidteecctiusrieosnafroer saenleectwteodr,kth’sesruebccyersesl,yuinsugaollnyadognoeodinoaptmimanizuaatliomnasntnraetre. gAystaosﬁtrnadigphrtofpoerwr waredigsthrtastewghyi,llearagtet,hme soasmtlye ftuimllye acvoonindeicntgedovaercrﬁhittteincgtu.rHesowareevseerl,elcatregde, pthaerrtseboyf rtehleyiﬁnngaol nneatwgooorkd oarpetirmedizuantidoannts.trIantetghye tboesﬁtncdasper,oplaerrgwe epiagrhtstsowf hthileenaetttwheorskambeectoimmee asivmoipdliyngirroevleevrﬁantttinfogr. Hlaotewreivnefer,relanrcgiengp.aIrntsthoef twhoerﬁstncaal snee,thwiogrhklyaprearraemduentedraiznet.dIanrcthheitebcetsutrceasshei,nldaergr eprpoapretsr oofpttihmeinzaettiwoonraknbdeaclolmowe tshime pealysyircrereleavtiaonnt ofofraldavteerrsienrfiearleenxcaimngp.lIens fthoeolwinogrstht ecanseet,whoigrkh.ly parameterized architectures hinder proper optimization and allow theAeaﬁsrystcsrteeaptioinn roefmaodvvienrgseirriraelleevxaanmtpalrecshfioteocltiunrgalthpearntestwlieosrkin. identifying those parts, which requires measuring the contribution of Aindﬁirvsitdsutaelp cionmrepmonoevnintsg siurrcehleavsannteaurrcohniste. cItnurparlepvaiortussliwesorikn,idheenutriifsytiicnsg bthasoesde poanrtuss, iwnghitchherweqeuigirhets dmisetarsibuuritniognthoef caonnetruibrountioans coof nitnrdibivuitdiounalmceoamsuproenheanvtse ssuhcohwnassonmeuerosuncsc. eIsns,pbruevt idoousnowt oprrko,vihdeeuraisptriocspebratsheedoroenticuaslinugndtehrestwanediginhgt.distribution of a neuron as conTthriebruetfioorne,mineaosuurrwe ohrakvewsehionwvenstsiogmatee gsuacmceestsh,ebourettdicomnoetaspurroevsi,dneaamperloyptehretShheaopreletiycavlauluned(eSrsVta),nidninogrd. er to separate relevant from irreTlheveraenftoprea,rtisn oofurawn oarrktiﬁwceiainl vneesutirgaal tneegtwamorekt.hWeoerebtiecgminebaysudreess,ignnaimngelya tchoeaSlihtiaopnlaelygvaamluee f(oSrVa)n, inarotirﬁdceiratlonseeupraarlanteetrweloervka,nwt fhroemre inrereulreovnasntfopramrtscooafliatnioanrstiaﬁncdialthneeuavraelrangeetwcoornkt.riWbuetiboengsinofbnyeduerosingsnitnogcoaacloitaiolintisonyaielldgatmoethfeorSahnapalretyiﬁcviaalluen.euInraol rndeetrwtoorkm, ewahsuerree nhoeuwrownesllfothrme Schoaapllietiyonvsaluanedmtehaesuarveesrathgee ccoonnttrriibbuuttiioonnsofoifnndeivuirdounasl ntoeucrooanlsit,iownesryemielodvetolotwhe-cSohnatrpibleuytinvgalunee.urIonnosradnedr mtoemaseuarseuirtes himowpawcteollnththeeSnheatpwloeyrkvpaelurfeomrmeaansucree.s the contribution of individual neurons, we remove low-contributing neurons and measure its imIpnacotuornexthpeernimetwenotrskwpeersfhoorwmathnacte.the Shapley value outperforms other heuristics for measuring the contribution of neurons.},
file         = {:done/2018StierAnalysing Analysing Neural Network Topologies//_ a Game Theoretic Approach.pdf:application/pdf},
groups       = {tesse:5, Game Theory},
langid       = {english},
shorttitle   = {Analysing Neural Network Topologies},
}
@Article{ShohamMultiagent,
author = {Shoham, Yoav},
title  = {Multiagent Systems: Algorithmic, Game-Theoretic, and Logical Foundations},
pages  = {532},
file   = {:done/ShohamMultiagent Multiagent Systems\\_ Algorithmic, Game Theoretic, and Logical Foundations.pdf:application/pdf},
groups = {tesse:2, Game Theory},
langid = {english},
}
@Book{2018SuttonReinforcement,
author     = {Sutton, Richard S. and Barto, Andrew G.},
date       = {2018},
title      = {Reinforcement learning: an introduction},
edition    = {Second edition},
isbn       = {978-0-262-03924-6},
location   = {Cambridge, Massachusetts},
pagetotal  = {526},
publisher  = {The MIT Press},
series     = {Adaptive computation and machine learning series},
abstract   = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
file       = {:done/2018SuttonReinforcement Reinforcement Learning\\_ an Introduction.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Reinforcement learning},
langid     = {english},
shorttitle = {Reinforcement learning},
}
@InProceedings{2017Pike-burkeOptimistic,
author    = {Pike-Burke, Ciara and Grunewalder, Steﬀen},
booktitle = {Proceedings of the 20th International Conference on Artificial Intelligence and Statistics (AISTATS)},
date      = {2017},
title     = {Optimistic Planning for the Stochastic Knapsack Problem},
pages     = {9},
abstract  = {The stochastic knapsack problem is a stochastic resource allocation problem that arises frequently and yet is exceptionally hard to solve. We derive and study an optimistic planning algorithm speciﬁcally designed for the stochastic knapsack problem. Unlike other optimistic planning algorithms for MDPs, our algorithm, OpStoK, avoids the use of discounting and is adaptive to the amount of resources available. We achieve this behavior by means of a concentration inequality that simultaneously applies to capacity and reward estimates. Crucially, we are able to guarantee that the aforementioned conﬁdence regions hold collectively over all time steps by an application of Doob’s inequality. We demonstrate that the method returns an -optimal solution to the stochastic knapsack problem with high probability. To the best of our knowledge, our algorithm is the ﬁrst which provides such guarantees for the stochastic knapsack problem. Furthermore, our algorithm is an anytime algorithm and will return a good solution even if stopped prematurely. This is particularly important given the diﬃculty of the problem. We also provide theoretical conditions to guarantee OpStoK does not expand all policies and demonstrate favorable performance in a simple experimental setting.},
file      = {:done/2017Pike-burkeOptimistic Optimistic Planning for the Stochastic Knapsack Problem.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2015ChrupalaLearning,
author    = {Chrupala, Grzegorz and Kadar, Akos and Alishahi, Afra and Zong, Chengqing and Strube, Michael},
date      = {2015},
title     = {Learning language through pictures},
isbn      = {978-1-941643-73-0},
note      = {OCLC: 6893507677},
publisher = {Association for Computational Linguistics},
abstract  = {We propose Imaginet, a model of learning visually grounded representations of language from coupled textual and visual input. The model consists of two Gated Recurrent Unit networks with shared word embeddings, and uses a multi-task objective by receiving a textual description of a scene and trying to concurrently predict its visual representation and the next word in the sentence. Like humans, it acquires meaning representations for individual words from descriptions of visual scenes. Moreover, it learns to effectively use sequential structure in semantic interpretation of multi-word phrases.},
file      = {:done/2015ChrupalaLearning Learning Language through Pictures.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{DeanApproximating,
author   = {Dean, Brian C. and Goemans, Michel X. and Vondrak, Jan},
title    = {Approximating the Stochastic Knapsack Problem: The Beneﬁt of Adaptivity},
pages    = {10},
abstract = {We consider a stochastic variant of the NP-hard 0/1 knapsack problem in which item values are deterministic and item sizes are independent random variables with known, arbitrary distributions. Items are placed in the knapsack sequentially, and the act of placing an item in the knapsack instantiates its size. Our goal is to compute a solution "policy" that maximizes the expected value of items placed in the knapsack, and we consider both non-adaptive policies (that designate a priori a ﬁxed sequence of items to insert) and adaptive policies (that can make dynamic choices based on the instantiated sizes of items placed in the knapsack thus far). We show that adaptivity provides only a constant-factor improvement by demonstrating a greedy non-adaptive algorithm that approximates the optimal adaptive policy within a factor of 7. We also design an adaptive polynomial-time algorithm which approximates the optimal adaptive policy within a factor of 5 + , for any constant > 0.},
file     = {:done/DeanApproximating Approximating the Stochastic Knapsack Problem/_ the Beneﬁt of Adaptivity.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Book{1999LevineNew,
author    = {Levine, Ross and Beck, Thorsten},
date      = {1999-07},
title     = {A New Database on Financial Development and Structure},
doi       = {10.1596/1813-9450-2146},
publisher = {The World Bank},
series    = {Policy Research Working Papers},
urldate   = {2019-03-26},
abstract  = {This paper introduces a new database of indicators of financial development and structure across countries and over time. This database is unique in that it unites a wide variety of indicators that measure the size, activity and efficiency of financial intermediaries and markets. It improves on previous efforts by presenting data on the public share of commercial banks, by introducing indicators of the size and activity of nonbank financial institutions and by presenting measures of the size of bond and primary equity markets. This paper describes the sources, the construction and the intuition for the different indicators and presents descriptive statistics.},
file      = {:done/1999LevineNew A New Database on Financial Development and Structure.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{1995LacherNeural,
author       = {Lacher, R. C. and Coats, Pamela K. and Sharma, Shanker C. and Fant, L. Franklin},
date         = {1995-08},
journaltitle = {European Journal of Operational Research},
title        = {A neural network for classifying the financial health of a firm},
doi          = {10.1016/0377-2217(93)E0274-2},
issn         = {0377-2217},
number       = {1},
pages        = {53--65},
url          = {http://linkinghub.elsevier.com/retrieve/pii/0377221793E02742},
urldate      = {2019-03-26},
volume       = {85},
abstract     = {We present here a neural network applied to a universal business problem: the estimation of the future fiscal health of a corporation. The commonly used accounting and financial tool for such classification and prediction is a multiple discriminant analysis (MDA) of financial ratios. But the MDA technique has limitations based on its assumptions of linear separability, multivariate normality, and independence of the predictive variableS. A neural network, being free from such constraining assumptions, is able to achieve superior results. Our neural network model is the Cascade-Correlation architecture recently developed by Scott E. Fahlman and Christian Lebiere at Carnegie Mellon University. This new approach solves the hidden architecture enigma encountered using other types of neural networks. Also, Cascade-Correlation manages error signals in a manner which significantly improves execution speed. Our research is the first to use Cascade-Correlation for corporate health estimation.},
file         = {:done/1995LacherNeural A Neural Network for Classifying the Financial Health of a Firm.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015TeshniziComparison,
author       = {Teshnizi, Saeed and Ayatollahi, Sayyed},
date         = {2015},
journaltitle = {Acta Informatica Medica},
title        = {A Comparison of Logistic Regression Model and Artificial Neural Networks in Predicting of Student's Academic Failure},
doi          = {10.5455/aim.2015.23.296-300},
issn         = {0353-8109},
number       = {5},
pages        = {296},
url          = {http://www.scopemed.org/fulltextpdf.php?mno=203919},
urldate      = {2019-03-26},
volume       = {23},
abstract     = {Background and objective: Artificial Neural Networks (ANNs) have recently been applied in situations where an analysis based on the logistic regression (LR) is a standard statistical approach; direct comparisons of the results, however, are seldom attempted. In this study, we compared both logistic regression models and feed-forward neural networks on the academic failure data set. Methods: The data for this study included 18 questions about study situation of 275 undergraduate students selected randomly from among nursing and midwifery and paramedic schools of Hormozgan University of Medical Sciences in 2013. Logistic regression with forward method and feed forward Artificial Neural Network with 15 neurons in hidden layer were fitted to the dataset. The accuracy of the models in predicting academic failure was compared by using ROC (Receiver Operating Characteristic) and classification accuracy. Results: Among nine ANNs, the ANN with 15 neurons in hidden layer was a better ANN compared with LR. The Area Under Receiver Operating Characteristics (AUROC) of the LR model and ANN with 15 neurons in hidden layers, were estimated as 0.55 and 0.89, respectively and ANN was significantly greater than the LR. The LR and ANN models respectively classified 77.5///////\\% and 84.3///////\\% of the students correctly. Conclusion: Based on this dataset, it seems the classification of the students in two groups with and without academic failure by using ANN with 15 neurons in the hidden layer is better than the LR model.},
langid       = {english},
}
@Article{2018LehtinenNoise2noise,
author       = {Lehtinen, Jaakko and Munkberg, Jacob and Hasselgren, Jon and Laine, Samuli and Karras, Tero and Aittala, Miika and Aila, Timo},
date         = {2018-03-12},
journaltitle = {arXiv:1803.04189 [cs, stat]},
title        = {Noise2Noise: Learning Image Restoration without Clean Data},
eprint       = {1803.04189},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1803.04189},
urldate      = {2019-03-26},
abstract     = {We apply basic statistical reasoning to signal reconstruction by machine learning — learning to map corrupted observations to clean signals —with a simple and powerful conclusion: under certain common circumstances, it is possible to learn to restore signals without ever observing clean ones, at performance close or equal to training using clean exemplars. We show applications in photographic noise removal, denoising of synthetic Monte Carlo images, and reconstruction of MRI scans from undersampled inputs, all based on only observing corrupted data.},
file         = {:done/2018LehtinenNoise2noise Noise2Noise/\\_ Learning Image Restoration without Clean Data.pdf:application/pdf;:todo/done/2018LehtinenNoise2noise Noise2Noise/\\_ Learning Image Restoration without Clean Data.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Noise2Noise},
}
@InProceedings{2018MasuyamaModal,
author     = {Masuyama, Yoshiki and Kusano, Tsubasa and Yatabe, Kohei and Oikawa, Yasuhiro},
booktitle  = {2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date       = {2018-04},
title      = {Modal Decomposition of Musical Instrument Sound Via Alternating Direction Method of Multipliers},
doi        = {10.1109/ICASSP.2018.8462350},
eventtitle = {ICASSP 2018 - 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn       = {978-1-5386-4658-8},
location   = {Calgary, AB},
pages      = {631--635},
publisher  = {IEEE},
url        = {https://ieeexplore.ieee.org/document/8462350/},
urldate    = {2019-03-26},
abstract   = {For a musical instrument sound containing partials, or modes, the behavior of modes around the attack time is particularly important. However, accurately decomposing it around the attack time is not an easy task, especially when the onset is sharp. This is because spectra of the modes are peaky while the sharp onsets need a broad one. In this paper, an optimization-based method of modal decomposition is proposed to achieve accurate decomposition around the attack time. The proposed method is formulated as a constrained optimization problem to enforce the perfect reconstruction property which is important for accurate decomposition. For optimization, the alternating direction method of multipliers (ADMM) is utilized, where the update of variables is calculated in closed form. The proposed method realizes accurate modal decomposition in the simulation and real piano sounds.},
langid     = {english},
}
@Article{2018EphratLooking,
author       = {Ephrat, Ariel and Mosseri, Inbar and Lang, Oran and Dekel, Tali and Wilson, Kevin and Hassidim, Avinatan and Freeman, William T. and Rubinstein, Michael},
date         = {2018-07-30},
journaltitle = {ACM Transactions on Graphics},
title        = {Looking to Listen at the Cocktail Party: A Speaker-Independent Audio-Visual Model for Speech Separation},
doi          = {10.1145/3197517.3201357},
eprint       = {1804.03619},
eprinttype   = {arxiv},
issn         = {0730-0301},
number       = {4},
pages        = {1--11},
url          = {http://arxiv.org/abs/1804.03619},
urldate      = {2019-03-26},
volume       = {37},
abstract     = {We present a joint audio-visual model for isolating a single speech signal from a mixture of sounds such as other speakers and background noise. Solving this task using only audio as input is extremely challenging and does not provide an association of the separated speech signals with speakers in the video. In this paper, we present a deep network-based model that incorporates both visual and auditory signals to solve this task. The visual features are used to "focus" the audio on desired speakers in a scene and to improve the speech separation quality. To train our joint audio-visual model, we introduce AVSpeech, a new dataset comprised of thousands of hours of video segments from the Web. We demonstrate the applicability of our method to classic speech separation tasks, as well as real-world scenarios involving heated interviews, noisy bars, and screaming children, only requiring the user to specify the face of the person in the video whose speech they want to isolate. Our method shows clear advantage over state-of-the-art audio-only speech separation in cases of mixed speech. In addition, our model, which is speaker-independent (trained once, applicable to any speaker), produces better results than recent audio-visual speech separation methods that are speaker-dependent (require training a separate model for each speaker of interest).},
file         = {:done/2018EphratLooking Looking to Listen at the Cocktail Party/\\_ a Speaker Independent Audio Visual Model for Speech Separation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Looking to Listen at the Cocktail Party},
}
@InProceedings{2017NecasovaSolving,
author     = {Nečasová, Gabriela and Kocina, Filip and Veigend, Petr and Chaloupka, Jan and Šátek, Václav and Kunovský, Jiří},
booktitle  = {AIP Conference Proceedings},
date       = {2017},
title      = {Solving wave equation using finite differences and Taylor series},
doi        = {10.1063/1.4992649},
eventtitle = {INTERNATIONAL CONFERENCE OF NUMERICAL ANALYSIS AND APPLIED MATHEMATICS (ICNAAM 2016)},
location   = {Rhodes, Greece},
pages      = {480013},
publisher  = {Author(s)},
urldate    = {2019-03-26},
abstract   = {The paper deals with the numerical solution of partial diﬀerential equations (PDEs), especially wave equation. Two methods are used to obtain numerical solution of the wave equation. The Finite Diﬀerence Method (FDM) is used for transformation of wave equation to the system of ordinary diﬀerential equations (ODEs), diﬀerent types of diﬀerence formulas are used. The inﬂuence of arithmetic to higher order diﬀerence formulas is also presented. The Modern Taylor Series Method (MTSM) allows to solve ODEs numerically with extremely high precision. An important feature of this method is an automatic integration order setting, i.e. using as many Taylor series terms as the deﬁned accuracy requires.},
file       = {:done/2017NecasovaSolving Solving Wave Equation Using Finite Differences and Taylor Series.pdf:application/pdf},
groups     = {tesse:5, Finite Difference Methods},
langid     = {english},
year       = {2017},
}
@Article{zappi_shader-based_nodate,
author   = {Zappi, Victor and Allen, Andrew and Fels, Sidney},
title    = {Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments},
pages    = {6},
abstract = {Physical modelling is a sophisticated synthesis technique, often used in the design of Digital Musical Instruments (DMIs). Some of the most precise physical simulations of sound propagation are based on Finite-Diﬀerence TimeDomain (FDTD) methods, which are stable, highly parameterizable but characterized by an extremely heavy computational load. This drawback hinders the spread of FDTD from the domain of oﬀ-line simulations to the one of DMIs. With this paper, we present a novel approach to real-time physical modelling synthesis, which implements a 2D FDTD solver as a shader program running on the GPU directly within the graphics pipeline. The result is a system capable of running fully interactive, massively sized simulation domains, suitable for novel DMI design. With the help of diagrams and code snippets, we provide the implementation details of a ﬁrst interactive application, a drum head simulator whose source code is available online. Finally, we evaluate the proposed system, showing how this new approach can work as a valuable alternative to classic GPGPU modelling.},
groups   = {tesse:4},
langid   = {english},
}
@Report{2017EisenachNonparametrically,
author      = {Eisenach, Carson and Wang, Zhaoran and Liu, Han},
date        = {2017},
institution = {Princeton University},
title       = {Nonparametrically learning activation functions in deep neural nets},
type        = {techreport},
pages       = {23},
abstract    = {We provide a principled framework for nonparametrically learning activation functions in deep neural networks. Currently, state-of-the-art deep networks treat choice of activation function as a hyper-parameter before training. By allowing activation functions to be estimated as part of the training procedure, we expand the class of functions that each node in the network can learn. We also provide a theoretical justiﬁcation for our choice of nonparametric activation functions and demonstrate that networks with our nonparametric activation functions generalize well. To demonstrate the power of our novel techniques, we test them on image recognition datasets and achieve up to a 15 /\\% relative increase in test performance compared to the baseline.},
file        = {:done/2017EisenachNonparametrically Nonparametrically Learning Activation Functions in Deep Neural Nets.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
year        = {2017},
}
@Article{2017LouizosLearning,
author       = {Louizos, Christos and Welling, Max and Kingma, Diederik P.},
date         = {2017-12-04},
journaltitle = {arXiv:1712.01312 [cs, stat]},
title        = {Learning Sparse Neural Networks through $L0$ Regularization},
eprint       = {1712.01312},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1712.01312},
urldate      = {2019-03-26},
abstract     = {We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 norm of the resulting gated weights is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by "stretching" a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efﬁcient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
langid       = {english},
}
@Article{2018IshikawaGeometric,
author       = {Ishikawa, Ai and Michels, Dominik L. and Yaguchi, Takaharu},
date         = {2018-07},
journaltitle = {Japan Journal of Industrial and Applied Mathematics},
title        = {Geometric-integration tools for the simulation of musical sounds},
doi          = {10.1007/s13160-017-0292-6},
issn         = {0916-7005, 1868-937X},
number       = {2},
pages        = {511--540},
urldate      = {2019-03-26},
volume       = {35},
abstract     = {During the last decade, much attention has been given to sound rendering and the simulation of acoustic phenomena by solving appropriate models described by Hamiltonian partial differential equations. In this contribution, we introduce a procedure to develop appropriate tools inspired from geometric integration in order to simulate musical sounds. Geometric integrators are numerical integrators of excellent quality that are designed exclusively for Hamiltonian ordinary differential equations. The introduced procedure is a combination of two techniques in geometric integration: the semi-discretization method by Celledoni et al. (J Comput Phys 231:6770–6789, 2012) and symplectic partitioned Runge–Kutta methods. This combination turns out to be a right procedure that derives numerical schemes that are effective and suitable for computation of musical sounds. By using this procedure we derive a series of explicit integration algorithms for a simple model describing piano sounds as a representative example for virtual instruments. We demonstrate the advantage of the numerical methods by evaluating a variety of numerical test cases.},
file         = {:done/2018IshikawaGeometric Geometric Integration Tools for the Simulation of Musical Sounds.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016JaderbergDecoupled,
author       = {Jaderberg, Max and Czarnecki, Wojciech Marian and Osindero, Simon and Vinyals, Oriol and Graves, Alex and Silver, David and Kavukcuoglu, Koray},
date         = {2016-08-18},
journaltitle = {arXiv:1608.05343 [cs]},
title        = {Decoupled Neural Interfaces using Synthetic Gradients},
eprint       = {1608.05343},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1608.05343},
urldate      = {2019-03-26},
abstract     = {Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one’s future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass – amounting to independent networks which co-learn such that they can be composed into a single functioning corporation.},
file         = {:done/2016JaderbergDecoupled Decoupled Neural Interfaces Using Synthetic Gradients.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2018HayashiNew,
author       = {Hayashi, Yukio},
date         = {2018-03},
journaltitle = {Network Science},
title        = {A new design principle of robust onion-like networks self-organized in growth},
doi          = {10.1017/nws.2017.25},
eprint       = {1706.03910},
eprinttype   = {arxiv},
issn         = {2050-1242, 2050-1250},
number       = {1},
pages        = {54--70},
url          = {http://arxiv.org/abs/1706.03910},
urldate      = {2019-03-26},
volume       = {6},
abstract     = {Today’s economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading eﬃciency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures.},
file         = {:done/2018HayashiNew A New Design Principle of Robust Onion like Networks Self Organized in Growth.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016MoriseWorld,
author       = {Morise, Masanori and Yokomori, Fumiya and Ozawa, Kenji},
date         = {2016},
journaltitle = {IEICE Transactions on Information and Systems},
title        = {WORLD: A Vocoder-Based High-Quality Speech Synthesis System for Real-Time Applications},
doi          = {10.1587/transinf.2015EDP7457},
issn         = {0916-8532, 1745-1361},
number       = {7},
pages        = {1877--1884},
url          = {https://www.jstage.jst.go.jp/article/transinf/E99.D/7/E99.D///////\\_2015EDP7457////////\\_article},
urldate      = {2019-03-26},
volume       = {E99.D},
abstract     = {A vocoder-based speech synthesis system, named WORLD, was developed in an eﬀort to improve the sound quality of realtime applications using speech. Speech analysis, manipulation, and synthesis on the basis of vocoders are used in various kinds of speech research. Although several high-quality speech synthesis systems have been developed, real-time processing has been diﬃcult with them because of their high computational costs. This new speech synthesis system has not only sound quality but also quick processing. It consists of three analysis algorithms and one synthesis algorithm proposed in our previous research. The eﬀectiveness of the system was evaluated by comparing its output with against natural speech including consonants. Its processing speed was also compared with those of conventional systems. The results showed that WORLD was superior to the other systems in terms of both sound quality and processing speed. In particular, it was over ten times faster than the conventional systems, and the real time factor (RTF) indicated that it was fast enough for real-time processing.},
langid       = {english},
shorttitle   = {WORLD},
}
@Article{2016SerafinVirtual,
author       = {Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C. and Nordahl, Rolf},
date         = {2016-09},
journaltitle = {Computer Music Journal},
title        = {Virtual Reality Musical Instruments: State of the Art, Design Principles, and Future Directions},
doi          = {10.1162/COMJ///////\\_a///////\\_00372},
issn         = {0148-9267, 1531-5169},
number       = {3},
pages        = {22--40},
urldate      = {2019-03-26},
volume       = {40},
abstract     = {The rapid development and availability of low-cost technologies have created a wide interest in virtual reality. In the field of computer music, the term "virtual musical instruments" has been used for a long time to describe software simulations, extensions of existing musical instruments, and ways to control them with new interfaces for musical expression. Virtual reality musical instruments (VRMIs) that include a simulated visual component delivered via a head-mounted display or other forms of immersive visualization have not yet received much attention. In this article, we present a field overview of VRMIs from the viewpoint of the performer. We propose nine design guidelines, describe evaluation methods, analyze case studies, and consider future challenges.},
file         = {:done/2016SerafinVirtual Virtual Reality Musical Instruments/\\_ State of the Art, Design Principles, and Future Directions.pdf:application/pdf;:todo/done/2016SerafinVirtual Virtual Reality Musical Instruments/\\_ State of the Art, Design Principles, and Future Directions.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Virtual Reality Musical Instruments},
}
@InProceedings{2016LiaoHow,
author    = {Liao, Qianli and Leibo, Joel Z. and Poggio, Tomaso},
booktitle = {Thirtieth AAAI Conference on Artificial Intelligence},
date      = {2016},
title     = {How Important Is Weight Symmetry in Backpropagation?},
pages     = {8},
abstract  = {Gradient backpropagation (BP) requires symmetric feedforward and feedback connections—the same weights must be used for forward and backward passes. This "weight transport problem" (Grossberg 1987) is thought to be one of the main reasons to doubt BP’s biologically plausibility. Using 15 different classiﬁcation datasets, we systematically investigate to what extent BP really depends on weight symmetry. In a study that turned out to be surprisingly similar in spirit to Lillicrap et al.’s demonstration (Lillicrap et al. 2014) but orthogonal in its results, our experiments indicate that: (1) the magnitudes of feedback weights do not matter to performance (2) the signs of feedback weights do matter—the more concordant signs between feedforward and their corresponding feedback connections, the better (3) with feedback weights having random magnitudes and 100 /\\% concordant signs, we were able to achieve the same or even better performance than SGD. (4) some normalizations/stabilizations are indispensable for such asymmetric BP to work, namely Batch Normalization (BN) (Ioffe and Szegedy 2015) and/or a "Batch Manhattan" (BM) update rule.},
file      = {:done/2016LiaoHow How Important Is Weight Symmetry in Backpropagation/\\_.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Misc{2006DongFinite,
author = {Dong, Shuonan},
date   = {2006},
title  = {Finite Difference Methods for the Hyperbolic Wave Partial Differential Equations},
url    = {https://w3.pppl.gov/m3d/1dwave/2006-04-12///////\\_18.085///////\\_Wave.pdf},
file   = {:done/2006DongFinite Finite Difference Methods for the Hyperbolic Wave Partial Differential Equations.pdf:application/pdf},
groups = {tesse:5, Finite Difference Methods},
langid = {english},
pages  = {27},
}
@Article{2016BalleEnd,
author       = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
date         = {2016-11-05},
journaltitle = {arXiv:1611.01704 [cs, math]},
title        = {End-to-end Optimized Image Compression},
eprint       = {1611.01704},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1611.01704},
urldate      = {2019-03-26},
abstract     = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear ﬁlters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate–distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate–distortion curve, as speciﬁed by a trade-off parameter. Across an independent set of test images, we ﬁnd that the optimized method generally exhibits better rate–distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
file         = {:done/2016BalleEnd End to End Optimized Image Compression.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016HighlanderVery,
author       = {Highlander, Tyler and Rodriguez, Andres},
date         = {2016-01-25},
journaltitle = {arXiv:1601.06815 [cs]},
title        = {Very Efficient Training of Convolutional Neural Networks using Fast Fourier Transform and Overlap-and-Add},
eprint       = {1601.06815},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1601.06815},
urldate      = {2019-03-26},
abstract     = {Convolutional neural networks (CNNs) are currently state-of-the-art for various classiﬁcation tasks, but are computationally expensive. Propagating through the convolutional layers is very slow, as each kernel in each layer must sequentially calculate many dot products for a single forward and backward propagation which equates to O(N2n2) per kernel per layer where the inputs are N × N arrays and the kernels are n × n arrays. Convolution can be efﬁciently performed as a Hadamard product in the frequency domain. The bottleneck is the transformation which has a cost of O(N2 log2 N) using the fast Fourier transform (FFT). However, the increase in efﬁciency is less signiﬁcant when N n as is the case in CNNs. We mitigate this by using the "overlap-and-add" technique reducing the computational complexity to O(N2 log2 n) per kernel. This method increases the algorithm’s efﬁciency in both the forward and backward propagation, reducing the training and testing time for CNNs. Our empirical results show our method reduces computational time by a factor of up to 16.3 times the traditional convolution implementation for a 8 × 8 kernel and a 224 × 224 image.},
langid       = {english},
}
@Article{2015ReyesSupersymmetrica,
author       = {Reyes, Marco A. and Arcos-Olalla, Rafael},
date         = {2015-10-13},
journaltitle = {arXiv:1510.03735 [math-ph, physics:quant-ph]},
title        = {Supersymmetric features of the Error and Dawson's functions},
eprint       = {1510.03735},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1510.03735},
urldate      = {2019-03-26},
abstract     = {Following a letter by Bassett, we show ﬁrst that it is possible to ﬁnd an analytical approximation to the error function in terms of a ﬁnite series of hyperbolic tangents from the supersymmetric (SUSY) solution of the Po¨schl-Teller eigenvalue problem in quantum mechanics (QM). Afterwards, we show that the second order diﬀerential equation for the derivatives of Dawson’s function can be found in another SUSY related eigenvalue problem, where the factorization of the simple harmonic oscillator Hamiltonian renders the wrong-sign Hermite diﬀerential equation, and that Dawson’s second order diﬀerential equation possess a singular SUSY type relation to this equation.},
file         = {:done/2015ReyesSupersymmetrica Supersymmetric Features of the Error and Dawson's Functions.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015SouzaFilhoMusica,
author       = {Souza Filho, N. E. and Gonçalves, B. A. and Oliveira, V. T.},
date         = {2015-06},
journaltitle = {Revista Brasileira de Ensino de Física},
title        = {Música para estudantes de engenharia: Síntese sonora de tema de jazz},
doi          = {10.1590/S1806-11173721804},
issn         = {1806-1117},
number       = {2},
pages        = {2313--1--2313--10},
url          = {http://www.scielo.br/scielo.php?script=sci///////\\_arttext///////\\&pid=S1806-11172015000200014///////\\&lng=pt///////\\&tlng=pt},
urldate      = {2019-03-26},
volume       = {37},
abstract     = {The production of sound by acoustic musical instruments is caused by the vibration of a resonant structure that can be described by signal corresponding to the temporal evolution of the vibration associated with the sound pressure. The fact that the sound can be characterized by a set of signals, suggests that a device can generate sound and therefore imitate sounds of acoustic instruments. Such device is called a synthesizer, its main component in sound production is an oscillator. This work presents the synthesis of a classic theme of West Coast Jazz that has as peculiarity an odd metric. All the notes were identiﬁed in the score and synthesized on computer.},
file         = {:done/2015SouzaFilhoMusica Música Para Estudantes De Engenharia/\\_ Síntese Sonora De Tema De Jazz.pdf:application/pdf},
groups       = {tesse:5},
langid       = {portuguese},
shorttitle   = {Música para estudantes de engenharia},
}
@Article{2015YoungHci,
author       = {Young, Gareth and Murphy, David},
date         = {2015},
journaltitle = {Unpublished},
title        = {HCI Models for Digital Musical Instruments: Methodologies for Rigorous Testing of Digital Musical Instruments},
doi          = {10.13140/rg.2.1.3949.9364},
urldate      = {2019-03-26},
abstract     = {Here we present an analysis of literature relating to the evaluation methodologies of Digital Musical Instruments (DMIs) derived from the field of Human Computer Interaction (HCI). We then apply choice aspects from these existing evaluation models and apply them to an optimized evaluation for assessing new DMIs.},
file         = {:done/2015YoungHci HCI Models for Digital Musical Instruments/\\_ Methodologies for Rigorous Testing of Digital Musical Instruments.pdf:application/pdf;:todo/done/2015YoungHci HCI Models for Digital Musical Instruments/\\_ Methodologies for Rigorous Testing of Digital Musical Instruments.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {HCI Models for Digital Musical Instruments},
}
@Article{2014VincentEfficient,
author       = {Vincent, Pascal and de Brébisson, Alexandre and Bouthillier, Xavier},
date         = {2014-12-22},
journaltitle = {arXiv:1412.7091 [cs]},
title        = {Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets},
eprint       = {1412.7091},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1412.7091},
urldate      = {2019-03-26},
abstract     = {An important class of problems involves training deep neural networks with sparse prediction targets of very high dimension D. These occur naturally in e.g. neural language models or the learning of word-embeddings, often posed as predicting the probability of next words among a vocabulary of size D (e.g. 200 000). Computing the equally large, but typically non-sparse D-dimensional output vector from a last hidden layer of reasonable dimension d (e.g. 500) incurs a prohibitive O(Dd) computational cost for each example, as does updating the D × d output weight matrix and computing the gradient needed for backpropagation to previous layers. While efﬁcient handling of large sparse network inputs is trivial, the case of large sparse targets is not, and has thus so far been sidestepped with approximate alternatives such as hierarchical softmax or sampling-based approximations during training. In this work we develop an original algorithmic approach which, for a family of loss functions that includes squared error and spherical softmax, can compute the exact loss, gradient update for the output weights, and gradient for backpropagation, all in O(d2) per example instead of O(Dd), remarkably without ever computing the D-dimensional output. The proposed algorithm yields aofsptheeedcuopmopfut4Dadt,ioi.nes.},
langid       = {english},
}
@InProceedings{2015HarrisonAlgorithm,
author    = {Harrison, Reginald L. and Bilbao, Stefan and Perry, James},
booktitle = {Proceedings of the 18th International Conference on Digital Audio Effects},
date      = {2015-11-30},
title     = {An algorithm for a valved brass instrument synthesis environment using finite-difference time-domain methods with performance optimisation},
language  = {English},
abstract  = {This paper presents a physical modelling sound synthesis environment for the production of valved brass instrument sounds. The governing equations of the system are solved using finite-difference time-domain (FDTD) methods and the environment is implemented in the C programming language. Users of the environment can create their own custom instruments and are able to control player parameters such as lip frequency, mouth pressure and valve openings through the use of instrument and score files.The algorithm for sound synthesis is presented in detail along with a discussion of optimisation methods used to reduce run time. Binaries for the environment are available for download online for multiple platforms.},
day       = {30},
file      = {:done/2015HarrisonAlgorithm An Algorithm for a Valved Brass Instrument Synthesis Environment Using Finite Difference Time Domain Methods with Performance Optimisation.pdf:application/pdf},
groups    = {tesse:5, Finite Difference Methods},
}
@Article{2014ChoProperties,
author       = {Cho, Kyunghyun and van Merrienboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
date         = {2014-09-03},
journaltitle = {arXiv:1409.1259 [cs, stat]},
title        = {On the Properties of Neural Machine Translation: Encoder-Decoder Approaches},
eprint       = {1409.1259},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1409.1259},
urldate      = {2019-03-26},
abstract     = {Neural machine translation is a relatively new approach to statistical machine translation based purely on neural networks. The neural machine translation models often consist of an encoder and a decoder. The encoder extracts a ﬁxed-length representation from a variable-length input sentence, and the decoder generates a correct translation from this representation. In this paper, we focus on analyzing the properties of the neural machine translation using two models; RNN Encoder–Decoder and a newly proposed gated recursive convolutional neural network. We show that the neural machine translation performs relatively well on short sentences without unknown words, but its performance degrades rapidly as the length of the sentence and the number of unknown words increase. Furthermore, we ﬁnd that the proposed gated recursive convolutional network learns a grammatical structure of a sentence automatically.},
file         = {:done/2014ChoProperties On the Properties of Neural Machine Translation/\\_ Encoder Decoder Approaches.pdf:application/pdf;:todo/done/2014ChoProperties On the Properties of Neural Machine Translation/\\_ Encoder Decoder Approaches.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {On the Properties of Neural Machine Translation},
}
@Article{2015HilleHow,
author       = {Hille, Adrian and Schupp, Jürgen},
date         = {2015-02},
journaltitle = {Economics of Education Review},
title        = {How learning a musical instrument affects the development of skills},
doi          = {10.1016/j.econedurev.2014.10.007},
issn         = {0272-7757},
pages        = {56--82},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0272775714000995},
urldate      = {2019-03-26},
volume       = {44},
abstract     = {Despite numerous studies on skill development, we know little about the effects of extracurricular music activities on cognitive and non-cognitive skills. This study examines how music training during childhood and youth affects the development of cognitive skills, school grades, personality, time use and ambition using data from the German Socio-Economic Panel (SOEP). Our ﬁndings suggest that adolescents with music training have better school grades, are more conscientious, open and ambitious. These effects are stronger among adolescents from lower socio-economic status. In order to address the non-random selection into playing music, we take into account detailed information on the child and its parents, which may determine both the decision to pursue music lessons and educational outcomes. While lacking truly exogenous variations in music activities, our results are robust to a large range of sensitivity tests. We thereby approach causality better than previous observational studies.},
file         = {:done/2015HilleHow How Learning a Musical Instrument Affects the Development of Skills.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015ChatziioannouEnergy,
author       = {Chatziioannou, Vasileios and van Walstijn, Maarten},
date         = {2015-03},
journaltitle = {Journal of Sound and Vibration},
title        = {Energy conserving schemes for the simulation of musical instrument contact dynamics},
doi          = {10.1016/j.jsv.2014.11.017},
issn         = {0022-460X},
pages        = {262--279},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0022460X14009146},
urldate      = {2019-03-26},
volume       = {339},
abstract     = {Collisions are an innate part of the function of many musical instruments. Due to the nonlinear nature of contact forces, special care has to be taken in the construction of numerical schemes for simulation and sound synthesis. Finite difference schemes and other time-stepping algorithms used for musical instrument modelling purposes are normally arrived at by discretising a Newtonian description of the system. However because impact forces are nonanalytic functions of the phase space variables, algorithm stability can rarely be established this way. This paper presents a systematic approach to deriving energy conserving schemes for frictionless impact modelling. The proposed numerical formulations follow from discretising Hamilton's equations of motion, generally leading to an implicit system of nonlinear equations that can be solved with Newton's method. The approach is first outlined for point mass collisions and then extended to distributed settings, such as vibrating strings and beams colliding with rigid obstacles. Stability and other relevant properties of the proposed approach are discussed and further demonstrated with simulation examples. The methodology is exemplified through a case study on tanpura string vibration, with the results confirming the main findings of previous studies on the role of the bridge in sound generation with this type of string instrument.},
file         = {:done/2015ChatziioannouEnergy Energy Conserving Schemes for the Simulation of Musical Instrument Contact Dynamics.pdf:application/pdf},
groups       = {tesse:5, Finite Difference Methods},
langid       = {english},
}
@Article{2013ParsaeianComparison,
author       = {Parsaeian, M. and Mohammad, K. and Mahmoudi, M. and Zeraati, H.},
date         = {2013},
journaltitle = {Iranian Journal of Public Health},
title        = {Comparison of Logistic Regression and Artificial Neural Network in Low Back Pain Prediction: Second National Health Survey},
pages        = {8},
abstract     = {Background: The purpose of this investigation was to compare empirically predictive ability of an artificial neural network with a logistic regression in prediction of low back pain. Methods: Data from the second national health survey were considered in this investigation. This data includes the information of low back pain and its associated risk factors among Iranian people aged 15 years and older. Artificial neural network and logistic regression models were developed using a set of 17294 data and they were validated in a test set of 17295 data. Hosmer and Lemeshow recommendation for model selection was used in fitting the logistic regression. A three-layer perceptron with 9 inputs, 3 hidden and 1 output neurons was employed. The efficiency of two models was compared by receiver operating characteristic analysis, root mean square and -2 Loglikelihood criteria. Results: The area under the ROC curve (SE), root mean square and -2Loglikelihood of the logistic regression was 0.752 (0.004), 0.3832 and 14769.2, respectively. The area under the ROC curve (SE), root mean square and -2Loglikelihood of the artificial neural network was 0.754 (0.004), 0.3770 and 14757.6, respectively. Conclusions: Based on these three criteria, artificial neural network would give better performance than logistic regression. Although, the difference is statistically significant, it does not seem to be clinically significant.},
file         = {:done/2013ParsaeianComparison Comparison of Logistic Regression and Artificial Neural Network in Low Back Pain Prediction/\\_ Second National Health Survey.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2012HinrichsenEntropy,
author       = {Hinrichsen, Haye},
date         = {2012-06},
journaltitle = {Revista Brasileira de Ensino de Física},
title        = {Entropy-based tuning of musical instruments},
doi          = {10.1590/S1806-11172012000200004},
issn         = {1806-1117},
number       = {2},
pages        = {1--8},
url          = {http://www.scielo.br/scielo.php?script=sci///////\\_arttext///////\\&pid=S1806-11172012000200004///////\\&lng=en///////\\&tlng=en},
urldate      = {2019-03-26},
volume       = {34},
abstract     = {The human sense of hearing perceives a combination of sounds ‘in tune’ if the corresponding harmonic spectra are correlated, meaning that the neuronal excitation pattern in the inner ear exhibits some kind of order. Based on this observation it is suggested that musical instruments such as pianos can be tuned by minimizing the Shannon entropy of suitably preprocessed Fourier spectra. This method reproduces not only the correct stretch curve but also similar pitch ﬂuctuations as in the case of high-quality aural tuning.},
file         = {:done/2012HinrichsenEntropy Entropy Based Tuning of Musical Instruments.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2011OmodhrainFramework,
author       = {O'Modhrain, Sile},
date         = {2011-03},
journaltitle = {Computer Music Journal},
title        = {A Framework for the Evaluation of Digital Musical Instruments},
doi          = {10.1162/COMJ///////\\_a///////\\_00038},
issn         = {0148-9267},
number       = {1},
pages        = {28--42},
urldate      = {2019-03-26},
volume       = {35},
file         = {:done/2011OmodhrainFramework A Framework for the Evaluation of Digital Musical Instruments.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2004SmithVirtual,
author       = {Smith, Julius O.},
date         = {2004-09},
journaltitle = {Journal of New Music Research},
title        = {Virtual Acoustic Musical Instruments: Review and Update},
doi          = {10.1080/0929821042000317859},
issn         = {0929-8215, 1744-5027},
number       = {3},
pages        = {283--304},
urldate      = {2019-03-26},
volume       = {33},
abstract     = {This article1 gives an overview of selected developments in musical sound synthesis based on physical models of musical instruments—sometimes called "virtual acoustic" sound synthesis. Emphasis is placed on techniques which yield the highest playability and sound quality in real time at a reasonable computational expense.},
file         = {:done/2004SmithVirtual Virtual Acoustic Musical Instruments/\\_ Review and Update.pdf:application/pdf;:todo/done/2004SmithVirtual Virtual Acoustic Musical Instruments/\\_ Review and Update.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Virtual Acoustic Musical Instruments},
}
@Article{2010Bioucas-diasMultiplicative,
author       = {Bioucas-Dias, José M. and Figueiredo, Mário A. T.},
date         = {2010-07},
journaltitle = {IEEE Transactions on Image Processing},
title        = {Multiplicative Noise Removal Using Variable Splitting and Constrained Optimization},
doi          = {10.1109/TIP.2010.2045029},
eprint       = {0912.1845},
eprinttype   = {arxiv},
issn         = {1057-7149, 1941-0042},
number       = {7},
pages        = {1720--1730},
url          = {http://arxiv.org/abs/0912.1845},
urldate      = {2019-03-26},
volume       = {19},
abstract     = {Multiplicative noise (also known as speckle noise) models are central to the study of coherent imaging systems, such as synthetic aperture radar and sonar, and ultrasound and laser imaging. These models introduce two additional layers of difﬁculties with respect to the standard Gaussian additive noise scenario: (1) the noise is multiplied by (rather than added to) the original image; (2) the noise is not Gaussian, with Rayleigh and Gamma being commonly used densities. These two features of multiplicative noise models preclude the direct application of most state-of-the-art algorithms, which are designed for solving unconstrained optimization problems where the objective has two terms: a quadratic data term (log-likelihood), reﬂecting the additive and Gaussian nature of the noise, plus a convex (possibly nonsmooth) regularizer (e.g., a total variation or wavelet-based regularizer/prior). In this paper, we address these difﬁculties by: (1) converting the multiplicative model into an additive one by taking logarithms, as proposed by some other authors; (2) using variable splitting to obtain an equivalent constrained problem; and (3) dealing with this optimization problem using the augmented Lagrangian framework. A set of experiments shows that the proposed method, which we name MIDAL (multiplicative image denoising by augmented Lagrangian), yields state-of-the-art results both in terms of speed and denoising performance.},
file         = {:done/2010Bioucas-diasMultiplicative Multiplicative Noise Removal Using Variable Splitting and Constrained Optimization.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2010MignotDigital,
author       = {Mignot, Rémi and Helie, Thomas and Matignon, Denis},
date         = {2010-05},
journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
title        = {Digital Waveguide Modeling for Wind Instruments: Building a State–Space Representation Based on the Webster–Lokshin Model},
doi          = {10.1109/TASL.2009.2038671},
issn         = {1558-7916, 1558-7924},
number       = {4},
pages        = {843--854},
url          = {http://ieeexplore.ieee.org/document/5446589/},
urldate      = {2019-03-26},
volume       = {18},
abstract     = {This paper deals with digital waveguide modeling of wind instruments. It presents the application of state–space representations for the reﬁned acoustic model of Webster–Lokshin. This acoustic model describes the propagation of longitudinal waves in axisymmetric acoustic pipes with a varying cross-section, visco-thermal losses at the walls, and without assuming planar or spherical waves. Moreover, three types of discontinuities of the shape can be taken into account (radius, slope, and curvature). The purpose of this work is to build low-cost digital simulations in the time domain based on the Webster–Lokshin model. First, decomposing a resonator into independent elementary parts and isolating delay operators lead to a Kelly–Lochbaum network of input/output systems and delays. Second, for a systematic assembling of elements, their state–space representations are derived in discrete time. Then, standard tools of automatic control are used to reduce the complexity of digital simulations in the time domain. The method is applied to a real trombone, and results of simulations are presented and compared with measurements. This method seems to be a promising approach in term of modularity, complexity of calculation, and accuracy, for any acoustic resonators based on tubes.},
langid       = {english},
shorttitle   = {Digital Waveguide Modeling for Wind Instruments},
}
@Article{2009SchneiderNeural,
author       = {Schneider, Peter and Wengenroth, Martina},
date         = {2009-06},
journaltitle = {Contemporary Music Review},
title        = {The Neural Basis of Individual Holistic and Spectral Sound Perception},
doi          = {10.1080/07494460903404402},
issn         = {0749-4467, 1477-2256},
number       = {3},
pages        = {315--328},
urldate      = {2019-03-26},
volume       = {28},
file         = {:done/2009SchneiderNeural The Neural Basis of Individual Holistic and Spectral Sound Perception.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2009EmbrechtsPanjer,
author       = {Embrechts, Paul and Frei, Marco},
date         = {2009-07},
journaltitle = {Mathematical Methods of Operations Research},
title        = {Panjer recursion versus FFT for compound distributions},
doi          = {10.1007/s00186-008-0249-2},
issn         = {1432-2994, 1432-5217},
number       = {3},
pages        = {497--508},
urldate      = {2019-03-26},
volume       = {69},
abstract     = {Numerical evaluation of compound distributions is an important task in insurance mathematics and quantitative risk management (QRM). In practice, both recursive methods as well as transform based techniques are widely used. We give a survey of these tools, point out the respective merits and provide some numerical examples.},
file         = {:done/2009EmbrechtsPanjer Panjer Recursion Versus FFT for Compound Distributions.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2008CharlesTutorial,
author       = {Charles, Jean-François},
date         = {2008-09},
journaltitle = {Computer Music Journal},
title        = {A Tutorial on Spectral Sound Processing Using Max/MSP and Jitter},
doi          = {10.1162/comj.2008.32.3.87},
issn         = {0148-9267, 1531-5169},
number       = {3},
pages        = {87--102},
urldate      = {2019-03-26},
volume       = {32},
file         = {:done/2008CharlesTutorial A Tutorial on Spectral Sound Processing Using Max/\\_MSP and Jitter.pdf:application/pdf;:todo/done/2008CharlesTutorial A Tutorial on Spectral Sound Processing Using Max/\\_MSP and Jitter.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2007ParentCase,
author       = {Parent, Milena M. and Deephouse, David L.},
date         = {2007-08-27},
journaltitle = {Journal of Business Ethics},
title        = {A Case Study of Stakeholder Identification and Prioritization by Managers},
doi          = {10.1007/s10551-007-9533-y},
issn         = {0167-4544},
number       = {1},
pages        = {1--23},
urldate      = {2019-03-26},
volume       = {75},
abstract     = {The purpose of this article is to examine stakeholder identification and prioritization by managers using the power, legitimacy, and urgency framework of Mitchell et al. (Academy of Management Review 22, 853–886; 1997). We use a multi-method, comparative case study of two large-scale sporting event organizing committees, with a particular focus on interviews with managers at three hierarchical levels. We support the positive relationship between number of stakeholder attributes and perceived stakeholder salience. Managers’ hierarchical level and role have direct and moderating effects on stakeholder identiﬁcation and perceived salience. We also found that most stakeholders were deﬁnitive, dominant, or dormant types –the other ﬁve types were rare. Power has the most important effect on salience, followed by urgency and legitimacy. Based on our case study, we offer several ways to advance the theory of stakeholder identiﬁcation and salience.},
file         = {:done/2007ParentCase A Case Study of Stakeholder Identification and Prioritization by Managers.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2016JackEffect,
author     = {Jack, Robert H. and Stockman, Tony and McPherson, Andrew},
booktitle  = {Proceedings of the Audio Mostly 2016 on - AM '16},
date       = {2016},
title      = {Effect of latency on performer interaction and subjective quality assessment of a digital musical instrument},
doi        = {10.1145/2986416.2986428},
eventtitle = {the Audio Mostly 2016},
isbn       = {978-1-4503-4822-5},
location   = {Norrkoping, Sweden},
pages      = {116--123},
publisher  = {ACM Press},
url        = {http://dl.acm.org/citation.cfm?doid=2986416.2986428},
urldate    = {2019-03-26},
abstract   = {When designing digital musical instruments the importance of low and consistent action-to-sound latency is widely accepted. This paper investigates the eﬀects of latency (020ms) on instrument quality evaluation and performer interaction. We present ﬁndings from an experiment conducted with musicians who performed on an percussive digital musical instrument with variable amounts of latency. Three latency conditions were tested against a zero latency condition, 10ms, 20ms and 10ms ± 3ms jitter. The zero latency condition was signiﬁcantly rated more positively than the 10ms with jitter and 20ms latency conditions in six quality measures, emphasising the importance of not only low, but stable latency in digital musical instruments. There was no signiﬁcant diﬀerence in rating between the zero latency condition and 10ms condition. A quantitative analysis of timing accuracy in a metronome task under latency conditions showed no signiﬁcant diﬀerence in mean synchronisation error. This suggests that the 20ms and 10ms with jitter latency conditions degrade subjective impressions of an instrument, but without signiﬁcantly aﬀecting the timing performance of our participants. These ﬁndings are discussed in terms of control intimacy and instrument transparency.},
langid     = {english},
}
@Article{kandus_fisica_nodate,
author       = {Kandus, Alejandra and Gutmann, Friedrich Wolfgang},
journaltitle = {A f},
title        = {A f´ısica das oscilac¸˜oes mecˆanicas em instrumentos musicais: Exemplo do berimbau},
pages        = {7},
abstract     = {In this work it is discussed the wave propagation in an elastic medium, particularly along strings and the air. The basic principles of the wave propagation are presented, taking as an example case a typical instrument from the state of Bahia, well known by all Brazilians, the berimbau.},
langid       = {portuguese},
}
@Article{erkut_finite_nodate,
author   = {Erkut, Cumhur and Karjalainen, Matti},
title    = {Finite difference method vs. digital waveguide method in string instrument modeling and synthesis},
pages    = {8},
abstract = {The one-dimensional digital waveguides, combined with the commuted synthesis method, allow modeling and high-quality synthesis of plucked string instrument tones in a very eﬃcient manner. However, the increasing computational power of the modern processors makes it feasible to experiment with more complex algorithms also for real-time sound synthesis purposes. By certain simpliﬁcations, time-domain methods based on ﬁnite diﬀerences (FDTD) are eﬃcient enough to run in real time on modern processors and yet they are more ﬂexible than the computationally less expensive commuted synthesis. The resulting structures, which are called 1-D FDTD waveguides, have previously been shown to be equal to or to approximate many properties of digital waveguides, including lossless and lossy propagation, input and output ports, terminations, and scattering junctions. The numerical stability of the 1-D FDTD waveguides, as well as their initialization and formation of the traveling waves are well understood. However, a careful comparison between 1-D FDTD waveguides and conventional digital waveguides in terms of their limitations, computational eﬃciency, accuracy, and interaction has not been carried out. The aim of this paper is to ﬁll this gap by highlighting important properties of both methods side by side in string instrument modeling and synthesis. We then try to combine the best properties of both methods and discuss the interaction of the two model structures. Synthetic tones and short musical phrases obtained by both synthesis models will be demonstrated during the presentation. These sound examples will also be available at http://www.acoustics.hut.fi/demos/.},
langid   = {english},
}
@Article{monti_department_2000,
author   = {Monti, Giuliano and Sandler, Mark},
date     = {2000},
title    = {Department of Electronic Engineering, King’s College London, Strand, London WC2R 2LS, UK},
pages    = {4},
abstract = {This paper describes an algorithm, which performs monophonic music transcription. A pitch tracker calculates the fundamental frequency of the signal from the autocorrelation function. A continuity-restoration block takes the extracted pitch and determines the score corresponding to the original performance. The signal envelope analysis completes the transcription system, calculating attack-sustain-decay-release times, which improves the synthesis process. Attention is also paid to the extraction of timbre and wavetable synthesis.},
langid   = {english},
}
@Article{1999AgleWho,
author       = {Agle, Bradley R.},
date         = {1999},
journaltitle = {Academy of Management Journal},
title        = {WHO MATTERS TO CEOS? AN INVESTIGATION OF STAKEHOLDER ATTRIBUTES AND SALIENCE, CORPORATE PERFORMANCE, AND CEO VALUES},
pages        = {19},
file         = {:done/1999AgleWho WHO MATTERS tO CEOS/\\_ aN INVESTIGATION oF STAKEHOLDER ATTRIBUTES aND SALIENCE, CORPORATE PERFORMANCE, aND CEO VALUES.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1999RowlandMissing,
author       = {Rowland, David R. and Pask, Colin},
date         = {1999-05},
journaltitle = {American Journal of Physics},
title        = {The missing wave momentum mystery},
doi          = {10.1119/1.19272},
issn         = {0002-9505, 1943-2909},
number       = {5},
pages        = {378--388},
urldate      = {2019-03-26},
volume       = {67},
file         = {:done/1999RowlandMissing The Missing Wave Momentum Mystery.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2004Costa-giomiEffects,
author       = {Costa-Giomi, Eugenia},
date         = {2004-04},
journaltitle = {Psychology of Music},
title        = {Effects of Three Years of Piano Instruction on Children’s Academic Achievement, School Performance and Self-Esteem},
doi          = {10.1177/0305735604041491},
issn         = {0305-7356, 1741-3087},
number       = {2},
pages        = {139--152},
urldate      = {2019-03-26},
volume       = {32},
file         = {:done/2004Costa-giomiEffects Effects of Three Years of Piano Instruction on Children’s Academic Achievement, School Performance and Self Esteem.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1999SmithBark,
author       = {Smith, J. O. and Abel, J. S.},
date         = {1999-11},
journaltitle = {IEEE Transactions on Speech and Audio Processing},
title        = {Bark and ERB bilinear transforms},
doi          = {10.1109/89.799695},
number       = {6},
pages        = {697--708},
url          = {http://ieeexplore.ieee.org/document/799695/},
urldate      = {2019-03-26},
volume       = {7},
abstract     = {Use of a bilinear conformal map to achieve a frequency warping nearly identical to that of the Bark frequency scale is described. Because the map takes the unit circle to itself, its form is that of the transfer function of a ﬁrst-order allpass ﬁlter. Since it is a ﬁrst-order map, it preserves the model order of rational systems, making it a valuable frequency warping technique for use in audio ﬁlter design. A closed-form weighted-equation-error method is derived which computes the optimal mapping coeﬃcient as a function of sampling rate, and the solution is shown to be generally indistinguishable from the optimal least-squares solution. The optimal Chebyshev mapping is also found to be essentially identical to the optimal least-squares solution. The expression 0.8517*sqrt(atan(0.06583*Fs))-0.1916 is shown to accurately approximate the optimal allpass coeﬃcient as a function of sampling rate Fs in kHz for sampling rates greater than 1 kHz. A ﬁlter design example is included which illustrates improvements due to carrying out the design over a Bark scale. Corresponding results are also given and compared for approximating the related "equivalent rectangular bandwidth (ERB) scale" of Moore and Glasberg using a ﬁrst-order allpass transformation. Due to the higher frequency resolution called for by the ERB scale, particularly at low frequencies, the ﬁrst-order conformal map is less able to follow the desired mapping, and the error is two to three times greater than the Bark-scale case, depending on the sampling rate.},
file         = {:done/1999SmithBark Bark and ERB Bilinear Transforms.pdf:application/pdf},
groups       = {tesse:5},
journal      = {IEEE Transactions on Speech and Audio Processing},
langid       = {english},
publisher    = {Institute of Electrical and Electronics Engineers (IEEE)},
year         = {1999},
}
@Report{2000SerraSound,
author      = {Serra, Xavier and Bonada, Jordi},
date        = {2000},
institution = {Pompeu Fabra University},
title       = {Sound Transformations Based on the SMS High Level Attributes},
type        = {resreport},
pages       = {6},
abstract    = {The basic Spectral Modeling Synthesis (SMS) technique models sounds as the sum of sinusoids plus a residual. Though this analysis/synthesis system has proved to be successful in transforming sounds, more powerful and intuitive musical transformations can be achieved by moving into the SMS high-level attribute plane. In this paper we describe how to extract high level sound attributes from the basic representation, modify them, and add them back before the synthesis stage. In this process new problems come up for which we propose some initial solutions.},
file        = {:done/2000SerraSound Sound Transformations Based on the SMS High Level Attributes.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
}
@Article{1991BrownMusical,
author       = {Brown, Judith C. and Zhang, Bin},
date         = {1991-05},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Musical frequency tracking using the methods of conventional and "narrowed" autocorrelation},
doi          = {10.1121/1.400923},
issn         = {0001-4966},
number       = {5},
pages        = {2346--2354},
urldate      = {2019-03-26},
volume       = {89},
file         = {:done/1991BrownMusical Musical Frequency Tracking Using the Methods of Conventional and "narrowed" Autocorrelation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1976ProvencherFourier,
author       = {Provencher, Stephen},
date         = {1976-01},
journaltitle = {Biophysical Journal},
title        = {A Fourier method for the analysis of exponential decay curves},
doi          = {10.1016/s0006-3495(76)85660-3},
number       = {1},
pages        = {27--41},
url          = {https://www.cell.com/biophysj/pdf/S0006-3495(76)85660-3.pdf},
urldate      = {2019-03-26},
volume       = {16},
abstract     = {A method based on the Fourier convolution theorem is developed for the analysis of data composed of random noise, plus an unknown constant "base line," plus a sum of (or an integral over a continuous spectrum of) exponential decay functions. The Fourier method's usual serious practical limitation of needing high accuracy data over a very wide time range is eliminated by the introduction of convergence parameters and a Gaussian taper window. A computer program is described for the analysis of discrete spectra, where the data involves only a sum of exponentials. The program is completely automatic in that the only necessary inputs are the raw data (not necessarily in equal intervals of time); no potentially biased initial guesses concerning either the number or the values of the components are needed. The outputs include the number of components, the amplitudes and time constants together with their estimated errors, and a spectral plot of the solution. The limiting resolving power of the method is studied by analyzing a wide range of simulated two-, three-, and four-component data. The results seem to indicate that the method is applicable over a considerably wider range of conditions than nonlinear least squares or the method of moments.},
file         = {:done/1976ProvencherFourier A Fourier Method for the Analysis of Exponential Decay Curves.pdf:application/pdf},
groups       = {tesse:5, Acoustics},
journal      = {Biophysical Journal},
langid       = {english},
month        = {January},
publisher    = {Elsevier BV},
year         = {1976},
}
@Article{2015LixinConstruction,
author       = {Lixin, Zhang and Long, Wang},
date         = {2015-10-08},
journaltitle = {The Open Cybernetics ////////\\\& Systemics Journal},
title        = {Construction of the Logistic Regression Estimation Model in Early Warning on Pure Financial Indicators},
doi          = {10.2174/1874110X01509012055},
issn         = {1874-110X},
number       = {1},
pages        = {2055--2059},
url          = {http://benthamopen.com/ABSTRACT/TOCSJ-9-2055},
urldate      = {2019-03-26},
volume       = {9},
abstract     = {In order to establish a reasonable and effective financial crisis early warning model, the article chooses some financial indicators and uses factor analysis to get the common factors to conduct the most salient financial indicators. In the case that multirole linearity is not significant, the article uses the logistic regression to analyse eligible financial data E and obtains the financial crisis warning model. Then the article found that this model has a high predictive accuracy.},
langid       = {english},
}
@Book{2017SchmidEvaluating,
author    = {Schmid, Gian-Marco},
date      = {2017},
title     = {Evaluating the Experiential Quality of Musical Instruments},
doi       = {10.1007/978-3-658-18420-9},
isbn      = {978-3-658-18419-3 978-3-658-18420-9},
location  = {Wiesbaden},
publisher = {Springer Fachmedien Wiesbaden},
urldate   = {2019-03-26},
file      = {:done/2017SchmidEvaluating Evaluating the Experiential Quality of Musical Instruments.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2011JensenComputational,
author    = {Jensen, Finn B. and Kuperman, William A. and Porter, Michael B. and Schmidt, Henrik},
date      = {2011},
title     = {Computational Ocean Acoustics},
doi       = {10.1007/978-1-4419-8678-8},
isbn      = {978-1-4419-8677-1 978-1-4419-8678-8},
location  = {New York, NY},
publisher = {Springer New York},
urldate   = {2019-03-26},
file      = {:done/2011JensenComputational Computational Ocean Acoustics.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{2018LokkiSpecial,
author       = {Lokki, Tapio and Müller, Meinard and Serafin, Stefania and Välimäki, Vesa},
date         = {2018-03-28},
journaltitle = {Applied Sciences},
title        = {Special Issue on "Sound and Music Computing"},
doi          = {10.3390/app8040518},
issn         = {2076-3417},
number       = {4},
pages        = {518},
url          = {http://www.mdpi.com/2076-3417/8/4/518},
urldate      = {2019-03-26},
volume       = {8},
file         = {:done/2018LokkiSpecial Special Issue on "Sound and Music Computing".pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{SalihSecond,
author = {Salih, A.},
title  = {Second-Order Wave Equation},
pages  = {24},
file   = {:done/SalihSecond Second Order Wave Equation.pdf:application/pdf},
groups = {Acoustics},
langid = {english},
}
@Misc{2005KnutWave,
author = {Knut, Andreas Lie},
date   = {2005},
title  = {The Wave Equation in 1D and 2D},
file   = {:done/2005KnutWave The Wave Equation in 1D and 2D.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
pages  = {48},
year   = {2005},
}
@Article{2012SimicInsolvency,
author       = {Simic, D. and Kovacevic, I. and Simic, S.},
date         = {2012-06-01},
journaltitle = {Logic Journal of IGPL},
title        = {Insolvency prediction for assessing corporate financial health},
doi          = {10.1093/jigpal/jzr009},
issn         = {1367-0751, 1368-9894},
number       = {3},
pages        = {536--549},
urldate      = {2019-03-26},
volume       = {20},
abstract     = {The prediction of corporate ﬁnancial failure, crucial for the prevention and mitigation of economic downturns in a national economy, requires the categorization of healthy and unhealthy companies. This study examines the case of Serbia and applies multivariant statistical methods and speciﬁc artiﬁcial neural network architectures—the self-organizing map (SOM)—to assess the corporate ﬁnancial health of various companies. Financial ratios drawn from corporate balance sheets become the independent variables in a multivariate discriminant analysis (MDA). These ﬁnancial ratios and the discriminant Z-score in the MDA form the input for the SOM, which creates a hybrid MDA-SOM model that is capable of predicting corporate ﬁnancial insolvency. The experimental results of this research correctly estimate company ﬁnancial health in 95 /\\% of cases. These are reliable predictions that are comparable with similar studies in other countries.},
file         = {:done/2012SimicInsolvency Insolvency Prediction for Assessing Corporate Financial Health.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2011MokhatabRafieiFinancial,
author       = {Mokhatab Rafiei, F. and Manzari, S. M. and Bostanian, S.},
date         = {2011-08},
journaltitle = {Expert Systems with Applications},
title        = {Financial health prediction models using artificial neural networks, genetic algorithm and multivariate discriminant analysis: Iranian evidence},
doi          = {10.1016/j.eswa.2011.02.082},
issn         = {0957-4174},
number       = {8},
pages        = {10210--10217},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0957417411002880},
urldate      = {2019-03-26},
volume       = {38},
abstract     = {The purpose of this study is to design a model to predict ﬁnancial health of companies. Financial ratios for 180 manufacturing companies quoted in Tehran Stock Exchange for one year (year ended March 21, 2008) have been used. Three models; based on artiﬁcial neural networks (ANN), genetic algorithm (GA), and multiple discriminant analysis (MDA) are utilized to classify the bankrupt from non bankrupt corporations. ANN model achieved 98.6 /\\% and 96.3 /\\% accuracy rates in training and holdout samples, respectively. To evaluate the reliability of the model, the data were examined with genetic algorithm and Multivariate discriminate analysis method. GA model attained only 92.5 /\\% and 91.5 /\\% accuracy rates and MDA reached 80.6 /\\% and 79.9 in training and holdout samples, respectively.},
file         = {:done/2011MokhatabRafieiFinancial Financial Health Prediction Models Using Artificial Neural Networks, Genetic Algorithm and Multivariate Discriminant Analysis/\\_ Iranian Evidence.pdf:application/pdf;:todo/done/2011MokhatabRafieiFinancial Financial Health Prediction Models Using Artificial Neural Networks, Genetic Algorithm and Multivariate Discriminant Analysis/\\_ Iranian Evidence.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Financial health prediction models using artificial neural networks, genetic algorithm and multivariate discriminant analysis},
}
@Article{freitas_universidade_nodate,
author = {Freitas, Jéssica Galdino De},
title  = {UNIVERSIDADE FEDERAL FLUMINENSE ESCOLA DE ENGENHARIA DEPARTAMENTO DE ENGENHARIA DE PRODUÇÃO MESTRADO EM ENGENHARIA DE PRODUÇÃO},
pages  = {133},
langid = {portuguese},
}
@Article{ghosh_comparative_nodate,
author   = {Ghosh, Arka},
title    = {Comparative study of Financial Time Series Prediction By Artificial Neural Network with Gradient Descent Learning},
pages    = {7},
abstract = {Financial forecasting is an example of a signal processing problem which is challenging due to Small sizes, high noise, non-stationarity, and non-linearity,but fast forecasting of stock market price is very important for strategic business planning.Present study is aimed to develop a comparative predictive model with Feedforward Multilayer Artificial Neural Network ///////\\& Recurrent Time Delay Neural Network for the Financial Timeseries Prediction.This study is developed with the help of historical stockprice dataset made available by GoogleFinance.To develop this prediction model Backpropagation method with Gradient Descent learning has been implemented.Finally the Neural Net ,learned with said algorithm is found to be skillful predictor for non-stationary noisy Financial Timeseries.},
langid   = {english},
}
@Article{2015VakhtinaCapital,
author       = {Vakhtina, Elena and Wosnitza, Jan Henrik},
date         = {2015-01},
journaltitle = {Physica A: Statistical Mechanics and its Applications},
title        = {Capital market based warning indicators of bank runs},
doi          = {10.1016/j.physa.2014.07.024},
issn         = {0378-4371},
pages        = {304--320},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0378437114005937},
urldate      = {2019-03-26},
volume       = {417},
abstract     = {In this investigation, we examine the univariate as well as the multivariate capabilities of the log-periodic [super-exponential] power law (LPPL) for the prediction of bank runs. The research is built upon daily CDS spreads of 40 international banks for the period from June 2007 to March 2010, i.e. at the heart of the global financial crisis. For this time period, 20 of the financial institutions received federal bailouts and are labeled as defaults while the remaining institutions are categorized as non-defaults. The employed multivariate pattern recognition approach represents a modification of the CORA3 algorithm. The approach is found to be robust regardless of reasonable changes of its inputs. Despite the fact that distinct alarm indices for banks do not clearly demonstrate predictive capabilities of the LPPL, the synchronized alarm indices confirm the multivariate discriminative power of LPPL patterns in CDS spread developments acknowledged by bootstrap intervals with 70 /\\% confidence level.},
file         = {:done/2015VakhtinaCapital Capital Market Based Warning Indicators of Bank Runs.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2013AliajBank’s,
author       = {Aliaj, Ada and Hoti, Ilir},
date         = {2013-10-01},
journaltitle = {Mediterranean Journal of Social Sciences},
title        = {Bank’s Rating a Need or Necessity in Albanian Banking System},
doi          = {10.5901/mjss.2013.v4n10p490},
issn         = {20399340, 20392117},
url          = {http://www.mcser.org/journal/index.php/mjss/article/view/1219},
urldate      = {2019-03-26},
abstract     = {Financial Transactions have increased and have become more complex over this 20 years. This paper examines the importance of credit ratings assigned to banks in Europe and the positive and negative impacts. We will also see Albanian Banking system over the last 5 years focusing on the risks it faces. The aim of this article is to analyze and identify the importance of Banks’ Rating. Actually, banks are the dominant sector within Albanian financial system, managing more than 95.5 /\\% of total financial assets. Although banks sometimes use internal models as a substitute for credit ratings for their credit assessments, the internal models themselves often tend to rely heavily on ratings for actual or methodological input. Investors’ reliance on credit ratings has increased over the past 30 years. Acquiring information is costly, particularly for fixed income investors, given collective action problems. Thus investors seek to outsource creditworthiness assessments to rating agencies. Finally we will see if there is a need of Banks Rating in Albanian`s Banks, the requirements of Basel III and the regulatory of the Bank of Albania.},
file         = {:done/2013AliajBank’s Bank’s Rating a Need or Necessity in Albanian Banking System.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{los_atendimento_nodate,
author = {Los, Geovana Zimmermann},
title  = {Atendimento de instituições financeiras às recomendações de evidenciação ambiental da Global Reporting Initiative (GRI)},
pages  = {23},
langid = {portuguese},
}
@Article{esumo_alise_nodate,
author   = {Esumo, R},
title    = {ANÁLISE COMPARATIVA DOS RELATÓRIOS DE SUSTENTABILIDADE DO GLOBAL REPORTING INITIATIVE COM ÊNFASE NAS EMPRESAS DE CAPITAL ABERTO COM ATUAÇÃO NO BRASIL},
pages    = {14},
abstract = {The environmental matter has been studied in several areas of the scientific knowledge due the scenery of development,. In this sense, the Global Reporting Initiative (GRI), an independent organization, is developing a strategic world-wide accepted model through guidelines based on principles. The goal of this article is identify the behavioral diversities regarding to the economic and environmental aspects of the partner capital open companies acting in Brazil that adopt GRI's Guidelines. Besides advancing, this global net which is ruled in sustainability allow that the companies choose the indicative they want to report, and also it does not demand the submission of reports to the external audit. The methodology of the research is data rising and bibliographical, with exploratory purpose. It was identified that GRI's Indicators contributes for the sustainable development as well as guarantee the inevitable tendency consisting in what is demonstrated, corresponding to the reality of the open capital companies with performance in Brazil, country that prioritizes GRI's Principles.},
langid   = {portuguese},
}
@Book{2009ClarkePrinciples,
author    = {Clarke, Bertrand S. and Fokoué, Ernest and Zhang, Hao Helen},
date      = {2009},
title     = {Principles and theory for data mining and machine learning},
isbn      = {978-0-387-98134-5 978-0-387-98135-2},
location  = {Dordrecht ; New York},
note      = {OCLC: ocn440103793},
pagetotal = {781},
publisher = {Springer},
series    = {Springer series in statistics},
file      = {:done/2009ClarkePrinciples Principles and Theory for Data Mining and Machine Learning.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Machine learning, Data mining, Data Mining, Maschinelles Lernen, Statistical methods, Statistik},
langid    = {english},
}
@Book{2002AgrestiCategorical,
author    = {Agresti, Alan},
date      = {2002},
title     = {Categorical data analysis},
edition   = {2nd ed},
isbn      = {978-0-471-36093-3},
location  = {New York},
pagetotal = {710},
publisher = {Wiley-Interscience},
series    = {Wiley series in probability and statistics},
file      = {:done/2002AgrestiCategorical Categorical Data Analysis.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Multivariate analysis},
langid    = {english},
}
@Book{2015YeeVector,
author     = {Yee, Thomas W.},
date       = {2015},
title      = {Vector generalized linear and additive models: with an implementation in R},
isbn       = {978-1-4939-2817-0},
location   = {New York, NY},
note       = {OCLC: ocn907271683},
pagetotal  = {589},
publisher  = {Springer},
series     = {Springer series in statistics},
file       = {:done/2015YeeVector Vector Generalized Linear and Additive Models/\\_ with an Implementation in R.pdf:application/pdf;:todo/done/2015YeeVector Vector Generalized Linear and Additive Models/\\_ with an Implementation in R.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Datenverarbeitung, Hochleistungsrechnen, Linear models (Statistics), Lineares Modell, Regression analysis, Statistisches Modell, Vector spaces, Wahrscheinlichkeitstheorie},
langid     = {english},
shorttitle = {Vector generalized linear and additive models},
}
@Article{1979DeMeersmanLeast,
author       = {De Meersman, R.},
date         = {1979-12},
journaltitle = {Journal of Computational and Applied Mathematics},
title        = {Least squares with non-linear equality constraints Application to closing of balances},
doi          = {10.1016/0771-050X(79)90044-5},
issn         = {0377-0427},
number       = {4},
pages        = {277--281},
url          = {https://linkinghub.elsevier.com/retrieve/pii/0771050X79900445},
urldate      = {2019-03-28},
volume       = {5},
abstract     = {It is shown how a least squares problem subject to equality constraints can be replaced by an unconstrained least squares problem. Constraints and equations may be non linear. Results seem to be too complicated to be applied to general cases but can quite successfully be used for special problems like the closing of balances for instance.},
file         = {:done/1979DeMeersmanLeast Least Squares with Non Linear Equality Constraints Application to Closing of Balances.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Book{2014PoularikasAdaptive,
author     = {Poularikas, Alexander},
date       = {2014-09-30},
title      = {Adaptive Filtering: Fundamentals of Least Mean Squares with MATLAB®},
doi        = {10.1201/b17464},
isbn       = {978-1-4822-5335-1 978-1-4822-5336-8},
publisher  = {CRC Press},
url        = {https://www.taylorfrancis.com/books/9781482253368},
urldate    = {2019-03-28},
file       = {:done/2014PoularikasAdaptive Adaptive Filtering/\\_ Fundamentals of Least Mean Squares with MATLAB®.pdf:application/pdf;:todo/done/2014PoularikasAdaptive Adaptive Filtering/\\_ Fundamentals of Least Mean Squares with MATLAB®.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Adaptive Filtering},
}
@Article{2014StankovicInstantaneous,
author       = {Stanković, Ljubiša and Djurović, Igor and Stanković, Srdjan and Simeunović, Marko and Djukanović, Slobodan and Daković, Miloš},
date         = {2014-12},
journaltitle = {Digital Signal Processing},
title        = {Instantaneous frequency in time–frequency analysis: Enhanced concepts and performance of estimation algorithms},
doi          = {10.1016/j.dsp.2014.09.008},
issn         = {1051-2004},
pages        = {1--13},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1051200414002838},
urldate      = {2019-03-28},
volume       = {35},
abstract     = {The instantaneous frequency (IF) is a very important feature of nonstationary signals in numerous applications. The ﬁrst overview of the concept and application of the IF estimators is presented in seminal papers by Boashash. Since then, a signiﬁcant knowledge has been gained about the performance of the IF estimators. This knowledge has been used not only for development of various IF estimators but also for introduction of novel time–frequency (TF) representations. The IF estimation in environments characterized by low signal-to-noise (SNR) has achieved signiﬁcant beneﬁts from these theoretical developments. In this paper, we review some of the most important developments in the last two decades related to the concept of the IF, performance analysis of IF estimators, and development of IF estimators for low SNR environments.},
file         = {:done/2014StankovicInstantaneous Instantaneous Frequency in Time–frequency Analysis//_ Enhanced Concepts and Performance of Estimation Algorithms.pdf:application/pdf;:todo/done/2014StankovicInstantaneous Instantaneous Frequency in Time–frequency Analysis//_ Enhanced Concepts and Performance of Estimation Algorithms.pdf:application/pdf},
groups       = {tesse:5, DSP},
langid       = {english},
shorttitle   = {Instantaneous frequency in time–frequency analysis},
}
@Book{2004StrikwerdaFinite,
author    = {Strikwerda, John C.},
date      = {2004},
title     = {Finite difference schemes and partial differential equations},
edition   = {2nd ed},
isbn      = {978-0-89871-567-5},
location  = {Philadelphia},
pagetotal = {435},
publisher = {Society for Industrial and Applied Mathematics},
file      = {:done/2004StrikwerdaFinite Finite Difference Schemes and Partial Differential Equations.djvu:Djvu},
groups    = {tesse:5, Finite Difference Methods},
keywords  = {Differential equations, Partial, Finite differences, Numerical solutions},
langid    = {english},
}
@Article{zhdanov_solving_nodate,
author       = {Zhdanov, Aleksandr Ivanovich and Gogoleva, Sofya Yuryevna},
journaltitle = {Solving Least Squares Problems},
title        = {Solving Least Squares Problems With Equality Constraints Based On Augmented Regularized Normal Equations},
pages        = {7},
abstract     = {This article is devoted to a new algorithm for solving least squares problems with linear equality constraints. The presented algorithm can help solve large dimension ill-conditioned problems e¢ ciently.},
langid       = {english},
}
@Article{2013RiskusImproved,
author       = {Riškus, Aleksas and Liutkus, Giedrius},
date         = {2013-12-12},
journaltitle = {Information Technology And Control},
title        = {An Improved Algorithm for the Approximation of a Cubic Bezier Curve and its Application for Approximating Quadratic Bezier Curve},
doi          = {10.5755/j01.itc.42.4.1707},
issn         = {2335-884X, 1392-124X},
number       = {4},
url          = {http://www.itc.ktu.lt/index.php/ITC/article/view/1707},
urldate      = {2019-03-28},
volume       = {42},
abstract     = {In this paper an improved version of an earlier proposed algorithm for approximating cubic Bezier curve by a set of circular arcs is presented. It is investigated how the improved algorithm fits for approximation of quadratic Bezier curves. These issues occur in CAD/CAM systems during data exchange into data formats which do not support Bezier curves. Experimental results on examples, widely used in the sources enlisted in references, are presented. Two typographical errors, made in the previous article, are corrected.},
langid       = {english},
}
@Thesis{torin_percussion_2015,
author = {Torin, Alberto},
date   = {2015},
title  = {Percussion Instrument Modelling In 3D: Sound Synthesis Through Time Domain Numerical Simulation},
type   = {phdthesis},
langid = {english},
}
@Book{2011BeckSociedade,
author     = {Beck, Ulrich and Nascimento, Sebastião},
date       = {2011},
title      = {Sociedade de risco: rumo a uma outra modernidade},
isbn       = {978-85-7326-450-0},
location   = {São Paulo},
note       = {OCLC: 753356287},
publisher  = {Editora 34},
file       = {:done/2011BeckSociedade Sociedade De Risco/\\_ Rumo a Uma Outra Modernidade.pdf:application/pdf},
groups     = {tesse:5},
langid     = {portuguese},
shorttitle = {Sociedade de risco},
}
@Book{2017HorstmannScala,
author    = {Horstmann, Cay S.},
date      = {2017},
title     = {Scala for the impatient},
edition   = {Second edition},
isbn      = {978-0-13-454056-6},
location  = {Boston},
note      = {OCLC: ocn964820811},
pagetotal = {359},
publisher = {Addison-Wesley},
file      = {:done/2017HorstmannScala Scala for the Impatient.pdf:application/pdf},
groups    = {tesse:5, Programming},
keywords  = {Computer programming, Functional programming (Computer science), Programming languages (Electronic computers), Scala (Computer program language)},
langid    = {english},
}
@Book{1997DodgeComputer,
author     = {Dodge, Charles and Jerse, Thomas A.},
date       = {1997},
title      = {Computer music: synthesis, composition, and performance},
edition    = {2. ed},
isbn       = {978-0-02-864682-4},
location   = {New York},
note       = {OCLC: 247058281},
pagetotal  = {455},
publisher  = {Schirmer Books [u.a.]},
langid     = {english},
shorttitle = {Computer music},
}
@Book{2014KoenigSpectral,
author    = {David M. Koenig},
date      = {2014-11-13},
title     = {Spectral Analysis of Musical Sounds with Emphasis on the Piano},
doi       = {10.1093/acprof:oso/9780198722908.001.0001},
isbn      = {978-0-19-872290-8},
publisher = {Oxford University Press},
urldate   = {2019-03-28},
file      = {:done/2014KoenigSpectral Spectral Analysis of Musical Sounds with Emphasis on the Piano.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
month     = {nov},
year      = {2014},
}
@Book{2008MontgomeryIntroduction,
author    = {Montgomery, Douglas C. and Jennings, Cheryl L. and Kulahci, Murat},
date      = {2008},
title     = {Introduction to Time Series Analysis and Forecasting},
isbn      = {978-0-471-65397-4},
pages     = {469},
publisher = {Wiley},
url       = {https://www.amazon.com/Introduction-Analysis-Forecasting-Douglas-Montgomery/dp/0471653977?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0471653977},
file      = {:done/2008MontgomeryIntroduction Introduction to Time Series Analysis and Forecasting.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Collection{2018KarasavvoglouEconomy,
date      = {2018},
editor    = {Karasavvoglou, Anastasios and Goić, Srećko and Polychronidou, Persefoni and Delias, Pavlos},
title     = {Economy, Finance and Business in Southeastern and Central Europe},
doi       = {10.1007/978-3-319-70377-0},
isbn      = {978-3-319-70376-3 978-3-319-70377-0},
location  = {Cham},
publisher = {Springer International Publishing},
series    = {Springer Proceedings in Business and Economics},
urldate   = {2019-03-28},
file      = {:done/2018KarasavvoglouEconomy Economy, Finance and Business in Southeastern and Central Europe.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Collection{2006KomaraEncyclopedia,
date      = {2006},
editor    = {Komara, Edward M.},
title     = {Encyclopedia of the blues},
isbn      = {978-0-415-92699-7 978-0-415-92700-0 978-0-415-92701-7},
location  = {New York},
pagetotal = {2},
publisher = {Routledge},
file      = {:done/2006KomaraEncyclopedia Encyclopedia of the Blues.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Biography Dictionaries, Blues (Music), Blues musicians, Encyclopedias},
langid    = {english},
}
@Article{1969HansonExtensions,
author       = {Hanson, Richard J. and Lawson, Charles L.},
date         = {1969-10},
journaltitle = {Mathematics of Computation},
title        = {Extensions and applications of the Householder algorithm for solving linear least squares problems},
doi          = {10.2307/2004965},
number       = {108},
pages        = {787},
url          = {https://www.jstor.org/stable/2004965?origin=crossref},
urldate      = {2019-03-28},
volume       = {23},
file         = {:done/1969HansonExtensions Extensions and Applications of the Householder Algorithm for Solving Linear Least Squares Problems.pdf:application/pdf},
groups       = {tesse:5},
journal      = {Mathematics of Computation},
langid       = {english},
publisher    = {American Mathematical Society (AMS)},
year         = {1969},
}
@Article{GallierFundamentals,
author = {Gallier, Jean and Quaintance, Jocelyn},
title  = {Fundamentals of Linear Algebra and Optimization},
pages  = {947},
file   = {:done/GallierFundamentals Fundamentals of Linear Algebra and Optimization.pdf:application/pdf},
groups = {tesse:2, Linear Algebra},
langid = {english},
}
@Book{2009GesserLibras?,
author     = {Gesser, Audrei},
date       = {2009},
title      = {Libras? que língua é essa?: crenças e preconceitos em torno da língua de sinais e da realidade surda},
isbn       = {978-85-7934-001-7},
location   = {São Paulo},
note       = {OCLC: 817147074},
publisher  = {Parábola Ed.},
langid     = {portuguese},
shorttitle = {Libras?},
}
@Article{myers_smarter_nodate,
author = {Myers, Mark},
title  = {A Smarter Way to Learn JavaScript: The new approach that uses technology to cut your effort in half},
pages  = {288},
langid = {english},
}
@Book{2013GolubMatrix,
author    = {Golub, Gene H. and Van Loan, Charles F.},
date      = {2013},
title     = {Matrix computations},
edition   = {Fourth edition},
isbn      = {978-1-4214-0794-4},
location  = {Baltimore},
note      = {OCLC: ocn824733531},
pagetotal = {756},
publisher = {The Johns Hopkins University Press},
series    = {Johns Hopkins studies in the mathematical sciences},
file      = {:done/2013GolubMatrix Matrix Computations.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Data processing, Matrices},
langid    = {english},
}
@InCollection{2013RossFront,
author    = {Ross, Sheldon M.},
booktitle = {Simulation},
date      = {2013},
title     = {Front Matter},
doi       = {10.1016/B978-0-12-415825-2.00014-0},
isbn      = {978-0-12-415825-2},
pages     = {i--ii},
publisher = {Elsevier},
url       = {https://linkinghub.elsevier.com/retrieve/pii/B9780124158252000140},
urldate   = {2019-03-28},
ean       = {9780124158252},
file      = {:done/2013RossFront Front Matter.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
pagetotal = {310},
year      = {2013},
}
@Article{1959GardnerMethod,
author         = {Gardner, Donald G. and Gardner, Jeanne C. and Laush, George and Meinke, W. Wayne},
date           = {1959},
journaltitle   = {The Journal of Chemical Physics},
title          = {Method for the Analysis of Multicomponent Exponential Decay Curves},
doi            = {10.1063/1.1730560},
issn           = {0021-9606, 1089-7690},
number         = {4},
pages          = {978--986},
urldate        = {2019-03-28},
volume         = {31},
file           = {:done/1959GardnerMethod Method for the Analysis of Multicomponent Exponential Decay Curves.pdf:application/pdf},
groups         = {tesse:5},
keywords       = {qualityAssured},
langid         = {english},
qualityassured = {qualityAssured},
}
@Article{1983KarplusDigital,
author       = {Karplus, Kevin and Strong, Alex},
date         = {1983},
journaltitle = {Computer Music Journal},
title        = {Digital Synthesis of Plucked-String and Drum Timbres},
doi          = {10.2307/3680062},
issn         = {0148-9267},
number       = {2},
pages        = {43},
url          = {https://www.jstor.org/stable/3680062?origin=crossref},
urldate      = {2019-03-28},
volume       = {7},
file         = {:done/1983KarplusDigital Digital Synthesis of Plucked String and Drum Timbres.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1994ChaigneNumericala,
author       = {Chaigne, Antoine and Askenfelt, Anders},
date         = {1994-02},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Numerical simulations of piano strings. A physical model for a struck string using finite difference methods},
doi          = {10.1121/1.408459},
issn         = {0001-4966},
number       = {2},
pages        = {1112--1118},
urldate      = {2019-03-28},
volume       = {95},
langid       = {english},
}
@Article{1998FontanaPhysical,
author       = {Fontana, Federico and Rocchesso, Davide},
date         = {1998},
journaltitle = {acta acustica},
title        = {Physical Modeling of Membranes for Percussion Instruments},
pages        = {15},
volume       = {84},
abstract     = {Recent research on Physical Modeling has led to 2-D discrete-time structures based on the Digital Waveguides. These structures are well suited for efficient yet accurate simulation of wave propagation in an ideal membrane. Nevertheless, real membranes exhibit a different behaviour, due to the environmental conditions and to the material they are made of. In this work we consider some aspects, crucial for the audio signal, of the physical phenomena concerning real membranes, and we will develop a 2-D waveguide model encompassing the effects of these aspects. In order to excite the simulated membrane, we will consider a hammer model previously developed for piano strings, and here adapted to fit the hammer-membrane interaction.},
file         = {:done/1998FontanaPhysical Physical Modeling of Membranes for Percussion Instruments.pdf:application/pdf},
groups       = {tesse:5, Acoustics, Digital Waveguides},
langid       = {english},
}
@Article{2004MoltenoExperimental,
author       = {Molteno, Timothy C. and Tufillaro, Nicholas B.},
date         = {2004-09},
journaltitle = {American Journal of Physics},
title        = {An experimental investigation into the dynamics of a string},
doi          = {10.1119/1.1764557},
issn         = {0002-9505, 1943-2909},
number       = {9},
pages        = {1157--1169},
urldate      = {2019-03-28},
volume       = {72},
file         = {:done/2004MoltenoExperimental An Experimental Investigation into the Dynamics of a String.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2006VaelimaekiDiscrete,
author       = {Välimäki, Vesa and Pakarinen, Jyri and Erkut, Cumhur and Karjalainen, Matti},
date         = {2006-01-01},
journaltitle = {Reports on Progress in Physics},
title        = {Discrete-time modelling of musical instruments},
doi          = {10.1088/0034-4885/69/1/R01},
issn         = {0034-4885, 1361-6633},
number       = {1},
pages        = {1--78},
url          = {http://stacks.iop.org/0034-4885/69/i=1/a=R01?key=crossref.fc944552766588c9a4a07777ff78c7f1},
urldate      = {2019-03-28},
volume       = {69},
abstract     = {This article describes physical modelling techniques that can be used for simulating musical instruments. The methods are closely related to digital signal processing. They discretize the system with respect to time, because the aim is to run the simulation using a computer. The physics-based modelling methods can be classiﬁed as mass–spring, modal, wave digital, ﬁnite difference, digital waveguide and source–ﬁlter models. We present the basic theory and a discussion on possible extensions for each modelling technique. For some methods, a simple model example is chosen from the existing literature demonstrating a typical use of the method. For instance, in the case of the digital waveguide modelling technique a vibrating string model is discussed, and in the case of the wave digital ﬁlter technique we present a classical piano hammer model. We tackle some nonlinear and time-varying models and include new results on the digital waveguide modelling of a nonlinear string. Current trends and future directions in physical modelling of musical instruments are discussed.},
file         = {:done/2006VaelimaekiDiscrete Discrete Time Modelling of Musical Instruments.pdf:application/pdf},
groups       = {tesse:5, Acoustics},
langid       = {english},
}
@Article{2014BilbaoNumerical,
author       = {Bilbao, Stefan and Torin, Alberto and Chatziioannou, Vasileios},
date         = {2014-05-11},
journaltitle = {arXiv:1405.2589 [math]},
title        = {Numerical Modeling of Collisions in Musical Instruments},
eprint       = {1405.2589},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1405.2589},
urldate      = {2019-03-28},
abstract     = {Collisions play an important role in many aspects of the physics of musical instruments. The striking action of a hammer or mallet in keyboard and percussion instruments is perhaps the most important example, but others include reed-beating eﬀects in wind instruments, the string/neck interaction in fretted instruments such as the guitar as well as in the sitar and the wire/membrane interaction in the snare drum. From a simulation perspective, whether the eventual goal is the validation of musical instrument models or sound synthesis, such highly nonlinear problems pose various diﬃculties, not the least of which is the risk of numerical instability. In this article, a novel ﬁnite diﬀerence time domain simulation framework for such collision problems is developed, where numerical stability follows from strict numerical energy conservation or dissipation, and where a a power law formulation for collisions is employed, as a potential function within a Hamiltonian formulation. The power law serves both as a model of deformable collision, and as a mathematical penalty under perfectly rigid, non-deformable collision. This formulation solves a major problem underlying previous work, where a Hamiltonian framework was not employed for collisions, and thus stability was not ensured. Various numerical examples, illustrating the unifying features of such methods across a wide variety of systems in musical acoustics are presented, including numerical stability and energy conservation/dissipation, bounds on spurious penetration in the case of rigid collisions, as well as various aspects of musical instrument physics.},
file         = {:done/2014BilbaoNumerical Numerical Modeling of Collisions in Musical Instruments.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016LyubimovMathematical,
author       = {Lyubimov, N. A. and Zakharov, E. V.},
date         = {2016-03},
journaltitle = {Acoustical Physics},
title        = {Mathematical model of acoustic speech production with mobile walls of the vocal tract},
doi          = {10.1134/S1063771016020093},
issn         = {1063-7710, 1562-6865},
number       = {2},
pages        = {225--234},
urldate      = {2019-03-28},
volume       = {62},
abstract     = {A mathematical speech production model is considered that describes acoustic oscillation prop agation in a vocal tract with mobile walls. The wave field function satisfies the Helmholtz equation with boundary conditions of the third kind (impedance type). The impedance mode corresponds to a three parameter pendulum oscillation model. The experimental research demonstrates the nonlinear character of how the mobility of the vocal tract walls influence the spectral envelope of a speech signal.},
langid       = {english},
}
@Article{2016CostaGraphical,
author       = {Costa, Helder Gomes},
date         = {2016-02-08},
journaltitle = {Journal of Modelling in Management},
title        = {Graphical interpretation of outranking principles: Avoiding misinterpretation results from ELECTRE I},
doi          = {10.1108/JM2-08-2013-0037},
issn         = {1746-5664},
number       = {1},
pages        = {26--42},
urldate      = {2019-03-28},
volume       = {11},
abstract     = {Purpose – This study aims to use a graphical approach to highlight the differences between outranking and preference relationships. The outranking principle is based on a structure of non-dominance, which differs from the usual structures of preferences.},
file         = {:done/2016CostaGraphical Graphical Interpretation of Outranking Principles/\\_ Avoiding Misinterpretation Results from ELECTRE I.pdf:application/pdf;:todo/done/2016CostaGraphical Graphical Interpretation of Outranking Principles/\\_ Avoiding Misinterpretation Results from ELECTRE I.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Graphical interpretation of outranking principles},
}
@InProceedings{1998LairdEfficient,
author     = {Laird, J. and Masri, P. and Canagarajah, C. N.},
booktitle  = {IEE Colloquium on Audio and Music Technology: the Challenge of Creative DSP},
date       = {1998},
title      = {Efficient and accurate synthesis of circular membranes using digital waveguides},
doi        = {10.1049/ic:19980829},
eventtitle = {IEE Colloquium on Audio and Music Technology: the Challenge of Creative DSP},
location   = {London, UK},
pages      = {12--12},
publisher  = {IEE},
urldate    = {2019-03-28},
volume     = {1998},
abstract   = {Digital waveguides may be used to construct efficient physical models of musical instruments and other resonant systems. They are commonly used to model systems with simple boundary - conditions, such as strings and square membranes. Membranes are modelled by connecting digital waveguides together to form a mesh; however when modelling circular membranes, such as those found in percussive drums or microphones, the mesh does not approximate well to the boundary shape. It was found that a two-dimensional triangular mesh structure, with rimguides attached around the edge could overcome this. In this paper several rimguide strategies have been explored. The best of these involves adjusting the phase delay so that the model is accurate for frequencies close to DC. The improvement is particularly noticeable for efficient (low sample rate) implementations.},
file       = {:done/1998LairdEfficient Efficient and Accurate Synthesis of Circular Membranes Using Digital Waveguides.pdf:application/pdf},
groups     = {tesse:5, Digital Waveguides},
langid     = {english},
}
@Article{2007MurphyAcoustic,
author       = {Murphy, D. and Kelloniemi, A. and Mullen, J. and Shelley, S.},
date         = {2007-03},
journaltitle = {IEEE Signal Processing Magazine},
title        = {Acoustic Modeling Using the Digital Waveguide Mesh},
doi          = {10.1109/MSP.2007.323264},
issn         = {1053-5888},
number       = {2},
pages        = {55--66},
url          = {http://ieeexplore.ieee.org/document/4117929/},
urldate      = {2019-03-28},
volume       = {24},
file         = {:done/2007MurphyAcoustic Acoustic Modeling Using the Digital Waveguide Mesh.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2010BankModal,
author       = {Bank, Balázs and Zambon, Stefano and Fontana, Federico},
date         = {2010-05},
journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
title        = {A Modal-Based Real-Time Piano Synthesizer},
doi          = {10.1109/TASL.2010.2040524},
issn         = {1558-7916, 1558-7924},
number       = {4},
pages        = {809--821},
url          = {http://ieeexplore.ieee.org/document/5446595/},
urldate      = {2019-03-28},
volume       = {18},
abstract     = {This paper presents a real-time piano synthesizer where both the transverse and longitudinal motion of the string is modeled by modal synthesis, resulting in a coherent and highly parallel model structure. The paper applies recent developments in piano modeling and focuses on the issues related to practical implementation (e.g., numerical stability, aliasing, and efﬁciency). A strong emphasis is given to modeling nonlinear string vibrations, and a new variation of earlier synthesis techniques is proposed which is particularly well suited for modal synthesis. For soundboard modeling, the possibilities of using FFT-based fast convolution and parallel second-order ﬁlters are discussed. Additionally, the paper describes the details of the software implementation and discusses the computational complexity of each model block. The piano model runs on current computer hardware with full polyphony in real time.},
file         = {:done/2010BankModal A Modal Based Real Time Piano Synthesizer.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2012BilbaoTime,
author       = {Bilbao, Stefan},
date         = {2012-01},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Time domain simulation and sound synthesis for the snare drum},
doi          = {10.1121/1.3651240},
issn         = {0001-4966},
number       = {1},
pages        = {914--925},
urldate      = {2019-03-28},
volume       = {131},
file         = {:done/2012BilbaoTime Time Domain Simulation and Sound Synthesis for the Snare Drum.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2008DiukObject,
author     = {Diuk, Carlos and Cohen, Andre and Littman, Michael L.},
booktitle  = {Proceedings of the 25th international conference on Machine learning - ICML '08},
date       = {2008},
title      = {An object-oriented representation for efficient reinforcement learning},
doi        = {10.1145/1390156.1390187},
eventtitle = {the 25th international conference},
isbn       = {978-1-60558-205-4},
location   = {Helsinki, Finland},
pages      = {240--247},
publisher  = {ACM Press},
url        = {http://portal.acm.org/citation.cfm?doid=1390156.1390187},
urldate    = {2019-03-28},
abstract   = {Rich representations in reinforcement learning have been studied for the purpose of enabling generalization and making learning feasible in large state spaces. We introduce Object-Oriented MDPs (OO-MDPs), a representation based on objects and their interactions, which is a natural way of modeling environments and offers important generalization opportunities. We introduce a learning algorithm for deterministic OO-MDPs and prove a polynomial bound on its sample complexity. We illustrate the performance gains of our representation and algorithm in the wellknown Taxi domain, plus a real-life videogame.},
file       = {:done/2008DiukObject An Object Oriented Representation for Efficient Reinforcement Learning.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@InProceedings{2010IoanaTime,
author     = {Ioana, Cornel and Mars, Jerome I. and Serbanescu, Alexandru and Stankovic, Srdjan},
booktitle  = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
date       = {2010},
title      = {Time-frequency-phase tracking approach : Application to underwater signals in a passive context},
doi        = {10.1109/ICASSP.2010.5495259},
eventtitle = {2010 IEEE International Conference on Acoustics, Speech and Signal Processing},
isbn       = {978-1-4244-4295-9},
location   = {Dallas, TX, USA},
pages      = {5634--5637},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/5495259/},
urldate    = {2019-03-28},
abstract   = {One of the most challenging applications of time-frequency representations deals with the analysis of the signal issued from natural environment. Recently, the interest for passive underwater context increased, basically due to the richness of the information carried out by the natural signals. Taken into account the non-linear multi-component time-frequency behavior of such signals, their analysis is a complex problem. In this paper we introduce a new time-frequency analysis concept that aims to extract the non-linear timefrequency components. The main feature of this technique is the joint use of time-amplitude, time-frequency and timephase information. This is materialized by a short-time polynomial phase modeling and the fusion of local information according to the best locally matches of local cubic frequency modulations. Tests provided on real data illustrate the benefits of the proposed approach.},
langid     = {english},
shorttitle = {Time-frequency-phase tracking approach},
}
@Article{2017LanctotUnified,
author       = {Lanctot, Marc and Zambaldi, Vinicius and Gruslys, Audrunas and Lazaridou, Angeliki and Tuyls, Karl and Perolat, Julien and Silver, David and Graepel, Thore},
date         = {2017-11-02},
journaltitle = {arXiv:1711.00832 [cs]},
title        = {A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning},
eprint       = {1711.00832},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1711.00832},
urldate      = {2019-03-28},
abstract     = {To achieve general intelligence, agents must learn how to interact with others in a shared environment: this is the challenge of multiagent reinforcement learning (MARL). The simplest form is independent reinforcement learning (InRL), where each agent treats its experience as part of its (non-stationary) environment. In this paper, we ﬁrst observe that policies learned using InRL can overﬁt to the other agents’ policies during training, failing to sufﬁciently generalize during execution. We introduce a new metric, joint-policy correlation, to quantify this effect. We describe an algorithm for general MARL, based on approximate best responses to mixtures of policies generated using deep reinforcement learning, and empirical game-theoretic analysis to compute meta-strategies for policy selection. The algorithm generalizes previous ones such as InRL, iterated best response, double oracle, and ﬁctitious play. Then, we present a scalable implementation which reduces the memory requirement using decoupled meta-solvers. Finally, we demonstrate the generality of the resulting policies in two partially observable settings: gridworld coordination games and poker.},
file         = {:done/2017LanctotUnified A Unified Game Theoretic Approach to Multiagent Reinforcement Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{GiannakopoulosDeep,
author   = {Giannakopoulos, Petros},
title    = {A Deep Q-Learning Agent for the L-Game with Variable Batch Training},
pages    = {6},
abstract = {We employ the Deep Q-Learning algorithm with Experience Replay to train an agent capable of achieving a high-level of play in the L-Game while selflearning from low-dimensional states. We also employ variable batch size for training in order to mitigate the loss of the rare reward signal and significantly accelerate training. Despite the large action space due to the number of possible moves, the low-dimensional state space and the rarity of rewards, which only come at the end of a game, DQL is successful in training an agent capable of strong play without the use of any search methods or domain knowledge.},
file     = {:done/GiannakopoulosDeep A Deep Q Learning Agent for the L Game with Variable Batch Training.pdf:application/pdf},
groups   = {tesse:2, Game Theory},
langid   = {english},
}
@Article{hu_multiagent_nodate,
author   = {Hu, Junling and Wellman, Michael P},
title    = {Multiagent Reinforcement LeAarlgnoinrigt:hmTheoretical Framework and an},
pages    = {9},
abstract = {In this paper, we adopt general-sum stochastic games as a framework for multiagent reinforcement learning. Our work extends previous work by Littman on zero-sum stochastic games to a broader framework. We design a multiagent Q-learning method under this framework, and prove that it converges to a Nash equilibrium under speci ed conditions. This algorithm is useful for nding the optimal strategy when there exists a unique Nash equilibrium in the game. When there exist multiple Nash equilibria in the game, this algorithm should be combined with other learning techniques to nd optimal strategies.},
langid   = {english},
}
@Article{horie_neural_nodate,
author   = {Horie, Ryota and Aiyoshi, Eitaro},
title    = {Neural Networks Realization of Searching Models for Nash Equilibr' and Their Application to Associative},
pages    = {6},
abstract = {We propose a new mutually coupled plural Neural Networks (N.N.) modules and its application to associative memories from the view point of noncooperative game theory. First, We propose a new dynamical searching model named Parallel Steepest Descent Method with Braking operators (PSDMB) which searches the Nash Equilibrium (NE) points under [0, 11-intervalor nonnegative constraints. Second, we propose a new mutually coupled plural N.N. modules named Game Neural Networks (GNN) to realize the proposed PSDMB with quadratic objective functions. In Addition, we indicate relations between the PSDMB, the GNN and the Lotka-Volterra equation, Last, for an application of the proposed GNN, we propose two kinds of multi modular associative memories which can associate the combined patterns composed of plural partial patterns: (1) the combined patterns are stored as the NE points and robust for noisy inputs; (2) the circulative sequence of the combined patterns are stored as saddles of a heteroclinic cycle.},
langid   = {english},
}
@Article{erev_predicting_nodate,
author = {Erev, IDo and Roth, Alvin E},
title  = {Predicting How People Play Games: Reinforcement Learning in Experimental Games with Unique, Mixed Strategy Equilibria},
pages  = {35},
langid = {english},
}
@Article{HuNash,
author   = {Hu, Junling and Wellman, Michael P.},
title    = {Nash Q-Learning for General-Sum Stochastic Games},
pages    = {31},
abstract = {We extend Q-learning to a noncooperative multiagent context, using the framework of generalsum stochastic games. A learning agent maintains Q-functions over joint actions, and performs updates based on assuming Nash equilibrium behavior over the current Q-values. This learning protocol provably converges given certain restrictions on the stage games (deﬁned by Q-values) that arise during learning. Experiments with a pair of two-player grid games suggest that such restrictions on the game structure are not necessarily required. Stage games encountered during learning in both grid environments violate the conditions. However, learning consistently converges in the ﬁrst grid game, which has a unique equilibrium Q-function, but sometimes fails to converge in the second, which has three different equilibrium Q-functions. In a comparison of ofﬂine learning performance in both games, we ﬁnd agents are more likely to reach a joint optimal path with Nash Q-learning than with a single-agent Q-learning method. When at least one agent adopts Nash Q-learning, the performance of both agents is better than using single-agent Q-learning. We have also implemented an online version of Nash Q-learning that balances exploration with exploitation, yielding improved performance.},
file     = {:done/HuNash Nash Q Learning for General Sum Stochastic Games.pdf:application/pdf},
groups   = {tesse:2, Game Theory},
langid   = {english},
}
@Article{2016SilverMastering,
author       = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
date         = {2016-01},
journaltitle = {Nature},
title        = {Mastering the game of Go with deep neural networks and tree search},
doi          = {10.1038/nature16961},
issn         = {0028-0836, 1476-4687},
number       = {7587},
pages        = {484--489},
url          = {http://www.nature.com/articles/nature16961},
urldate      = {2019-03-28},
volume       = {529},
file         = {:done/2016SilverMastering Mastering the Game of Go with Deep Neural Networks and Tree Search.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Book{2014KamlerHelping,
author     = {Kamler, Barbara and Thomson, Pat},
date       = {2014},
title      = {Helping doctoral students write: pedagogies for supervision},
edition    = {Second edition},
isbn       = {978-0-415-82348-7 978-0-415-82349-4},
location   = {London ; New York},
pagetotal  = {189},
publisher  = {Routledge},
abstract   = {"Helping Doctoral Students Write offers a proven approach to effective doctoral writing. By treating research as writing and writing as research, the authors offer pedagogical strategies for doctoral supervisors that will assist the production of well-argued and lively dissertations. It is clear that many doctoral candidates find research writing complicated and difficult, but the advice they receive often glosses over the complexities of writing and/or locates the problem in the writer. Kamler and Thomson provide a highly effective framework for scholarly work that is located in personal, institutional and cultural contexts. The pedagogical approach developed in the book is based on the notion of writing as a social practice. This approach allows supervisors to think of doctoral writers as novices who need to learn new ways with words as they enter the discursive practices of scholarly communities. This involves learning sophisticated writing practices with specific sets of conventions and textual characteristics. The authors offer supervisors practical advice on helping with commonly encountered writing tasks such as the proposal, the journal abstract, the literature review and constructing the dissertation argument. The first edition of this book has helped many academics and thousands of research students produce better written material. Now fully updated the second edition includes: Examples from a broader range of academic disciplines A new chapter on writing from the thesis More advice on reading and note taking, performance and conferences, Further information on developing a personal academic writing style, and Advice on the use of social media (blogs, tweets and wikis) to create trans-disciplinary and trans-national networks and conversations"--},
file       = {:done/2014KamlerHelping Helping Doctoral Students Write/\\_ Pedagogies for Supervision.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Dissertations, Academic, EDUCATION / General, EDUCATION / Higher, EDUCATION / Research},
langid     = {english},
shorttitle = {Helping doctoral students write},
}
@Article{2007SgroiNeural,
author       = {Sgroi, Daniel and Zizzo, Daniel J.},
date         = {2007-03},
journaltitle = {Physica A: Statistical Mechanics and its Applications},
title        = {Neural networks and bounded rationality},
doi          = {10.1016/j.physa.2006.10.026},
issn         = {0378-4371},
number       = {2},
pages        = {717--725},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S037843710601048X},
urldate      = {2019-03-28},
volume       = {375},
abstract     = {Traditionally the emphasis in neural network research has been on improving their performance as a means of pattern recognition. Here we take an alternative approach and explore the remarkable similarity between the under-performance of neural networks trained to behave optimally in economic situations and observed human performance in the laboratory under similar circumstances. In particular, we show that neural networks are consistent with observed laboratory play in two very important senses. Firstly, they select a rule for behavior which appears very similar to that used by laboratory subjects. Secondly, using this rule they perform optimally only approximately 60 /\\% of the time.},
file         = {:done/2007SgroiNeural Neural Networks and Bounded Rationality.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2008MarchioriPredicting,
author       = {Marchiori, D. and Warglien, M.},
date         = {2008-02-22},
journaltitle = {Science},
title        = {Predicting Human Interactive Learning by Regret-Driven Neural Networks},
doi          = {10.1126/science.1151185},
issn         = {0036-8075, 1095-9203},
number       = {5866},
pages        = {1111--1113},
urldate      = {2019-03-28},
volume       = {319},
file         = {:done/2008MarchioriPredicting Predicting Human Interactive Learning by Regret Driven Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{SpiliopoulosLearning,
author   = {Spiliopoulos, Leonidas},
title    = {Learning backward induction: a neural network agent approach},
pages    = {10},
abstract = {This paper addresses the question of whether neural networks (NNs), a realistic cognitive model of human information processing, can learn to backward induce in a two-stage game with a unique subgame-perfect Nash equilibrium. The NNs were found to predict the Nash equilibrium approximately 70 /\\% of the time in new games. Similarly to humans, the neural network agents are also found to suffer from subgame and truncation inconsistency, supporting the contention that they are appropriate models of general learning in humans. The agents were found to behave in a bounded rational manner as a result of the endogenous emergence of decision heuristics. In particular a very simple heuristic socialmax, that chooses the cell with the highest social payoff explains their behavior approximately 60 /\\% of the time, whereas the ownmax heuristic that simply chooses the cell with the maximum payoff for that agent fares worse explaining behavior roughly 38 /\\%, albeit still signiﬁcantly better than chance. These two heuristics were found to be ecologically valid for the backward induction problem as they predicted the Nash equilibrium in 67 /\\% and 50 /\\% of the games respectively. Compared to various standard classiﬁcation algorithms, the NNs were found to be only slightly more accurate than standard discriminant analyses. However, the latter do not model the dynamic learning process and have an ad hoc postulated functional form. In contrast, a NN agent’s behavior evolves with experience and is capable of taking on any functional form according to the universal approximation theorem.},
file     = {:done/SpiliopoulosLearning Learning Backward Induction\\_ a Neural Network Agent Approach.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@InProceedings{2009BabesQ,
author    = {Babes, Monica and Wunder, Michael and Littman, Michael},
booktitle = {Proc. of 8th Int. Conf. on Autonomous Agents and Multiagent Systems (AAMAS 2009)},
date      = {2009},
title     = {Q-learning in Two-Player Two-Action Games},
pages     = {4},
abstract  = {Q-learning is a simple, powerful algorithm for behavior learning. It was derived in the context of single agent decision making in Markov decision process environments, but its applicability is much broader—in experiments in multiagent environments, Q-learning has also performed well. Our preliminary analysis using dynamical systems ﬁnds that Qlearning’s indirect control of behavior via estimates of value contributes to its beneﬁcial performance in general-sum 2player games like the Prisoner’s Dilemma.},
file      = {:done/2009BabesQ Q Learning in Two Player Two Action Games.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{2010WierstraRecurrent,
author       = {Wierstra, D. and Forster, A. and Peters, J. and Schmidhuber, J.},
date         = {2010-10-01},
journaltitle = {Logic Journal of IGPL},
title        = {Recurrent policy gradients},
doi          = {10.1093/jigpal/jzp049},
issn         = {1367-0751, 1368-9894},
number       = {5},
pages        = {620--634},
urldate      = {2019-03-28},
volume       = {18},
abstract     = {Reinforcement learning for partially observable Markov decision problems (POMDPs) is a challenge as it requires policies with an internal state. Traditional approaches suﬀer signiﬁcantly from this shortcoming and usually make strong assumptions on the problem domain such as perfect system models, state-estimators and a Markovian hidden system. Recurrent neural networks (RNNs) oﬀer a natural framework for dealing with policy learning using hidden state and require only few limiting assumptions. As they can be trained well using gradient descent, they are suited for policy gradient approaches.},
file         = {:done/2010WierstraRecurrent Recurrent Policy Gradients.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2010VassiliadesMultiagent,
author     = {Vassiliades, Vassilis and Christodoulou, Chris},
booktitle  = {The 2010 International Joint Conference on Neural Networks (IJCNN)},
date       = {2010-07},
title      = {Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma: Fast cooperation through evolved payoffs},
doi        = {10.1109/IJCNN.2010.5596937},
eventtitle = {2010 International Joint Conference on Neural Networks (IJCNN)},
isbn       = {978-1-4244-6916-1},
location   = {Barcelona, Spain},
pages      = {1--8},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/5596937/},
urldate    = {2019-03-28},
abstract   = {In this paper, we investigate the importance of rewards in Multiagent Reinforcement Learning in the context of the Iterated Prisoner's Dilemma. We use an evolutionary algorithm to evolve valid payoff structures with the aim of encouraging mutual cooperation. An exhaustive analysis is performed by investigating the effect of: i) the lower and upper bounds of the search space of the payoff values, ii) the reward sign, iii) the population size, and iv) the mutation operators used. Our results indicate that valid structures that encourage cooperation can quickly be obtained, while their analysis shows that: i) they should contain a mixture of positive and negative values and ii) the magnitude of the positive values should be much smaller than the magnitude of the negative values.},
file       = {:done/2010VassiliadesMultiagent Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma/\\_ Fast Cooperation through Evolved Payoffs.pdf:application/pdf;:todo/done/2010VassiliadesMultiagent Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma/\\_ Fast Cooperation through Evolved Payoffs.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma},
}
@Article{2014HeNeural,
author       = {He, Xing and Yu, Junzhi and Huang, Tingwen and Li, Chuandong and Li, Chaojie},
date         = {2014-09},
journaltitle = {Neural Networks},
title        = {Neural network for solving Nash equilibrium problem in application of multiuser power control},
doi          = {10.1016/j.neunet.2014.06.002},
issn         = {0893-6080},
pages        = {73--78},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014001348},
urldate      = {2019-03-28},
volume       = {57},
abstract     = {In this paper, based on an equivalent mixed linear complementarity problem, we propose a neural network to solve multiuser power control optimization problems (MPCOP), which is modeled as the noncooperative Nash game in modern digital subscriber line (DSL). If the channel crosstalk coeﬃcients matrix is positive semideﬁnite, it is shown that the proposed neural network is stable in the sense of Lyapunov and global convergent to an Nash equilibrium, and the Nash equilibrium is unique if the channel crosstalk coeﬃcients matrix is positive deﬁnite. Finally, simulation results on two numerical examples show the eﬀectiveness and performance of the proposed neural network.},
langid       = {english},
}
@Article{2015NarasimhanLanguage,
author       = {Narasimhan, Karthik and Kulkarni, Tejas and Barzilay, Regina},
date         = {2015-06-30},
journaltitle = {arXiv:1506.08941 [cs]},
title        = {Language Understanding for Text-based Games Using Deep Reinforcement Learning},
eprint       = {1506.08941},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1506.08941},
urldate      = {2019-03-28},
abstract     = {In this paper, we consider the task of learning control policies for text-based games. In these games, all interactions in the virtual world are through text and the underlying state is not observed. The resulting language barrier makes such environments challenging for automatic game players. We employ a deep reinforcement learning framework to jointly learn state representations and action policies using game rewards as feedback. This framework enables us to map text descriptions into vector representations that capture the semantics of the game states. We evaluate our approach on two game worlds, comparing against baselines using bag-of-words and bag-of-bigrams for state representations. Our algorithm outperforms the baselines on both worlds demonstrating the importance of learning expressive representations.},
file         = {:done/2015NarasimhanLanguage Language Understanding for Text Based Games Using Deep Reinforcement Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{schuurmans_deep_nodate,
author   = {Schuurmans, Dale and Zinkevich, Martin A},
title    = {Deep Learning Games},
pages    = {9},
abstract = {We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods. For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game. We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem. Based on these connections we investigate alternative learning methods, and ﬁnd that regret matching can achieve competitive training performance while producing sparser models than current deep learning strategies.},
langid   = {english},
}
@Article{2015HausknechtDeep,
author       = {Hausknecht, Matthew and Stone, Peter},
date         = {2015-07-23},
journaltitle = {arXiv:1507.06527 [cs]},
title        = {Deep Recurrent Q-Learning for Partially Observable MDPs},
eprint       = {1507.06527},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1507.06527},
urldate      = {2019-03-28},
abstract     = {Deep Reinforcement Learning has yielded proﬁcient controllers for complex tasks. However, these controllers have limited memory and rely on being able to perceive the complete game screen at each decision point. To address these shortcomings, this article investigates the effects of adding recurrency to a Deep Q-Network (DQN) by replacing the ﬁrst post-convolutional fully-connected layer with a recurrent LSTM. The resulting Deep Recurrent Q-Network (DRQN), although capable of seeing only a single frame at each timestep, successfully integrates information through time and replicates DQN’s performance on standard Atari games and partially observed equivalents featuring ﬂickering game screens. Additionally, when trained with partial observations and evaluated with incrementally more complete observations, DRQN’s performance scales as a function of observability. Conversely, when trained with full observations and evaluated with partial observations, DRQN’s performance degrades less than DQN’s. Thus, given the same length of history, recurrency is a viable alternative to stacking a history of frames in the DQN’s input layer and while recurrency confers no systematic advantage when learning to play the game, the recurrent net can better adapt at evaluation time if the quality of observations changes.},
file         = {:done/2015HausknechtDeep Deep Recurrent Q Learning for Partially Observable MDPs.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017UrbanGaussian,
author       = {Urban, Sebastian and Basalla, Marcus and van der Smagt, Patrick},
date         = {2017-11-29},
journaltitle = {arXiv:1711.11059 [cs, stat]},
title        = {Gaussian Process Neurons Learn Stochastic Activation Functions},
eprint       = {1711.11059},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1711.11059},
urldate      = {2019-03-28},
abstract     = {We propose stochastic, non-parametric activation functions that are fully learnable and individual to each neuron. Complexity and the risk of overﬁtting are controlled by placing a Gaussian process prior over these functions. The result is the Gaussian process neuron, a probabilistic unit that can be used as the basic building block for probabilistic graphical models that resemble the structure of neural networks. The proposed model can intrinsically handle uncertainties in its inputs and self-estimate the conﬁdence of its predictions. Using variational Bayesian inference and the central limit theorem, a fully deterministic loss function is derived, allowing it to be trained as efﬁciently as a conventional neural network using mini-batch gradient descent. The posterior distribution of activation functions is inferred from the training data alongside the weights of the network.},
file         = {:done/2017UrbanGaussian Gaussian Process Neurons Learn Stochastic Activation Functions.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017HarperReinforcement,
author       = {Harper, Marc and Knight, Vincent and Jones, Martin and Koutsovoulos, Georgios and Glynatsi, Nikoleta E. and Campbell, Owen},
date         = {2017-12-11},
journaltitle = {PLOS ONE},
title        = {Reinforcement learning produces dominant strategies for the Iterated Prisoner’s Dilemma},
doi          = {10.1371/journal.pone.0188046},
editor       = {Deng, Yong},
issn         = {1932-6203},
number       = {12},
pages        = {e0188046},
urldate      = {2019-03-28},
volume       = {12},
langid       = {english},
}
@InProceedings{2017KorukhovaTraining,
author     = {Korukhova, Yulia and Kuryshev, Sergey},
booktitle  = {Proceedings of the 9th International Conference on Agents and Artificial Intelligence},
date       = {2017},
title      = {Training Agents with Neural Networks in Systems with Imperfect Information:},
doi        = {10.5220/0006242102960301},
eventtitle = {9th International Conference on Agents and Artificial Intelligence},
isbn       = {978-989-758-219-6 978-989-758-220-2},
location   = {Porto, Portugal},
pages      = {296--301},
publisher  = {SCITEPRESS - Science and Technology Publications},
url        = {http://www.scitepress.org/DigitalLibrary/Link.aspx?doi=10.5220/0006242102960301},
urldate    = {2019-03-28},
abstract   = {Multi-agent Systems, Neural Networks, Dominated Strategies.},
file       = {:done/2017KorukhovaTraining Training Agents with Neural Networks in Systems with Imperfect Information/\\_.pdf:application/pdf;:todo/done/2017KorukhovaTraining Training Agents with Neural Networks in Systems with Imperfect Information/\\_.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Training Agents with Neural Networks in Systems with Imperfect Information},
}
@Article{KamraPolicy,
author   = {Kamra, Nitin and Gupta, Umang and Fang, Fei and Liu, Yan and Tambe, Milind},
title    = {Policy Learning for Continuous Space Security Games Using Neural Networks},
pages    = {10},
abstract = {A wealth of algorithms centered around (integer) linear programming have been proposed to compute equilibrium strategies in security games with discrete states and actions. However, in practice many domains possess continuous state and action spaces. In this paper, we consider a continuous space security game model with inﬁnite-size action sets for players and present a novel deep learning based approach to extend the existing toolkit for solving security games. Specifically, we present (i) OptGradFP, a novel and general algorithm that searches for the optimal defender strategy in a parameterized continuous search space, and can also be used to learn policies over multiple game states simultaneously; (ii) OptGradFP-NN, a convolutional neural network based implementation of OptGradFP for continuous space security games. We demonstrate the potential to predict good defender strategies via experiments and analysis of OptGradFP and OptGradFP-NN on discrete and continuous game settings.},
file     = {:done/KamraPolicy Policy Learning for Continuous Space Security Games Using Neural Networks.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2018WangTowards,
author       = {Wang, Weixun and Hao, Jianye and Wang, Yixi and Taylor, Matthew},
date         = {2018-02-28},
journaltitle = {arXiv:1803.00162 [cs]},
title        = {Towards Cooperation in Sequential Prisoner's Dilemmas: a Deep Multiagent Reinforcement Learning Approach},
eprint       = {1803.00162},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1803.00162},
urldate      = {2019-03-28},
abstract     = {The Iterated Prisoner’s Dilemma has guided research on social dilemmas for decades. However, it distinguishes between only two atomic actions: cooperate and defect. In real-world prisoner’s dilemmas, these choices are temporally extended and different strategies may correspond to sequences of actions, reﬂecting grades of cooperation. We introduce a Sequential Prisoner’s Dilemma (SPD) game to better capture the aforementioned characteristics. In this work, we propose a deep multiagent reinforcement learning approach that investigates the evolution of mutual cooperation in SPD games. Our approach consists of two phases. The ﬁrst phase is ofﬂine: it synthesizes policies with different cooperation degrees and then trains a cooperation degree detection network. The second phase is online: an agent adaptively selects its policy based on the detected degree of opponent cooperation. The effectiveness of our approach is demonstrated in two representative SPD 2D games: the ApplePear game and the Fruit Gathering game. Experimental results show that our strategy can avoid being exploited by exploitative opponents and achieve cooperation with cooperative opponents.},
langid       = {english},
shorttitle   = {Towards Cooperation in Sequential Prisoner's Dilemmas},
}
@Article{2017HausknechtMachine,
author       = {Hausknecht, Matthew and Li, Wen-Ke and Mauk, Michael and Stone, Peter},
date         = {2017-03},
journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
title        = {Machine Learning Capabilities of a Simulated Cerebellum},
doi          = {10.1109/TNNLS.2015.2512838},
issn         = {2162-237X, 2162-2388},
number       = {3},
pages        = {510--522},
url          = {http://ieeexplore.ieee.org/document/7393590/},
urldate      = {2019-03-28},
volume       = {28},
abstract     = {This article describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks - eyelid conditioning, pendulum balancing, PID control, robot balancing, pattern recognition, and MNIST handwritten digit recognition. These tasks span several paradigms of machine learning including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, reinforcement learning and temporal pattern recognition both prove problematic due to the delayed nature of error signals and the simulator’s inability to solve the credit assignment problem. These results are consistent with previous ﬁndings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning while the cerebellum handles supervised learning.},
file         = {:done/2017HausknechtMachine Machine Learning Capabilities of a Simulated Cerebellum.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Collection{1995ChauvinBackpropagation,
date       = {1995},
editor     = {Chauvin, Yves and Rumelhart, David E.},
title      = {Backpropagation: theory, architectures, and applications},
isbn       = {978-0-8058-1258-9 978-0-8058-1259-6},
location   = {Hillsdale, N.J},
pagetotal  = {561},
publisher  = {Lawrence Erlbaum Associates},
series     = {Developments in connectionist theory},
file       = {:done/1995ChauvinBackpropagation Backpropagation/\\_ Theory, Architectures, and Applications.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Back propagation (Artificial intelligence)},
langid     = {english},
shorttitle = {Backpropagation},
}
@Unpublished{2014SathyanarayanaGentle,
author = {Sathyanarayana, Shashi},
date   = {2014},
title  = {A Gentle Introduction to Backpropagation},
file   = {:done/2014SathyanarayanaGentle A Gentle Introduction to Backpropagation.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
pages  = {15},
}
@Article{2016ZagoruykoWide,
author       = {Zagoruyko, Sergey and Komodakis, Nikos},
date         = {2016-05-23},
journaltitle = {arXiv:1605.07146 [cs]},
title        = {Wide Residual Networks},
eprint       = {1605.07146},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1605.07146},
urldate      = {2019-03-28},
abstract     = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efﬁciency all previous deep residual networks, including thousand-layerdeep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and signiﬁcant improvements on ImageNet. Our code and models are available at https: //github.com/szagoruyko/wide-residual-networks.},
file         = {:done/2016ZagoruykoWide Wide Residual Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2018DonahueAdversarial,
author       = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
date         = {2018-02-12},
journaltitle = {arXiv:1802.04208 [cs]},
title        = {Adversarial Audio Synthesis},
eprint       = {1802.04208},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1802.04208},
urldate      = {2019-03-28},
abstract     = {While Generative Adversarial Networks (GANs) have seen wide success at the problem of synthesizing realistic images, they have seen little application to audio generation. Unlike for images, a barrier to success is that the best discriminative representations for audio tend to be non-invertible, and thus cannot be used to synthesize listenable outputs. In this paper, we introduce WaveGAN, a ﬁrst attempt at applying GANs to raw audio synthesis in an unsupervised setting. Our experiments on speech demonstrate that WaveGAN can produce intelligible words from a small vocabulary of human speech, as well as synthesize audio from other domains such as bird vocalizations, drums, and piano. Qualitatively, we ﬁnd that human judges prefer the generated examples from WaveGAN over those from a method which na¨ıvely apply GANs on image-like audio feature representations.},
file         = {:done/2018DonahueAdversarial Adversarial Audio Synthesis.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{1999SilvescuFourier,
author     = {Silvescu, A.},
booktitle  = {IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
date       = {1999},
title      = {Fourier neural networks},
doi        = {10.1109/IJCNN.1999.831544},
eventtitle = {International Conference on Neural Networks},
isbn       = {978-0-7803-5529-3},
location   = {Washington, DC, USA},
pages      = {488--491},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/831544/},
urldate    = {2019-03-28},
volume     = {1},
abstract   = {A new kind of neuron model that has a Fourier-like IN/OUT function is introduced. The model is discussed in a general theoretical framework and some completeness theorems are presented. Current experimental results show that the new model outperforms by a large margin both in representational power and convergence speed the classical mathematical model of neuron based on weighted sum of inputs ﬁltered by a nonlinear function. The new model is also appealing from a neurophysiological point of view because it produces a more realistic representation by considering the inputs as oscillations.},
file       = {:done/1999SilvescuFourier Fourier Neural Networks.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Article{2002StanleyEvolving,
author       = {Stanley, Kenneth O. and Miikkulainen, Risto},
date         = {2002-06},
journaltitle = {Evolutionary Computation},
title        = {Evolving Neural Networks through Augmenting Topologies},
doi          = {10.1162/106365602320169811},
issn         = {1063-6560, 1530-9304},
number       = {2},
pages        = {99--127},
urldate      = {2019-03-28},
volume       = {10},
abstract     = {An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT) that outperforms the best ﬁxed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efﬁciency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signiﬁcantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution.},
file         = {:done/2002StanleyEvolving Evolving Neural Networks through Augmenting Topologies.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2007YadavTime,
author       = {Yadav, R. N. and Kalra, P. K. and John, J.},
date         = {2007-08},
journaltitle = {Applied Soft Computing},
title        = {Time series prediction with single multiplicative neuron model},
doi          = {10.1016/j.asoc.2006.01.003},
issn         = {1568-4946},
number       = {4},
pages        = {1157--1163},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S156849460600007X},
urldate      = {2019-03-28},
volume       = {7},
abstract     = {Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network.},
file         = {:done/2007YadavTime Time Series Prediction with Single Multiplicative Neuron Model.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2013GravesGenerating,
author       = {Graves, Alex},
date         = {2013-08-04},
journaltitle = {arXiv:1308.0850 [cs]},
title        = {Generating Sequences With Recurrent Neural Networks},
eprint       = {1308.0850},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1308.0850},
urldate      = {2019-03-28},
abstract     = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
file         = {:done/2013GravesGenerating Generating Sequences with Recurrent Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2013LinNetwork,
author       = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
date         = {2013-12-16},
journaltitle = {arXiv:1312.4400 [cs]},
title        = {Network In Network},
eprint       = {1312.4400},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1312.4400},
urldate      = {2019-03-28},
abstract     = {We propose a novel deep network structure called "Network In Network"(NIN) to enhance model discriminability for local patches within the receptive ﬁeld. The conventional convolutional layer uses linear ﬁlters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive ﬁeld. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classiﬁcation layer, which is easier to interpret and less prone to overﬁtting than traditional fully connected layers. We demonstrated the state-of-the-art classiﬁcation performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
file         = {:done/2013LinNetwork Network in Network.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{hahn_expressive_nodate,
author = {Hahn, Henrik},
title  = {Expressive sampling synthesis. Learning extended source-filter models from instrument sound databases for expressive sample manipulations},
pages  = {193},
langid = {english},
}
@Article{2016WangFinancial,
author       = {Wang, Jie and Wang, Jun and Fang, Wen and Niu, Hongli},
date         = {2016},
journaltitle = {Computational Intelligence and Neuroscience},
title        = {Financial Time Series Prediction Using Elman Recurrent Random Neural Networks},
doi          = {10.1155/2016/4742515},
issn         = {1687-5265, 1687-5273},
pages        = {1--14},
url          = {http://www.hindawi.com/journals/cin/2016/4742515/},
urldate      = {2019-03-28},
volume       = {2016},
file         = {:done/2016WangFinancial Financial Time Series Prediction Using Elman Recurrent Random Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{NayebiGruva,
author   = {Nayebi, Aran and Vitelli, Matt},
title    = {GRUV: Algorithmic Music Generation using Recurrent Neural Networks},
pages    = {6},
abstract = {We compare the performance of two different types of recurrent neural networks (RNNs) for the task of algorithmic music generation, with audio waveforms as input. In particular, we focus on RNNs that have a sophisticated gating mechanism, namely, the Long Short-Term Memory (LSTM) network and the recently introduced Gated Recurrent Unit (GRU). Our results indicate that the generated outputs of the LSTM network were signiﬁcantly more musically plausible than those of the GRU.},
file     = {:done/NayebiGruva GRUV\\_ Algorithmic Music Generation Using Recurrent Neural Networks.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2016BarrowCross,
author       = {Barrow, Devon K. and Crone, Sven F.},
date         = {2016-10},
journaltitle = {International Journal of Forecasting},
title        = {Cross-validation aggregation for combining autoregressive neural network forecasts},
doi          = {10.1016/j.ijforecast.2015.12.011},
issn         = {0169-2070},
number       = {4},
pages        = {1120--1137},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0169207016300188},
urldate      = {2019-03-28},
volume       = {32},
abstract     = {This paper evaluates k-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to k-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
langid       = {english},
}
@Article{2016ShinDeep,
author       = {Shin, Hoo-Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
date         = {2016-05},
journaltitle = {IEEE Transactions on Medical Imaging},
title        = {Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning},
doi          = {10.1109/TMI.2016.2528162},
issn         = {0278-0062, 1558-254X},
number       = {5},
pages        = {1285--1298},
url          = {http://ieeexplore.ieee.org/document/7404017/},
urldate      = {2019-03-28},
volume       = {35},
abstract     = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets (i.e. ImageNet) and the revival of deep convolutional neural networks (CNN). CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufﬁcient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classiﬁcation: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised ﬁne-tuning. Another effective method is transfer learning, i.e., ﬁne-tuning CNN models (supervised) pre-trained from natural image dataset to medical image tasks (although domain transfer between two medical image datasets is also possible).},
langid       = {english},
shorttitle   = {Deep Convolutional Neural Networks for Computer-Aided Detection},
}
@Article{2016DingFacenet2expnet,
author       = {Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
date         = {2016-09-21},
journaltitle = {arXiv:1609.06591 [cs]},
title        = {FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition},
eprint       = {1609.06591},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1609.06591},
urldate      = {2019-03-28},
abstract     = {Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although ﬁne-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redundant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We ﬁrst propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the reﬁning stage, we append fullyconnected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
file         = {:done/2016DingFacenet2expnet FaceNet2ExpNet/\\_ Regularizing a Deep Face Recognition Net for Expression Recognition.pdf:application/pdf;:todo/done/2016DingFacenet2expnet FaceNet2ExpNet/\\_ Regularizing a Deep Face Recognition Net for Expression Recognition.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {FaceNet2ExpNet},
}
@Article{2016WuInvestigating,
author       = {Wu, Zhizheng and King, Simon},
date         = {2016-01-11},
journaltitle = {arXiv:1601.02539 [cs]},
title        = {Investigating gated recurrent neural networks for speech synthesis},
eprint       = {1601.02539},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1601.02539},
urldate      = {2019-03-28},
abstract     = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve signiﬁcantly better performance on SPSS than deep feedforward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simpliﬁed architecture. The simpliﬁed architecture has signiﬁcantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
file         = {:done/2016WuInvestigating Investigating Gated Recurrent Neural Networks for Speech Synthesis.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017ValinHybrid,
author       = {Valin, Jean-Marc},
date         = {2017-09-24},
journaltitle = {arXiv:1709.08243 [cs, eess]},
title        = {A Hybrid DSP/Deep Learning Approach to Real-Time Full-Band Speech Enhancement},
eprint       = {1709.08243},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1709.08243},
urldate      = {2019-03-28},
abstract     = {Despite noise suppression being a mature area in signal processing, it remains highly dependent on ﬁne tuning of estimator algorithms and parameters. In this paper, we demonstrate a hybrid DSP/deep learning approach to noise suppression. A deep neural network with four hidden layers is used to estimate ideal critical band gains, while a more traditional pitch ﬁlter attenuates noise between pitch harmonics. The approach achieves signiﬁcantly higher quality than a traditional minimum mean squared error spectral estimator, while keeping the complexity low enough for real-time operation at 48 kHz on a low-power processor.},
langid       = {english},
}
@Article{2017JanaiComputer,
author       = {Janai, Joel and Güney, Fatma and Behl, Aseem and Geiger, Andreas},
date         = {2017-04-18},
journaltitle = {arXiv:1704.05519 [cs]},
title        = {Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art},
eprint       = {1704.05519},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1704.05519},
urldate      = {2019-03-28},
abstract     = {Recent years have witnessed amazing progress in AI related ﬁelds such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing ﬁeld, however, it becomes increasingly diﬃcult to stay up-to-date or enter the ﬁeld as a beginner. While several topic speciﬁc survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several speciﬁc topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we ﬁrst provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
file         = {:done/2017JanaiComputer Computer Vision for Autonomous Vehicles/\\_ Problems, Datasets and State of the Art.pdf:application/pdf;:todo/done/2017JanaiComputer Computer Vision for Autonomous Vehicles/\\_ Problems, Datasets and State of the Art.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Computer Vision for Autonomous Vehicles},
}
@Article{2016SaxenaConvolutional,
author       = {Saxena, Shreyas and Verbeek, Jakob},
date         = {2016-06-08},
journaltitle = {arXiv:1606.02492 [cs]},
title        = {Convolutional Neural Fabrics},
eprint       = {1606.02492},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1606.02492},
urldate      = {2019-03-28},
abstract     = {Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on backpropagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classiﬁcation on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
file         = {:done/2016SaxenaConvolutional Convolutional Neural Fabrics.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{ChenAccelerator,
author = {Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
title  = {Accelerator for Deep Convolutional Neural Networks},
pages  = {4},
file   = {:done/ChenAccelerator Accelerator for Deep Convolutional Neural Networks.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Article{2016ZhaoPeak,
author       = {Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
date         = {2016-07-24},
journaltitle = {arXiv:1607.06997 [cs]},
title        = {Peak-Piloted Deep Network for Facial Expression Recognition},
eprint       = {1607.06997},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1607.06997},
urldate      = {2019-03-28},
abstract     = {Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A specialpurpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper deﬁnition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
file         = {:done/2016ZhaoPeak Peak Piloted Deep Network for Facial Expression Recognition.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1983JaffeExtensions,
author       = {Jaffe, David A. and Smith, Julius O.},
date         = {1983},
journaltitle = {Computer Music Journal},
title        = {Extensions of the Karplus-Strong Plucked-String Algorithm},
doi          = {10.2307/3680063},
issn         = {0148-9267},
number       = {2},
pages        = {56--69},
url          = {https://www.jstor.org/stable/3680063},
urldate      = {2019-03-28},
volume       = {7},
file         = {:done/1983JaffeExtensions Extensions of the Karplus Strong Plucked String Algorithm.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{2018FukudaStochastic,
author     = {Fukuda, Y. and Kawahara, T.},
booktitle  = {2018 7th International Symposium on Next Generation Electronics (ISNE)},
date       = {2018-05},
title      = {Stochastic weights binary neural networks on FPGA},
doi        = {10.1109/ISNE.2018.8394726},
eventtitle = {2018 7th International Symposium on Next Generation Electronics (ISNE)},
pages      = {1--3},
abstract   = {To achieve an advanced Internet of Things (IoT), it is necessary to combine artificial intelligence (AI) with IoT. Compact circuits that can operate AI functions will be useful for this purpose. Therefore, we propose stochastic weights binary neural networks (SWBNN). SWBNNs are more accurate than binary neural networks (BNN) with small circuits. BNNs can be realized with small circuits since binary calculation needs simpler circuits than real number calculation. However, BNNs have lower accuracy than networks with real numbers. Thus, the proposed SWBNNs are BNNs that behave stochastically, which makes them more accurate than BNNs. Moreover, SWBNNs can still be achieved with small circuits since they execute binary calculation. As a result, the accuracy for the test data of SWBNNs is closer to the accuracy for learning data than the accuracy for the test data of BNNs is. Especially when using the CIFAR10 database, the difference in the identification accuracy rate between learning data and test data decreased from 6 /\\% for BNNs to 2 /\\% for SWBNNs. From results of a field-programmable gate array (FPGA) implementation, circuits of SWBNNs are sufficiently small although they are 10 /\\% bigger than those of BNNs. Therefore, SWBNNs are more accurate than BNNs, and the circuit costs ofintroducing stochastic weights are low.},
file       = {:done/2018FukudaStochastic Stochastic Weights Binary Neural Networks on FPGA.pdf:application/pdf},
groups     = {tesse:5},
}
@Article{2009SaitisPhysical,
author       = {Saitis, Charalampos and Orr, Sarah and van Walstijn, Maarten},
date         = {2009-04},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Physical modeling of the piano: An investigation into the effect of string stiffness on the hammer string interaction.},
doi          = {10.1121/1.4784255},
issn         = {0001-4966},
number       = {4},
pages        = {2684--2684},
urldate      = {2019-03-28},
volume       = {125},
langid       = {english},
shorttitle   = {Physical modeling of the piano},
}
@Unpublished{2001AirdTowards,
author = {Aird, Marc and Laird, Joel},
date   = {2001},
title  = {Towards material modelling in physical models using digital waveguides},
file   = {:done/2001AirdTowards Towards Material Modelling in Physical Models Using Digital Waveguides.pdf:application/pdf},
groups = {tesse:5, Digital Waveguides},
}
@Book{2014MontgomeryApplied,
author    = {Montgomery, Douglas C. and Runger, George C.},
date      = {2014},
title     = {Applied statistics and probability for engineers},
edition   = {Sixth edition},
isbn      = {978-1-118-53971-2},
location  = {Hoboken, NJ},
pagetotal = {811},
publisher = {John Wiley and Sons, Inc},
abstract  = {"This best-selling engineering statistics text provides a practical approach that is more oriented to engineering and the chemical and physical sciences than many similar texts. It is packed with unique problem sets that reflect realistic situations engineers will encounter in their working lives. This text shows how statistics, the science of data is just as important for engineers as the mechanical, electrical, and materials sciences"--},
file      = {:done/2014MontgomeryApplied Applied Statistics and Probability for Engineers.pdf:application/pdf},
groups    = {tesse:5, Probability},
}
@Thesis{1989WatkinsLearning,
author      = {Watkins, Christopher John Cornish Hellaby},
date        = {1989},
institution = {King's College, Cambridge},
title       = {Learning from delayed rewards},
type        = {phdthesis},
file        = {:done/1989WatkinsLearning Learning from Delayed Rewards.pdf:application/pdf},
groups      = {tesse:5},
}
@InProceedings{1988LecunTheoretical,
author    = {LeCun, Yann and Touresky, D. and Hinton, G. and Sejnowski, T.},
booktitle = {Proceedings of the 1988 connectionist models summer school},
date      = {1988},
title     = {A theoretical framework for back-propagation},
pages     = {21--28},
publisher = {CMU, Pittsburgh, Pa: Morgan Kaufmann},
volume    = {1},
file      = {:done/1988LecunTheoretical A Theoretical Framework for Back Propagation.pdf:application/pdf},
groups    = {tesse:5},
}
@Misc{2018PeirceSolving,
author    = {Peirce, Anthony},
date      = {2018},
title     = {Solving the Heat, Laplace and Wave quations using finite difference methods},
note      = {Introductory lecture notes on Partial Differential Equations},
url       = {https://www.math.ubc.ca//textasciitilde peirce/M257///////\\_316///////\\_2012///////\\_Lecture///////\\_8.pdf},
abstract  = {In this lecture we introduce the finite difference method that is widely used for approximating PDEs using the computer. We use the definition of the derivative and Taylor series to derive finite difference approximations to the first and second derivatives of a function. We then use these finite difference quotients to approximate the derivatives in the heat equation and to derive a finite difference approximation to the heat equation. Similarly, the technique is applied to the wave equation and Laplace’s Equation. The technique is illustrated using EXCEL spreadsheets.},
file      = {:done/2018PeirceSolving Solving the Heat, Laplace and Wave Quations Using Finite Difference Methods.pdf:application/pdf},
groups    = {tesse:5, Finite Difference Methods},
langid    = {english},
publisher = {The University of British Columbia},
}
@Article{1994MartucciSymmetric,
author       = {Martucci, S. A.},
date         = {1994-05},
journaltitle = {IEEE Transactions on Signal Processing},
title        = {Symmetric convolution and the discrete sine and cosine transforms},
doi          = {10.1109/78.295213},
issn         = {1053-587X},
number       = {5},
pages        = {1038--1051},
volume       = {42},
abstract     = {This paper discusses the use of symmetric convolution and the discrete sine and cosine transforms (DSTs and DCTs) for general digital signal processing. The operation of symmetric convolution is a formalized approach to convolving symmetrically extended sequences. The result is the same as that obtained by taking an inverse discrete trigonometric transform (DTT) of the product of the forward DTTs of those two sequences. There are 16 members in the family of DTTs. Each provides a representation for a corresponding distinct type of symmetric-periodic sequence. The author defines symmetric convolution, relates the DSTs and DCTs to symmetric-periodic sequences, and then use these principles to develop simple but powerful convolution-multiplication properties for the entire family of DSTs and DCTs. Symmetric convolution can be used for discrete linear filtering when the filter is symmetric or antisymmetric. The filtering will be efficient because fast algorithms exist for all versions of the DTTs. Conventional linear convolution is possible if one first zero-pad the input data. Symmetric convolution and its fast implementation using DTTs are now an alternative to circular convolution and the DFT.<<ETX>>},
file         = {:done/1994MartucciSymmetric Symmetric Convolution and the Discrete Sine and Cosine Transforms.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{1996SandholmMultiagent,
author       = {Sandholm, Tuomas W. and Crites, Robert H.},
date         = {1996-01-01},
journaltitle = {Biosystems},
title        = {Multiagent reinforcement learning in the Iterated Prisoner's Dilemma},
doi          = {10.1016/0303-2647(95)01551-5},
issn         = {0303-2647},
number       = {1},
pages        = {147--166},
url          = {http://www.sciencedirect.com/science/article/pii/0303264795015515},
urldate      = {2019-03-28},
volume       = {37},
abstract     = {Reinforcement learning (RL) is based on the idea that the tendency to produce an action should be strengthened (reinforced) if it produces favorable results, and weakened if it produces unfavorable results. Q-learning is a recent RL algorithm that does not need a model of its environment and can be used on-line. Therefore, it is well suited for use in repeated games against an unknown opponent. Most RL research has been confined to single-agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the Iterated Prisoner's Dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. RL is considerably more difficult in such a domain. This paper investigates the ability of a variety of Q-learning agents to play the IPD game against an unknown opponent. In some experiments, the opponent is the fixed strategy Tit-For-Tat, while in others it is another Q-learner. All the Q-learners learned to play optimally against Tit-For-Tat. Playing against another learner was more difficult because the adaptation of the other learner created a non-stationary environment, and because the other learner was not endowed with any a priori knowledge about the IPD game such as a policy designed to encourage cooperation. The learners that were studied varied along three dimensions: the length of history they received as context, the type of memory they employed (lookup tables based on restricted history windows or recurrent neural networks that can theoretically store features from arbitrarily deep in the past), and the exploration schedule they followed. Although all the learners faced difficulties when playing against other learners, agents with longer history windows, lookup table memories, and longer exploration schedules fared best in the IPD games.},
file         = {:done/1996SandholmMultiagent Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma.pdf:application/pdf},
groups       = {tesse:5},
shortjournal = {Biosystems},
}
@Book{2012HarringtonMachine,
author    = {Harrington, Peter},
date      = {2012},
title     = {Machine learning in action},
isbn      = {978-1-61729-018-3},
location  = {Shelter Island, N.Y},
note      = {OCLC: ocn746834657},
pagetotal = {354},
publisher = {Manning Publications Co},
file      = {:done/2012HarringtonMachine Machine Learning in Action.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Machine learning, Handbooks, manuals, etc},
langid    = {english},
}
@Book{1992HollandAdaptation,
author     = {Holland, John H.},
date       = {1992},
title      = {Adaptation in natural and artificial systems: an introductory analysis with applications to biology, control, and artificial intelligence},
edition    = {1st MIT Press ed},
isbn       = {978-0-262-08213-6 978-0-262-58111-0},
location   = {Cambridge, Mass},
pagetotal  = {211},
publisher  = {MIT Press},
series     = {Complex adaptive systems},
file       = {:done/1992HollandAdaptation Adaptation in Natural and Artificial Systems/\\_ an Introductory Analysis with Applications to Biology, Control, and Artificial Intelligence.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Adaptation (Biology), Adaptive control systems, Mathematical models},
langid     = {english},
shorttitle = {Adaptation in natural and artificial systems},
}
@Article{2018WangPacking,
author       = {Wang, Y. and Xu, C. and Xu, C. and Tao, D.},
date         = {2018},
journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
title        = {Packing Convolutional Neural Networks in the Frequency Domain},
doi          = {10.1109/TPAMI.2018.2857824},
issn         = {0162-8828},
pages        = {1--1},
abstract     = {Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present a series of approaches for compressing and speeding up CNNs in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. Furthermore, we explore a data-driven method for removing redundancies in both spatial and frequency domains, which allows us to discard more useless weights by keeping similar accuracies. After obtaining the optimal sparse CNN in the frequency domain, we relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.},
}
@Book{2017McclureTensorflow,
author    = {McClure, Nick},
date      = {2017},
title     = {TensorFlow Machine Learning Cookbook},
isbn      = {978-1-78646-216-9},
location  = {s.l},
note      = {OCLC: 992150633},
pagetotal = {370},
publisher = {Packt Publishing},
file      = {:done/2017McclureTensorflow TensorFlow Machine Learning Cookbook.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2016BonninBuilding,
author = {Bonnin, Rodolfo},
date   = {2016},
title  = {Building Machine Learning Projects with TensorFlow},
file   = {:done/2016BonninBuilding Building Machine Learning Projects with TensorFlow.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
}
@InCollection{2006WerbosBackwards,
author     = {Werbos, Paul J.},
booktitle  = {Automatic Differentiation: Applications, Theory, and Implementations},
date       = {2006},
title      = {Backwards Differentiation in AD and Neural Nets: Past Links and New Opportunities},
doi        = {10.1007/3-540-28438-9///////\\_2},
editor     = {Bücker, Martin and Corliss, George and Naumann, Uwe and Hovland, Paul and Norris, Boyana},
isbn       = {978-3-540-28403-1},
location   = {Berlin/Heidelberg},
pages      = {15--34},
publisher  = {Springer-Verlag},
urldate    = {2019-03-28},
volume     = {50},
abstract   = {Backwards calculation of derivatives – sometimes called the reverse mode, the full adjoint method, or backpropagation, has been developed and applied in many fields. This paper reviews several strands of history, advanced capabilities and types of application – particularly those which are crucial to the development of brain-like capabilities in intelligent control and artificial intelligence.},
file       = {:done/2006WerbosBackwards Backwards Differentiation in AD and Neural Nets/\\_ Past Links and New Opportunities.pdf:application/pdf;:todo/done/2006WerbosBackwards Backwards Differentiation in AD and Neural Nets/\\_ Past Links and New Opportunities.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Backwards Differentiation in AD and Neural Nets},
}
@Book{2008PoliField,
author    = {Poli, Riccardo and Langdon, William B. and McPhee, Nicholas F. and Koza, John R.},
date      = {2008},
title     = {A field guide to genetic programming},
isbn      = {978-1-4092-0073-4},
location  = {[Morrisville, NC},
note      = {OCLC: 837998350},
pagetotal = {233},
publisher = {Lulu Press]},
file      = {:done/2008PoliField A Field Guide to Genetic Programming.pdf:application/pdf},
groups    = {tesse:5, Programming},
langid    = {english},
}
@Article{2004LengLine,
author       = {Leng, Gang and Prasad, Girijesh and McGinnity, Thomas Martin},
date         = {2004-12},
journaltitle = {Neural Networks},
title        = {An on-line algorithm for creating self-organizing fuzzy neural networks},
doi          = {10.1016/j.neunet.2004.07.009},
issn         = {0893-6080},
number       = {10},
pages        = {1477--1493},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0893608004001698},
urldate      = {2019-03-28},
volume       = {17},
abstract     = {This paper presents a new on-line algorithm for creating a self-organizing fuzzy neural network (SOFNN) from sample patterns to implement a singleton or Takagi-Sugeno (TS) type fuzzy model. The SOFNN is based on ellipsoidal basis function (EBF) neurons consisting of a center vector and a width vector. New methods of the structure learning and the parameter learning, based on new adding and pruning techniques and a recursive on-line learning algorithm, are proposed and developed. A proof of the convergence of both the estimation error and the linear network parameters is also given in the paper. The proposed methods are very simple and effective and generate a fuzzy neural model with a high accuracy and compact structure. Simulation work shows that the SOFNN has the capability of self-organization to determine the structure and parameters of the network automatically.},
file         = {:done/2004LengLine An on Line Algorithm for Creating Self Organizing Fuzzy Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017LiFrequency,
author       = {Li, Junxuan and You, Shaodi and Robles-Kelly, Antonio},
date         = {2017-12-08},
journaltitle = {arXiv:1712.03037 [cs]},
title        = {A Frequency Domain Neural Network for Fast Image Super-resolution},
eprint       = {1712.03037},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1712.03037},
urldate      = {2019-03-28},
abstract     = {In this paper, we present a frequency domain neural network for image super-resolution. The network employs the convolution theorem so as to cast convolutions in the spatial domain as products in the frequency domain. Moreover, the non-linearity in deep nets, often achieved by a rectifier unit, is here cast as a convolution in the frequency domain. This not only yields a network which is very computationally efficient at testing but also one whose parameters can all be learnt accordingly. The network can be trained using back propagation and is devoid of complex numbers due to the use of the Hartley transform as an alternative to the Fourier transform. Moreover, the network is potentially applicable to other problems elsewhere in computer vision and image processing which are often cast in the frequency domain. We show results on super-resolution and compare against alternatives elsewhere in the literature. In our experiments, our network is one to two orders of magnitude faster than the alternatives with an imperceptible loss of performance.},
file         = {:done/2017LiFrequency A Frequency Domain Neural Network for Fast Image Super Resolution.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{2016ChenCompressing,
author     = {Chen, Wenlin and Wilson, James and Tyree, Stephen and Weinberger, Kilian Q. and Chen, Yixin},
booktitle  = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD '16},
date       = {2016},
title      = {Compressing Convolutional Neural Networks in the Frequency Domain},
doi        = {10.1145/2939672.2939839},
eventtitle = {the 22nd ACM SIGKDD International Conference},
isbn       = {978-1-4503-4232-2},
location   = {San Francisco, California, USA},
pages      = {1475--1484},
publisher  = {ACM Press},
url        = {http://dl.acm.org/citation.cfm?doid=2939672.2939839},
urldate    = {2019-03-28},
file       = {:done/2016ChenCompressing Compressing Convolutional Neural Networks in the Frequency Domain.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Collection{1979DenkerErgodic,
date       = {1979},
editor     = {Denker, Manfred and Jacobs, Konrad},
title      = {Ergodic theory: proceedings, Oberwolfach, Germany, June 11-17, 1978},
isbn       = {978-0-387-09517-2},
location   = {Berlin ; New York},
pagetotal  = {209},
publisher  = {Springer-Verlag},
series     = {Lecture notes in mathematics ; 729},
shorttitle = {Ergodic theory},
}
@Article{2018MukhometzianovCapsnet,
author       = {Mukhometzianov, Rinat and Carrillo, Juan},
date         = {2018-05-28},
journaltitle = {arXiv:1805.11195 [cs, stat]},
title        = {CapsNet comparative performance evaluation for image classification},
eprint       = {1805.11195},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1805.11195},
urldate      = {2019-03-28},
abstract     = {Image classification has become one of the main tasks in the field of computer vision technologies. In this context, a recent algorithm called CapsNet that implements an approach based on activity vectors and dynamic routing between capsules may overcome some of the limitations of the current state of the art artificial neural networks (ANN) classifiers, such as convolutional neural networks (CNN). In this paper, we evaluated the performance of the CapsNet algorithm in comparison with three well-known classifiers (Fisher-faces, LeNet, and ResNet). We tested the classification accuracy on four datasets with a different number of instances and classes, including images of faces, traffic signs, and everyday objects. The evaluation results show that even for simple architectures, training the CapsNet algorithm requires significant computational resources and its classification performance falls below the average accuracy values of the other three classifiers. However, we argue that CapsNet seems to be a promising new technique for image classification, and further experiments using more robust computation resources and re-fined CapsNet architectures may produce better outcomes.},
file         = {:done/2018MukhometzianovCapsnet CapsNet Comparative Performance Evaluation for Image Classification.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{2007FastlCritical,
author = {Fastl, Hugo and Zwicker, Eberhard},
date   = {2007},
title  = {Critical Bands and Excitation},
note   = {OCLC: 7328866217},
pages  = {149--173},
file   = {:done/2007FastlCritical Critical Bands and Excitation.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Article{CookReal,
author = {Cook, Perry R. (Author)},
title  = {Real Sound Synthesis for Interactive Applications},
pages  = {269},
file   = {:done/CookReal Real Sound Synthesis for Interactive Applications.pdf:application/pdf},
groups = {tesse:2, DSP},
langid = {english},
}
@Book{2003TrautmannDigital,
author    = {Trautmann, Lutz and Rabenstein, Rudolf},
date      = {2003},
title     = {Digital Sound Synthesis by Physical Modeling Using the Functional Transformation Method},
doi       = {10.1007/978-1-4615-0049-0},
isbn      = {978-1-4613-4900-6 978-1-4615-0049-0},
location  = {Boston, MA},
publisher = {Springer US},
urldate   = {2019-03-31},
langid    = {english},
}
@Book{2003JaynesProbability,
author     = {Jaynes, Edwin T and Bretthorst, G. Larry and Publishing, EBSCO},
date       = {2003},
title      = {Probability theory: the logic of science},
isbn       = {978-0-511-06589-7},
location   = {Cambridge},
note       = {OCLC: 982265136},
publisher  = {Cambridge University Press},
url        = {http://www5.unitn.it/Biblioteca/it/Web/LibriElettroniciDettaglio/50847},
urldate    = {2019-03-31},
abstract   = {The standard rules of probability can be interpreted as uniquely valid principles in logic. In this book, E.T. Jaynes dispels the imaginary distinction between'probability theory'and'statistical inference', leaving a logical unity and simplicity, which provides greater technical power and flexibility in applications. This book goes beyond the conventional mathematics of probability theory, viewing the subject in a wider context. New results are discussed, along with applications of probability theory to a wide variety of problems in physics, mathematics, economics, chemistry and biology. It contains many exercises and problems, and is suitable for use as a textbook on graduate level courses involving data analysis. The material is aimed at readers who are already familiar with applied mathematics at an advanced undergraduate level or higher. The book will be of interest to scientists working in any area where inference from incomplete information is necessary. (A cura dell'editore).},
langid     = {english},
shorttitle = {Probability theory},
}
@Book{2013FieldDiscovering,
author    = {Field, Andy and Miles, Jeremy and Field, Zoë},
date      = {2013},
title     = {Discovering statistics using R},
edition   = {Reprint},
isbn      = {978-1-4462-0045-2 978-1-4462-0046-9},
location  = {Los Angeles, Calif.},
note      = {OCLC: 854989686},
pagetotal = {957},
publisher = {Sage},
file      = {:done/2013FieldDiscovering Discovering Statistics Using R.pdf:application/pdf},
groups    = {tesse:5, Probability},
langid    = {english},
}
@Book{2012TijmsUnderstanding,
author    = {Tijms, Henk C.},
date      = {2012},
title     = {Understanding probability},
edition   = {3. ed},
isbn      = {978-1-107-65856-1},
location  = {Cambridge},
note      = {OCLC: 803790744},
pagetotal = {562},
publisher = {Cambridge Univ. Press},
abstract  = {"Understanding Probability is a unique and stimulating approach to a first course in probability. The first part of the book demystifies probability and uses many wonderful probability applications from everyday life to help the reader develop a feel for probabilities. The second part, covering a wide range of topics, teaches clearly and simply the basics of probability. This fully revised third edition has been packed with even more exercises and examples, and it includes new sections on Bayesian inference, Markov chain Monte Carlo simulation, hitting probabilities in random walks and Brownian motion, and a new chapter on continuous-time Markov chains with applications. Here you will find all the material taught in an introductory probability course. The first part of the book, with its easy-going style, can be read by anybody with a reasonable background in high school mathematics. The second part of the book requires a basic course in calculus"--},
file      = {:done/2012TijmsUnderstanding Understanding Probability.pdf:application/pdf},
groups    = {tesse:5, Probability},
keywords  = {Probabilities, Chance, Mathematical analysis, MATHEMATICS / Probability /\\& Statistics / General},
langid    = {english},
}
@Article{KennedyHilbert,
author = {Kennedy, Rodney A. and Sadeghi, Parastoo},
title  = {Hilbert Space Methods in Signal Processing},
pages  = {440},
file   = {:done/KennedyHilbert Hilbert Space Methods in Signal Processing.pdf:application/pdf},
groups = {tesse:2, DSP},
langid = {english},
}
@Book{2017FanoTwenty,
author     = {Fano, Guido and Blinder, S. M.},
date       = {2017},
title      = {Twenty-First Century Quantum Mechanics: Hilbert Space to Quantum Computers},
doi        = {10.1007/978-3-319-58732-5},
isbn       = {978-3-319-58731-8 978-3-319-58732-5},
location   = {Cham},
publisher  = {Springer International Publishing},
series     = {UNITEXT for Physics},
urldate    = {2019-03-31},
file       = {:done/2017FanoTwenty Twenty First Century Quantum Mechanics//_ Hilbert Space to Quantum Computers.pdf:application/pdf},
groups     = {tesse:5, Quantum Mechanics},
langid     = {english},
shorttitle = {Twenty-First Century Quantum Mechanics},
}
@Book{2017SamuelProgramming,
author     = {Samuel, Stephen and Bocutiu, Stefan},
date       = {2017},
title      = {Programming Kotlin: familiarize yourself with all of Kotlin's features with this in-depth guide},
langid     = {english},
shorttitle = {Programming Kotlin},
}
@Article{1982Jacquet-lagrezeAssessing,
author       = {Jacquet-Lagreze, E. and Siskos, J.},
date         = {1982-06},
journaltitle = {European Journal of Operational Research},
title        = {Assessing a set of additive utility functions for multicriteria decision-making, the UTA method},
doi          = {10.1016/0377-2217(82)90155-2},
issn         = {0377-2217},
number       = {2},
pages        = {151--164},
url          = {https://linkinghub.elsevier.com/retrieve/pii/0377221782901552},
urldate      = {2019-03-31},
volume       = {10},
file         = {:done/1982Jacquet-lagrezeAssessing Assessing a Set of Additive Utility Functions for Multicriteria Decision Making, the UTA Method.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2001BeutheComparative,
author       = {Beuthe, Michel and Scannella, Giuseppe},
date         = {2001},
journaltitle = {European Journal of Operational Research},
title        = {Comparative analysis of UTA multicriteria methods q},
pages        = {17},
abstract     = {This paper reviews the main variants of the utility additive (UTA) multicriteria method, and systematically compares their predictive performance on two sets of data. It analyses both those cases where the model provides a ranking with errors and those without errors. First, it shows that the reference projects should be chosen carefully in order to elicit as much information as possible from the decision maker: a set of projects satisfying a fractional factorial plan is recommended. Second, it discusses the use of alternative post-optimality methods for solving the problem of multiple solutions and their dierent predictive performances. Third, it presents the results of simulations based on utility functions involving interdependence between criteria, and shows that UTA handles this problem eectively by an adjustment of its coecients. Finally, the in¯uence of the model's parameters on the predictive performance is also investigated. Ó 2001 Elsevier Science B.V. All rights reserved.},
file         = {:done/2001BeutheComparative Comparative Analysis of UTA Multicriteria Methods Q.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InCollection{2005SiskosUta,
author    = {Siskos, Yannis and Grigoroudis, Evangelos and Matsatsinis, Nikolaos F.},
booktitle = {Multiple Criteria Decision Analysis: State of the Art Surveys},
date      = {2005},
title     = {UTA Methods},
doi       = {10.1007/0-387-23081-5///////\\_8},
isbn      = {978-0-387-23067-2},
location  = {New York},
pages     = {297--334},
publisher = {Springer-Verlag},
urldate   = {2019-03-31},
volume    = {78},
abstract  = {Abstract: UTA methods refer to the philosophy of assessing a set of value or utility functions, assuming the axiomatic basis of MAUT and adopting the preference disaggregation principle. UTA methodology uses linear programming techniques in order to optimally infer additive value/utility functions, so that these functions are as consistent as possible with the global decision-maker’s preferences (inference principle). The main objective of this chapter is to analytically present the UTA method and its variants and to summarize the progress made in this field. The historical background and the philosophy of the aggregation-disaggregation approach are firstly given in this chapter. The detailed presentation of the basic UTA algorithm is presented, including the discussion on the stability and sensitivity analyses. Several variants of the UTA method, which incorporate different forms of optimality criteria used in the LP formulation, are also discussed. The implementation of the UTA methods is illustrated by a general overview of UTA-based DSSs, as well as real-world decision-making applications. Finally, several potential future research developments of the UTA methodologies within the context of MCDA are discussed through this chapter.},
file      = {:done/2005SiskosUta UTA Methods.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{1979HwangMultiple,
author      = {Hwang, Ching-Lai and Masud, Abu Syed Md.},
date        = {1979},
title       = {Multiple Objective Decision Making — Methods and Applications},
doi         = {10.1007/978-3-642-45511-7},
editorb     = {Beckmann, M. and Künzi, H. P.},
isbn        = {978-3-540-09111-0},
location    = {Berlin, Heidelberg},
publisher   = {Springer Berlin Heidelberg},
series      = {Lecture Notes in Economics and Mathematical Systems},
urldate     = {2019-03-31},
volume      = {164},
editorbtype = {redactor},
file        = {:done/1979HwangMultiple Multiple Objective Decision Making — Methods and Applications.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
year        = {1979},
}
@Collection{2010ZopounidisHandbook,
date      = {2010},
editor    = {Zopounidis, Constantin},
title     = {Handbook of multicriteria analysis},
isbn      = {978-3-540-92827-0 978-3-540-92828-7},
location  = {Heidelberg},
note      = {OCLC: 845476314},
pagetotal = {455},
publisher = {Springer},
series    = {Applied optimization},
file      = {:done/2010ZopounidisHandbook Handbook of Multicriteria Analysis.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2013IshizakaMulti,
author     = {Ishizaka, Alessio and Nemery, Philippe},
date       = {2013},
title      = {Multi-criteria decision analysis: methods and software},
isbn       = {978-1-119-97407-9 978-1-118-64490-4},
location   = {Chichester},
note       = {OCLC: 931031284},
pagetotal  = {296},
publisher  = {Wiley},
file       = {:done/2013IshizakaMulti Multi Criteria Decision Analysis/\\_ Methods and Software.pdf:application/pdf;:todo/done/2013IshizakaMulti Multi Criteria Decision Analysis/\\_ Methods and Software.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Multi-criteria decision analysis},
}
@Book{2000TriantaphyllouMulti,
author      = {Triantaphyllou, Evangelos},
date        = {2000},
title       = {Multi-criteria Decision Making Methods: A Comparative Study},
doi         = {10.1007/978-1-4757-3157-6},
editorb     = {Pardalos, Panos M. and Hearn, Donald},
isbn        = {978-1-4419-4838-0 978-1-4757-3157-6},
location    = {Boston, MA},
publisher   = {Springer US},
series      = {Applied Optimization},
urldate     = {2019-03-31},
volume      = {44},
editorbtype = {redactor},
langid      = {english},
shorttitle  = {Multi-criteria Decision Making Methods},
}
@Book{2009BritainMulti,
author     = {Britain, Great and for Communities, Department and Government, Local},
date       = {2009},
title      = {Multi-criteria analysis: a manual.},
isbn       = {978-1-4098-1023-0},
location   = {Wetherby},
note       = {OCLC: 502304922},
publisher  = {Communities and Local Government},
url        = {http://www.communities.gov.uk/documents/corporate/pdf/1132618.pdf},
urldate    = {2019-03-31},
langid     = {english},
shorttitle = {Multi-criteria analysis},
}
@Book{2002BeltonMultiple,
author    = {Belton, Valerie and Stewart, Theodor J.},
date      = {2002},
title     = {Multiple Criteria Decision Analysis an Integrated Approach},
isbn      = {978-1-4613-5582-3 978-1-4615-1495-4},
location  = {Boston, MA},
note      = {OCLC: 903189757},
publisher = {Springer US},
abstract  = {The field of multiple criteria decision analysis (MCDA), also termed multiple criteria decision aid, or multiple criteria decision making (MCDM), has developed rapidly over the past quarter century and in the process a number of divergent schools of thought have emerged. This can make it difficult for a new entrant into the field to develop a comprehensive appreciation of the range of tools and approaches which are available to assist decision makers in dealing with the ever-present difficulties of seeking compromise or consensus between conflicting inter ests and goals, i.e. the "multiple criteria". The diversity of philosophies and models makes it equally difficult for potential users of MCDA, i.e. management scientists and/or decision makers facing problems involving conflicting goals, to gain a clear understanding of which methodologies are appropriate to their particular context. Our intention in writing this book has been to provide a compre hensive yet widely accessible overview of the main streams of thought within MCDA. We aim to provide readers with sufficient awareness of the underlying philosophies and theories, understanding of the practi cal details of the methods, and insight into practice to enable them to implement any of the approaches in an informed manner. As the title of the book indicates, our emphasis is on developing an integrated view of MCDA, which we perceive to incorporate both integration of differ ent schools of thought within MCDA, and integration of MCDA with broader management theory, science and practice.},
file      = {:done/2002BeltonMultiple Multiple Criteria Decision Analysis an Integrated Approach.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2010TrappenbergFundamentals,
author    = {Trappenberg, Thomas P.},
date      = {2010},
title     = {Fundamentals of computational neuroscience},
edition   = {Second edition},
isbn      = {978-0-19-956841-3},
location  = {Oxford New York Auckland},
note      = {OCLC: 699333421},
pagetotal = {390},
publisher = {Oxford University Press},
file      = {:done/2010TrappenbergFundamentals Fundamentals of Computational Neuroscience.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2015DharaniBiology,
author     = {Dharani, Krishnagopal},
date       = {2015},
title      = {The biology of thought: a neuronal mechanism in the generation of thought - a new molecular model},
isbn       = {978-0-12-800900-0},
location   = {Amsterdam},
note       = {OCLC: 931711206},
pagetotal  = {229},
publisher  = {Elsevier Acad. Press},
langid     = {english},
shorttitle = {The biology of thought},
}
@Book{2016BearNeuroscience,
author     = {Bear, Mark F. and Paradiso, Michael A. and Connors, Barry W.},
date       = {2016},
title      = {Neuroscience: exploring the brain},
isbn       = {978-0-7817-7817-6},
location   = {Hagerstown, Md.},
note       = {OCLC: 1090148410},
publisher  = {Lippincott Williams //\\\& Wilkins},
file       = {:done/2016BearNeuroscience Neuroscience/\\_ Exploring the Brain.pdf:application/pdf;:todo/done/2016BearNeuroscience Neuroscience/\\_ Exploring the Brain.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Neuroscience},
}
@Book{2015SterlingPrinciples,
author    = {Sterling, Peter and Laughlin, Simon},
date      = {2015},
title     = {Principles of neural design},
isbn      = {978-0-262-02870-7},
location  = {Cambridge, Massachusetts},
note      = {OCLC: 915145186},
pagetotal = {542},
publisher = {The MIT Press},
file      = {:done/2015SterlingPrinciples Principles of Neural Design.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Brain, Learning, Neural circuitry, Neural Pathways, physiology, Physiology},
langid    = {english},
}
@Article{LedouxHow,
author = {LeDOUX, J. O. S. E. P. H.},
title  = {HOW OUR BRAINS BECOME WHO WE ARE},
pages  = {160},
file   = {:done/LedouxHow HOW OUR BRAINS BECOME WHO WE ARE.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Article{DayanTheoretical,
author = {Dayan, Peter and Abbott, L. F.},
title  = {THEORETICAL NEUROSCIENCE},
pages  = {432},
file   = {:done/DayanTheoretical THEORETICAL NEUROSCIENCE.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Book{2014VanderbeiLinear,
author     = {Vanderbei, Robert J.},
date       = {2014},
title      = {Linear programming: foundations and extensions},
edition    = {4. ed},
isbn       = {978-1-4614-7629-0 978-1-4614-7630-6},
location   = {New York, NY},
note       = {OCLC: 859535863},
number     = {196},
pagetotal  = {414},
publisher  = {Springer},
series     = {International series in operations research //\\\& management science},
file       = {:done/2014VanderbeiLinear Linear Programming//_ Foundations and Extensions.pdf:application/pdf;:todo/done/2014VanderbeiLinear Linear Programming//_ Foundations and Extensions.pdf:application/pdf},
groups     = {tesse:5, Optimization},
langid     = {english},
shorttitle = {Linear programming},
}
@Book{2000SchrijverTheory,
author    = {Schrijver, Alexander},
date      = {2000},
title     = {Theory of linear and integer programming},
edition   = {Reprinted},
isbn      = {978-0-471-98232-6},
location  = {Chichester},
note      = {OCLC: 247967491},
pagetotal = {471},
publisher = {Wiley},
series    = {Wiley-Interscience series in discrete mathematics and optimization},
file      = {:done/2000SchrijverTheory Theory of Linear and Integer Programming.pdf:application/pdf},
groups    = {tesse:5, Optimization},
langid    = {english},
}
@Book{2016ChiconeInvitation,
author = {Chicone, University Of Mi, Carmen (professor Of Mathematics},
date   = {2016},
title  = {Invitation to applied mathematics - differential equations, modeling, and c.},
isbn   = {978-0-12-804153-6},
note   = {OCLC: 1023278769},
file   = {:done/2016ChiconeInvitation Invitation to Applied Mathematics Differential Equations, Modeling, and C..pdf:application/pdf},
groups = {tesse:5},
langid = {english},
}
@Book{2013LukeEssentials,
author     = {Luke, Sean},
date       = {2013},
title      = {Essentials of metaheuristics: a set of undergraduate lecture notes},
edition    = {Second edition, online version 2.0},
isbn       = {978-1-300-54962-8},
location   = {Morrisville, N.C.},
note       = {OCLC: 876159426},
pagetotal  = {239},
publisher  = {lulu.com},
file       = {:done/2013LukeEssentials Essentials of Metaheuristics/\\_ a Set of Undergraduate Lecture Notes.pdf:application/pdf;:todo/done/2013LukeEssentials Essentials of Metaheuristics/\\_ a Set of Undergraduate Lecture Notes.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Essentials of metaheuristics},
}
@InProceedings{1992VemulapatiSolving,
author     = {Vemulapati, U. B.},
booktitle  = {Proceedings Scalable High Performance Computing Conference SHPCC-92.},
date       = {1992},
title      = {Solving equality constrained least squares problems},
doi        = {10.1109/SHPCC.1992.232669},
eventtitle = {Scalable High Performance Computing Conference SHPCC-92.},
isbn       = {978-0-8186-2775-0},
location   = {Williamsburg, VA, USA},
pages      = {380--384},
publisher  = {IEEE Comput. Soc. Press},
url        = {http://ieeexplore.ieee.org/document/232669/},
urldate    = {2019-03-31},
file       = {:done/1992VemulapatiSolving Solving Equality Constrained Least Squares Problems.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Book{2015BjoerckNumerical,
author    = {Björck, Åke},
date      = {2015},
title     = {Numerical Methods in Matrix Computations},
doi       = {10.1007/978-3-319-05089-8},
isbn      = {978-3-319-05088-1 978-3-319-05089-8},
location  = {Cham},
publisher = {Springer International Publishing},
series    = {Texts in Applied Mathematics},
urldate   = {2019-03-31},
volume    = {59},
file      = {:done/2015BjoerckNumerical Numerical Methods in Matrix Computations.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{SelesnickLeast,
author = {Selesnick, Ivan},
title  = {Least Squares with Examples in Signal Processing},
pages  = {15},
file   = {:done/SelesnickLeast Least Squares with Examples in Signal Processing.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Collection{2017JawaidGreen,
date       = {2017},
editor     = {Jawaid, Mohammad and Salit, Mohd Sapuan and Alothman, Othman Y.},
title      = {Green biocomposites: design and applications},
isbn       = {978-3-319-49381-7 978-3-319-49382-4},
location   = {Cham},
pagetotal  = {345},
publisher  = {Springer International Publishing},
series     = {Green energy and technology},
file       = {:done/2017JawaidGreen Green Biocomposites/\\_ Design and Applications.pdf:application/pdf;:todo/done/2017JawaidGreen Green Biocomposites/\\_ Design and Applications.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Green biocomposites},
}
@Collection{2017JawaidGreena,
date       = {2017},
editor     = {Jawaid, Hohammad and Sapuan, Salit Mohd and Alothman, Othman Y.},
title      = {Green biocomposites: manufacturing and properties},
isbn       = {978-3-319-46609-5 978-3-319-46610-1},
location   = {Cham},
note       = {OCLC: 964576721},
pagetotal  = {409},
publisher  = {Springer International Publishing},
series     = {Green energy and technology},
file       = {:done/2017JawaidGreena Green Biocomposites/\\_ Manufacturing and Properties.pdf:application/pdf;:todo/done/2017JawaidGreena Green Biocomposites/\\_ Manufacturing and Properties.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Green biocomposites},
}
@Article{PhillipsBio,
author = {Phillips, Steven},
title  = {BIO-COMPOSITE MATERIAL APPLICATIONS TO MUSICAL INSTRUMENTS},
pages  = {97},
file   = {:done/PhillipsBio BIO COMPOSITE MATERIAL APPLICATIONS tO MUSICAL INSTRUMENTS.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Collection{2013EbnesajjadHandbook,
date       = {2013},
editor     = {Ebnesajjad, Sina},
title      = {Handbook of biopolymers and biodegradable plastics: properties, processing and applications},
isbn       = {978-1-4557-2834-3},
location   = {Amsterdam},
note       = {OCLC: 820108309},
pagetotal  = {462},
publisher  = {Elsevier},
series     = {Plastics design library (PDL)},
langid     = {english},
shorttitle = {Handbook of biopolymers and biodegradable plastics},
}
@Article{PillaHandbook,
author = {Pilla, Srikanth},
title  = {Handbook of Bioplastics and Biocomposites Engineering Applications},
pages  = {622},
file   = {:done/PillaHandbook Handbook of Bioplastics and Biocomposites Engineering Applications.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Article{2015KesslerGreen,
author       = {Kessler, Michael R. and Thakur, Vijay Kumar},
date         = {2015-07-01},
journaltitle = {Choice Reviews Online},
title        = {Green biorenewable biocomposites: from knowledge to industrial applications},
doi          = {10.5860/CHOICE.191046},
issn         = {0009-4978, 1523-8253},
number       = {11},
pages        = {52--5901--52--5901},
urldate      = {2019-03-31},
volume       = {52},
file         = {:done/2015KesslerGreen Green Biorenewable Biocomposites\\_ from Knowledge to Industrial Applications.pdf:application/pdf},
groups       = {tesse:5},
journal      = {Choice Reviews Online},
langid       = {english},
month        = {June},
publisher    = {American Library Association},
shorttitle   = {Green biorenewable biocomposites},
year         = {2015},
}
@Book{2016BucurHandbook,
author    = {Bucur, Voichita},
date      = {2016},
title     = {Handbook of Materials for String Musical Instruments},
doi       = {10.1007/978-3-319-32080-9},
isbn      = {978-3-319-32078-6 978-3-319-32080-9},
location  = {Cham},
publisher = {Springer International Publishing},
urldate   = {2019-03-31},
file      = {:done/2016BucurHandbook Handbook of Materials for String Musical Instruments.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{LewisIntroduction,
author = {Lewis, Mark C. and Lacher, Lisa L.},
title  = {INTRODUCTION TO PROGRAMMING AND PROBLEM-SOLVING USING SCALA},
pages  = {591},
file   = {:done/LewisIntroduction INTRODUCTION tO PROGRAMMING aND PROBLEM SOLVING USING SCALA.pdf:application/pdf},
groups = {tesse:2, Programming},
langid = {english},
}
@Book{2005BucklandProgramming,
author    = {Buckland, Mat},
date      = {2005},
title     = {Programming game AI by example},
isbn      = {978-1-55622-078-4},
location  = {Plano, Tex},
note      = {OCLC: 249763995},
pagetotal = {495},
publisher = {Wordware Publ},
series    = {Wordware game developer's library},
file      = {:done/2005BucklandProgramming Programming Game AI by Example.pdf:application/pdf},
groups    = {tesse:5, Programming},
langid    = {english},
}
@Article{bugnion_scala:applied_nodate,
author = {Bugnion, Pascal},
title  = {Scala:Applied Machine Learning},
pages  = {1623},
langid = {english},
}
@Article{1997SussmanStructure,
author       = {Sussman, Gerald Jay and Abelson, Hal},
date         = {1997-02},
journaltitle = {Computers //\\\& Mathematics with Applications},
title        = {Structure and interpretation of computer programs},
doi          = {10.1016/S0898-1221(97)90051-1},
issn         = {0898-1221},
number       = {4},
pages        = {133},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0898122197900511},
urldate      = {2019-03-31},
volume       = {33},
file         = {:done/1997SussmanStructure Structure and Interpretation of Computer Programs.pdf:application/pdf},
groups       = {tesse:5},
journal      = {Computers \\\& Mathematics with Applications},
langid       = {english},
month        = {February},
publisher    = {Elsevier BV},
year         = {1997},
}
@Book{2017BallWriting,
author = {Ball, Thorsten},
date   = {2017},
title  = {Writing An Interpreter In Go},
file   = {:done/2017BallWriting Writing an Interpreter in Go.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
}
@Book{2016EngQt5,
author     = {Eng, Lee Zhi},
date       = {2016},
title      = {Qt5 C++ GUI programming cookbook: use Qt5 to design and build a graphical user interface that is functional, appealing, and user-friendly for your software application},
isbn       = {978-1-78328-027-8 978-1-78328-028-5},
location   = {Birmingham Mumbai},
note       = {OCLC: 960408279},
pagetotal  = {283},
publisher  = {Packt Publishing},
series     = {Quick answers to common problems},
langid     = {english},
shorttitle = {Qt5 C++ GUI programming cookbook},
}
@Book{2009HaykinNeural,
author    = {Haykin, Simon S.},
date      = {2009},
title     = {Neural networks and learning machines},
edition   = {3. ed},
isbn      = {978-0-13-147139-9},
location  = {New York},
note      = {OCLC: 857737780},
pagetotal = {906},
publisher = {Pearson},
file      = {:done/2009HaykinNeural Neural Networks and Learning Machines.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{deng_deep_nodate,
author       = {Deng, Li and Yu, Dong},
journaltitle = {Deep Learning},
title        = {Deep Learning: Methods and Applications},
pages        = {197},
volume       = {7},
langid       = {english},
}
@Collection{2014KowaliwGrowing,
date       = {2014},
editor     = {Kowaliw, Taras and Bredeche, Nicolas and Doursat, René},
title      = {Growing adaptive machines: combining development and learning in artificial neural networks},
isbn       = {978-3-642-55336-3 978-3-642-55337-0},
location   = {Heidelberg},
note       = {OCLC: 892794596},
number     = {557},
pagetotal  = {261},
publisher  = {Springer},
series     = {Studies in computational intelligence},
langid     = {english},
shorttitle = {Growing adaptive machines},
}
@Article{CilimkovicNeural,
author   = {Cilimkovic, Mirza},
title    = {Neural Networks and Back Propagation Algorithm},
pages    = {12},
abstract = {Neural Networks (NN) are important data mining tool used for classiﬁcation and clustering. It is an attempt to build machine that will mimic brain activities and be able to learn. NN usually learns by examples. If NN is supplied with enough examples, it should be able to perform classiﬁcation and even discover new trends or patterns in data. Basic NN is composed of three layers, input, output and hidden layer. Each layer can have number of nodes and nodes from input layer are connected to the nodes from hidden layer. Nodes from hidden layer are connected to the nodes from output layer. Those connections represent weights between nodes.},
file     = {:done/CilimkovicNeural Neural Networks and Back Propagation Algorithm.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Book{2016ShanmuganathanArtificial,
author   = {Shanmuganathan, Subana and Samarasinghe, Sandhya},
date     = {2016},
title    = {Artificial neural network modelling},
doi      = {10.1007/978-3-319-28495-8},
isbn     = {978-3-319-28495-8 978-3-319-28493-4},
note     = {OCLC: 939520392},
urldate  = {2019-03-31},
abstract = {This book covers theoretical aspects as well as recent innovative applications of Artificial Neural networks (ANNs) in natural, environmental, biological, social, industrial and automated systems. It presents recent results of ANNs in modelling small, large and complex systems under three categories, namely, 1) Networks, Structure Optimisation, Robustness and Stochasticity 2) Advances in Modelling Biological and Environmental Systems and 3) Advances in Modelling Social and Economic Systems. The book aims at serving undergraduates, postgraduates and researchers in ANN computational modelling.},
file     = {:done/2016ShanmuganathanArtificial Artificial Neural Network Modelling.pdf:application/pdf},
groups   = {tesse:5},
langid   = {english},
}
@Book{2019VasilevPython,
author     = {Vasilev, Ivan},
date       = {2019},
title      = {Python deep learning: exploring deep learning techniques and neural network architectures with PyTorch, Keras, and TensorFlow},
isbn       = {978-1-78934-970-2},
note       = {OCLC: 1086398837},
url        = {http://proquest.safaribooksonline.com/?fpi=9781789348460},
urldate    = {2019-03-31},
langid     = {english},
shorttitle = {Python deep learning},
}
@Book{1999AnthonyNeural,
author     = {Anthony, Martin and Bartlett, Peter L.},
date       = {1999},
title      = {Neural Network Learning: Theoretical Foundations},
doi        = {10.1017/CBO9780511624216},
isbn       = {978-0-511-62421-6},
location   = {Cambridge},
publisher  = {Cambridge University Press},
url        = {http://ebooks.cambridge.org/ref/id/CBO9780511624216},
urldate    = {2019-03-31},
langid     = {english},
shorttitle = {Neural Network Learning},
}
@Book{2007GalushkinNeural,
author    = {Galushkin, Alexander I.},
date      = {2007},
title     = {Neural Networks Theory},
isbn      = {978-3-540-48125-6},
location  = {Berlin, Heidelberg},
note      = {OCLC: 938669307},
publisher = {Springer-Verlag Berlin Heidelberg},
file      = {:done/2007GalushkinNeural Neural Networks Theory.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2016LayLinear,
author   = {Lay, David C. and Lay, Steven R. and McDonald, Judith},
date     = {2016},
title    = {Linear algebra and its applications},
isbn     = {978-0-321-98238-4},
note     = {OCLC: 875056178},
abstract = {With traditional linear algebra texts, the course is relatively easy for students during the early stages as material is presented in a familiar, concrete setting. However, when abstract concepts are introduced, students often hit a wall. Instructors seem to agree that certain concepts (such as linear independence, spanning, subspace, vector space, and linear transformations) are not easily understood and require time to assimilate. These concepts are fundamental to the study of linear algebra, so students' understanding of them is vital to mastering the subject. This text makes these concepts more accessible by introducing them early in a familiar, concrete Rn setting, developing them gradually, and returning to them throughout the text so that when they are discussed in the abstract, students are readily able to understand.},
file     = {:done/2016LayLinear Linear Algebra and Its Applications.pdf:application/pdf},
groups   = {tesse:5, Linear Algebra},
langid   = {english},
}
@Book{2008Leyton-brownEssentials,
author     = {Leyton-Brown, Kevin and Shoham, Yoav},
date       = {2008},
title      = {Essentials of game theory: a concise, multidisciplinary introduction},
isbn       = {978-1-59829-593-1 978-1-59829-594-8},
location   = {San Rafael, California},
note       = {OCLC: 236164649},
number     = {Lecture 3},
pagetotal  = {88},
publisher  = {Morgan ////////\\\& Claypool Publishers},
series     = {Synthesis lectures on artificial intelligence and machine learning},
langid     = {english},
shorttitle = {Essentials of game theory},
}
@Book{2009GintisGame,
author     = {Gintis, Herbert},
date       = {2009},
title      = {Game theory evolving: a problem-centered introduction to modeling strategic interaction},
edition    = {2. ed},
isbn       = {978-0-691-14050-6 978-0-691-14051-3},
location   = {Princeton, NJ},
note       = {OCLC: 845420030},
pagetotal  = {390},
publisher  = {Princeton Univ. Press},
langid     = {english},
shorttitle = {Game theory evolving},
}
@Book{2005HayesSchaums,
author    = {Hayes, Monson H.},
date      = {2005},
title     = {Schaum's outline of theory and problems of digital signal processing},
edition   = {Nachdr.},
isbn      = {978-0-07-027389-4},
location  = {New York},
note      = {OCLC: 255732140},
pagetotal = {432},
publisher = {McGraw Hill},
series    = {Schaum's outline series},
file      = {:done/2005HayesSchaums Schaum's Outline of Theory and Problems of Digital Signal Processing.pdf:application/pdf},
groups    = {tesse:5, DSP},
langid    = {english},
}
@Book{2016AmiotMusic,
author    = {Amiot, Emmanuel},
date      = {2016},
title     = {Music Through Fourier Space},
doi       = {10.1007/978-3-319-45581-5},
isbn      = {978-3-319-45580-8 978-3-319-45581-5},
location  = {Cham},
publisher = {Springer International Publishing},
series    = {Computational Music Science},
urldate   = {2019-03-31},
file      = {:done/2016AmiotMusic Music through Fourier Space.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2018BroughtonDiscrete,
author     = {Broughton, S. Allen and Bryan, Kurt},
date       = {2018},
title      = {Discrete fourier analysis and wavelets: applications to signal and image processing},
edition    = {Second edition},
isbn       = {978-1-119-25824-7 978-1-119-25822-3 978-1-119-25823-0},
location   = {Hoboken, NJ},
note       = {OCLC: 1030990110},
pagetotal  = {442},
publisher  = {John Wiley},
abstract   = {Vector spaces, signals, and images -- The discrete fourier transform -- The discrete cosine transform -- Convolution and filtering -- Windowing and localization -- Frames -- Filter banks -- Lifting for filter banks and wavelets -- Wavelets},
langid     = {english},
shorttitle = {Discrete fourier analysis and wavelets},
}
@Book{2014AndreescuComplex,
author   = {Andreescu, Titu and Andrica, D and Andrica, D},
date     = {2014},
title    = {Complex numbers from A to ... Z},
isbn     = {978-0-8176-8414-3},
note     = {OCLC: 868044080},
abstract = {"The book reflects the unique experience of the authors. It distills a vast mathematical literature, most of which is unknown to the western public, and captures the essence of an abundant problem culture. The target audience includes undergraduates, high school students and their teachers, mathematical contestants (such as those training for Olympiads or the W.L. Putnam Mathematical Competition) and their coaches, as well as anyone interested in essential mathematics."--Page 4 of cover.},
langid   = {english},
}
@Book{1998NeedhamVisual,
author    = {Needham, Tristan},
date      = {1998},
title     = {Visual complex analysis},
editor    = {Needham, Tristan},
isbn      = {978-0-19-853447-1},
location  = {Oxford},
note      = {OCLC: 632965949},
pagetotal = {592},
publisher = {Clarendon Press},
file      = {:done/1998NeedhamVisual Visual Complex Analysis.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
year      = {1998},
}
@Book{2004OdianPrinciples,
author    = {Odian, George G.},
date      = {2004},
title     = {Principles of polymerization},
edition   = {4. ed},
isbn      = {978-0-471-27400-1},
location  = {Hoboken, NJ},
note      = {OCLC: 249002704},
pagetotal = {812},
publisher = {Wiley-Interscience},
file      = {:done/2004OdianPrinciples Principles of Polymerization.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2012ClaydenOrganic,
author    = {Clayden, Jonathan and Greeves, Nick and Warren, Stuart G.},
date      = {2012},
title     = {Organic chemistry},
edition   = {Second edition},
isbn      = {978-0-19-927029-3},
location  = {Oxford},
note      = {OCLC: 775825174},
pagetotal = {1234},
publisher = {Oxford University Press},
file      = {:done/2012ClaydenOrganic Organic Chemistry.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Textbooks, Chemistry, Organic},
langid    = {english},
}
@Book{2007HiemenzPolymer,
author    = {Hiemenz, Paul C. and Lodge, Timothy},
date      = {2007},
title     = {Polymer chemistry},
edition   = {2. ed},
isbn      = {978-1-57444-779-8},
location  = {Boca Raton, Fla.},
note      = {OCLC: 255638513},
pagetotal = {587},
publisher = {CRC Press},
file      = {:done/2007HiemenzPolymer Polymer Chemistry.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{MorrisFundamentals,
author = {Morris, Carla C.},
title  = {Fundamentals of Calculus},
pages  = {371},
file   = {:done/MorrisFundamentals Fundamentals of Calculus.pdf:application/pdf},
groups = {tesse:2, Calculus},
langid = {english},
}
@Book{2015KaabarFriendly,
author    = {Kaabar, Mohammed K. A.},
date      = {2015},
title     = {A Friendly Introduction to Differential Equations},
doi       = {10.13140/2.1.4168.4806},
publisher = {CreateSpace},
urldate   = {2019-03-31},
file      = {:done/2015KaabarFriendly A Friendly Introduction to Differential Equations.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2014BronsonSchaums,
author    = {Bronson, Richard and Costa, Gabriel},
date      = {2014},
title     = {Schaum's Outline of Differential Equations, 4th Edition.},
isbn      = {978-0-07-182485-9},
location  = {Blacklick},
note      = {OCLC: 887191871},
publisher = {McGraw-Hill Companies, The},
url       = {http://accessengineeringlibrary.com/browse/schaums-outline-of-differential-equations-fourth-edition},
urldate   = {2019-03-31},
file      = {:done/2014BronsonSchaums Schaum's Outline of Differential Equations, 4th Edition..pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2008HolznerDifferential,
author    = {Holzner, Steven},
date      = {2008-05-23},
title     = {Differential Equations For Dummies},
isbn      = {978-0-470-17814-0},
pages     = {363},
pagetotal = {360},
publisher = {John Wiley and Sons Ltd},
url       = {https://www.ebook.de/de/product/7119629/steven///////\\_holzner///////\\_differential///////\\_equations///////\\_for///////\\_dummies.html},
ean       = {9780470178140},
file      = {:done/2008HolznerDifferential Differential Equations for Dummies.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
year      = {2008},
}
@Book{2012NagleFundamentals,
author    = {Nagle, R. Kent and Saff, E. B. and Snider, Arthur David},
date      = {2012},
title     = {Fundamentals of differential equations},
isbn      = {978-0-321-74773-0 978-0-321-75820-0},
location  = {Boston},
note      = {OCLC: 701619880},
publisher = {Pearson Education},
file      = {:done/2012NagleFundamentals Fundamentals of Differential Equations.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2016AdzievskiIntroduction,
author    = {Adzievski, Kuzman},
date      = {2016-04-19},
title     = {Introduction to Partial Differential Equations for Scientists and Engineers Using Mathematica},
doi       = {10.1201/b15775},
edition   = {1},
isbn      = {978-1-4665-1057-9},
publisher = {Chapman and Hall/CRC},
url       = {https://www.taylorfrancis.com/books/9781466510579},
urldate   = {2019-03-31},
langid    = {english},
}
@Book{2015LoganFirst,
author    = {Logan, J. David},
date      = {2015},
title     = {A first course in differential equations},
edition   = {Third edition},
isbn      = {978-3-319-17851-6 978-3-319-17852-3},
location  = {Cham Heidelberg New York Dordrecht London},
note      = {OCLC: 915389739},
pagetotal = {369},
publisher = {Springer},
series    = {Undergraduate texts in mathematics},
file      = {:done/2015LoganFirst A First Course in Differential Equations.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{RudnickiFundamentals,
author = {Rudnicki, John W.},
title  = {Fundamentals of Continuum Mechanics},
pages  = {220},
file   = {:done/RudnickiFundamentals Fundamentals of Continuum Mechanics.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Book{2008GonzalezFirst,
author     = {Gonzalez, Oscar and Stuart, Andrew M.},
date       = {2008},
title      = {A First Course in Continuum Mechanics},
doi        = {10.1017/CBO9780511619571},
publisher  = {Cambridge University Press},
series     = {Cambridge Texts in Applied Mathematics},
collection = {Cambridge Texts in Applied Mathematics},
file       = {:done/2008GonzalezFirst A First Course in Continuum Mechanics.pdf:application/pdf},
groups     = {tesse:5},
place      = {Cambridge},
}
@Book{2014MuellerExpedition,
author    = {Müller, Wolfgang H.},
date      = {2014},
title     = {An Expedition to Continuum Theory},
doi       = {10.1007/978-94-007-7799-6},
isbn      = {978-94-007-7798-9 978-94-007-7799-6},
location  = {Dordrecht},
publisher = {Springer Netherlands},
series    = {Solid Mechanics and Its Applications},
urldate   = {2019-03-31},
volume    = {210},
file      = {:done/2014MuellerExpedition An Expedition to Continuum Theory.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2008IrgensContinuum,
author     = {Irgens, Fridtjov},
date       = {2008},
title      = {Continuum mechanics: with 4 tables},
isbn       = {978-3-540-74297-5 978-3-540-74298-2},
location   = {Berlin},
note       = {OCLC: 253962701},
pagetotal  = {661},
publisher  = {Springer},
file       = {:done/2008IrgensContinuum Continuum Mechanics/\\_ with 4 Tables.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Continuum mechanics},
}
@Book{2003SteinComplex,
author    = {Stein, Elias M. and Shakarchi, Rami},
date      = {2003},
title     = {Complex analysis},
isbn      = {978-0-691-11385-2},
location  = {Princeton, N.J. Oxford},
number    = {2},
pagetotal = {392},
publisher = {Princeton University Press},
series    = {Princeton lectures in analysis},
file      = {:done/2003SteinComplex Complex Analysis.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2001RaoTransform,
author    = {Rao, Kamisetty Ramamohan and Yip, Pat C.},
date      = {2001},
title     = {The transform and data compression handbook},
editor    = {Rao, Kamisetty Ramamohan and Yip, Pat C.},
isbn      = {978-0-8493-3692-8},
location  = {Boca Raton, Fla.},
note      = {OCLC: 247573781},
pagetotal = {388},
publisher = {CRC Press},
series    = {The electrical engineering and signal processing series},
file      = {:done/2001RaoTransform The Transform and Data Compression Handbook.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Misc{2006UlrichEnvelope,
author = {Ulrich, T. J.},
date   = {2006},
title  = {Envelope calculation from the Hilbert transform},
file   = {:done/2006UlrichEnvelope Envelope Calculation from the Hilbert Transform.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
pages  = {6},
}
@Book{2013DangeloHermitian,
author    = {D'Angelo, John P.},
date      = {2013},
title     = {Hermitian Analysis},
doi       = {10.1007/978-1-4614-8526-1},
isbn      = {978-1-4614-8525-4 978-1-4614-8526-1},
location  = {New York, NY},
publisher = {Springer New York},
series    = {Cornerstones},
urldate   = {2019-03-31},
file      = {:done/2013DangeloHermitian Hermitian Analysis.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@InCollection{2013BoschEarly,
author      = {Bosch, Jan and Holmström Olsson, Helena and Björk, Jens and Ljungblad, Jens},
booktitle   = {Lean Enterprise Software and Systems},
date        = {2013},
title       = {The Early Stage Software Startup Development Model: A Framework for Operationalizing Lean Principles in Software Startups},
doi         = {10.1007/978-3-642-44930-7///////\\_1},
editor      = {Fitzgerald, Brian and Conboy, Kieran and Power, Ken and Valerdi, Ricardo and Morgan, Lorraine and Stol, Klaas-Jan},
editorb     = {van der Aalst, Wil and Mylopoulos, John and Rosemann, Michael and Shaw, Michael J. and Szyperski, Clemens},
isbn        = {978-3-642-44929-1 978-3-642-44930-7},
location    = {Berlin, Heidelberg},
pages       = {1--15},
publisher   = {Springer Berlin Heidelberg},
urldate     = {2019-03-31},
volume      = {167},
abstract    = {Software startups are more popular than ever and growing in numbers. They operate under conditions of extreme uncertainty and face many challenges. Often, agile development practices and lean principles are suggested as ways to increase the odds of succeeding as a startup, as they both advocate close customer collaboration and short feedback cycles focusing on delivering direct customer value. However, based on an interview study we see that despite guidance and support in terms of well-known and documented development methods, practitioners find it difficult to implement and apply these in practice. To explore this further, and to propose operational support for software startup companies, this study aims at investigating (1) what are the typical challenges when finding a product idea worth scaling, and (2) what solution would serve to address these challenges. To this end, we propose the ‘Early Stage Software Startup Development Model’ (ESSSDM). The model extends already existing lean principles, but offers novel support for practitioners for investigating multiple product ideas in parallel, for determining when to move forward with a product idea, and for deciding when to abandon a product idea. The model was evaluated in a software startup project, as well as with industry professionals within the software startup domain.},
editorbtype = {redactor},
file        = {:done/2013BoschEarly The Early Stage Software Startup Development Model/\\_ a Framework for Operationalizing Lean Principles in Software Startups.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
shorttitle  = {The Early Stage Software Startup Development Model},
}
@InCollection{2015GiardinoKey,
author    = {Giardino, Carmine and Bajwa, Sohaib Shahid and Wang, Xiaofeng and Abrahamsson, Pekka},
booktitle = {Agile Processes in Software Engineering and Extreme Programming},
date      = {2015},
title     = {Key Challenges in Early-Stage Software Startups},
doi       = {10.1007/978-3-319-18612-2///////\\_5},
editor    = {Lassenius, Casper and Dingsøyr, Torgeir and Paasivaara, Maria},
isbn      = {978-3-319-18611-5 978-3-319-18612-2},
location  = {Cham},
pages     = {52--63},
publisher = {Springer International Publishing},
urldate   = {2019-03-31},
volume    = {212},
file      = {:done/2015GiardinoKey Key Challenges in Early Stage Software Startups.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2001RoadsMicrosound,
author    = {Roads, Curtis},
date      = {2001},
title     = {Microsound},
isbn      = {978-0-262-18215-7},
location  = {Cambridge, Mass.},
note      = {OCLC: 834185525},
pagetotal = {409},
publisher = {MIT Press},
file      = {:done/2001RoadsMicrosound Microsound.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Misc{2002SaloFinite,
author   = {Salo, Maaria},
date     = {2002},
title    = {Finite Difference Method in Sound Synthesis},
url      = {http://math.aalto.fi//textasciitilde ksalo2/akusem/sem///////\\_FDM.pdf},
abstract = {This paper reviews the ﬁnite difference method in the sound synthesis of string instruments. The mathematical basis for the method and the evaluation of the recursion equations are considered. Some stability conditions are discussed. Initial and boundary conditions are reviewed for piano- and guitar-like strings.},
file     = {:done/2002SaloFinite Finite Difference Method in Sound Synthesis.pdf:application/pdf},
groups   = {tesse:5, Finite Difference Methods},
langid   = {english},
pages    = {12},
}
@Book{1999WoodardNation,
author      = {Woodard, Komozi},
date        = {1999},
title       = {A nation within a nation: Amiri Baraka (LeRoi Jones) and Black Power politics},
editora     = {Baraka, Imamu Amiri},
isbn        = {978-0-8078-2457-3 978-0-8078-4761-9},
location    = {Chapel Hill, NC},
note        = {OCLC: 237335993},
pagetotal   = {329},
publisher   = {Univ. of North Carolina Press},
editoratype = {collaborator},
shorttitle  = {A nation within a nation},
}
@Article{2012TrevinoAudio,
author       = {Trevino, Jeffrey and Allen, Drew},
date         = {2012-06},
journaltitle = {Computer Music Journal},
title        = {The Audio Programming Book, Edited by Richard Boulanger and Victor Lazzarini},
doi          = {10.1162/COMJ///////\\_r///////\\_00121},
issn         = {0148-9267, 1531-5169},
number       = {2},
pages        = {85--89},
urldate      = {2019-03-31},
volume       = {36},
file         = {:done/2012TrevinoAudio The Audio Programming Book, Edited by Richard Boulanger and Victor Lazzarini.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Collection{2009CormenIntroduction,
date      = {2009},
editor    = {Cormen, Thomas H. and Leiserson, Charles Eric and Rivest, Ronald Linn and Stein, Clifford},
title     = {Introduction to algorithms},
edition   = {3. ed},
isbn      = {978-0-262-03384-8 978-0-262-53305-8},
location  = {Cambridge, Mass.},
note      = {OCLC: 698955316},
pagetotal = {1292},
publisher = {MIT Press},
abstract  = {I. Foundations. The role of algorithms in computing -- Getting started -- Growth of functions -- Divide-and-conquer -- Probabilistic analysis and randomized algorithms -- II. Sorting and order statistics. Heapsort -- Quicksort -- Sorting in linear time -- Medians and order statistics -- III. Data structures. Elementary data structures -- Hash tables -- Binary search trees -- Red-black trees -- Augmenting data structures -- IV. Advanced design and analysis techniques. Dynamic programming -- Greedy algorithms -- Amortized analysis -- V. Advanced data structures. B-trees -- Fibonacci heaps -- van Emde Boas trees -- Data structures for disjoint sets -- VI. Graph algorithms. Elementary graph algorithms -- Minimum spanning trees -- Single-source shortest paths -- All-pairs shortest paths -- Maximun flow -- VII. Selected topics. Multithreaded algorithms -- Matrix operations -- Linear programming -- Polynomials and the FFT -- Number-theoretic algorithms -- String matching -- Computational geometry -- NP-completeness -- Approximation algorithms -- VIII. Appendix: Mathematical background. Summations -- Sets, etc. -- Counting and probability -- Matrices},
file      = {:done/2009CormenIntroduction Introduction to Algorithms.pdf:application/pdf},
groups    = {tesse:5, Programming},
langid    = {english},
}
@Article{2010RabensteinTubular,
author       = {Rabenstein, Rudolf and Koch, Tilman and Popp, Christian},
date         = {2010-05},
journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
title        = {Tubular Bells: A Physical and Algorithmic Model},
doi          = {10.1109/TASL.2009.2035214},
issn         = {1558-7916},
number       = {4},
pages        = {881--890},
url          = {http://ieeexplore.ieee.org/document/5299084/},
urldate      = {2019-03-31},
volume       = {18},
abstract     = {Tubular bells are geometrically simple representatives of three-dimensional vibrating structures. Under certain assumptions, a tubular bell can be modeled as a rectangular plate with different types of homogeneous boundary conditions. Suitable functional transformations with respect to time and space turn the corresponding initial-boundary value problem into a two-dimensional transfer function. An algorithmic model follows according to the functional transformation method in digital sound synthesis. As with simpler vibrating structures (strings, membranes) the synthesis algorithms consist of a parallel arrangement of second-order sections. Their coefﬁcients are obtained by simple analytic expressions directly from the physical parameters of the tubular bell.},
file         = {:done/2010RabensteinTubular Tubular Bells/\\_ a Physical and Algorithmic Model.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Tubular Bells},
}
@Article{1993ItohCurve,
author       = {Itoh, Koichi and Ohno, Yoshio},
date         = {1993},
journaltitle = {Electronic publishing},
title        = {A curve fitting algorithm for character fonts},
number       = {3},
pages        = {195--205},
volume       = {6},
file         = {:done/1993ItohCurve A Curve Fitting Algorithm for Character Fonts.pdf:application/pdf},
groups       = {tesse:5},
publisher    = {Citeseer},
}
@Unpublished{2012BarendrechtGentle,
author = {Barendrecht, P. J.},
date   = {2012},
title  = {A gentle introduction to rational Bézier curves and NURBS},
file   = {:done/2012BarendrechtGentle A Gentle Introduction to Rational Bézier Curves and NURBS.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
}
@Misc{2008BertkaIntroduction,
author = {Bertka, Benjamin T},
date   = {2008},
title  = {An Introduction to Bezier Curves, B-Splines, and Tensor Product Surfaces with History and Applications},
langid = {english},
}
@Collection{2001RogersIntroduction,
date       = {2001},
editor     = {Rogers, David F.},
title      = {An introduction to NURBS: with historical perspective},
isbn       = {978-1-55860-669-2},
location   = {San Francisco, Calif.},
note       = {OCLC: 247648741},
pagetotal  = {324},
publisher  = {Morgan Kaufmann Publishers},
langid     = {english},
shorttitle = {An introduction to NURBS},
}
@Misc{2012SederbergComputer,
author = {Sederberg, Thomas W.},
date   = {2012},
title  = {Computer Aided Geometric Design},
file   = {:done/2012SederbergComputer Computer Aided Geometric Design.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
pages  = {289},
year   = {2012},
}
@Book{2006FarinCurves,
author     = {Farin, Gerald E.},
date       = {2006},
title      = {Curves and surfaces for CAGD: a practical guide},
edition    = {5. ed., [Nachdr.]},
isbn       = {978-1-55860-737-8},
location   = {San Francisco, Calif.},
note       = {OCLC: 254200301},
pagetotal  = {499},
publisher  = {Morgan Kaufmann Publ},
series     = {The Morgan Kaufmann series in computer graphics and geometric modeling},
file       = {:done/2006FarinCurves Curves and Surfaces for CAGD/\\_ a Practical Guide.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Curves and surfaces for CAGD},
}
@Article{2011BostockD³,
author       = {Bostock, M. and Ogievetsky, V. and Heer, J.},
date         = {2011-12},
journaltitle = {IEEE Transactions on Visualization and Computer Graphics},
title        = {D³ Data-Driven Documents},
doi          = {10.1109/TVCG.2011.185},
issn         = {1077-2626},
number       = {12},
pages        = {2301--2309},
url          = {http://ieeexplore.ieee.org/document/6064996/},
urldate      = {2019-03-31},
volume       = {17},
abstract     = {Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-speciﬁc abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efﬁciency and retaining powerful declarative components. Immediate evaluation of operators further simpliﬁes debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations.},
file         = {:done/2011BostockD³ D³ Data Driven Documents.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{cabrinha_bezier_nodate,
author   = {Cabrinha, Mark},
title    = {From Bézier to NURBS: Integrating Material and Digital Techniques through a Plywood Shell},
pages    = {14},
abstract = {The development of digital fabrication has reintroduced material processes with digital processes. There has been much discussion about the tool and the objects of the tool, but little discussion of the implication of the material process on the digital process. A brief historical review on the development of computer numerical control and the origins of the Bézier curve reveals an instrumental fact: computer numerical controlled tools necessitated advancements in computational surfaces which eventually led to NURBS (Non-Uniform Rational B-Splines) surfaces. In other words, the origins of NURBS surfaces resides in its relation to material processes, rather than many current approaches that develop free form surfaces and then force the tool onto the material without regard to the material properties.},
langid   = {english},
}
@Article{RamshawMultiplying,
author = {Ramshaw, Lyle},
title  = {On multiplying points: the paired algebras of forms and sites},
pages  = {173},
file   = {:done/RamshawMultiplying On Multiplying Points\\_ the Paired Algebras of Forms and Sites.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Book{2010PatrikalakisShape,
author    = {Patrikalakis, Nicholas M. and Maekawa, Takashi},
date      = {2010},
title     = {Shape Interrogation for Computer Aided Design and Manufacturing},
doi       = {10.1007/978-3-642-04074-0},
isbn      = {978-3-642-04073-3 978-3-642-04074-0},
location  = {Berlin, Heidelberg},
publisher = {Springer Berlin Heidelberg},
urldate   = {2019-03-31},
file      = {:done/2010PatrikalakisShape Shape Interrogation for Computer Aided Design and Manufacturing.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{2012FaroukiBernstein,
author       = {Farouki, Rida T.},
date         = {2012-08},
journaltitle = {Computer Aided Geometric Design},
title        = {The Bernstein polynomial basis: A centennial retrospective},
doi          = {10.1016/j.cagd.2012.03.001},
issn         = {0167-8396},
number       = {6},
pages        = {379--419},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0167839612000192},
urldate      = {2019-03-31},
volume       = {29},
abstract     = {One hundred years after the introduction of the Bernstein polynomial basis, we survey the historical development and current state of theory, algorithms, and applications associated with this remarkable method of representing polynomials over ﬁnite domains. Originally introduced by Sergei Natanovich Bernstein to facilitate a constructive proof of the Weierstrass approximation theorem, the leisurely convergence rate of Bernstein polynomial approximations to continuous functions caused them to languish in obscurity, pending the advent of digital computers. With the desire to exploit the power of computers for geometric design applications, however, the Bernstein form began to enjoy widespread use as a versatile means of intuitively constructing and manipulating geometric shapes, spurring further development of basic theory, simple and eﬃcient recursive algorithms, recognition of its excellent numerical stability properties, and an increasing diversiﬁcation of its repertoire of applications. This survey provides a brief historical perspective on the evolution of the Bernstein polynomial basis, and a synopsis of the current state of associated algorithms and applications.},
file         = {:done/2012FaroukiBernstein The Bernstein Polynomial Basis/\\_ a Centennial Retrospective.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {The Bernstein polynomial basis},
}
@Book{1995LawsonSolving,
author    = {Lawson, Charles L. and Hanson, Richard J.},
date      = {1995},
title     = {Solving least squares problems},
edition   = {10. Dr.},
isbn      = {978-0-89871-356-5},
location  = {Philadelphia},
note      = {OCLC: 844911591},
number    = {15},
pagetotal = {337},
publisher = {SIAM},
series    = {Classics in applied mathematics},
file      = {:done/1995LawsonSolving Solving Least Squares Problems.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2000BloomfieldFourier,
author     = {Bloomfield, Peter},
date       = {2000-01-21},
title      = {Fourier Analysis of Time Series: An Introduction},
doi        = {10.1002/0471722235},
isbn       = {978-0-471-72223-6 978-0-471-88948-9},
location   = {Hoboken, NJ, USA},
publisher  = {John Wiley ////////\\\& Sons, Inc.},
series     = {Wiley Series in Probability and Statistics},
url        = {https://iujfk.files.wordpress.com/2013/04/bloomfield-2000-fourier-analysis-of-time-series-an-introduction-2ed.pdf},
urldate    = {2019-03-31},
shorttitle = {Fourier Analysis of Time Series},
}
@Thesis{2002AirdMusical,
author      = {Aird, Marc-Laurent},
date        = {2002},
institution = {University of Bath},
title       = {Musical instrument modelling using digital waveguides},
type        = {phdthesis},
file        = {:done/2002AirdMusical Musical Instrument Modelling Using Digital Waveguides.pdf:application/pdf},
groups      = {tesse:5, Digital Waveguides},
}
@InProceedings{2002BakkerReinforcement,
author    = {Bakker, Bram},
booktitle = {In NIPS},
date      = {2002},
title     = {Reinforcement Learning with Long Short-Term Memory},
pages     = {1475--1482},
publisher = {MIT Press},
url       = {https://papers.nips.cc/paper/1953-reinforcement-learning-with-long-short-term-memory.pdf},
abstract  = {This paper presents reinforcement learning with a Long ShortTerm Memory recurrent neural network: RL-LSTM. Model-free RL-LSTM using Advantage /\\# /\\# /\\# learning and directed exploration can solve non-Markovian tasks with long-term dependencies between relevantevents. This is demonstrated in a T-maze task, as well as in a di /\\#cult variation of the pole balancing task. 1},
file      = {:done/2002BakkerReinforcement Reinforcement Learning with Long Short Term Memory.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2012PieglNurbs,
author    = {Piegl, Les and Tiller, Wayne},
date      = {2012},
title     = {The NURBS book},
isbn      = {978-3-540-61545-3},
publisher = {Springer Science //\\\& Business Media},
file      = {:done/2012PieglNurbs The NURBS Book.pdf:application/pdf},
groups    = {tesse:5},
}
@InProceedings{2005SmithViewpoints,
author    = {Smith, Julius O.},
booktitle = {Proceedings of the International Computer Music Conference},
date      = {2005},
title     = {Viewpoints on the history of digital synthesis},
pages     = {1--1},
publisher = {INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
url       = {https://ccrma.stanford.edu//textasciitilde jos/kna/kna.pdf},
file      = {:done/2005SmithViewpoints Viewpoints on the History of Digital Synthesis.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{2004Martinez-friasSegmentation,
author       = {Martínez-Frías, M. L.},
date         = {2004-07-15},
journaltitle = {American Journal of Medical Genetics. Part A},
title        = {Segmentation anomalies of the vertebras and ribs: one expression of the primary developmental field},
doi          = {10.1002/ajmg.a.30016},
issn         = {1552-4825},
number       = {2},
pages        = {127--131},
volume       = {128A},
pmid         = {15214001},
shortjournal = {Am. J. Med. Genet. A},
shorttitle   = {Segmentation anomalies of the vertebras and ribs},
}
@Collection{2017InstituteGuide,
date      = {2017},
editor    = {Institute, Project Management},
title     = {A guide to the project management body of knowledge / Project Management Institute},
edition   = {Sixth edition},
isbn      = {978-1-62825-184-5},
location  = {Newtown Square, PA},
pagetotal = {756},
publisher = {Project Management Institute},
series    = {PMBOK guide},
abstract  = {"The PMBOK},
}
@Book{2017AnthonyFullstack,
author     = {Anthony, Accomazzo and Nathaniel, Murray and Ari, Lerner},
date       = {2017-09-12},
title      = {Fullstack React: The Complete Guide to ReactJS and Friends},
isbn       = {978-0-9913446-2-8},
pagetotal  = {836},
publisher  = {Fullstack.io},
abstract   = {Stop wasting your time learning React with incomplete and confusing tutorials. There are so many incorrect, confusing, and out-of-date blog articles One tutorial says one thing and another says something completely different. There are too many options There are fifty different boilerplates and a dozen different Flux implementations. Which one is best? What if you could master the entire framework in less time, with solid foundations, without beating your head against the wall? Imagine how quickly you can get all of your work done with the right tools and best practices. Seriously, let's stop wasting time scouring Google, searching through incorrect, out-of-date, blog posts and get everything you need to be productive in one, well-organized place, complete with both simple and complex examples to get your app up and running. You'll learn what you need to know to work professionally and build solid, well-tested, optimized apps with ReactJS. This book is your definitive guide.},
file       = {:done/2017AnthonyFullstack Fullstack React/\\_ the Complete Guide to ReactJS and Friends.pdf:application/pdf},
groups     = {tesse:5},
shorttitle = {Fullstack React},
}
@Book{2015GottschlingDiscovering,
author     = {Gottschling, Peter},
date       = {2015-12-27},
title      = {Discovering Modern C++: An Intensive Course for Scientists, Engineers, and Programmers},
edition    = {1 edition},
isbn       = {978-0-13-438358-3},
location   = {Boston},
pagetotal  = {480},
publisher  = {Addison-Wesley Professional},
abstract   = {As scientific and engineering projects grow larger and more complex, it is increasingly likely that those projects will be written in C++. With embedded hardware growing more powerful, much of its software is moving to C++, too. Mastering C++ gives you strong skills for programming at nearly every level, from "close to the hardware" to the highest-level abstractions. In short, C++ is a language that scientific and technical practitioners need to know. Peter Gottschling’s Discovering Modern C++ is an intensive introduction that guides you smoothly to sophisticated approaches based on advanced features. Gottschling introduces key concepts using examples from many technical problem domains, drawing on his extensive experience training professionals and teaching C++ to students of physics, math, and engineering. This book is designed to help you get started rapidly and then master increasingly robust features, from lambdas to expression templates. You’ll also learn how to take advantage of the powerful libraries available to C++ programmers: both the Standard Template Library (STL) and scientific libraries for arithmetic, linear algebra, differential equations, and graphs. Throughout, Gottschling demonstrates how to write clear and expressive software using object orientation, generics, metaprogramming, and procedural techniques. By the time you’re finished, you’ll have mastered all the abstractions you need to write C++ programs with exceptional quality and performance.},
shorttitle = {Discovering Modern C++},
}
@Book{2014DineenMultivariate,
author    = {Dineen, Seán},
date      = {2014},
title     = {Multivariate Calculus and Geometry},
edition   = {3},
isbn      = {978-1-4471-6418-0},
location  = {London},
publisher = {Springer-Verlag},
series    = {Springer Undergraduate Mathematics Series},
url       = {https://www.springer.com/gp/book/9781447164180},
urldate   = {2019-03-31},
abstract  = {Multivariate calculus can be understood best by combining geometric insight, intuitive arguments, detailed explanations and mathematical reasoning. This textbook not only follows this programme, but additionally provides a solid description of the basic concepts, via familiar examples, which are then tested in technically demanding situations. In this new edition the introductory chapter and two of the chapters on the geometry of surfaces have been revised. Some exercises have been replaced and others provided with expanded solutions. Familiarity with partial derivatives and a course in linear algebra are essential prerequisites for readers of this book. Multivariate Calculus and Geometry is aimed primarily at higher level undergraduates in the mathematical sciences. The inclusion of many practical examples involving problems of several variables will appeal to mathematics, science and engineering students.},
file      = {:done/2014DineenMultivariate Multivariate Calculus and Geometry.epub:ePUB},
groups    = {tesse:5, Calculus},
langid    = {english},
}
@Book{1965FinneyPrinciples,
author    = {Finney, Harry Hanson and Miller, Herbert Elmer},
date      = {1965},
title     = {Principles of accounting intermediate},
isbn      = {978-0-13-702852-8},
location  = {Englewood Cliffs, N.J.},
note      = {OCLC: 799320107},
publisher = {Prentice-Hall},
}
@Misc{2017LessardEquality,
author = {Lessard, Laurent},
date   = {2017},
title  = {Equality constraints and tradeoffs},
note   = {Introduction to Optimization},
url    = {https://laurentlessard.com/teaching/cs524/slides/9///////\\%20-///////\\%20equality///////\\%20constraints///////\\%20and///////\\%20tradeoffs.pdf},
file   = {:done/2017LessardEquality Equality Constraints and Tradeoffs.pdf:application/pdf},
groups = {tesse:5},
}
@Article{2008DonosoFisica,
author       = {Donoso, José Pedro and Tannús, Alberto and Guimarães, Francisco and Freitas, Thiago Corrêa de},
date         = {2008},
journaltitle = {Revista Brasileira de Ensino de Física},
title        = {A física do violino},
doi          = {10.1590/S1806-11172008000200006},
issn         = {1806-1117},
number       = {2},
pages        = {23051--230521},
url          = {http://www.scielo.br/scielo.php?pid=S0102-47442008000200006///////\\&script=sci///////\\_arttext},
urldate      = {2019-03-31},
volume       = {30},
file         = {:done/2008DonosoFisica A Física Do Violino.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{2013SantosBalanco,
author       = {dos Santos, Anderson Pires and Rosa, Luciênio and Júnior, Silva},
date         = {2013},
journaltitle = {Revista UNEMAT de Contabilidade},
title        = {BALANÇO SOCIAL: UMA ANÁLISE COMPARATIVA DOS INDICADORES SOCIAIS INTERNOS E EXTERNOS DAS INSTITUIÇÕES FINANCEIRAS CAIXA E BANCO DO BRASIL 2002 A 2011},
number       = {4},
volume       = {2},
shorttitle   = {BALANÇO SOCIAL},
}
@MvBook{2008DahlquistNumerical,
author    = {Dahlquist, Germund},
date      = {2008},
title     = {Numerical Methods in Scientific Computing},
isbn      = {978-1-4122-2499-4},
note      = {Google-Books-ID: 2i7TjgEACAAJ},
pagetotal = {book},
publisher = {Society for Industrial and Applied Mathematics},
volumes   = {2},
file      = {:done/2008DahlquistNumerical Numerical Methods in Scientific Computing.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
volume    = {2},
}
@Book{2016LibbyResponsive,
author    = {Libby, Alex and Gupta, Gaurav and Talesra, Asoj},
date      = {2016-08-29},
title     = {Responsive Web Design with HTML5 and CSS3 Essentials},
isbn      = {978-1-78355-307-5},
location  = {Birmingham, UK},
pagetotal = {174},
publisher = {Packt Publishing - ebooks Account},
abstract  = {Design and deliver an optimal user experience for all devices About This Book Get to grips with the core functionality of RWD through examples Discover how to make layouts, content and media flexible, and explore why a content-first approach is more effective Maximize the performance of your web pages so that they work across all browsers and devices irrespective of the screen size Who This Book Is For This book is for web designers who are familiar with HTML and CSS, and want to begin with responsive web design. Web development experience and knowledge of HTML5, CSS3 is assumed. What You Will Learn Explore various layout options Understand what can be achieved in the browser, without the use of third-party tools Executing media queries to benefit responsive designs Understand the basics of responsive workflow and boilerplate frameworks Improve performance of responsive web design Maintain compatibility across various browsers In Detail Responsive web design (RWD) is a web design approach aimed at crafting sites to provide an optimal viewing and interaction experience—providing easy reading and navigation with minimum resizing, panning, and scrolling—and all of this across a wide range of devices from desktop computer monitors to mobile phones. Responsive web design is becoming more important as the amount of mobile traffic now accounts for more than half of the Internet’s total traffic. This book will give you in depth knowledge about the basics of responsive web design. You will embark on a journey of building effective responsive web pages that work across a range of devices, from mobile phones to smart TVs, with nothing more than standard markup and styling techniques. You'll begin by getting an understanding of what RWD is and its significance to the modern web. Building on the basics, you'll learn about layouts and media queries. Following this, we’ll dive into creating layouts using grid based templates. We’ll also cover the important topic of performance management, and discover how to tackle cross-browser challenges. Style and approach This is a practical example-based book which will delve into various elements and benefits of a responsive web design. It will help you understand the essential skills needed to create responsive web sites and guide you through the basics of building responsive web pages for any device. The topics are a blend of theoretical and practical essentials which will assist you to explore more about responsive web design.},
file      = {:done/2016LibbyResponsive Responsive Web Design with HTML5 and CSS3 Essentials.epub:ePUB},
groups    = {tesse:5},
}
@Book{2016HolecekDream,
author     = {Holecek, Andrew and LaBerge, Stephen},
date       = {2016-07-01},
title      = {Dream Yoga: Illuminating Your Life Through Lucid Dreaming and the Tibetan Yogas of Sleep},
pagetotal  = {352},
publisher  = {Sounds True},
abstract   = {Lucid dreaming—becoming fully conscious in the dream state—has attracted legions of those seeking to explore their vast inner worlds. Yet our states of sleep offer much more than entertainment. Combining modern lucid dreaming principles with the time-tested insights of Tibetan dream yoga makes this astonishing yet elusive experience both easier to access and profoundly life-changing. With Dream Yoga, Andrew Holecek presents a practical guide for meditators, lucid dreamers ready to go deeper, and complete beginners. Topics include: meditations and techniques for dream induction and lucidity, enhancing dream recall, dream interpretation, working with nightmares, and more.},
shorttitle = {Dream Yoga},
}
@Book{2017GulliDeep,
author     = {Gullì, Antonio and Pal, Sujit},
date       = {2017},
title      = {Deep learning with Keras: implement neural networks with Keras on Theano and TensorFlow},
isbn       = {978-1-78712-842-2 978-1-78712-903-0},
location   = {Birmingham Mumbai},
pagetotal  = {304},
publisher  = {Packt},
shorttitle = {Deep learning with Keras},
}
@Book{2004FeigenbaumTotal,
author    = {Feigenbaum, Armand V.},
date      = {2004-08-01},
title     = {Total Quality Control},
edition   = {3 edition},
isbn      = {978-0-07-022003-4},
location  = {New York; London},
pagetotal = {896},
publisher = {McGraw-Hill Professional},
abstract  = {Total Quality Control integrates an organization's quality development with existing business practices to produce quantifiable customer satisfaction. The heavily-revised Fourth Edition introduces new TQM methodologies and shows how to achieve productivity, market penetration, and thrive in a global economy. * New emphasis on managing, operating, and integrating the key areas of a company's quality value chain * Six Sigma and its role in Total Quality Management * Ensuring customer satisfaction and retention},
file      = {:done/2004FeigenbaumTotal Total Quality Control.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{1992WidrowBackpropagation,
author       = {Widrow, B. and Lehr, M.},
date         = {1992},
journaltitle = {Proceedings of the INNS Summer Workshop on Neural Network Computing for the Electric Power Industry},
title        = {Backpropagation and its Applications},
url          = {http://www-isl.stanford.edu//textasciitilde widrow/papers/c1992backpropagationand.pdf},
file         = {:done/1992WidrowBackpropagation Backpropagation and Its Applications.pdf:application/pdf},
groups       = {tesse:5},
}
@Book{2015AzadCalculus,
author     = {Azad, Kalid},
date       = {2015},
title      = {Calculus, Better Explained: A Guide To Developing Lasting Intuition},
pagetotal  = {87},
abstract   = {Calculus, Better Explained is the calculus primer you wish you had in school. Learn the essential concepts using concrete analogies and vivid diagrams, not mechanical definitions. Calculus isn't a set of rules, it's a specific, practical viewpoint we can apply to everyday thinking.Frustrated With Abstract, Mechanical Lessons? I was too. Despite years of classes, I didn't have a strong understanding of calculus concepts. Sure, I could follow mechanical steps, but I had no lasting intuition.The classes I've seen are too long, taught in the wrong order, and without solid visualizations. Here's how this course is different:1) It gets to the point. A typical class plods along, saving concepts like Integrals until Week 8. I want to see what calculus can offer by Minute 8. Each compact, tightly-written lesson can be read in 15 minutes.2) Concepts are taught in their natural order. Most classes begin with the theory of limits, a technical concept discovered 150 years after calculus was invented. That's like putting a new driver into a Formula-1 racecar on day 1. We can begin with the easy-to-grasp concepts discovered 2000 years ago.3) It has vivid analogies and visualizations. Calculus is usually defined as the "study of change"... which sounds like history or geology. Instead of an abstract definition, we'll see calculus a step-by-step viewpoint to explore patterns.4) It's written by a human, for humans. I'm not a haughty professor or strict schoolmarm. I'm a friend who saw a fun way to internalize some difficult ideas. This course is a chat over coffee, not a keep-your-butt-in-your-seat lecture.The goal is to help you grasp the Aha! moments behind calculus in hours, not a painful semester (or a decade, in my case).Join Thousands Of Happy ReadersHere's a few samples of anonymous feedback as people went through the course. The material covers a variety of levels, whether you're looking for intuitive appreciation or the specifics of the rules."I've done all of this stuff before, and I do understand calculus intuitively, but this was the most fun I've had going through this kind of thing. The informal writing and multitude of great analogies really helps this become an enjoyable read and the rest is simple after that - you make this seem easy, but at the same time, you aren't doing it for us…This is what math education is supposed to be like :)""I have psychology and medicine background so I relate your ideas to my world. To me the most useful idea was what each circle production feels like. Rings are natural growth…Slices are automatable chunks and automation cheapens production… Boards in the shape on an Arch are psychologically most palatable for work (wind up, hard part, home stretch). Brilliant and kudos, from one INTP to another.""I like how you're introducing both derivatives and integrals at the same time - it's really helps with understanding the relationship between them. Also, I appreciate how you're coming from such a different angle than is traditionally taken - it's always interesting to see where you decide to go next.""That was breathtaking. Seriously, mail my air back please, I've grown used to it. Beautiful work, thank you. Lesson 15 was masterful. I am starting to feel calculus. "d/dx is good" (sorry, couldn't resist!)."},
shorttitle = {Calculus, Better Explained},
}
@Book{1979GareyComputers,
author     = {Garey, Michael R. and Johnson, David S.},
date       = {1979-01-15},
title      = {Computers and Intractability: A Guide to the Theory of NP-Completeness},
edition    = {1st Edition edition},
isbn       = {978-0-7167-1045-5},
location   = {New York u.a},
pagetotal  = {340},
publisher  = {W. H. Freeman},
abstract   = {A readable, straightforward guide by two authors with extensive experience in the field. This text shows how to recognize NP-complete problems and offers practical suggestions for dealing with them effectively. It is suitable as a supplement to courses in algorithm design, computational complexity, operations research, or combinatorial mathematics, and as a text for seminars on approximation algorithms or computational complexity.},
file       = {:done/1979GareyComputers Computers and Intractability/\\_ a Guide to the Theory of NP Completeness.pdf:application/pdf},
groups     = {tesse:5},
shorttitle = {Computers and Intractability},
}
@Book{1973GillespieQuantum,
author    = {Gillespie, Daniel T.},
date      = {1973},
title     = {A quantum mechanics primer},
isbn      = {978-0-7002-2290-2},
location  = {Scranton, Pa},
pagetotal = {137},
publisher = {International Textbook Co},
url       = {https://www.amazon.com/quantum-mechanics-primer-Daniel-Gillespie/dp/0700222901?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0700222901},
abstract  = {Book by Gillespie, Daniel T},
file      = {:done/1973GillespieQuantum A Quantum Mechanics Primer.pdf:application/pdf},
groups    = {tesse:5, Quantum Mechanics},
year      = {1973},
}
@Book{1997MarcondesIniciacao,
author     = {Marcondes, Danilo},
date       = {1997},
title      = {Iniciação à história da filosofia: Dos pré-socráticos a Wittgenstein},
pagetotal  = {435},
publisher  = {Zahar},
abstract   = {Resultado de mais de quinze anos dedicados ao ensino da filosofia, este verdadeiro guia da história do pensamento ocidental situa pensadores e correntes filosóficas em seu contexto histórico, discute idéias e conceitos e, quando necessário, apresenta os textos mais relevantes dos filósofos em questão, como o mito da caverna de Platão ou a tabela dos juízos e categorias de Kant. Dividido cronologicamente em quatro partes - filosofia antiga, medieval, moderna e contemporânea -, após cada capítulo o livro traz quadros sinóticos que recapitulam a matéria estudada, uma seleção de leituras sugeridas, além de propor questões e temas para discussão em sala de aula, o que reforça e traduz o caráter expressamente didático da obra. Conta também com índice remissivo.},
shorttitle = {Iniciação à história da filosofia},
}
@Book{ceddia_pure_nodate,
author     = {Ceddia, Dave},
title      = {Pure React: A step-by-step guide to mastering React.},
pagetotal  = {191},
abstract   = {Front-end development today is massively overwhelming.The React ecosystem is huge – Redux, React Router, Webpack, Babel… Where do you even start?Most people begin by reading tutorials, but sifting through them to find the one that makes it 'click' is an exercise in frustration. On top of that, they dish out pages of code and leave it up to you to "learn by osmosis."If you've tried to learn React on your own, you've probably had the same experience I did: after sifting through countless tutorials and video, you still can’t cobble together your own app. One second you’re nodding along, sure that you understand. The next, you’re staring at a blinking cursor wondering what to type.Thousands of people are already using React in their jobs or startups, and you can too -- once you figure it out.But what if you could skip straight to being as productive with React as you are with your current framework of choice?What if you were able to code as fast as you can think, using the JavaScript you know and love?Writing apps could be fun again! And fast.Teaching yourself React can be overwhelming… but it doesn’t have to be. You can master the fundamentals in a matter of days.Here's the thing: you need real practice. Not just nodding along to tutorials, but actually building examples and exercises. That's how you gain mastery, and this book will show you how.You’ll get hands-on practice building a series of small components and micro apps – no big monolithic app here. The bite-size apps will have you reviewing concepts until they’re second nature.With the included exercises you will be writing your own code right from the start – this isn’t a "copy, paste, watch it run" tutorial.Don't worry, we won't build any "ToDo" apps. The world already has enough of those.Learn to turn mockups directly into code, how to "think in components," and how to handle changing state in React's world of one-way data flow.Here's what the book covers:- Easy project setup with Create React App (you’ll be running code within minutes)- Debugging strategies for when things go wrong- Mastering JSX syntax, including "if"s, loops, and dynamic child components- Using props to communicate between components, and make them as reusable as possible- How PropTypes can save you time debugging and help "future you" remember how to use the components you wrote- Using the "children" prop to render dynamic content- How to write React in the modern ES6 style, with a gentle introduction to ES6 syntax (you don't need to know ES6!)- How input controls work in React (controlled and uncontrolled)- Where and how to properly use component state in a React application- Stateful vs Stateless components- The lifecycle of a component, and how to use it to your advantageThe book includes examples large and small, and 25 exercises to hone your knowledge.Buy Pure React and start learning React today!},
shorttitle = {Pure React},
}
@Book{2014PooleLinear,
author     = {Poole, David},
date       = {2014-01-08},
title      = {Linear Algebra: A Modern Introduction},
edition    = {4 edition},
isbn       = {978-1-285-46324-7},
location   = {Stamford, CT},
pagetotal  = {720},
publisher  = {Cengage Learning},
abstract   = {David Poole's innovative LINEAR ALGEBRA: A MODERN INTRODUCTION, 4e emphasizes a vectors approach and better prepares students to make the transition from computational to theoretical mathematics. Balancing theory and applications, the book is written in a conversational style and combines a traditional presentation with a focus on student-centered learning. Theoretical, computational, and applied topics are presented in a flexible yet integrated way. Stressing geometric understanding before computational techniques, vectors and vector geometry are introduced early to help students visualize concepts and develop mathematical maturity for abstract thinking. Additionally, the book includes ample applications drawn from a variety of disciplines, which reinforce the fact that linear algebra is a valuable tool for modeling real-life problems.},
file       = {:done/2014PooleLinear Linear Algebra/\\_ a Modern Introduction.pdf:application/pdf},
groups     = {tesse:5, Linear Algebra},
shorttitle = {Linear Algebra},
}
@Book{2015McfarlandCss,
author     = {McFarland, David Sawyer},
date       = {2015-08-13},
title      = {CSS: The Missing Manual},
edition    = {4 edition},
pagetotal  = {720},
publisher  = {O'Reilly Media},
abstract   = {CSS lets you create professional-looking websites, but learning its finer points can be tricky—even for seasoned web developers. This fully updated edition provides the most modern and effective tips, tricks, and tutorial-based instruction on CSS available today. Learn how to use new tools such as Flexbox and Sass to build web pages that look great and run fast on any desktop or mobile device. Ideal for casual and experienced designers alike.The important stuff you need to know:Start with the basics. Write CSS-friendly HTML, including the HTML5 tags recognized by today’s browsers.Design for mobile devices. Create web pages that look great when visitors use them on the go.Make your pages work for you. Add animations that capture the imagination, and forms that get the job done.Take control of page layouts. Use professional design techniques such as floats and positioning.Make your layouts more flexible. Design websites with Flexbox that adjust to different devices and screen sizes.Work more efficiently. Write less CSS code and work with smaller files, using Syntactically Awesome Stylesheets (Sass).},
file       = {:done/2015McfarlandCss CSS/\\_ the Missing Manual.epub:ePUB},
groups     = {tesse:5},
shorttitle = {CSS},
}
@Book{lazyprogrammer_deep_nodate,
author     = {LazyProgrammer},
title      = {Deep Learning: Recurrent Neural Networks in Python: LSTM, GRU, and more RNN machine learning architectures in Python and Theano},
pagetotal  = {56},
abstract   = {LSTM, GRU, and more advanced recurrent neural networksLike Markov models, Recurrent Neural Networks are all about learning sequences - but whereas Markov Models are limited by the Markov assumption, Recurrent Neural Networks are not - and as a result, they are more expressive, and more powerful than anything we’ve seen on tasks that we haven’t made progress on in decades.In the first section of the course we are going to add the concept of time to our neural networks.I’ll introduce you to the Simple Recurrent Unit, also known as the Elman unit.We are going to revisit the XOR problem, but we’re going to extend it so that it becomes the parity problem - you’ll see that regular feedforward neural networks will have trouble solving this problem but recurrent networks will work because the key is to treat the input as a sequence.In the next section of the book, we are going to revisit one of the most popular applications of recurrent neural networks - language modeling.One popular application of neural networks for language is word vectors or word embeddings. The most common technique for this is called Word2Vec, but I’ll show you how recurrent neural networks can also be used for creating word vectors.In the section after, we’ll look at the very popular LSTM, or long short-term memory unit, and the more modern and efficient GRU, or gated recurrent unit, which has been proven to yield comparable performance.We’ll apply these to some more practical problems, such as learning a language model from Wikipedia data and visualizing the word embeddings we get as a result.All of the materials required for this course can be downloaded and installed for FREE. We will do most of our work in Numpy, Matplotlib, and Theano. I am always available to answer your questions and help you along your data science journey.See you in class!"Hold up... what’s deep learning and all this other crazy stuff you’re talking about?"If you are completely new to deep learning, you might want to check out my earlier books and courses on the subject:Deep Learning in Python https://www.amazon.com/dp/B01CVJ19E8Deep Learning in Python Prerequisities https://www.amazon.com/dp/B01D7GDRQ2Much like how IBM’s Deep Blue beat world champion chess player Garry Kasparov in 1996, Google’s AlphaGo recently made headlines when it beat world champion Lee Sedol in March 2016.What was amazing about this win was that experts in the field didn’t think it would happen for another 10 years. The search space of Go is much larger than that of chess, meaning that existing techniques for playing games with artificial intelligence were infeasible. Deep learning was the technique that enabled AlphaGo to correctly predict the outcome of its moves and defeat the world champion.Deep learning progress has accelerated in recent years due to more processing power (see: Tensor Processing Unit or TPU), larger datasets, and new algorithms like the ones discussed in this book.},
shorttitle = {Deep Learning},
}
@Book{2019RaoNatural,
author     = {Rao, Delip and McMahan, Brian},
date       = {2019-02-11},
title      = {Natural Language Processing with PyTorch: Build Intelligent Language Applications Using Deep Learning},
edition    = {1 edition},
isbn       = {978-1-4919-7823-8},
location   = {Beijing Boston Farnham},
pagetotal  = {256},
publisher  = {O'Reilly Media},
abstract   = {Natural Language Processing (NLP) provides boundless opportunities for solving problems in artificial intelligence, making products such as Amazon Alexa and Google Translate possible. If you’re a developer or data scientist new to NLP and deep learning, this practical guide shows you how to apply these methods using PyTorch, a Python-based deep learning library.Authors Delip Rao and Brian McMahon provide you with a solid grounding in NLP and deep learning algorithms and demonstrate how to use PyTorch to build applications involving rich representations of text specific to the problems you face. Each chapter includes several code examples and illustrations.Explore computational graphs and the supervised learning paradigmMaster the basics of the PyTorch optimized tensor manipulation libraryGet an overview of traditional NLP concepts and methodsLearn the basic ideas involved in building neural networksUse embeddings to represent words, sentences, documents, and other featuresExplore sequence prediction and generate sequence-to-sequence modelsLearn design patterns for building production NLP systems},
shorttitle = {Natural Language Processing with PyTorch},
}
@Book{SportoElm,
author = {sporto},
title  = {Elm Tutorial},
url    = {https://github.com/sporto/elm-tutorial},
file   = {:done/SportoElm Elm Tutorial.epub:ePUB},
groups = {tesse:2, Programming},
}
@Book{1990MooreElements,
author    = {Moore, F. Richard},
date      = {1990-02-19},
title     = {Elements of Computer Music},
edition   = {1st edition},
isbn      = {978-0-13-252552-7},
location  = {Englewood Cliffs, N.J},
pagetotal = {560},
publisher = {Prentice Hall},
abstract  = {This is a general introduction to the theory of computer music, giving details on sound, digital signal processing, math, and C programming. It assumes a strong knowledge of music.},
file      = {:done/1990MooreElements Elements of Computer Music.djvu:Djvu},
groups    = {tesse:5},
}
@Thesis{2017MartinApplication,
author      = {Martin, Muzika},
date        = {2017},
institution = {Czech Technical University in Prague},
title       = {Application of Game Theoretic Algorithms to Gomoku},
type        = {phdthesis},
url         = {https://dspace.cvut.cz/bitstream/handle/10467/70078/F3-BP-2017-Muzika-Martin-Application///////\\%20of///////\\%20Game///////\\%20Theoretic///////\\%20Algorithms.pdf},
file        = {:done/2017MartinApplication Application of Game Theoretic Algorithms to Gomoku.pdf:application/pdf},
groups      = {tesse:5},
}
@Book{2015KelleherFundamentals,
author     = {Kelleher, John D. and Namee, Brian Mac and D'Arcy, Aoife},
date       = {2015-07-31},
title      = {Fundamentals of Machine Learning for Predictive Data Analytics: Algorithms, Worked Examples, and Case Studies},
edition    = {1 edition},
pagetotal  = {624},
publisher  = {The MIT Press},
abstract   = {A comprehensive introduction to the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications.Machine learning is often used to build predictive models by extracting patterns from large datasets. These models are used in predictive data analytics applications including price prediction, risk assessment, predicting customer behavior, and document classification. This introductory textbook offers a detailed and focused treatment of the most important machine learning approaches used in predictive data analytics, covering both theoretical concepts and practical applications. Technical and mathematical material is augmented with explanatory worked examples, and case studies illustrate the application of these models in the broader business context.After discussing the trajectory from data to insight to decision, the book describes four approaches to machine learning: information-based learning, similarity-based learning, probability-based learning, and error-based learning. Each of these approaches is introduced by a nontechnical explanation of the underlying concept, followed by mathematical models and algorithms illustrated by detailed worked examples. Finally, the book considers techniques for evaluating prediction models and offers two case studies that describe specific data analytics projects through each phase of development, from formulating the business problem to implementation of the analytics solution. The book, informed by the authors' many years of teaching machine learning, and working on predictive data analytics projects, is suitable for use by undergraduates in computer science, engineering, mathematics, or statistics; by graduate students in disciplines with applications for predictive data analytics; and as a reference for professionals.},
shorttitle = {Fundamentals of Machine Learning for Predictive Data Analytics},
}
@Book{2013SpanielGame,
author     = {Spaniel, William},
date       = {2013},
title      = {Game Theory 101: The Complete Textbook},
pagetotal  = {275},
abstract   = {Game Theory 101: The Complete Textbook is a no-nonsense, games-centered introduction to strategic form (matrix) and extensive form (game tree) games. From the first lesson to the last, this textbook introduces games of increasing complexity and then teaches the game theoretical tools necessary to solve them. Inside, you will find:1) All the basics fully explained, including pure strategy Nash equilibrium, mixed strategy Nash equilibrium, the mixed strategy algorithm, how to calculate payoffs, strict dominance, weak dominance, iterated elimination of strictly dominated strategies, iterated elimination of weakly dominated strategies, subgame perfect equilibrium, backward induction, forward induction, and more.2) Dozens of games solved, including the prisoner’s dilemma, stag hunt, matching pennies, zero sum games, battle of the sexes/Bach or Stravinsky, chicken/snowdrift, pure coordination, deadlock, safety in numbers, Selten's game, the escalation game, the ultimatum game, the pirate game, nim, the centipede game, the hawk-dove game, the volunteer's dilemma, and rock-paper-scissors.3) Rich descriptions of important economic concepts such as commitment problems, burning bridges, perverse incentives, and the chain store paradox.4) Advanced topics such as generalized games, comparative statics, and knife-edge conditions.5) Real world applications including wars, firm entry/exit, tournament strategy, arms races, advertising, game shows, soccer, baseball, video games, and more.6) Crystal clear, line-by-line calculations of every step, with more than 400 images so you don’t miss a thing.Quick, efficient, and to the point, Game Theory 101: The Complete Textbook is perfect for introductory game theory, intermediate microeconomics, and political science.},
shorttitle = {Game Theory 101},
}
@Book{2007BinmoreGame,
author     = {Binmore, Ken},
date       = {2007-10-25},
title      = {Game Theory: A Very Short Introduction},
edition    = {1 edition},
isbn       = {978-0-19-921846-2},
location   = {Oxford},
pagetotal  = {201},
publisher  = {OUP Oxford},
abstract   = {Games are everywhere: Drivers manoeuvring in heavy traffic are playing a driving game. Bargain hunters bidding on eBay are playing an auctioning game. A firm negotiating next year's wage is playing a bargaining game. The opposing candidates in an election are playing a political game. The supermarket's price for corn flakes is decided by playing an economic game.Game theory is about how to play such games in a rational way. Even when the players have not thought everything out in advance, game theory often works for the same reason that mindless animals sometimes end up behaving very cleverly: evolutionary forces eliminate irrational play because it is unfit.Game theory has seen spectacular successes in evolutionary biology and economics, and is beginning to revolutionize other disciplines from psychology to political science. This Very Short Introduction introduces the fascinating world of game theory, showing how it can be understood without mathematical equations, and revealing that everything from how to play poker optimally to the sex ratio among bees can be understood by anyone willing to think seriously about the problem.ABOUT THE SERIES: The Very Short Introductions series from Oxford University Press contains hundreds of titles in almost every subject area. These pocket-sized books are the perfect way to get ahead in a new subject quickly. Our expert authors combine facts, analysis, perspective, new ideas, and enthusiasm to make interesting and challenging topics highly readable.},
shorttitle = {Game Theory},
}
@Book{2008DahlquistNumericala,
author     = {Dahlquist, Germund and Björck, Åke},
date       = {2008-09-04},
title      = {Numerical Methods in Scientific Computing: Volume 1},
isbn       = {978-0-89871-644-3},
location   = {Philadelphia},
pagetotal  = {746},
publisher  = {Society for Industrial and Applied Mathematics},
url        = {http://fmipa.umri.ac.id/wp-content/uploads/2016/03/Dahlquist///////\\_G.///////\\_Bjoerck///////\\_A.///////\\_Vol.1.///////\\_Numerical///////\\_methodBookZZ.org///////\\_.pdf},
abstract   = {This new book from the authors of the classic book Numerical Methods addresses the increasingly important role of numerical methods in science and engineering. More cohesive and comprehensive than any other modern textbook in the field, it combines traditional and well-developed topics with other material that is rarely found in numerical analysis texts, such as interval arithmetic, elementary functions, operator series, convergence acceleration, and continued fractions. Although this volume is self-contained, more comprehensive treatments of matrix computations will be given in a forthcoming volume. A supplementary Website contains three appendices: an introduction to matrix computations; a description of Mulprec, a MATLAB® multiple precision package; and a guide to literature, algorithms, and software in numerical analysis. Review questions, problems, and computer exercises are also included. For use in an introductory graduate course in numerical analysis and for researchers who use numerical methods in science and engineering.},
shorttitle = {Numerical Methods in Scientific Computing},
}
@Book{2018LazarMastering,
author     = {Lazar, Guillaume and Penea, Robin},
date       = {2018},
title      = {Mastering Qt 5: create stunning cross-platform applications using C++ with Qt widgets and QML with QT Quick},
isbn       = {978-1-78899-539-9},
location   = {Birmingham, UK},
note       = {OCLC: 1039917696},
publisher  = {Packt},
url        = {http://www.arxen.com/descargas/Books/Mastering///////\\%20Qt///////\\%205///////\\%20-///////\\%20Guillaume///////\\%20Lazar,///////\\%20Robin///////\\%20Penea.pdf},
shorttitle = {Mastering Qt 5},
}
@Book{2004ScheyDiv,
author     = {Schey, H. M.},
date       = {2004-12-01},
title      = {Div, Grad, Curl, and All That: An Informal Text on Vector Calculus},
edition    = {4th edition},
isbn       = {978-0-393-92516-6},
location   = {New York},
pagetotal  = {176},
publisher  = {W. W. Norton ////////\\\& Company},
abstract   = {This new fourth edition of the acclaimed and bestselling Div, Grad, Curl, and All That has been carefully revised and now includes updated notations and seven new example exercises. Since the publication of the First Edition over thirty years ago, Div, Grad, Curl, and All That has been widely renowned for its clear and concise coverage of vector calculus, helping science and engineering students gain a thorough understanding of gradient, curl, and Laplacian operators without required knowledge of advanced mathematics.},
shorttitle = {Div, Grad, Curl, and All That},
}
@Book{2015GrolemundHands,
author    = {Grolemund, Garrett},
date      = {2015-08-01},
title     = {Hands-On Programming with R},
isbn      = {978-1-4493-5901-0},
pagetotal = {247},
publisher = {O'Reilly UK Ltd.},
url       = {https://d1b10bmlvqabco.cloudfront.net/attach/ighbo26t3ua52t/igp9099yy4v10/igz7vp4w5su9/OReilly///////\\_HandsOn///////\\_Programming///////\\_with///////\\_R///////\\_2014.pdf},
urldate   = {2019-04-01},
abstract  = {Learn how to program by diving into the R language, and then use your newfound skills to solve practical data science problems. With this book, you’ll learn how to load data, assemble and disassemble data objects, navigate R’s...},
ean       = {9781449359010},
file      = {:done/2015GrolemundHands Hands on Programming with R.epub:ePUB},
groups    = {tesse:5, Programming},
langid    = {english},
year      = {2015},
}
@Book{2017PercivalTest,
author     = {Percival, Harry},
date       = {2017-08-31},
title      = {Test-Driven Development with Python: Obey the Testing Goat: Using Django, Selenium, and JavaScript},
edition    = {2 edition},
isbn       = {978-1-4919-5870-4},
location   = {Sebastopol, CA},
pagetotal  = {624},
publisher  = {O'Reilly Media},
url        = {http://www.obeythetestinggoat.com/pages/book.html///////\\#toc},
abstract   = {By taking you through the development of a real web application from beginning to end, the second edition of this hands-on guide demonstrates the practical advantages of test-driven development (TDD) with Python. You’ll learn how to write and run tests before building each part of your app, and then develop the minimum amount of code required to pass those tests. The result? Clean code that works.In the process, you’ll learn the basics of Django, Selenium, Git, jQuery, and Mock, along with current web development techniques. If you’re ready to take your Python skills to the next level, this book—updated for Python 3.6—clearly demonstrates how TDD encourages simple designs and inspires confidence.Dive into the TDD workflow, including the unit test/code cycle and refactoringUse unit tests for classes and functions, and functional tests for user interactions within the browserLearn when and how to use mock objects, and the pros and cons of isolated vs. integrated testsTest and automate your deployments with a staging serverApply tests to the third-party plugins you integrate into your siteRun tests automatically by using a Continuous Integration environmentUse TDD to build a REST API with a front-end Ajax interface},
shorttitle = {Test-Driven Development with Python},
}
@Book{2015EcoHow,
author     = {Eco, Umberto and Erspamer, Francesco},
date       = {2015-03-06},
title      = {How to Write a Thesis},
edition    = {Translation edition},
isbn       = {978-0-262-52713-2},
location   = {Cambridge, Massachusetts},
pagetotal  = {256},
publisher  = {The MIT Press},
translator = {Farina, Caterina Mongiat and Farina, Geoff},
abstract   = {Umberto Eco's wise and witty guide to researching and writing a thesis, published in English for the first time. By the time Umberto Eco published his best-selling novel The Name of the Rose, he was one of Italy's most celebrated intellectuals, a distinguished academic and the author of influential works on semiotics. Some years before that, in 1977, Eco published a little book for his students, How to Write a Thesis, in which he offered useful advice on all the steps involved in researching and writing a thesis―from choosing a topic to organizing a work schedule to writing the final draft. Now in its twenty-third edition in Italy and translated into seventeen languages, How to Write a Thesis has become a classic. Remarkably, this is its first, long overdue publication in English. Eco's approach is anything but dry and academic. He not only offers practical advice but also considers larger questions about the value of the thesis-writing exercise. How to Write a Thesis is unlike any other writing manual. It reads like a novel. It is opinionated. It is frequently irreverent, sometimes polemical, and often hilarious. Eco advises students how to avoid "thesis neurosis" and he answers the important question "Must You Read Books?" He reminds students "You are not Proust" and "Write everything that comes into your head, but only in the first draft." Of course, there was no Internet in 1977, but Eco's index card research system offers important lessons about critical thinking and information curating for students of today who may be burdened by Big Data.How to Write a Thesis belongs on the bookshelves of students, teachers, writers, and Eco fans everywhere. Already a classic, it would fit nicely between two other classics: Strunk and White and The Name of the Rose.ContentsThe Definition and Purpose of a Thesis • Choosing the Topic • Conducting Research • The Work Plan and the Index Cards • Writing the Thesis • The Final Draft},
file       = {:done/2015EcoHow How to Write a Thesis.epub:ePUB},
groups     = {tesse:5},
}
@Book{1982MichaelsonHow,
author    = {Michaelson, Herbert B.},
date      = {1982},
title     = {How to write and publish engineering papers and reports},
isbn      = {978-0-89495-016-2},
location  = {Philadelphia},
pagetotal = {158},
publisher = {ISI Press},
series    = {The Professional writing series},
url       = {https://www.amazon.com/Publish-Engineering-Reports-Professional-writing/dp/0894950169?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0894950169},
file      = {:done/1982MichaelsonHow How to Write and Publish Engineering Papers and Reports.djvu:Djvu},
groups    = {tesse:5, Writing},
year      = {1982},
}
@Book{2017PastineIntroducing,
author     = {Pastine, Ivan and Pastine, Tuvana},
date       = {2017-03-02},
title      = {Introducing Game Theory: A Graphic Guide},
pagetotal  = {176},
publisher  = {Icon Books Ltd},
abstract   = {When should you adopt an aggressive business strategy? How do we make decisions when we don’t have all the information? What makes international environmental cooperation possible?Game theory is the study of how we make a decision when the outcome of our moves depends on the decisions of someone else. Economists Ivan and Tuvana Pastine explain why, in these situations, we sometimes cooperate, sometimes clash, and sometimes act in a way that seems completely random.Stylishly brought to life by award-winning cartoonist Tom Humberstone, Game Theory will help readers understand behaviour in everything from our social lives to business, global politics to evolutionary biology. It provides a thrilling new perspective on the world we live in.},
shorttitle = {Introducing Game Theory},
}
@Book{2010MixermanZen,
author    = {Mixerman},
date      = {2010-10-01},
title     = {Zen and the Art of Mixing},
pagetotal = {350},
publisher = {Hal Leonard},
abstract  = {(Technical Reference). In his first book, The Daily Adventures of Mixerman , the author detailed the frustrating and often hilarious goings on during the process of recording a major-label band. Musicians, engineers, and producers laughed and cried at the crazy goings-on they'd never imagined or recognized all too well. Now Mixerman turns his razor-sharp gaze to the art of mixing and gives followers and the uninitiated reason to hope if not for logic and civility in the recording studio then at least for a good sounding record. With a firm commitment to art over technology and to maintaining a grasp of each, Mixerman outlines his own approach to recording success, based on his years mixing records in all genres of music for all kinds of artists, often under trying circumstances. As he states in his introduction to the new volume, "Even if you're not a professional mixer, even if you're a musician trying to mix your own work or a studio owner in a smaller market, you have your own set of pressures to deal with while you're mixing. Regardless of what those pressures are, it's important to identify and recognize them, if for no other reason than so you can learn to completely ignore them." But how? "That's where the Zen comes in."},
}
@Book{2003KunderaArt,
author    = {Kundera, Milan},
date      = {2003-04-01},
title     = {The Art of the Novel},
edition   = {Reprint edition},
isbn      = {978-0-06-009374-7},
location  = {New York},
pagetotal = {176},
publisher = {Harper Perennial Modern Classics},
abstract  = {Kundera brilliantly examines the work of such important and diverse figures as Rabelais, Cervantes, Sterne, Diderot, Flaubert, Tolstoy, and Musil. He is especially penetrating on Hermann Broch, and his exploration of the world of Kafka's novels vividly reveals the comic terror of Kafka's bureaucratized universe.Kundera's discussion of his own work includes his views on the role of historical events in fiction, the meaning of action, and the creation of character in the post-psychological novel.},
}
@Book{2018SinghIonic,
author     = {Singh, Indermohan and Phan, Hoc},
date       = {2018-04-30},
title      = {Ionic Cookbook: Recipes to create cutting-edge, real-time hybrid mobile apps with Ionic, 3rd Edition},
edition    = {3 edition},
pagetotal  = {392},
publisher  = {Packt Publishing},
abstract   = {Solve all your Ionic-related issues through dedicated recipes that will help you get the best out of Ionic. Working with Ionic components to find out the best way to share data between them effectively.Key FeaturesLeverage Ionic 3.9 and its exciting new features to create cutting-edge, real-time apps Work through simple recipes to address your problems directly and solve them effectively Get examples at each step to guide you on your learning curve with AngularBook DescriptionIonic is the preferred choice for JavaScript developers to develop real-time hybrid applications. This book will get you started with Ionic 3.9 and help you create Angular 5 components that interact with templates.You will work with Ionic components and find out how to share data efficiently between them. You'll discover how to make the best use of the REST API to handle back-end services and then move on to animating your application to make it look pretty. You then learn to add in a local push notification in order to test the app. Then you'll work with Cordova to support native functionalities on both iOS and Android. From there, you'll get to grips with using the default themes for each platform and customizing your own. We then take you through the advanced Ionic features like lazy loading, deep linking, localizing ionic apps etc. Finally, you'll see how best to deploy your app to different platforms.This book will solve all your Ionic-related issues through dedicated recipes that will help you get the best out of Ionic.What you will learnHelp readers to jump-start Ionic appsExplore essential features of Ionic with examplesLearn how to use native device functionalitiesMake the best use of the REST API to handle back-end servicesWork with Cordova to support native functionalities on both iOS and Android.Master advanced topics in app development such as deep linking and lazy loadingWho This Book Is ForThis book targets JavaScript developers. No previous knowledge of Ionic is necessary, but prior knowledge of web development techniques would be useful.Table of ContentsCreating Our First App with Ionic 3Adding Ionic 3 ComponentsExtending Ionic 3 with Angular 4 Building BlocksValidating Forms and Making HTTP RequestsAdding AnimationUser Authentication and Push NotificationSupporting Device Functionalities Using Ionic NativeTheming the AppTaking your app to the next levelPublishing the App for Different Platforms},
shorttitle = {Ionic Cookbook},
}
@Book{2017GalowiczC++17,
author     = {Galowicz, Jacek},
date       = {2017-06-28},
title      = {C++17 STL Cookbook: Discover the latest enhancements to functional programming and lambda expressions},
edition    = {1 edition},
pagetotal  = {534},
publisher  = {Packt Publishing},
abstract   = {Key FeaturesLearn the latest features of C++ and how to write better code by using the Standard Library (STL). Reduce the development time for your applications.Understand the scope and power of STL features to deal with real-world problems.Compose your own algorithms without forfeiting the simplicity and elegance of the STL way.Book DescriptionC++ has come a long way and is in use in every area of the industry. Fast, efficient, and flexible, it is used to solve many problems. The upcoming version of C++ will see programmers change the way they code. If you want to grasp the practical usefulness of the C++17 STL in order to write smarter, fully portable code, then this book is for you.Beginning with new language features, this book will help you understand the language's mechanics and library features, and offers insight into how they work. Unlike other books, ours takes an implementation-specific, problem-solution approach that will help you quickly overcome hurdles. You will learn the core STL concepts, such as containers, algorithms, utility classes, lambda expressions, iterators, and more, while working on practical real-world recipes. These recipes will help you get the most from the STL and show you how to program in a better way.By the end of the book, you will be up to date with the latest C++17 features and save time and effort while solving tasks elegantly using the STL.What you will learnLearn about the new core language features and the problems they were intended to solveUnderstand the inner workings and requirements of iterators by implementing themExplore algorithms, functional programming style, and lambda expressionsLeverage the rich, portable, fast, and well-tested set of well-designed algorithms provided in the STLWork with strings the STL way instead of handcrafting C-style codeUnderstand standard support classes for concurrency and synchronization, and how to put them to workUse the filesystem library addition available with the C++17 STLAbout the AuthorJacek Galowicz obtained his master of science in electrical engineering/computer engineering at RWTH Aachen University, Germany. While at university, he enjoyed working as a student assistant in teaching and research, and he participated in several scientific publications. During and after his studies, he worked as a freelancer and implemented applications as well as kernel drivers in C and C++, touching various areas, including 3D graphics programming, databases, network communication, and physics simulation. In recent years, he has been programming performance- and security-sensitive microkernel operating systems for Intel x86 virtualization at Intel and FireEye in Braunschweig, Germany. He has a strong passion for modern C++ implementations of low-level software, and he tries hard to combine high performance with an elegant coding style. Learning purely functional programming and Haskell in recent years triggered his drive to implement generic code with the aid of meta programming.Table of ContentsThe New C++17 FeaturesSTL ContainersIteratorsLambda ExpressionsSTL Algorithm BasicsAdvanced Use of STL AlgorithmsStrings, Stream Classes, and Regular ExpressionsUtility ClasssesParallelism and ConcurrencyFilesystem},
shorttitle = {C++17 STL Cookbook},
}
@Book{2014HorsleyUnlimited,
author     = {Horsley, Kevin},
date       = {2014-01-26},
title      = {Unlimited Memory: How to Use Advanced Learning Strategies to Learn Faster, Remember More and be More Productive},
edition    = {2 edition},
pagetotal  = {189},
publisher  = {TCK Publishing},
abstract   = {Kevin Horsley Broke a World Memory Record in 2013...And You're About to Learn How to Use His Memory Strategies to Learn Faster, Be More Productive and Achieve More SuccessWith over 200,000 copies sold, Unlimited Memory is a Wall Street Journal Best Seller and has been the ///////\\#1 memory book on Amazon for more than two years. It has been translated into more than a dozen languages including French, Chinese, Russian, Korean, Ukrainian, and Lithuanian.Most people never tap into 10///////\\% of their potential for memory.In this book, you're about to learn:How the World's Top Memory Experts Concentrate and Remember Any Information at Will, and How You Can TooDo you ever feel like you're too busy, too stressed or just too distracted to concentrate and get work done?In Unlimited Memory, you'll learn how the world's best memory masters get themselves to concentrate at will, anytime they want. When you can easily focus and concentrate on the task at hand, and store and recall useful information, you can easily double your productivity and eliminate wasted time, stress and mistakes at work.In this book, you'll find all the tools, strategies and techniques you need to improve your memory.Here’s just a taste of the memory methods you'll learn in this book: The 3 bad habits that keep you from easily remembering important information How a simple pattern of thinking can stop you from imprinting and remembering key facts, figures and ideas, and how to break this old pattern so you’ll never again be known as someone with a "bad memory" How to master your attention so you can focus and concentrate longer, even during challenging or stressful situations How to use your car to remember anything you want (like long lists or information you need to remember for your studies or personal life) without writing anything down Simple methods that allow you to nail down tough information or complex concepts quickly and easily How to combine your long-term memory (things you already know and will never forget) and short-term memory (information you want to remember right now) to create instant recall for tests, presentations and important projects The simple, invisible mental technique for remembering names without social awkwardness or anxiety How using your imagination to bring boring information to life can help you dramatically improve your attention span and recall An incredible strategy for remembering numbers (the same system Kevin used to remember Pi to 10,000 digits and beat the world memory record by 14 minutes) How to use a mental map to lock in and connect hundreds or even thousands of ideas in your long-term memory (this method will allow you to become a leading expert in your field faster than you ever dreamed possible)If you're ready to harness the incredible power of your mind to remember more in less time, this book is for you.About the AuthorFor over 25 years, Kevin Horsley has been analyzing the mind and memory and its capacity for brilliance. He is one of only a few people in the world to have received the title of International Grandmaster of Memory. He is a World Memory Championship medalist, and a two-time World Record holder for The Everest of memory tests. Kevin is also an author of four books, and the designer of a mathematics game with the Serious Games Institute at North-West University Vaal Campus.His work has been featured in The Oprah Magazine, Time, Forbes, Inc., The Huffington Post, ELLE, Longevity and on numerous TV and radio shows. Kevin is an International professional speaker and has spoken in many different countries. He assists organizations in improving their learning, motivation, creativity and thinking.},
shorttitle = {Unlimited Memory},
}
@Book{2015VerouCss,
author     = {Verou, Lea},
date       = {2015-07-03},
title      = {CSS Secrets: Better Solutions to Everyday Web Design Problems},
edition    = {1 edition},
isbn       = {978-1-4493-7263-7},
location   = {Sebastopol, CA},
pagetotal  = {392},
publisher  = {O'Reilly Media},
abstract   = {In this practical guide, CSS expert Lea Verou provides 47 undocumented techniques and tips to help intermediate-to advanced CSS developers devise elegant solutions to a wide range of everyday web design problems.Rather than focus on design, CSS Secrets shows you how to solve problems with code. You'll learn how to apply Lea's analytical approach to practically every CSS problem you face to attain DRY, maintainable, flexible, lightweight, and standards-compliant results.Inspired by her popular talks at over 60 international web development conferences, Lea Verou provides a wealth of information for topics including:Backgrounds and BordersShapesVisual EffectsTypographyUser ExperienceStructure and LayoutTransitions and Animations},
file       = {:done/2015VerouCss CSS Secrets/\\_ Better Solutions to Everyday Web Design Problems.epub:ePUB},
groups     = {tesse:5},
shorttitle = {CSS Secrets},
}
@Book{1997BieglerSystematic,
author    = {Biegler, Lorenz T. and Grossmann, Ignacio E. and Westerberg, Arthur W.},
date      = {1997-04-06},
title     = {Systematic Methods of Chemical Process Design},
edition   = {1 edition},
isbn      = {978-0-13-492422-9},
location  = {Upper Saddle River, N.J},
pagetotal = {796},
publisher = {Prentice Hall},
abstract  = {Brings together all the information engineers and researchers need to develop efficient, cost-effective chemical production processes. The book presents a systematic approach to chemical process design, covering both continuous and batch processes. Starting with the basics, the book then moves on to advanced topics. Among the topics covered are: flowsheet synthesis, mass and energy balances, equipment sizing and costing, economic evaluation, process simulation and optimization. The book also covers specific chemical processes such as distillation systems, reactor networks, separation, and heat exchange networks. It shows how to build more flexible processes, including multiproduct batch processes. Any researcher or practicing engineer involved in designing chemical processes.},
file      = {:done/1997BieglerSystematic Systematic Methods of Chemical Process Design.djvu:Djvu},
groups    = {tesse:5},
}
@Book{2017MoskalaAndroid,
author     = {Moskala, Marcin and Wojda, Igor},
date       = {2017-08-30},
title      = {Android Development with Kotlin: Enhance your skills for Android development using Kotlin},
edition    = {1 edition},
pagetotal  = {440},
publisher  = {Packt Publishing},
abstract   = {Learn how to make Android development much faster using a variety of Kotlin features, from basics to advanced, to write better quality code.Key FeaturesLeverage specific features of Kotlin to ease Android application developmentWrite code based on both object oriented and functional programming to build robust applicationsFilled with various practical examples so you can easily apply your knowledge to real world scenariosIdentify the improved way of dealing with common Java patternsBook DescriptionNowadays, improved application development does not just mean building better performing applications. It has become crucial to find improved ways of writing code. Kotlin is a language that helps developers build amazing Android applications easily and effectively. This book discusses Kotlin features in context of Android development. It demonstrates how common examples that are typical for Android development, can be simplified using Kotlin. It also shows all the benefits, improvements and new possibilities provided by this language.The book is divided in three modules that show the power of Kotlin and teach you how to use it properly. Each module present features in different levels of advancement. The first module covers Kotlin basics. This module will lay a firm foundation for the rest of the chapters so you are able to read and understand most of the Kotlin code. The next module dives deeper into the building blocks of Kotlin, such as functions, classes, and function types. You will learn how Kotlin brings many improvements to the table by improving common Java concepts and decreasing code verbosity. The last module presents features that are not present in Java. You will learn how certain tasks can be achieved in simpler ways thanks to Kotlin.Through the book, you will learn how to use Kotlin for Android development. You will get to know and understand most important Kotlin features, and how they can be used. You will be ready to start your own adventure with Android development with Kotlin.What you will learnRun a Kotlin application and understand the integration with Android StudioIncorporate Kotlin into new/existing Android Java based projectLearn about Kotlin type system to deal with null safety and immutabilityDefine various types of classes and deal with propertiesDefine collections and transform them in functional wayDefine extensions, new behaviours to existing libraries and Android framework classesUse generic type variance modifiers to define subtyping relationship between generic typesBuild a sample applicationTable of ContentsBeginning Your Kotlin AdventureLaying a FoundationPlaying with FunctionsClasses and ObjectsFunctions as First-Class CitizensGenerics Are Your FriendsExtension Functions and PropertiesDelegatesMaking Your Marvel Gallery Application},
shorttitle = {Android Development with Kotlin},
}
@Book{2017BancilaModern,
author     = {Bancila, Marius},
date       = {2017-05-15},
title      = {Modern C++ Programming Cookbook: Recipes to explore data structure, multithreading, and networking in C++17},
isbn       = {978-1-78646-518-4},
pagetotal  = {590},
publisher  = {Packt Publishing - ebooks Account},
abstract   = {Over 100 recipes to help you overcome your difficulties with C++ programming and gain a deeper understanding of the working of modern C++About This BookExplore the most important language and library features of C++17, including containers, algorithms, regular expressions, threads, and more,Get going with unit testing frameworks Boost.Test, Google Test and Catch,Extend your C++ knowledge and take your development skills to new heights by making your applications fast, robust, and scalable.Who This Book Is ForIf you want to overcome difficult phases of development with C++ and leverage its features using modern programming practices, then this book is for you. The book is designed for both experienced C++ programmers as well as people with strong knowledge of OOP concepts.What You Will LearnGet to know about the new core language features and the problems they were intended to solveUnderstand the standard support for threading and concurrency and know how to put them on work for daily basic tasksLeverage C++'s features to get increased robustness and performanceExplore the widely-used testing frameworks for C++ and implement various useful patterns and idiomsWork with various types of strings and look at the various aspects of compilationExplore functions and callable objects with a focus on modern featuresLeverage the standard library and work with containers, algorithms, and iteratorsUse regular expressions for find and replace string operationsTake advantage of the new filesystem library to work with files and directories},
shorttitle = {Modern C++ Programming Cookbook},
}
@Book{2018LapanDeep,
author     = {Lapan, Maxim},
date       = {2018-06-21},
title      = {Deep Reinforcement Learning Hands-On: Apply modern RL methods, with deep Q-networks, value iteration, policy gradients, TRPO, AlphaGo Zero and more},
isbn       = {978-1-78883-424-7},
location   = {Birmingham Mumbai},
pagetotal  = {546},
publisher  = {Packt Publishing},
abstract   = {This practical guide will teach you how deep learning (DL) can be used to solve complex real-world problems.Key FeaturesExplore deep reinforcement learning (RL), from the first principles to the latest algorithmsEvaluate high-profile RL methods, including value iteration, deep Q-networks, policy gradients, TRPO, PPO, DDPG, D4PG, evolution strategies and genetic algorithmsKeep up with the very latest industry developments, including AI-driven chatbotsBook DescriptionRecent developments in reinforcement learning (RL), combined with deep learning (DL), have seen unprecedented progress made towards training agents to solve complex problems in a human-like way. Google's use of algorithms to play and defeat the well-known Atari arcade games has propelled the field to prominence, and researchers are generating new ideas at a rapid pace.Deep Reinforcement Learning Hands-On is a comprehensive guide to the very latest DL tools and their limitations. You will evaluate methods including Cross-entropy and policy gradients, before applying them to real-world environments. Take on both the Atari set of virtual games and family favorites such as Connect4. The book provides an introduction to the basics of RL, giving you the know-how to code intelligent learning agents to take on a formidable array of practical tasks. Discover how to implement Q-learning on 'grid world' environments, teach your agent to buy and trade stocks, and find out how natural language models are driving the boom in chatbots.What you will learnUnderstand the DL context of RL and implement complex DL modelsLearn the foundation of RL: Markov decision processesEvaluate RL methods including Cross-entropy, DQN, Actor-Critic, TRPO, PPO, DDPG, D4PG and othersDiscover how to deal with discrete and continuous action spaces in various environmentsDefeat Atari arcade games using the value iteration methodCreate your own OpenAI Gym environment to train a stock trading agentTeach your agent to play Connect4 using AlphaGo ZeroExplore the very latest deep RL research on topics including AI-driven chatbotsWho This Book Is ForSome fluency in Python is assumed. Basic deep learning (DL) approaches should be familiar to readers and some practical experience in DL will be helpful. This book is an introduction to deep reinforcement learning (RL) and requires no background in RL.Table of ContentsWhat is Reinforcement Learning?OpenAI GymDeep Learning with PyTorchThe Cross-Entropy MethodTabular Learning and the Bellman EquationDeep Q-NetworksDQN ExtensionsStocks Trading Using RLPolicy Gradients – An AlternativeThe Actor-Critic MethodAsynchronous Advantage Actor-CriticChatbots Training with RLWeb NavigationContinuous Action SpaceTrust Regions – TRPO, PPO, and ACKTRBlack-Box Optimization in RLBeyond Model-Free – ImaginationAlphaGo Zero},
shorttitle = {Deep Reinforcement Learning Hands-On},
}
@Book{2016CostandiNeuroplasticity,
author    = {Costandi, Moheb},
date      = {2016-08-19},
title     = {Neuroplasticity},
isbn      = {978-0-262-52933-4},
location  = {Cambridge, MA},
pagetotal = {192},
publisher = {The MIT Press},
abstract  = {The real story of how our brains and nervous systems change throughout our lifetimes―with or without "brain training."Fifty years ago, neuroscientists thought that a mature brain was fixed like a fly in amber, unable to change. Today, we know that our brains and nervous systems change throughout our lifetimes. This concept of neuroplasticity has captured the imagination of a public eager for self-improvement―and has inspired countless Internet entrepreneurs who peddle dubious "brain training" games and apps. In this book, Moheb Costandi offers a concise and engaging overview of neuroplasticity for the general reader, describing how our brains change continuously in response to our actions and experiences.Costandi discusses key experimental findings, and describes how our thinking about the brain has evolved over time. He explains how the brain changes during development, and the "synaptic pruning" that takes place before brain maturity. He shows that adult brains can grow new cells (citing, among many other studies, research showing that sexually mature male canaries learn a new song every year). He describes the kind of brain training that can bring about improvement in brain function. It's not gadgets and games that promise to "rewire your brain" but such sustained cognitive tasks as learning a musical instrument or a new language. (Costandi also notes that London cabbies increase their gray matter after rigorous training in their city's complicated streets.) He tells how brains compensate after stroke or injury; describes addiction and pain as maladaptive forms of neuroplasticity; and considers brain changes that accompany childhood, adolescence, parenthood, and aging. Each of our brains is custom-built. Neuroplasticity is at the heart of what makes us human.},
file      = {:done/2016CostandiNeuroplasticity Neuroplasticity.epub:ePUB},
groups    = {tesse:5},
}
@Book{2015NiouStrategy,
author     = {Niou, Emerson and Ordeshook, Peter C.},
date       = {2015-05-28},
title      = {Strategy and Politics: An Introduction to Game Theory},
edition    = {1 edition},
isbn       = {978-1-138-01948-5},
location   = {New York, NY},
pagetotal  = {432},
publisher  = {Routledge},
abstract   = {Strategy and Politics: An Introduction to Game Theory is designed to introduce students with no background in formal theory to the application of game theory to modeling political processes. This accessible text covers the essential aspects of game theory while keeping the reader constantly in touch with why political science as a whole would benefit from considering this method. Examining the very phenomena that power political machineries―elections, legislative and committee processes, and international conflict, the book attempts to answer fundamental questions about their nature and function in a clear, accessible manner. Included at the end of each chapter is a set of exercises designed to allow students to practice the construction and analysis of political models. Although the text assumes only an elementary-level training in algebra, students who complete a course around this text will be equipped to read nearly all of the professional literature that makes use of game theoretic analysis.},
shorttitle = {Strategy and Politics},
}
@Book{2016RashidMake,
author    = {Rashid, Tariq},
date      = {2016-03-31},
title     = {Make Your Own Neural Network},
edition   = {1 edition},
isbn      = {978-1-5308-2660-5},
location  = {s.l.},
pagetotal = {222},
publisher = {CreateSpace Independent Publishing Platform},
abstract  = {A step-by-step gentle journey through the mathematics of neural networks, and making your own using the Python computer language. Neural networks are a key element of deep learning and artificial intelligence, which today is capable of some truly impressive feats. Yet too few really understand how neural networks actually work. This guide will take you on a fun and unhurried journey, starting from very simple ideas, and gradually building up an understanding of how neural networks work. You won't need any mathematics beyond secondary school, and an accessible introduction to calculus is also included. The ambition of this guide is to make neural networks as accessible as possible to as many readers as possible - there are enough texts for advanced readers already!You'll learn to code in Python and make your own neural network, teaching it to recognise human handwritten numbers, and performing as well as professionally developed networks. Part 1 is about ideas. We introduce the mathematical ideas underlying the neural networks, gently with lots of illustrations and examples. Part 2 is practical. We introduce the popular and easy to learn Python programming language, and gradually builds up a neural network which can learn to recognise human handwritten numbers, easily getting it to perform as well as networks made by professionals. Part 3 extends these ideas further. We push the performance of our neural network to an industry leading 98 /\\% using only simple ideas and code, test the network on your own handwriting, take a privileged peek inside the mysterious mind of a neural network, and even get it all working on a Raspberry Pi. All the code in this has been tested to work on a Raspberry Pi Zero.},
file      = {:done/2016RashidMake Make Your Own Neural Network.epub:ePUB},
groups    = {tesse:5, Neural Networks and Sustainability, Neural Networks},
}
@Book{2014TalwalkarJoy,
author     = {Talwalkar, Presh},
date       = {2014-08-08},
title      = {The Joy of Game Theory: An Introduction to Strategic Thinking},
isbn       = {978-1-5004-9744-6},
pagetotal  = {154},
publisher  = {CreateSpace Independent Publishing Platform},
abstract   = {This book is a selection of the best articles from Game Theory Tuesdays, a column from the blog Mind Your Decisions. Articles from Game Theory Tuesdays have been referenced in The Freakonomics Blog, Yahoo Finance, and CNN.com. Game theory is the study of interactive decision making--that is, in situations where each person's action affects the outcome for the whole group. Game theory is a beautiful subject and this book will teach you how to understand the theory and practically implement solutions through a series of stories and the aid of over 30 illustrations. This book has two primary objectives. (1) To help you recognize strategic games, like the Prisoner's Dilemma, Bertrand Duopoly, Hotelling's Game, the Game of Chicken, and Mutually Assured Destruction. (2) To show you how to make better decisions and change the game, a powerful concept that can transform no-win situations into mutually beneficial outcomes. You'll learn how to negotiate better by making your threats credible, sometimes limiting options or burning bridges, and thinking about new ways to create better outcomes. As these goals indicate, game theory is about more than board games and gambling. It all seems so simple, and yet that definition belies the complexity of game theory. While it may only take seconds to get a sense of game theory, it takes a lifetime to appreciate and master it. This book will get you started.},
shorttitle = {The Joy of Game Theory},
}
@Book{2012RamachandranTell,
author     = {Ramachandran, V. S.},
date       = {2012-04-05},
title      = {The Tell-Tale Brain: Unlocking the Mystery of Human Nature},
isbn       = {978-0-09-953759-5},
pagetotal  = {384},
publisher  = {Windmill Books},
abstract   = {John, aged sixty, suffered a stroke and recovered fully, except in one respect: although he can see perfectly, he can no longer recognise faces, even his own reflection in a mirror.Whenever Francesca touches a particular texture, she experiences a vivid emotion: denim = extreme sadness; wax = embarrassment; orange peel = shock.Jimmie, whose left arm was recently amputated, can still feel it - and it's itchy.Our brains are the most enchanting and complex things in the known universe - but what happens when they go wrong? Dr V. S. Ramachandran, 'the Sherlock Holmes of brain science' and one of the world's leading neuroscientists, has spent a lifetime working with patients who suffer from rare and baffling brain conditions. In The Tell-Tale Brain, he tells their stories, and explores what they reveal about the greatest mystery of them all: how our minds work, and what makes each of us so uniquely human.},
file       = {:done/2012RamachandranTell The Tell Tale Brain/\\_ Unlocking the Mystery of Human Nature.epub:ePUB},
groups     = {tesse:5},
shorttitle = {The Tell-Tale Brain},
}
@Collection{2012KandelPrinciples,
date      = {2012-10-26},
editor    = {Kandel, Eric R. and Schwartz, James H. and Jessell, Thomas M. and Siegelbaum, Steven A. and Hudspeth, A. J.},
title     = {Principles of Neural Science},
edition   = {5th edition},
isbn      = {978-0-07-139011-8},
location  = {New York},
pagetotal = {1760},
publisher = {McGraw-Hill Education / Medical},
abstract  = {Publisher's Note: Products purchased from Third Party sellers are not guaranteed by the publisher for quality, authenticity, or access to any online entitlements included with the product.Now updated: the definitive neuroscience resource―from Eric R. Kandel, MD (winner of the Nobel Prize in 2000); James H. Schwartz, MD, PhD; Thomas M. Jessell, PhD; Steven A. Siegelbaum, PhD; and A. J. Hudspeth, PhDA Doody's Core Title for 2017!900 full-color illustrationsDeciphering the link between the human brain and behavior has always been one of the most intriguing―and often challenging―aspects of scientific endeavor. The sequencing of the human genome, and advances in molecular biology, have illuminated the pathogenesis of many neurological diseases and have propelled our knowledge of how the brain controls behavior.To grasp the wider implications of these developments and gain a fundamental understanding of this dynamic, fast-moving field, Principles of Neuroscience stands alone as the most authoritative and indispensible resource of its kind.In this classic text, prominent researchers in the field expertly survey the entire spectrum of neural science, giving an up-to-date, unparalleled view of the discipline for anyone who studies brain and mind. Here, in one remarkable volume, is the current state of neural science knowledge―ranging from molecules and cells, to anatomic structures and systems, to the senses and cognitive functions―all supported by more than 900 precise, full-color illustrations. In addition to clarifying complex topics, the book also benefits from a cohesive organization, beginning with an insightful overview of the interrelationships between the brain, nervous system, genes, and behavior. Principles of Neural Science then proceeds with an in-depth examination of the molecular and cellular biology of nerve cells, synaptic transmission, and the neural basis of cognition. The remaining sections illuminate how cells, molecules, and systems give us sight, hearing, touch, movement, thought, learning, memories, and emotions.The new fifth edition of Principles of Neural Science is thoroughly updated to reflect the tremendous amount of research, and the very latest clinical perspectives, that have significantly transformed the field within the last decade.Ultimately, Principles of Neural Science affirms that all behavior is an expression of neural activity, and that the future of clinical neurology and psychiatry hinges on the progress of neural science. Far exceeding the scope and scholarship of similar texts, this unmatched guide offers a commanding, scientifically rigorous perspective on the molecular mechanisms of neural function and disease―one that you’ll continually rely on to advance your comprehension of brain, mind, and behavior.FEATURESThe cornerstone reference in the field of neuroscience that explains how the nerves, brain, and mind functionClear emphasis on how behavior can be examined through the electrical activity of both individual neurons and systems of nerve cellsCurrent focus on molecular biology as a tool for probing the pathogenesis of many neurological diseases, including muscular dystrophy, Huntington disease, and certain forms of Alzheimer’s diseaseMore than 900 engaging full-color illustrations―including line drawings, radiographs, micrographs, and medical photographs clarify often-complex neuroscience conceptsOutstanding section on the development and emergence of behavior, including important coverage of},
file      = {:done/2012KandelPrinciples Principles of Neural Science.epub:ePUB},
groups    = {tesse:5},
}
@Book{Qt5,
title  = {Qt5 Cadaques},
url    = {https://qmlbook.github.io/},
file   = {:done/Qt5 Qt5 Cadaques.epub:ePUB},
groups = {tesse:2, Programming},
}
@Book{2015SusskindQuantum,
author     = {Susskind, Leonard and Friedman, Art},
date       = {2015-05-12},
title      = {Quantum Mechanics: The Theoretical Minimum},
isbn       = {978-0-465-06290-4},
location   = {New York},
pagetotal  = {384},
publisher  = {Basic Books},
abstract   = {From the bestselling author of The Theoretical Minimum, a DIY introduction to the math and science of quantum mechanics.First he taught you classical mechanics. Now, physicist Leonard Susskind has teamed up with data engineer Art Friedman to present the theory and associated mathematics of the strange world of quantum mechanics.In this follow-up to the New York Times best-selling The Theoretical Minimum, Susskind and Friedman provide a lively introduction to this famously difficult field, which attempts to understand the behavior of sub-atomic objects through mathematical abstractions. Unlike other popularizations that shy away from quantum mechanics' weirdness, Quantum Mechanics embraces the utter strangeness of quantum logic. The authors offer crystal-clear explanations of the principles of quantum states, uncertainty and time dependence, entanglement, and particle and wave states, among other topics, and each chapter includes exercises to ensure mastery of each area. Like The Theoretical Minimum, this volume runs parallel to Susskind's eponymous Stanford University-hosted continuing education course.An approachable yet rigorous introduction to a famously difficult topic, Quantum Mechanics provides a tool kit for amateur scientists to learn physics at their own pace.},
file       = {:done/2015SusskindQuantum Quantum Mechanics//_ the Theoretical Minimum.pdf:application/pdf},
groups     = {tesse:5, Quantum Mechanics},
shorttitle = {Quantum Mechanics},
}
@Book{2006RileyMathematical,
author     = {Riley, K. F. and Hobson, M. P. and Bence, S. J.},
date       = {2006-03-13},
title      = {Mathematical Methods for Physics and Engineering: A Comprehensive Guide},
edition    = {3 edition},
isbn       = {978-0-521-67971-8},
location   = {Cambridge ; New York},
pagetotal  = {1359},
publisher  = {Cambridge University Press},
abstract   = {The third edition of this highly acclaimed undergraduate textbook is suitable for teaching all the mathematics for an undergraduate course in any of the physical sciences. As well as lucid descriptions of all the topics and many worked examples, it contains over 800 exercises. New stand-alone chapters give a systematic account of the 'special functions' of physical science, cover an extended range of practical applications of complex variables, and give an introduction to quantum operators. Further tabulations, of relevance in statistics and numerical integration, have been added. In this edition, half of the exercises are provided with hints and answers and, in a separate manual available to both students and their teachers, complete worked solutions. The remaining exercises have no hints, answers or worked solutions and can be used for unaided homework; full solutions are available to instructors on a password-protected web site, www.cambridge.org/9780521679718.},
shorttitle = {Mathematical Methods for Physics and Engineering},
}
@Misc{WangIterated,
author = {Wang, Keven},
title  = {Iterated Prisoners Dilemma with Reinforcement Learning.pdf},
url    = {http://web.stanford.edu/class/psych209/Readings/2017ProjectExamples/wangkeven///////\\_17581///////\\_1628229///////\\_psych209///////\\_paper.pdf},
file   = {:done/WangIterated Iterated Prisoners Dilemma with Reinforcement Learning.pdf.pdf:application/pdf},
groups = {tesse:2, Game Theory},
}
@Report{2018Clarivate2018,
author      = {Clarivate},
date        = {2018},
institution = {Clarivate Analytics},
title       = {2018 Journal Citation Reports},
type        = {techreport},
url         = {https://clarivate.com/wp-content/uploads/2018/06/Crv///////\\_JCR///////\\_Full-Marketing-List///////\\_A4///////\\_2018///////\\_v4.pdf},
file        = {:done/2018Clarivate2018 2018 Journal Citation Reports.pdf:application/pdf},
groups      = {tesse:5},
publisher   = {Clarivate Analytics},
}
@Book{1996HayesStatistical,
author    = {Hayes, Monson H.},
date      = {1996-04-11},
title     = {Statistical DSP},
edition   = {1 edition},
isbn      = {978-0-471-59431-4},
location  = {New York},
pagetotal = {628},
publisher = {WILEY},
abstract  = {The main thrust is to provide students with a solid understanding of a number of important and related advanced topics in digital signal processing such as Wiener filters, power spectrum estimation, signal modeling and adaptive filtering. Scores of worked examples illustrate fine points, compare techniques and algorithms and facilitate comprehension of fundamental concepts. Also features an abundance of interesting and challenging problems at the end of every chapter.},
file      = {:done/1996HayesStatistical Statistical DSP.pdf:application/pdf},
groups    = {tesse:5, DSP},
}
@Article{SangamMapping,
author     = {Sangam, S. L. and Mogali, Miss Shivaranjini S.},
title      = {Mapping and Visualization Softwares tools: a review},
url        = {https://edisciplinas.usp.br/pluginfile.php/4131773/mod///////\\_folder/content/0/Mapping///////\\%20and///////\\%20Visualisation///////\\%20Software///////\\%20Tools///////\\%20A///////\\%20Review///////\\%20.pdf?forcedownload=1},
file       = {:done/SangamMapping Mapping and Visualization Softwares Tools\\_ a Review.pdf:application/pdf},
groups     = {tesse:2},
shorttitle = {Mapping and Visualization Softwares tools},
}
@Book{2007PressNumerical,
author     = {Press, William H},
date       = {2007},
title      = {Numerical recipes: the art of scientific computing},
isbn       = {978-0-511-33555-6},
location   = {Cambridge, UK; New York},
note       = {OCLC: 212427139},
publisher  = {Cambridge University Press},
shorttitle = {Numerical recipes},
}
@Article{1996SchneiderNurb,
author       = {Schneider, Philip},
date         = {1996},
journaltitle = {Develop},
title        = {NURB Curves: A Guide for the Uninitiated},
url          = {http://digiitalarchfab.com/portal/wp-content/uploads/2011/03/Nurbs-Curve-A-Guide-for-the-Uninitiated.pdf},
file         = {:done/1996SchneiderNurb NURB Curves/\\_ a Guide for the Uninitiated.pdf:application/pdf},
groups       = {tesse:5},
}
@Book{2014PirkleDesigning,
author     = {Pirkle, Will},
date       = {2014-11-26},
title      = {Designing Software Synthesizer Plug-Ins in C++: For RackAFX, VST3, and Audio Units},
edition    = {1 edition},
isbn       = {978-1-138-78707-0},
location   = {Burlington, MA},
pagetotal  = {760},
publisher  = {Routledge},
abstract   = {Bridging the gap from theory to programming, Designing Software Synthesizer Plug-Ins in C++ For RackAFX, VST3 and Audio Units contains complete code for designing and implementing software synthesizers for both Windows and Mac platforms. You will learn synthesizer operation, starting with the underlying theory of each synthesizer component, and moving on to the theory of how these components combine to form fully working musical instruments that function on a variety of target digital audio workstations (DAWs). Containing some of the latest advances in theory and algorithm development, this book contains information that has never been published in textbook form, including several unique algorithms of the author’s own design. The book is broken into three parts: plug-in programming, theory and design of the central synthesizer components of oscillators, envelope generators, and filters, and the design and implementation of six complete polyphonic software synthesizer musical instruments, which can be played in real time. The instruments implement advanced concepts including a user-programmable modulation matrix. The final chapter shows you the theory and code for a suite of delay effects to augment your synthesizers, introducing you to audio effect processing. The companion website, www.focalpress.com/cw/pirkle, gives you access to free software to guide you through the application of concepts discussed in the book, and code for both Windows and Mac platforms. In addition to the software, it features bonus projects, application notes, and video tutorials. A reader forum, monitored by the author, gives you the opportunity for questions and information exchange.},
shorttitle = {Designing Software Synthesizer Plug-Ins in C++},
}
@Article{1992EbdonIntroduction,
author       = {Ebdon, J. R.},
date         = {1992},
journaltitle = {Polymer International},
title        = {Introduction to polymers (second edition) R. J. Young and P. A. Lovell Chapman and Hall, London, 1991. pp. 443, price £16.95. ISBN 0-412-30640-9 (PB); ISBN 0–412–30630–1 (HB)},
doi          = {10.1002/pi.4990270217},
issn         = {09598103, 10970126},
number       = {2},
pages        = {207--208},
urldate      = {2019-04-01},
volume       = {27},
langid       = {english},
}
@Book{1991YoungIntroduction,
author    = {Young, Robert J. and Lovell, P. A.},
date      = {1991},
title     = {Introduction to polymers},
edition   = {2nd ed},
isbn      = {978-0-412-30630-3},
location  = {London ; New York},
pagetotal = {443},
publisher = {Chapman //\\\& Hall},
url       = {https://www.amazon.com/Introduction-Polymers-Robert-J-Young/dp/0412306301?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=0412306301},
file      = {:done/1991YoungIntroduction Introduction to Polymers.pdf:application/pdf},
groups    = {tesse:5},
year      = {1991},
}
@Book{2005TemamMathematical,
author    = {Temam, Roger},
date      = {2005-06-20},
title     = {Mathematical Modeling in Continuum Mechanics},
edition   = {2 edition},
isbn      = {978-0-521-61723-9},
location  = {Cambridge, UK ; New York},
pagetotal = {356},
publisher = {Cambridge University Press},
abstract  = {Temam and Miranville present core topics within the general themes of fluid and solid mechanics. The brisk style allows the text to cover a wide range of topics including viscous flow, magnetohydrodynamics, atmospheric flows, shock equations, turbulence, nonlinear solid mechanics, solitons, and the nonlinear Schrödinger equation. This second edition will be a unique resource for those studying continuum mechanics at the advanced undergraduate and beginning graduate level whether in engineering, mathematics, physics or the applied sciences. Exercises and hints for solutions have been added to the majority of chapters, and the final part on solid mechanics has been substantially expanded. These additions have now made it appropriate for use as a textbook, but it also remains an ideal reference book for students and anyone interested in continuum mechanics.},
file      = {:done/2005TemamMathematical Mathematical Modeling in Continuum Mechanics.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2009NollJoint,
author     = {Noll, Terrie},
date       = {2009-03-21},
title      = {The Joint Book: The Complete Guide to Wood Joinery},
edition    = {Edição: Spi},
isbn       = {978-0-7858-2227-1},
location   = {London},
publisher  = {Chartwell Books},
abstract   = {This hardcover book with internal spiral binding is 6.5in x 8in, a perfect size for carpenters and woodworkers to keep near their workbench or toolbox for quick access.The design of this book allows it to lay open flat, which allows for easy and frequent reference, and the interior photographs, illustrations and diagrams, make the learning process simple and fun for beginners, and provides useful tips for more advanced readers.The Joint Book contains easy-to-follow step-by-step instructions for creating edge and scarf joints, lapped and housed joints, mortise and tenon joints, miters and bevels, dovetails, dowels and biscuits, and provides detailed descriptions of fasteners, hardware, and knockdown joints. This book is the perfect companion for any woodworker interested in improving their joint-making skills.},
file       = {:done/2009NollJoint The Joint Book/\\_ the Complete Guide to Wood Joinery.pdf:application/pdf},
groups     = {tesse:5},
shorttitle = {The Joint Book},
}
@Book{2018BoydIntroduction,
author     = {Boyd, Stephen and Vandenberghe, Lieven},
date       = {2018-06-30},
title      = {Introduction to Applied Linear Algebra: Vectors, Matrices, and Least Squares},
edition    = {1 edition},
pagetotal  = {457},
publisher  = {Cambridge University Press},
abstract   = {This groundbreaking textbook combines straightforward explanations with a wealth of practical examples to offer an innovative approach to teaching linear algebra. Requiring no prior knowledge of the subject, it covers the aspects of linear algebra – vectors, matrices, and least squares – that are needed for engineering applications, discussing examples across data science, machine learning and artificial intelligence, signal and image processing, tomography, navigation, control, and finance. The numerous practical exercises throughout allow students to test their understanding and translate their knowledge into solving real-world problems, with lecture slides, additional computational exercises in Julia and MATLAB, and data sets accompanying the book online at https://web.stanford.edu//textasciitilde boyd/vmls/. Suitable for both one-semester and one-quarter courses, as well as self-study, this self-contained text provides beginning students with the foundation they need to progress to more advanced study.},
file       = {:done/2018BoydIntroduction Introduction to Applied Linear Algebra/\\_ Vectors, Matrices, and Least Squares.pdf:application/pdf},
groups     = {tesse:5, Linear Algebra},
shorttitle = {Introduction to Applied Linear Algebra},
}
@Book{1999McclellandPsychological,
author    = {McClelland, James L. and Rumelhart, David E. and McClelland, James L.},
date      = {1999},
title     = {Psychological and Biological Models},
isbn      = {978-0-262-63110-5},
location  = {Cambridge},
note      = {OCLC: 248384622},
number    = {Vol. 2},
pagetotal = {611},
series    = {Parallel Distributed Processing. Explorations in the Microstructure of Cognition},
}
@Book{1986RumelhartParallel,
author      = {Rumelhart, David E. and McClelland, James L.},
date        = {1986},
title       = {Parallel distributed processing: explorations in the microstructure of cognition},
editora     = {University of California, San Diego},
isbn        = {978-0-262-18120-4 978-0-262-13218-3},
location    = {Cambridge, Mass},
pagetotal   = {2},
publisher   = {MIT Press},
series      = {Computational models of cognition and perception},
editoratype = {collaborator},
file        = {:done/1986RumelhartParallel Parallel Distributed Processing/\\_ Explorations in the Microstructure of Cognition.pdf:application/pdf},
groups      = {tesse:5},
shorttitle  = {Parallel distributed processing},
}
@Book{2015DiezOpenintro,
author     = {Diez, David M. and Barr, Christopher D. and Çetinkaya-Rundel, Mine},
date       = {2015-07-02},
title      = {OpenIntro Statistics},
edition    = {3 edition},
isbn       = {978-1-943450-04-6},
pagetotal  = {436},
publisher  = {OpenIntro, Inc.},
url        = {https://www.openintro.org/stat/textbook.php?stat///////\\_book=os},
abstract   = {NO COLOR in the text or graphs in this "budget edition". For a FULL COLOR textbook option, see openintro.org/os/amazonhc, which redirects to the proper Amazon page. The OpenIntro project was founded in 2009 to improve the quality and availability of education by producing exceptional books and teaching tools that are free to use and easy to modify. Our inaugural effort is OpenIntro Statistics. Probability is optional, inference is key, and we feature real data whenever possible. Files for the entire book are freely available at openintro.org, and anybody can purchase a paperback copy from amazon.com for about 10. OpenIntro has grown through the involvement and enthusiasm of our community. Visit our website, openintro.org. We provide videos, labs for R and SAS, teaching resources like slides, and many other helpful resources.},
file       = {:done/2015DiezOpenintro OpenIntro Statistics.pdf:application/pdf},
groups     = {tesse:5, Probability},
shorttitle = {OpenIntro Statistics},
}
@InProceedings{2013PassonneauAutomated,
author     = {Passonneau, Rebecca Jane and Chent, Emily and Guot, Weiwei and Perin, Dolores},
booktitle  = {Short Papers},
date       = {2013-01-01},
title      = {Automated pyramid scoring of summaries using distributional semantics},
eventtitle = {51st Annual Meeting of the Association for Computational Linguistics, ACL 2013},
pages      = {143--147},
publisher  = {Association for Computational Linguistics (ACL)},
url        = {https://pennstate.pure.elsevier.com/en/publications/automated-pyramid-scoring-of-summaries-using-distributional-seman},
urldate    = {2019-04-01},
}
@Book{noauthor_proceedings_2013,
date  = {2013},
title = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, ACL 2013},
isbn  = {978-1-937284-51-0},
url   = {https://aclweb.org/anthology/P13-2000},
}
@Misc{farnum_review_nodate,
author   = {Farnum, Larissa},
title    = {Review of differentiation and integration rules from Calculus I and II for Ordinary Differential Equations},
url      = {https://www.academia.edu/8924647/Review///////\\_of///////\\_differentiation///////\\_and///////\\_integration///////\\_rules///////\\_from///////\\_Calculus///////\\_I///////\\_and///////\\_II///////\\_for///////\\_Ordinary///////\\_Differential///////\\_Equations///////\\_3301},
urldate  = {2019-04-01},
abstract = {Review of differentiation and integration rules from Calculus I and II for Ordinary Differential Equations, 3301},
langid   = {english},
}
@Reference{2019LearningStatistical,
date   = {2019},
editor = {Learning, Hawkes},
title  = {Statistical Tables},
url    = {http://www.hawkeslearning.com/documents/statdatasets/stat///////\\_tables.pdf},
file   = {:done/2019LearningStatistical Statistical Tables.pdf:application/pdf},
groups = {tesse:5, Probability},
}
@Article{2011IlhanAdaptive,
author       = {İlhan, Taylan and Iravani, Seyed M. R. and Daskin, Mark S.},
date         = {2011},
journaltitle = {Operations research},
title        = {The adaptive knapsack problem with stochastic rewards},
number       = {1},
pages        = {242--248},
url          = {http://users.iems.northwestern.edu//textasciitilde iravani/Stochastic///////\\_Knapsack.pdf},
volume       = {59},
file         = {:done/2011IlhanAdaptive The Adaptive Knapsack Problem with Stochastic Rewards.pdf:application/pdf},
groups       = {tesse:5},
}
@Misc{PoczosIntroduction,
author = {Poczos, Barnabas},
title  = {Introduction to Machine Learning (Lecture Notes)},
file   = {:done/PoczosIntroduction Introduction to Machine Learning (Lecture Notes).pdf:application/pdf},
groups = {tesse:2},
}
@Article{KoernerVectors,
author = {Körner, T. W.},
title  = {Vectors, Pure and Applied},
pages  = {458},
file   = {:done/KoernerVectors Vectors, Pure and Applied.pdf:application/pdf},
groups = {tesse:2, Mathematics},
langid = {english},
}
@Book{2009StewartProbability,
author     = {Stewart, William J.},
date       = {2009},
title      = {Probability, Markov chains, queues, and simulation: the mathematical basis of performance modeling},
isbn       = {978-0-691-14062-9},
location   = {Princeton, N.J},
note       = {OCLC: ocn255018592},
pagetotal  = {758},
publisher  = {Princeton University Press},
keywords   = {Computer simulation, Markov processes, Probabilities, Queuing theory},
langid     = {english},
shorttitle = {Probability, Markov chains, queues, and simulation},
}
@Article{2014FowlerSequences,
author = {Fowler, Jim and Snapp, Bart},
date   = {2014},
title  = {Sequences and Series},
pages  = {147},
file   = {:done/2014FowlerSequences Sequences and Series.pdf:application/pdf},
groups = {tesse:2, Mathematics},
langid = {english},
}
@Book{2016PinedoScheduling,
author     = {Pinedo, Michael},
date       = {2016},
title      = {Scheduling: theory, algorithms, and systems},
edition    = {Fifth Edition},
isbn       = {978-3-319-26578-0 978-3-319-26580-3},
location   = {Cham Heidelberg New York Dordrecht London},
note       = {OCLC: 945375528},
pagetotal  = {670},
publisher  = {Springer},
file       = {:done/2016PinedoScheduling Scheduling/\\_ Theory, Algorithms, and Systems.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Production scheduling},
langid     = {english},
shorttitle = {Scheduling},
}
@Book{2004BruceMastering,
author     = {Bruce, Robert and Mercer, Brian},
date       = {2004},
title      = {Mastering astral projection: 90-day guide to out-of-body experience},
edition    = {1st ed},
isbn       = {978-0-7387-0467-8},
location   = {St. Paul, Minn},
pagetotal  = {483},
publisher  = {Llewellyn},
keywords   = {Astral projection},
langid     = {english},
shorttitle = {Mastering astral projection},
}
@Article{2015LermanPatent,
author       = {Lerman, Celia},
date         = {2015},
journaltitle = {SSRN Electronic Journal},
title        = {Patent Strategies of Technology Startups: An Empirical Study},
doi          = {10.2139/ssrn.2610433},
issn         = {1556-5068},
url          = {http://www.ssrn.com/abstract=2610433},
urldate      = {2019-04-01},
abstract     = {How does a patent strategy affect a tech startup company’s growth? This is a fundamental question for technology entrepreneurs, investors, lawyers and the innovation system as a whole. In this study, I shed light on this issue by conducting an empirical analysis of the patenting strategies of technology startups, examining the relationship between a company’s patent applications and different events over the company’s life: rounds of investment received, company acquisition and closure. I provide the first comprehensive cross-industry analysis of this question, by analyzing the patent portfolios of United States startups listed in CrunchBase, a crowd-sourced registry of tech companies used by the startup industry. By looking into these companies’ public patent applications from the United States Patent and Trademark Office (USPTO) database between 2008 and 2012, I examine the patenting patterns of startups as they progress through funding rounds.},
file         = {:done/2015LermanPatent Patent Strategies of Technology Startups/\\_ an Empirical Study.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Patent Strategies of Technology Startups},
}
@Book{2018NiederstRobbinsLearning,
author     = {Niederst Robbins, Jennifer},
date       = {2018},
title      = {Learning Web Design: a beginner's guide to HTML, CSS, Javascript, and Web Graphics},
edition    = {Fifth Edition},
isbn       = {978-1-4919-6020-2},
location   = {Beijing ; Sebastopol, CA},
pagetotal  = {790},
publisher  = {O'Reilly},
keywords   = {Design, Handbooks, manuals, etc, Authoring programs, Cascading style sheets, Computer graphics, HTML (Document markup language), JavaScript (Computer program language), Web sites},
langid     = {english},
shorttitle = {Learning Web Design},
}
@Book{2012DegrootProbability,
author    = {DeGroot, Morris H. and Schervish, Mark J.},
date      = {2012},
title     = {Probability and statistics},
edition   = {4th ed},
isbn      = {978-0-321-50046-5},
location  = {Boston},
note      = {OCLC: ocn502674206},
pagetotal = {893},
publisher = {Addison-Wesley},
file      = {:done/2012DegrootProbability Probability and Statistics.pdf:application/pdf},
groups    = {tesse:5, Probability},
keywords  = {Probabilities, Mathematical statistics, Textbooks},
langid    = {english},
}
@Book{2018BlennowMathematical,
author = {Blennow, Mattias},
date   = {2018},
title  = {Mathematical Methods for Physics and Engineering},
isbn   = {978-1-138-05690-9},
file   = {:done/2018BlennowMathematical Mathematical Methods for Physics and Engineering.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
}
@Book{2018ShoresApplied,
author    = {Shores, Thomas S.},
date      = {2018},
title     = {Applied Linear Algebra and Matrix Analysis},
doi       = {10.1007/978-3-319-74748-4},
isbn      = {978-3-319-74747-7},
location  = {Cham},
publisher = {Springer International Publishing},
series    = {Undergraduate Texts in Mathematics},
urldate   = {2019-04-01},
file      = {:done/2018ShoresApplied Applied Linear Algebra and Matrix Analysis.pdf:application/pdf},
groups    = {tesse:5, Linear Algebra},
langid    = {english},
}
@Book{2012GreeneEconometric,
author    = {Greene, William H.},
date      = {2012},
title     = {Econometric analysis},
edition   = {7th ed},
isbn      = {978-0-13-139538-1},
location  = {Boston},
note      = {OCLC: ocn692292382},
pagetotal = {1188},
publisher = {Prentice Hall},
file      = {:done/2012GreeneEconometric Econometric Analysis.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Econometrics},
langid    = {english},
}
@Book{2018ImanishiSynthesis,
author     = {Imanishi, Yukio},
date       = {2018-01-10},
title      = {Synthesis of Biocomposite Materials: Chemical and Biological Modifications of Natural Polymers},
doi        = {10.1201/9781351077064},
edition    = {1},
editor     = {Imanishi, Yukio},
isbn       = {978-1-351-07706-4},
publisher  = {CRC Press},
url        = {https://www.taylorfrancis.com/books/9781351085519},
urldate    = {2019-04-01},
langid     = {english},
shorttitle = {Synthesis of Biocomposite Materials},
}
@Book{2003Buss3d,
author     = {Buss, Samuel R.},
date       = {2003-05-19},
title      = {3D Computer Graphics: A Mathematical Introduction with OpenGL},
edition    = {1 edition},
isbn       = {978-0-521-82103-2},
location   = {New York},
pagetotal  = {396},
publisher  = {Cambridge University Press},
abstract   = {This introduction to 3D computer graphics emphasizes fundamentals and the mathematics underlying computer graphics, while also covering programming techniques using OpenGL, a platform-independent graphics programming environment. The minimal prerequisites make it suitable for self-study or for use as an advanced undergraduate or introductory graduate text as the author leads step-by-step from the basics of transformations to advanced topics such as animations and kinematics. Accompanying software, including source code for a ray tracing software package, is available freely from the book's web site.},
shorttitle = {3D Computer Graphics},
}
@Book{2016GeorgeMastering,
author     = {George, Nigel},
date       = {2016-12-06},
title      = {Mastering Django: Core},
isbn       = {978-1-78728-114-1},
pagetotal  = {694},
publisher  = {Packt Publishing},
file       = {:done/2016GeorgeMastering Mastering Django/\\_ Core.epub:ePUB},
groups     = {tesse:5},
shorttitle = {Mastering Django},
}
@Book{2014ChaconPro,
author    = {Chacon, Scott and Straub, Ben},
date      = {2014-11-09},
title     = {Pro Git},
edition   = {2nd ed. edition},
isbn      = {978-1-4842-0077-3},
location  = {New York, NY},
pagetotal = {456},
publisher = {Apress},
abstract  = {Pro Git (Second Edition) is your fully-updated guide to Git and its usage in the modern world. Git has come a long way since it was first developed by Linus Torvalds for Linux kernel development. It has taken the open source world by storm since its inception in 2005, and this book teaches you how to use it like a pro. Effective and well-implemented version control is a necessity for successful web projects, whether large or small. With this book you’ll learn how to master the world of distributed version workflow, use the distributed features of Git to the full, and extend Git to meet your every need. Written by Git pros Scott Chacon and Ben Straub, Pro Git (Second Edition) builds on the hugely successful first edition, and is now fully updated for Git version 2.0, as well as including an indispensable chapter on GitHub. It’s the best book for all your Git needs.},
file      = {:done/2014ChaconPro Pro Git.epub:ePUB},
groups    = {tesse:5},
}
@Book{2013PavlinaAstral,
author     = {Pavlina, Erin},
date       = {2013-10-18},
title      = {The Astral Projection Guidebook: Mastering the Art of Astral Travel},
edition    = {1 edition},
isbn       = {978-1-4912-4697-9},
pagetotal  = {156},
publisher  = {CreateSpace Independent Publishing Platform},
abstract   = {Learn to Master Astral Travel Would you like to walk through walls, fly around the world, reconnect with deceased loved ones, and explore time and space? Have you had a terrifying out of body encounter that left you fearful of ever exploring astral projection? Would you like to know how to travel the astral realms safely and protect yourself from unwanted projections? Whether you are a novice or an experienced astral traveler, the Astral Projection Guidebook will teach you how to master astral projection safely and effectively. In this guide, you’ll learn how to: • Prepare yourself for a positive astral journey • Achieve separation from your body • Explore the astral realms – alone or with friends • Enjoy mind-blowing astral sex • Protect yourself from dark entities • Train to become an astral warrior Get ready to have fun exploring the astral realms!},
file       = {:done/2013PavlinaAstral The Astral Projection Guidebook/\\_ Mastering the Art of Astral Travel.epub:ePUB},
groups     = {tesse:5},
shorttitle = {The Astral Projection Guidebook},
}
@Book{2012BruceAstral,
author     = {Bruce, Robert},
date       = {2012},
title      = {Astral Dynamics: A NEW Approach to Out-of-Body Experience},
edition    = {Edição: 1.1},
pagetotal  = {517},
publisher  = {Magic Light Press},
abstract   = {This is the fabulous FIRST EDITION of Astral DynamicsThe first edition of Astral Dynamics (the book you are now reading about) contains a great deal of content that was dropped from the 2009 second edition to save space. These are essentially two quite different books. Most people want both these amazing books, so we convinced Robert Bruce to produce an eBook version of the original First Edition of Astral Dynamics, with full color illustrations and other improvements. This First Edition went out of print in 2009. THE FIRST EDITION OF ASTRAL DYNAMICS - In one fascinating volume, Robert Bruce gathered together a personal narrative, a "how-to", a troubleshooting guide, and a theoretical perspective on the nonphysical structure underlying the strange and multidimensional life we all lead. Whether you are a skeptic, a veteran astral projector, a newbie, or an armchair traveler, there's treasure here. It's in a class by itself. What he says here rings true. In fact, it does a lot more than ring true. It opens doors. Astral Dynamics provides the intelligent and motivated reader with everything needed to put theory into practice. The book's six parts may be read each on their own, but they have been placed to build nicely one upon the other. Part One, "Elements of Projection", presents Robert's theory of what actually goes on when the projectable astral double leaves the physical body. This in itself, right out of the gate, is fascinating terrain. His theory of the consequences of the mind-split that results from projection is itself worth the price of the book. Part Two, "NEW Energy Ways", presents his stunningly practical method of raising energy and awareness by using touch, which he calls Tactile Imaging. I have tried this method on half a dozen people, selected more or less at random, each of whom obtained the desired awareness within seconds! And then, as if the new method of visualization weren't enough, Robert proceeds to describe the nature and anatomy of our energy bodies, a description firmly rooted in his own personal explorations. Part Three, "Core Skills", builds on this foundation, demonstrating how to succeed at the three tasks that are essential to success in astral projection: deep physical relaxation, taming the mind, and attaining the trance state. Part Four, "Projection Exit and Technique", tells you what you need to know to stop reading about astral projection and actually do it. Part Five, "The Akashic Connection", proceeds into the realm of the theoretical, not for the sake of getting lost in theory, but in order to make sense of things seen and heard. Particularly interesting is Robert's description of the nature and meaning of what he calls the Akashic Pulse. As to his description and analysis of the astral planes, the silver cord, the etheric body, and the Akashic record — I doubt that these have been equaled anywhere in the subject's extensive modern literature. I am confident that they have not been excelled. Part Six, "Strange Astral Phenomena", takes on a few conundrums that are worth exploring. Projection into higher realms; reality fluctuations; astral noise; what Robert calls astral wildlife, and astral combat... he covers the turf as it hasn't been covered to date. And he does it so casually, in so unpretentious a manner, his mood ranging from deepest awe to casually joking, with all the range between. If you have any interest at all in the subject of astral projection, lucid dreaming — higher consciousness in general — you're going to love this book. I predict that it will become a classic, read and valued for many years to come. Frank DeMarco - Chairman, Hampton Roads Publishing Company, Inc., VA.About the Author:Robert Bruce is the author of six groundbreaking books exploring such mysteries as the human energy body, the out-of-body experience, Kundalini, mind's-eye vision, spiritual and psychic development, metaphysics, clairvoyance, and psychi},
file       = {:done/2012BruceAstral Astral Dynamics/\\_ a NEW Approach to Out of Body Experience.epub:ePUB},
groups     = {tesse:5},
shorttitle = {Astral Dynamics},
}
@Book{1999KostekSoft,
author     = {Kostek, Bozena},
date       = {1999},
title      = {Soft Computing in Acoustics: Applications of Neural Networks, Fuzzy Logic and Rough Sets to Musical Acoustics},
isbn       = {978-3-7908-1190-2},
publisher  = {Physica-Verlag Heidelberg},
series     = {Studies in Fuzziness and Soft Computing},
url        = {https://www.springer.com/br/book/9783790811902},
urldate    = {2019-04-04},
abstract   = {Applications of some selected soft computing methods to acoustics and sound engineering are presented in this book. The aim of this research study is the implementation of soft computing methods to musical signal analysis and to the recognition of musical sounds and phrases. Accordingly, some methods based on such learning algorithms as neural networks, rough sets and fuzzy-logic were conceived, implemented and tested. Additionally, the above-mentioned methods were applied to the analysis and verification of subjective testing results. The last problem discussed within the framework of this book was the problem of fuzzy control of the classical pipe organ instrument. The obtained results show that computational intelligence and soft computing may be used for solving some vital problems in both musical and architectural acoustics.},
file       = {:done/1999KostekSoft Soft Computing in Acoustics/\\_ Applications of Neural Networks, Fuzzy Logic and Rough Sets to Musical Acoustics.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Soft Computing in Acoustics},
}
@Book{2017DangetiStatistics,
author     = {Dangeti, Pratap},
date       = {2017-07-21},
title      = {Statistics for Machine Learning: Techniques for exploring supervised, unsupervised, and reinforcement learning models with Python and R},
isbn       = {978-1-78829-575-8},
pagetotal  = {442},
publisher  = {Packt Publishing},
abstract   = {Key FeaturesLearn about the statistics behind powerful predictive models with p-value, ANOVA, and F- statistics.Implement statistical computations programmatically for supervised and unsupervised learning through K-means clustering.Master the statistical aspect of Machine Learning with the help of this example-rich guide to R and Python.Book DescriptionComplex statistics in Machine Learning worry a lot of developers. Knowing statistics helps you build strong Machine Learning models that are optimized for a given problem statement. This book will teach you all it takes to perform complex statistical computations required for Machine Learning. You will gain information on statistics behind supervised learning, unsupervised learning, reinforcement learning, and more. Understand the real-world examples that discuss the statistical side of Machine Learning and familiarize yourself with it. You will also design programs for performing tasks such as model, parameter fitting, regression, classification, density collection, and more.By the end of the book, you will have mastered the required statistics for Machine Learning and will be able to apply your new skills to any sort of industry problem.What you will learnUnderstand the Statistical and Machine Learning fundamentals necessary to build modelsUnderstand the major differences and parallels between the statistical way and the Machine Learning way to solve problemsLearn how to prepare data and feed models by using the appropriate Machine Learning algorithms from the more-than-adequate R and Python packagesAnalyze the results and tune the model appropriately to your own predictive goalsUnderstand the concepts of required statistics for Machine LearningIntroduce yourself to necessary fundamentals required for building supervised ///////\\& unsupervised deep learning modelsLearn reinforcement learning and its application in the field of artificial intelligence domainAbout the AuthorPratap Dangeti develops machine learning and deep learning solutions for structured, image, and text data at TCS, analytics and insights, innovation lab in Bangalore. He has acquired a lot of experience in both analytics and data science. He received his master's degree from IIT Bombay in its industrial engineering and operations research program. He is an artificial intelligence enthusiast. When not working, he likes to read about next-gen technologies and innovative methodologies.Table of ContentsJourney from Statistics to Machine LearningParallelism of Statistics and Machine LearningLogistic Regression vs. Random ForestTree-Based Machine Learning modelsK-Nearest Neighbors ///////\\& Naive BayesSupport Vector Machines ///////\\& Neural NetworksRecommendation EnginesUnsupervised LearningReinforcement Learning},
shorttitle = {Statistics for Machine Learning},
}
@Book{2011LoyMusimathics,
author     = {Loy, Gareth},
date       = {2011-08-19},
title      = {Musimathics: The Mathematical Foundations of Music I},
edition    = {Reprint edition},
isbn       = {978-0-262-51655-6},
location   = {Cambridge, Mass.},
pagetotal  = {504},
publisher  = {The MIT Press},
volume     = {1},
abstract   = {A commonsense, self-contained introduction to the mathematics and physics of music; essential reading for musicians, music engineers, and anyone interested in the intersection of art and science."Mathematics can be as effortless as humming a tune, if you know the tune," writes Gareth Loy. In Musimathics, Loy teaches us the tune, providing a friendly and spirited tour of the mathematics of music―a commonsense, self-contained introduction for the nonspecialist reader. It is designed for musicians who find their art increasingly mediated by technology, and for anyone who is interested in the intersection of art and science.In Volume 1, Loy presents the materials of music (notes, intervals, and scales); the physical properties of music (frequency, amplitude, duration, and timbre); the perception of music and sound (how we hear); and music composition. Calling himself "a composer seduced into mathematics," Loy provides answers to foundational questions about the mathematics of music accessibly yet rigorously. The examples given are all practical problems in music and audio.Additional material can be found at http://www.musimathics.com.},
file       = {:done/2011LoyMusimathics Musimathics/\\_ the Mathematical Foundations of Music I.djvu:Djvu},
groups     = {tesse:5},
shorttitle = {Musimathics},
}
@Book{2011LoyMusimathicsa,
author     = {Loy, Gareth and Chowning, John M.},
date       = {2011},
title      = {Musimathics: The Mathematical Foundations of Music II},
edition    = {1st MIT Press pbk. ed},
isbn       = {978-0-262-51656-3 978-0-262-12285-6},
location   = {Cambridge, Mass.},
note       = {OCLC: 935185987},
pagetotal  = {562},
publisher  = {MIT Press},
volume     = {2},
file       = {:done/2011LoyMusimathicsa Musimathics/\\_ the Mathematical Foundations of Music II.djvu:Djvu},
groups     = {tesse:5},
shorttitle = {Musimathics},
}
@Book{2017BarrosFirst,
author    = {de Barros, Laécio Carvalho and Bassanezi, Rodney Carlos and Lodwick, Weldon Alexander},
date      = {2017},
title     = {A First Course in Fuzzy Logic, Fuzzy Dynamical Systems, and Biomathematics},
doi       = {10.1007/978-3-662-53324-6},
isbn      = {978-3-662-53322-2},
location  = {Berlin, Heidelberg},
publisher = {Springer Berlin Heidelberg},
series    = {Studies in Fuzziness and Soft Computing},
urldate   = {2019-04-04},
volume    = {347},
file      = {:done/2017BarrosFirst A First Course in Fuzzy Logic, Fuzzy Dynamical Systems, and Biomathematics.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2016ArgueellesMendezPractical,
author    = {Argüelles Mendez, Luis},
date      = {2016},
title     = {A Practical Introduction to Fuzzy Logic using LISP},
doi       = {10.1007/978-3-319-23186-0},
isbn      = {978-3-319-23185-3 978-3-319-23186-0},
location  = {Cham},
publisher = {Springer International Publishing},
series    = {Studies in Fuzziness and Soft Computing},
urldate   = {2019-04-04},
volume    = {327},
file      = {:done/2016ArgueellesMendezPractical A Practical Introduction to Fuzzy Logic Using LISP.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Collection{2015TamirFifty,
date      = {2015},
editor    = {Tamir, Dan E. and Rishe, Naphtali D. and Kandel, Abraham},
title     = {Fifty years of fuzzy logic and its applications},
isbn      = {978-3-319-19682-4 978-3-319-19683-1},
location  = {Cham},
note      = {OCLC: 908374518},
number    = {326},
pagetotal = {684},
publisher = {Springer},
series    = {Studies in fuzziness and soft computing},
file      = {:done/2015TamirFifty Fifty Years of Fuzzy Logic and Its Applications.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2008RossFuzzy,
author    = {Ross, Timothy J.},
date      = {2008-03-11},
title     = {Fuzzy Logic with Engineering Applications},
edition   = {2 edition},
pagetotal = {650},
publisher = {Wiley},
abstract  = {Fuzzy logic refers to a large subject dealing with a set of methods to characterize and quantify uncertainty in engineering systems that arise from ambiguity, imprecision, fuzziness, and lack of knowledge. Fuzzy logic is a reasoning system based on a foundation of fuzzy set theory, itself an extension of classical set theory, where set membership can be partial as opposed to all or none, as in the binary features of classical logic. Fuzzy logic is a relatively new discipline in which major advances have been made over the last decade or so with regard to theory and applications. Following on from the successful first edition, this fully updated new edition is therefore very timely and much anticipated. Concentration on the topics of fuzzy logic combined with an abundance of worked examples, chapter problems and commercial case studies is designed to help motivate a mainstream engineering audience, and the book is further strengthened by the inclusion of an online solutions manual as well as dedicated software codes. Senior undergraduate and postgraduate students in most engineering disciplines, academics and practicing engineers, plus some working in economics, control theory, operational research etc, will all find this a valuable addition to their bookshelves.},
file      = {:done/2008RossFuzzy Fuzzy Logic with Engineering Applications.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2002MoravcsikMusical,
author     = {Moravcsik, Michael J. and Rosenbluth, Darrel},
date       = {2002},
title      = {Musical sound: an introduction to the physics of music},
isbn       = {978-0-306-46710-3},
location   = {New York, NY},
note       = {OCLC: 248884612},
pagetotal  = {316},
publisher  = {Kluwer Acad./Plenum},
langid     = {english},
shorttitle = {Musical sound},
}
@Article{1983McintyreOscillations,
author       = {McIntyre, M. E. and Schumacher, R. T. and Woodhouse, J.},
date         = {1983-11},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {On the oscillations of musical instruments},
doi          = {10.1121/1.390157},
issn         = {0001-4966},
number       = {5},
pages        = {1325--1345},
urldate      = {2019-04-04},
volume       = {74},
file         = {:done/1983McintyreOscillations On the Oscillations of Musical Instruments.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1998KarjalainenPlucked,
author       = {Karjalainen, Matti and Valimaki, Vesa and Tolonen, Tero},
date         = {1998},
journaltitle = {Computer Music Journal},
title        = {Plucked-String Models: From the Karplus-Strong Algorithm to Digital Waveguides and beyond},
doi          = {10.2307/3681155},
issn         = {0148-9267},
number       = {3},
pages        = {17},
url          = {https://www.jstor.org/stable/3681155?origin=crossref},
urldate      = {2019-04-04},
volume       = {22},
langid       = {english},
shorttitle   = {Plucked-String Models},
}
@Book{2011VeselicDamped,
author     = {Veselić, Krešimir},
date       = {2011},
title      = {Damped oscillations of linear systems: a mathematical introduction},
isbn       = {978-3-642-21334-2},
location   = {Berlin ; London ; New York},
note       = {OCLC: ocn747814290},
number     = {2023},
pagetotal  = {209},
publisher  = {Springer},
series     = {Lecture notes in mathematics},
abstract   = {"The theory of linear damped oscillations was originally developed more than hundred years ago and is still of vital research interest to engineers, mathematicians and physicists alike. This theory plays a central role in explaining the stability of mechanical structures in civil engineering, but it also has applications in other fields such as electrical network systems and quantum mechanics. This volume gives an introduction to linear finite dimensional damped systems as they are viewed by an applied mathematician. After a short overview of the physical principles leading to the linear systems model, a largely self-contained mathematical theory for this model is presented. this includes the geometry of the underlying indefinite eigenvalue problem. Particular attention is paid to the sensitivity issues which influence numerical computations. finally, several recent research developments are included, e.g. Lyapunov stability and the perturbation of the time evolution."--P. [4] of cover},
file       = {:done/2011VeselicDamped Damped Oscillations of Linear Systems//_ a Mathematical Introduction.pdf:application/pdf},
groups     = {tesse:5, Acoustics},
keywords   = {Textbooks, Harmonic oscillators, Linear systems},
langid     = {english},
shorttitle = {Damped oscillations of linear systems},
}
@Article{Waves,
title  = {Waves and Oscillations, Second Edition},
pages  = {394},
file   = {:done/Waves Waves and Oscillations, Second Edition.pdf:application/pdf},
groups = {tesse:2, Acoustics},
langid = {english},
}
@Book{2009CorduneanuAlmost,
author    = {Corduneanu, C.},
date      = {2009},
title     = {Almost periodic oscillations and waves},
isbn      = {978-0-387-09818-0},
location  = {New York},
note      = {OCLC: ocn248978163},
pagetotal = {308},
publisher = {Springer},
file      = {:done/2009CorduneanuAlmost Almost Periodic Oscillations and Waves.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Almost periodic functions, Differential equations, Oscillation theory, Oscillations},
langid    = {english},
}
@Book{2001AnfilovPhysics,
author     = {Anfilov, Gleb},
date       = {2001-07-01},
title      = {Physics and Music},
isbn       = {978-0-89875-419-3},
location   = {Honolulu, Hawaii},
pagetotal  = {272},
publisher  = {University Press of the Pacific},
translator = {Kuznetsov, Boris},
abstract   = {From the primitive reed pipe to modern music "written" by computers is quite a journey. Here, in informal text and about a score of plates, is a story that takes the teenage layman on this interesting trip. The younger reader, like a good musicologist, follows the steps in the evolution of the most important instruments that make up today's symphony orchestra, and the development of music itself (scales, modes, keys, and temperaments). Physics and music is also a source, although, of necessity a modest one, of information about the music research that has been underway in the Soviet Union, especially in the scientific manufacture of the violin, and in electrophonic and synthetic music. This why the foreign reader might think of a degree of "bias" on the part of the author. Yet, it gives him an insight into what is going on in a country that has given the world quite a number of great composers.},
file       = {:done/2001AnfilovPhysics Physics and Music.pdf:application/pdf},
groups     = {tesse:5},
}
@Book{2010FletcherPhysics,
author    = {Fletcher, Neville H. and Rossing, Thomas D.},
date      = {2010},
title     = {The physics of musical instruments},
edition   = {2. ed., [Nachdr.]},
isbn      = {978-1-4419-3120-7 978-0-387-21603-4},
location  = {New York, NY},
note      = {OCLC: 780101082},
pagetotal = {756},
publisher = {Springer},
file      = {:done/2010FletcherPhysics The Physics of Musical Instruments.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Thesis{2004DavidssonStructure,
author      = {Davidsson, Peter},
date        = {2004},
institution = {Univ.},
title       = {Structure-acoustic analysis: finite element modelling and reduction methods},
type        = {phdthesis},
location    = {Lund},
note        = {OCLC: 186476406},
langid      = {english},
shorttitle  = {Structure-acoustic analysis},
}
@Article{1996HainesDetermination,
author       = {Haines, DanielW. and Leban, Jean-Michel and Herb, Christian},
date         = {1996-08},
journaltitle = {Wood Science and Technology},
title        = {Determination of Young's modulus for spruce, fir and isotropic materials},
doi          = {10.1007/BF00229348},
issn         = {0043-7719, 1432-5225},
number       = {4},
subtitle     = {by the resonance flexure method with comparisons to static flexure and other dynamic methods},
titleaddon   = {by the resonance flexure method with comparisons to static flexure and other dynamic methods},
urldate      = {2019-04-05},
volume       = {30},
abstract     = {Dynamic methods provide rapid and accurate means to determine Young's modulus, i.e. the modulus of elasticity, of wood. For dry, clear specimens of picba commun (Norway spruce, picea excelsa) and sapin pictin (silver fir, abies amabilis) we present a comparison of results from tests by a resonance flexure method with results obtained from four-point static flexure tests. For a wide range of specimen size the resonance flexure method provides a simpler, more rapidly performed alternative to the classical static flexure method, giving Young's modulus values which are for the spruce and fir specimens of this study, nearly identical to those calculated from the static flexure tests. Results are also presented which show that a resonance longitudinal method yields higher values of Young's modulus and an ultrasonic method yields still higher values. We provide also a comparison of the four test methods applied to isotropic materials.},
file         = {:done/1996HainesDetermination Determination of Young's Modulus for Spruce, Fir and Isotropic Materials.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1997MarmarasErgonomic,
author       = {Marmaras, Nikos and Zarboutis, Nikos},
date         = {1997-02},
journaltitle = {Applied Ergonomics},
title        = {Ergonomic redesign of the electric guitar},
doi          = {10.1016/s0003-6870(96)00032-4},
number       = {1},
pages        = {59--67},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0003687096000324},
urldate      = {2019-04-05},
volume       = {28},
abstract     = {The present study deals with the redesign of the electric guitar, considering ergonomic criteria. Diffkulties met by novice musicians, neuro-muscular fatigue caused by guitar playing and occupational diseases occurring to professional guitarists, justify this study. Characteristics of existing electric guitar models which add to musician’s fatigue or difficulty, were identified. A number of ergonomic requirements were then derived. The redesign process tried to satisfy these requirements, considering at the same time musical requirements and technical constraints. A comparative evaluation of the designed ergonomic guitar with three existing electric guitar models, showed1 that although the designed electric guitar preserves the main features of the instrument, it achieves better user fit. Copyright @ 1996 Elsevier Science Ltd.},
file         = {:done/1997MarmarasErgonomic Ergonomic Redesign of the Electric Guitar.pdf:application/pdf},
groups       = {tesse:5},
journal      = {Applied Ergonomics},
langid       = {english},
month        = {February},
publisher    = {Elsevier BV},
year         = {1997},
}
@Article{2012TorresExperimental,
author       = {Torres, J Alejandro and Villarreal, J Luis and Ramırez, R},
date         = {2012},
journaltitle = {Rev. Mex. Fis. E},
title        = {Experimental and simulated exploration of structural deﬂections and acoustic waves of guitar top plates},
pages        = {6},
langid       = {english},
}
@Article{PedgleyMaterials,
author = {Pedgley, Owain and Norman, Eddie and Armstrong, Rob},
title  = {MATERIALS-INSPIRED INNOVATION FOR ACOUSTIC GUITAR DESIGN},
pages  = {20},
file   = {:done/PedgleyMaterials MATERIALS INSPIRED INNOVATION fOR ACOUSTIC GUITAR DESIGN.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Article{2014PaivaModal,
author = {Paiva, G.},
date   = {2014},
title  = {Modal Analysis of a Brazilian Guitar Body},
pages  = {7},
file   = {:done/2014PaivaModal Modal Analysis of a Brazilian Guitar Body.pdf:application/pdf},
groups = {tesse:2, Acoustics},
langid = {english},
}
@Article{2005BecacheNumerical,
author       = {Bécache, Eliane and Chaigne, Antoine and Derveaux, Gregoire and Joly, Patrick},
date         = {2005-01},
journaltitle = {Computers //\\\& Structures},
title        = {Numerical simulation of a guitar},
doi          = {10.1016/j.compstruc.2004.04.018},
issn         = {0045-7949},
number       = {2},
pages        = {107--126},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045794904002974},
urldate      = {2019-04-05},
volume       = {83},
abstract     = {The purpose of this study is to present a time-domain numerical modeling of the guitar. The model involves the transverse displacement of the string excited by a force pulse, the ﬂexural motion of the soundboard and the sound radiation in the air. We use a speciﬁc spectral method for solving the Kirchhoﬀ–LoveÕs dynamic plate model for orthotropic material, a ﬁctitious domain method for solving the ﬂuid–structure interaction and a conservative scheme for the time discretization. One of the originality of the proposed scheme is a stable coupling method between a continuous time resolution and a discrete one.},
file         = {:done/2005BecacheNumerical Numerical Simulation of a Guitar.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2007ShlychkovNumerical,
author       = {Shlychkov, S. V.},
date         = {2007-05},
journaltitle = {Mechanics of Composite Materials},
title        = {Numerical-experimental investigation of resonance characteristics of a sounding board},
doi          = {10.1007/s11029-007-0026-y},
issn         = {0191-5665, 1573-8922},
number       = {3},
pages        = {269--276},
urldate      = {2019-04-05},
volume       = {43},
langid       = {english},
}
@Article{2010HsiaoProduct,
author       = {Hsiao, Shih-Wen and Chiu, Fu-Yuan and Lu, Shu-Hong},
date         = {2010-05},
journaltitle = {International Journal of Industrial Ergonomics},
title        = {Product-form design model based on genetic algorithms},
doi          = {10.1016/j.ergon.2010.01.009},
issn         = {0169-8141},
number       = {3},
pages        = {237--246},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0169814110000107},
urldate      = {2019-04-05},
volume       = {40},
abstract     = {Industrial design attempts to enhance quality of life by designing products that meet consumer requirements. Combining concepts from various ﬁelds, including design, computer technology, aesthetics, and economics, industrial designers seek to improve quality of life by designing products that meet consumer needs. Industrial designers focus on customers' perceptions of products and their preferences for certain shapes, textures, colors, styles, linguistic variables, prices, and functions. Because new products are continuously being released, manufacturers must continually design products to satisfy customer needs to avoid displacement by market competitors. When planning strategies for marketing products to various users and consumers, managers must often consider multiple combinations of product shapes and must design products that cater to consumer tastes to minimize the risk of their products being rejected by the market. Companies with highly-skilled designers have more ideas, better and more competitive products, and shorter production times than companies with weak designers. This study analyzed product styles by applying genetic algorithms and Kansei Engineering Type II (AHP and Quantiﬁcation Theory Type I). This research transforms the psychological conceptions of consumers into linguistic variables. A MATLAB program was constructed to enable designers to simulate consumer logic. The cognitive dissonance between virtual and real models was minimized by using a 3D CAD model, and the virtual model of optimum solutions in this study employed a rapid prototyping machine to generate real models efﬁciently. Future genetic algorithm models applying different decision theories may achieve even faster and more accurate results.},
file         = {:done/2010HsiaoProduct Product Form Design Model Based on Genetic Algorithms.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014ShepherdEffects,
author       = {Shepherd, Micah R. and Hambric, Stephen A. and Wess, Dennis B.},
date         = {2014-11},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {The effects of wood variability on the free vibration of an acoustic guitar top plate},
doi          = {10.1121/1.4898740},
issn         = {0001-4966},
number       = {5},
pages        = {EL357--EL361},
urldate      = {2019-04-05},
volume       = {136},
abstract     = {A ﬁnite element model of a bare top plate with braces and a bridge plate was created using orthotropic material properties. The natural variation of the wood properties including dependence on moisture content was also determined. The simulated modes were then compared to experimentally obtained modes from top plate prototypes. Uncertainty analysis was also performed to determine the statistical bound of natural variability between wood samples. The natural frequencies of the model fall within the computed error bound. These results reinforce the importance of obtaining accurate material properties for acoustic guitar modeling.},
langid       = {english},
}
@InProceedings{2008ZoranReacoustic,
author     = {Zoran, Amit and Maes, Pattie},
booktitle  = {ACM SIGGRAPH 2008 posters on - SIGGRAPH '08},
date       = {2008},
title      = {The reAcoustic eGuitar},
doi        = {10.1145/1400885.1400928},
eventtitle = {ACM SIGGRAPH 2008 posters},
isbn       = {978-1-60558-466-9},
location   = {Los Angeles, California},
pages      = {1},
publisher  = {ACM Press},
url        = {http://portal.acm.org/citation.cfm?doid=1400885.1400928},
urldate    = {2019-04-05},
abstract   = {We present a new approach for designing guitars making use of the digital environment. We aim to preserve the physical connection between a user and the instrument, while offering innovative sound design. The reAcoustic eGuitar is a digitally fabricated instrument to design sounds. A user shapes it by modifying six separate acoustic chambers. We contextualize the need for such an artifact within the evolution of the guitar.},
file       = {:done/2008ZoranReacoustic The ReAcoustic EGuitar.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Article{2003DerveauxTime,
author       = {Derveaux, Grégoire and Chaigne, Antoine and Joly, Patrick and Bécache, Eliane},
date         = {2003-12},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Time-domain simulation of a guitar: Model and method},
doi          = {10.1121/1.1629302},
issn         = {0001-4966},
number       = {6},
pages        = {3368--3383},
urldate      = {2019-04-05},
volume       = {114},
langid       = {english},
shorttitle   = {Time-domain simulation of a guitar},
}
@Article{2012Gorrostieta-hurtadoVibration,
author       = {Gorrostieta-Hurtado, Efren},
date         = {2012-03-23},
journaltitle = {International Journal of the Physical Sciences},
title        = {Vibration analysis in the design and construction of an acoustic guitar},
doi          = {10.5897/IJPS11.1603},
issn         = {1992-1950},
number       = {13},
url          = {http://www.academicjournals.org/IJPS/abstracts/abstracts/abstract2012/23Mar/Gorrostieta-Hurtado///////\\%20et///////\\%20al.htm},
urldate      = {2019-04-05},
volume       = {7},
abstract     = {One of the main problems encountered in developing and building an acoustic guitar is to establish a formal methodology that allows us to observe the impact of some parameters involved in the process. This paper propose a modal analysis in the different stages of construction, and it is performed by -a finite element analysis of the soundboard of the instrument according to the relationship that that have the- frequencies and vibration modes, here, a set of certain parameters in the design are used to improve sound performance.},
file         = {:done/2012Gorrostieta-hurtadoVibration Vibration Analysis in the Design and Construction of an Acoustic Guitar.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2011ChouGestalt–minimalism,
author       = {Chou, Jyh-Rong},
date         = {2011-11},
journaltitle = {International Journal of Industrial Ergonomics},
title        = {A Gestalt–Minimalism-based decision-making model for evaluating product form design},
doi          = {10.1016/j.ergon.2011.07.006},
issn         = {0169-8141},
number       = {6},
pages        = {607--616},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0169814111000886},
urldate      = {2019-04-05},
volume       = {41},
abstract     = {This paper presents a novel methodology for dealing with decision-making problems in product design ﬁelds. The purpose of this study is to evaluate product form design in terms of the perspectives of Gestalt psychology and Minimalist principles. Unlike traditional AHP methods, the proposed decision-making model uses distinct mathematical tools to establish priorities for the criteria and synthesize the evaluation results. A case study was conducted to illustrate the practicability of this proposed model. It has shown a credible result. In addition to product form design, this model can be applied to related design ﬁelds, such as plane design and other visual design.},
langid       = {english},
}
@Article{2002HsiaoNeural,
author       = {Hsiao, Shih-Wen and Huang, H. C.},
date         = {2002-01},
journaltitle = {Design Studies},
title        = {A neural network based approach for product form design},
doi          = {10.1016/S0142-694X(01)00015-1},
issn         = {0142-694X},
number       = {1},
pages        = {67--84},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0142694X01000151},
urldate      = {2019-04-05},
volume       = {23},
abstract     = {A neural network based approach for product design is addressed in this article. Computer modeling, fuzzy set theory and semantic difference method are applied to set up an experiment. The experimental results are analyzed by applied back-propagation neural network, which establish the relationships between product–form parameters and adjective image words. A database for the connections among the design elements, product images and shape generation rules was constructed. A computer-aided system for product–form design was then developed based on this database. With the aid of this design system, a designer can generate 3D models of any product with different images by providing basic design elements and shape generation rules. Simultaneously, a rendered 3D model of the designed product and its images are also presented by this system. Therefore, changing the conﬁguration parameter(s) until the product shape is acceptable can modify the image of a product. In this manner, the designed product can ﬁt more closely to the consumers’ desire. Chair design is taken as a case study; but this method can be used to develop other products. kc 2001 Elsevier Science Ltd. All rights reserved.},
file         = {:done/2002HsiaoNeural A Neural Network Based Approach for Product Form Design.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2012ZoranPlatform,
author       = {Zoran, Amit and Welch, Stephen and Hunt, William D.},
date         = {2012-04},
journaltitle = {Applied Acoustics},
title        = {A platform for manipulation and examination of the acoustic guitar: The Chameleon Guitar},
doi          = {10.1016/j.apacoust.2011.10.004},
issn         = {0003-682X},
number       = {4},
pages        = {338--347},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0003682X11002738},
urldate      = {2019-04-05},
volume       = {73},
abstract     = {A platform for manipulation and examination the acoustic guitar is presented, based on a novel guitar design – the Chameleon Guitar – featuring a replaceable acoustic resonator functioning as the soundboard of the instrument. The goal of the design process is to create a tone as sonically close to that of a traditional guitar as possible, while maintaining an easily replaceable soundboard. An iterative, data driven approach was used, each design step coming under examination from one or more measurement tools: ﬁnite-element method, acoustic impulse testing, and laser vibrometry. Ideal resonator geometry, bridge location, and piezoelectric sensor positions were determined. The ﬁnished instrument was then examined with laser vibrometry to conﬁrm earlier results, evaluate the behavior and chosen sensor positions for various tonewoods, and examine the acoustic effects of adding sensors and wax ﬁnish. The conclusions drawn are diverse and point to the signiﬁcance of attention to detail in each step of instrument construction. For example, when changing instrument material from one softwood to another, ideal locations for piezoelectric sensors are subject to change. We conclude that detailed acoustic analysis can signiﬁcantly aid in the construction of new instruments by quantifying the impact of instrument geometry and material properties.},
langid       = {english},
shorttitle   = {A platform for manipulation and examination of the acoustic guitar},
}
@Article{odonnell_study_2012,
author   = {O’Donnell, J and McRobbie, G},
date     = {2012},
title    = {A Study into the Acoustic and Vibrational Effects of Carbon Fiber Reinforced Plastic as a Sole Manufacturing Material for Acoustic Guitars},
pages    = {4},
abstract = {This study will research a modern design of acoustic guitar by analysis of the vibrational modes. The guitar that will undergo testing has been provided by Emerald Guitars and is solely constructed using Carbon Fiber Reinforced Plastic (CFRP). With the use of COMSOL Multiphysics© the soundboard of the guitar will be simulated and analysis will be carried out to determine the first 10 eigenfrequencies and the modal shapes these create. This paper will detail the preliminary results obtained using the physical data collected through experimental testing in a previous study. The paper will demonstrate an application of the finite element method in the field of musical acoustics.},
langid   = {english},
}
@Article{2015CaucaAcoustic,
author       = {del Cauca, Universidad and Idrobo-Ávila, Ennio Hugo and Vargas-Cañas, Rubiel and del Cauca, Universidad},
date         = {2015-12},
journaltitle = {Revista Facultad de Ingeniería Universidad de Antioquia},
title        = {Acoustic and mechanic characterization of materials used in manufacturing the soundboard of the spanish guitar: influence in the sonority},
doi          = {10.17533/udea.redin.n76a04},
issn         = {0120-6230},
number       = {76},
url          = {http://aprendeenlinea.udea.edu.co/revistas/index.php/ingenieria/article/view/21021},
urldate      = {2019-04-05},
langid       = {english},
shorttitle   = {Acoustic and mechanic characterization of materials used in manufacturing the soundboard of the spanish guitar},
}
@Misc{2013GualandriAnalysis,
author = {Gualandri, Daniel},
date   = {2013},
title  = {Analysis of an Acoustic Guitar},
url    = {https://courses.physics.illinois.edu/phys406/sp2017/Student///////\\_Projects/Spring13/Dan///////\\_Gualandri///////\\_P406///////\\_Final///////\\_Project///////\\_Report///////\\_Sp13.pdf},
file   = {:done/2013GualandriAnalysis Analysis of an Acoustic Guitar.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
pages  = {12},
}
@Article{2005HsiaoApplying,
author       = {Hsiao, Shih-Wen and Tsai, Hung-Cheng},
date         = {2005-05},
journaltitle = {International Journal of Industrial Ergonomics},
title        = {Applying a hybrid approach based on fuzzy neural network and genetic algorithm to product form design},
doi          = {10.1016/j.ergon.2004.10.007},
issn         = {0169-8141},
number       = {5},
pages        = {411--428},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0169814104002070},
urldate      = {2019-04-05},
volume       = {35},
abstract     = {When generating new design concepts, most industrial designers tend to draw upon stereotypical images and their own personal design experiences. The evaluation of each individual design candidate in terms of its ability to meet the demands of the marketplace is a crucial step within the conceptual design stage. Consequently, this paper proposes a method which enables an automatic product form search or product image evaluation by means of fuzzy neural network and genetic algorithm. Initially, a feature-based hierarchical computer-aided design (CAD) model is constructed, in which the related form parameters are thoroughly deﬁned in applicable domains to facilitate the automatic generation of new product forms. A fuzzy neural network algorithm is then applied to establish the relationships between the input form parameters and a series of adjectival image words. In a reverse process, genetic algorithm is employed to search for a near-optimal design which satisﬁes the designer’s required product image by using the trained neural network as a ﬁtness function. The proposed method provides an automatic design system, which gives designers the ability to rapidly obtain a product form and its corresponding image, or to search for the ideal form which ﬁts a required image in a shorter lead-time. An electronic door lock design is chosen as the subject of the current investigation. However, the proposed method is equally applicable to the design of other products.},
langid       = {english},
}
@Article{2008HsiaoApplying,
author       = {Hsiao, Shih-Wen and Chiu, Fu-Yuan and Chen, Chong Shian},
date         = {2008-11},
journaltitle = {International Journal of Industrial Ergonomics},
title        = {Applying aesthetics measurement to product design},
doi          = {10.1016/j.ergon.2008.02.009},
issn         = {0169-8141},
number       = {11},
pages        = {910--920},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S016981410800036X},
urldate      = {2019-04-05},
volume       = {38},
abstract     = {In the highly competitive market, varying product color to change its image is one of the best solutions to improve the product competitiveness. In this paper, the relationships among the product image, color area, and aesthetic measurement of the product are studied. The pixels of an area of color are used to obtain the proportionate relationship between different colored areas in a given solid visual angle. Based on the relationship among the Hue, Value, Chroma and colored area proposed by Munsell, the other factors are integrated to set up one formula for evaluating the aesthetic degree of color matching. The aesthetics measurement is considered to be inﬂuenced by the color environments, color areas, component colors and display angles of the product. The color planning for developing a cell phone was performed based on this model. The experimental results veriﬁed this model can be used for color planning in product design.},
file         = {:done/2008HsiaoApplying Applying Aesthetics Measurement to Product Design.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Book{1999LavilleConstrucao,
author     = {Laville, Christian and Dionne, Jean},
date       = {1999},
title      = {A construção do saber: manual de metodologia da pesquisa em ciências humanas},
isbn       = {978-85-7307-489-5},
location   = {Belo Horizonte; Porto Alegre (RS)},
note       = {OCLC: 817592435},
publisher  = {Ed. da UFMG : ARTMED},
file       = {:done/1999LavilleConstrucao A Construção Do Saber/\\_ Manual De Metodologia Da Pesquisa Em Ciências Humanas.pdf:application/pdf},
groups     = {tesse:5},
langid     = {portuguese},
shorttitle = {A construção do saber},
}
@Article{2006AssisMetodologia,
author = {Assis, Maria Cristina De},
date   = {2006},
title  = {METODOLOGIA DO TRABALHO CIENTÍFICO},
pages  = {48},
file   = {:done/2006AssisMetodologia METODOLOGIA DO TRABALHO CIENTÍFICO.pdf:application/pdf},
groups = {tesse:2},
langid = {portuguese},
}
@Book{2012AbreuArte,
author = {Abreu, Antônio Suárez},
date   = {2012},
title  = {A Arte de Argumentar},
isbn   = {978-8585851811},
file   = {:done/2012AbreuArte A Arte De Argumentar.epub:ePUB},
groups = {tesse:5},
langid = {portuguese},
}
@Book{2009GilComo,
author    = {Gil, Antonio Carlos},
date      = {2009},
title     = {Como elaborar projetos de pesquisa},
isbn      = {978-85-224-3169-4},
location  = {São Paulo},
note      = {OCLC: 422878585},
publisher = {Atlas},
file      = {:done/2009GilComo Como Elaborar Projetos De Pesquisa.pdf:application/pdf},
groups    = {tesse:5},
langid    = {portuguese},
}
@Article{ZanonO,
author = {Zanon, Fábio},
title  = {O violão no Brasil depois de Villa-Lobos},
pages  = {8},
file   = {:done/ZanonO O Violão No Brasil Depois De Villa Lobos.pdf:application/pdf},
groups = {tesse:2},
langid = {portuguese},
}
@Article{2002HeijinkComplexity,
author       = {Heijink, Hank and Meulenbroek, Ruud G. J.},
date         = {2002-12},
journaltitle = {Journal of Motor Behavior},
title        = {On the Complexity of Classical Guitar Playing: Functional Adaptations to Task Constraints},
doi          = {10.1080/00222890209601952},
issn         = {0022-2895, 1940-1027},
number       = {4},
pages        = {339--351},
urldate      = {2019-04-05},
volume       = {34},
abstract     = {The authors performed a behavioral study of the complexity of left-hand finger movements in classical guitar playing. Six professional guitarists played movement sequences in a fixed tempo. Left-hand finger movements were recorded in 3 dimensions, and the guitar sound was recorded synchronously. Assuming that performers prefer to avoid extreme joint angles when moving, the authors hypothesized 3 complexity factors. The results showed differential effects of the complexity factors on the performance measures and on participants’ judgments of complexity. The results demonstrated that keeping the joints in the middle of their range is an important principle in guitar playing, and players exploit the available tolerance in timing and placement of the left-hand fingers to control the acoustic output variability.},
file         = {:done/2002HeijinkComplexity On the Complexity of Classical Guitar Playing/\\_ Functional Adaptations to Task Constraints.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {On the Complexity of Classical Guitar Playing},
}
@Article{balleste_organizacao_2009,
author   = {Ballesté, Adriana Olinto},
date     = {2009},
title    = {ORGANIZAÇÃO DO CONHECIMENTO E SUA APLICAÇÃO AO UNIVERSO DE CORDAS DEDILHADAS NO RIO DE JANEIRO OITOCENTISTA},
pages    = {11},
abstract = {On the musicology, in particular in the organology, terminological problems often occur. A typical example can be observed in the plunked musical instruments universe in Brazil. This paper demonstrates the importance of the application of knowledge organization methodologies in the musical domain, especially in the plunked musical instruments domain.},
langid   = {portuguese},
}
@Article{2011Zoran3d,
author       = {Zoran, Amit},
date         = {2011-12},
journaltitle = {Journal of New Music Research},
title        = {The 3D Printed Flute: Digital Fabrication and Design of Musical Instruments},
doi          = {10.1080/09298215.2011.621541},
issn         = {0929-8215, 1744-5027},
number       = {4},
pages        = {379--387},
urldate      = {2019-04-05},
volume       = {40},
abstract     = {This paper considers the controversy of modern acoustic instruments, which may have come to an evolutionary impasse, due to its high standardization that makes it diﬃcult to explore design modiﬁcations. A new approach for the design and fabrication of an acoustic instrument is presented, using digital fabrication technologies, and speciﬁcally 3D printing, which has the potential to inﬂuence new designs, and to lead to new acoustics and ergonomic innovations. This paper describes the key concepts of this approach, presenting the development process of such a 3D printed instrument—a prototype of a 3D printed concert ﬂute, some other 3D printed elements, and a conceptual example of an innovative trumpet—discussing the potential of the new technology in fabricating and designing of musical instruments.},
file         = {:done/2011Zoran3d The 3D Printed Flute/\\_ Digital Fabrication and Design of Musical Instruments.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {The 3D Printed Flute},
}
@Article{2012RichardsonThree,
author = {Richardson, Bernard and Johnson, Helen and Joslin, Alexandra and Perry, Ian},
date   = {2012},
title  = {The three-mass model for the classical guitar revisited},
pages  = {7},
file   = {:done/2012RichardsonThree The Three Mass Model for the Classical Guitar Revisited.pdf:application/pdf},
groups = {tesse:2, Acoustics},
langid = {english},
}
@Article{2013HeemskerkEye,
author       = {Heemskerk, Jacques},
date         = {2013-11},
journaltitle = {Europhysics News},
title        = {An eye-witness report on how the CD came about},
doi          = {10.1051/epn/2013601},
issn         = {0531-7479, 1432-1092},
number       = {6},
pages        = {21--24},
urldate      = {2019-04-05},
volume       = {44},
abstract     = {The economics of French instrumentmaking is mostly made up of very small handicraft enterprises. But craftsmen do not always have the means to embark, by themselves, on an innovation strategy. The idea is to try to respond to current challenges in instrument making; for example, the reduction in costs of design times or the adaptation to customer needs. All of this necessitates the development of low cost tools for characterising and prototyping of instruments dedicated to their use in workshops. Examples of collaborative approach between instrument makers and research laboratories are presented. Mainly, the PAFI project, for "Instrument-making Aid Platform" aims to develop characterisation tools for all the instrument families. The initiative’s originality lies in the fact that "pilot craftsmen" are associated with every stage of development. The process involves a research program and the support of craftsmen for developing the hardware and software. Bearing in mind the international economic context, these experiences may act as a basis for broadening and pursuing this initiative on an international scale. The open and progressive nature of the work means that we can consider such a prospect with a view to maintaining the smallscale production of high-quality instruments.},
file         = {:done/2013HeemskerkEye An Eye Witness Report on How the CD Came about.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2002ElejabarrietaCoupled,
author       = {Elejabarrieta, M. J. and Ezcurra, A. and Santamarı́a, C.},
date         = {2002},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {Coupled modes of the resonance box of the guitar},
doi          = {10.1121/1.1470163},
issn         = {0001-4966},
number       = {5},
pages        = {2283},
urldate      = {2019-04-05},
volume       = {111},
file         = {:done/2002ElejabarrietaCoupled Coupled Modes of the Resonance Box of the Guitar.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{probert_design_nodate,
author = {Probert, Stephen M},
title  = {Design, Manufacture and Analysis of a Carbon Fiber Epoxy Composite Acoustic Guitar},
pages  = {193},
langid = {english},
}
@Article{GreenMechanical,
author = {Green, David W. and Winandy, Jerrold E. and Kretschmann, David E.},
title  = {Mechanical Properties of Wood},
pages  = {46},
file   = {:done/GreenMechanical Mechanical Properties of Wood.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Article{2012ZillDifferential,
author = {Zill, Dennis G.},
date   = {2012},
title  = {Differential Equations with Boundary-Value Problems},
pages  = {673},
file   = {:done/2012ZillDifferential Differential Equations with Boundary Value Problems.pdf:application/pdf},
groups = {tesse:2},
langid = {english},
}
@Book{2005SerwayModern,
author    = {Serway, Raymond A. and Moses, Clement J. and Moyer, Curt A.},
date      = {2005},
title     = {Modern physics},
edition   = {3rd ed},
isbn      = {978-0-534-49339-4 978-0-534-40624-0},
location  = {Belmont, CA},
pagetotal = {1},
publisher = {Thomson Brooks/Cole},
file      = {:done/2005SerwayModern Modern Physics.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Textbooks, Physics},
langid    = {english},
}
@Book{2018ShabanaComputational,
author    = {Shabana, Ahmed A.},
date      = {2018},
title     = {Computational continuum mechanics},
edition   = {Third edition},
isbn      = {978-1-119-29323-1},
location  = {Hoboken, NJ, USA},
pagetotal = {1},
publisher = {Wiley},
file      = {:done/2018ShabanaComputational Computational Continuum Mechanics.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Continuum mechanics, Engineering mathematics},
langid    = {english},
}
@Book{1981GurtinIntroduction,
author    = {Gurtin, Morton E.},
date      = {1981-12-12},
title     = {An Introduction to Continuum Mechanics},
edition   = {1 edition},
isbn      = {978-0-12-309750-7},
location  = {New York},
pagetotal = {265},
publisher = {Academic Press},
url       = {https://www.ebook.de/de/product/3659321/morton///////\\_e///////\\_gurtin///////\\_an///////\\_introduction///////\\_to///////\\_continuum///////\\_mechanics.html},
abstract  = {This book presents an introduction to the classical theories of continuum mechanics; in particular, to the theories of ideal, compressible, and viscous fluids, and to the linear and nonlinear theories of elasticity. These theories are important, not only because they are applicable to a majority of the problems in continuum mechanics arising in practice, but because they form a solid base upon which one can readily construct more complex theories of material behavior. Further, although attention is limited to the classical theories, the treatment is modern with a major emphasis on foundations and structure},
ean       = {9780123097507},
file      = {:done/1981GurtinIntroduction An Introduction to Continuum Mechanics.djvu:Djvu},
groups    = {tesse:5},
year      = {1981},
}
@Book{2016HastieElements,
author     = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
date       = {2016},
title      = {The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Second Edition},
edition    = {2nd edition},
isbn       = {978-0-387-84857-0},
location   = {New York, NY},
pagetotal  = {745},
publisher  = {Springer},
abstract   = {This book describes the important ideas in a variety of fields such as medicine, biology, finance, and marketing in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of colour graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression ///////\\& path algorithms for the lasso, non-negative matrix factorisation, and spectral clustering. There is also a chapter on methods for "wide'' data (p bigger than n), including multiple testing and false discovery rates.},
shorttitle = {The Elements of Statistical Learning},
}
@Article{2016BasTraining,
author       = {Bas, Eren},
date         = {2016-01-01},
journaltitle = {Journal of Artificial Intelligence and Soft Computing Research},
title        = {The Training Of Multiplicative Neuron Model Based Artificial Neural Networks With Differential Evolution Algorithm For Forecasting},
doi          = {10.1515/jaiscr-2016-0001},
issn         = {2083-2567},
number       = {1},
pages        = {5--11},
url          = {http://content.sciendo.com/view/journals/jaiscr/6/1/article-p5.xml},
urldate      = {2019-04-05},
volume       = {6},
abstract     = {In recent years, artiﬁcial neural networks have been commonly used for time series forecasting by researchers from various ﬁelds. There are some types of artiﬁcial neural networks and feed forward artiﬁcial neural networks model is one of them. Although feed forward artiﬁcial neural networks gives successful forecasting results they have a basic problem. This problem is architecture selection problem. In order to eliminate this problem, Yadav et al. (2007) proposed multiplicative neuron model artiﬁcial neural network. In this study, differential evolution algorithm is proposed for the training of multiplicative neuron model for forecasting. The proposed method is applied to two well-known different real world time series data.},
langid       = {english},
}
@Article{2016TuMapping,
author       = {Tu, Enmei and Kasabov, Nikola and Yang, Jie},
date         = {2016-03-17},
journaltitle = {arXiv:1603.05594 [cs, stat]},
title        = {Mapping Temporal Variables into the NeuCube for Improved Pattern Recognition, Predictive Modelling and Understanding of Stream Data},
eprint       = {1603.05594},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1603.05594},
urldate      = {2019-04-05},
abstract     = {This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The ﬁrst one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of trafﬁc in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.},
langid       = {english},
}
@InProceedings{2015LiFacial,
author     = {Li, Junnan and Lam, Edmund Y.},
booktitle  = {2015 IEEE International Conference on Imaging Systems and Techniques (IST)},
date       = {2015-09},
title      = {Facial expression recognition using deep neural networks},
doi        = {10.1109/IST.2015.7294547},
eventtitle = {2015 IEEE International Conference on Imaging Systems and Techniques (IST)},
isbn       = {978-1-4799-8633-0},
location   = {Macau, China},
pages      = {1--6},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/7294547/},
urldate    = {2019-04-05},
abstract   = {We develop a technique using deep neural network for human facial expression recognition. Images of human faces are preprocessed with photometric normalization and histogram manipulation to remove illumination variance. Facial features are then extracted by convolving each preprocessed image with 40 Gabor filters. Kernel PCA is applied to features before feeding them into the deep neural network that consists of 1 input layer, 2 hidden layers and a softmax classifier. The deep network is trained using greedy layer-wise strategy. We use the Extended CohnKanade Dataset for training and testing. Recognition tests are performed on six basic expressions (i.e. surprise, fear, disgust, anger, happiness, sadness). To test the robustness of the classification system further, and for benchmark comparison, we add a seventh emotion, namely "contempt", for additional recognition tests. We construct confusion matrix to evaluate the performance of the deep network. It is demonstrated that the network generalizes to new images fairly successfully with an average recognition rate of 96.8 /\\% for six emotions and 91.7 /\\% for seven emotions. In comparison with shallower neural networks and SVM methods, the proposed deep network method can provide better recognition performance.},
file       = {:done/2015LiFacial Facial Expression Recognition Using Deep Neural Networks.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Article{2015BalleDensity,
author       = {Ballé, Johannes and Laparra, Valero and Simoncelli, Eero P.},
date         = {2015-11-19},
journaltitle = {arXiv:1511.06281 [cs]},
title        = {Density Modeling of Images using a Generalized Normalization Transformation},
eprint       = {1511.06281},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1511.06281},
urldate      = {2019-04-05},
abstract     = {We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectiﬁed and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a signiﬁcantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efﬁciently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
file         = {:done/2015BalleDensity Density Modeling of Images Using a Generalized Normalization Transformation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017HuangLearning,
author       = {Huang, Ke-Kun and Dai, Dao-Qing and Ren, Chuan-Xian and Lai, Zhao-Rong},
date         = {2017-05},
journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
title        = {Learning Kernel Extended Dictionary for Face Recognition},
doi          = {10.1109/TNNLS.2016.2522431},
issn         = {2162-237X, 2162-2388},
number       = {5},
pages        = {1082--1094},
url          = {http://ieeexplore.ieee.org/document/7407377/},
urldate      = {2019-04-05},
volume       = {28},
abstract     = {A sparse representation classiﬁer (SRC) and a kernel discriminant analysis (KDA) are two successful methods for face recognition. An SRC is good at dealing with occlusion, while a KDA does well in suppressing intraclass variations. In this paper, we propose kernel extended dictionary (KED) for face recognition, which provides an efﬁcient way for combining KDA and SRC. We ﬁrst learn several kernel principal components of occlusion variations as an occlusion model, which can represent the possible occlusion variations efﬁciently. Then, the occlusion model is projected by KDA to get the KED, which can be computed via the same kernel trick as new testing samples. Finally, we use structured SRC for classiﬁcation, which is fast as only a small number of atoms are appended to the basic dictionary, and the feature dimension is low. We also extend KED to multikernel space to fuse different types of features at kernel level. Experiments are done on several large-scale data sets, demonstrating that not only does KED get impressive results for nonoccluded samples, but it also handles the occlusion well without overﬁtting, even with a single gallery sample per subject.},
file         = {:done/2017HuangLearning Learning Kernel Extended Dictionary for Face Recognition.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1999JiangImage,
author       = {Jiang, J.},
date         = {1999-07},
journaltitle = {Signal Processing: Image Communication},
title        = {Image compression with neural networks – A survey},
doi          = {10.1016/S0923-5965(98)00041-1},
issn         = {0923-5965},
number       = {9},
pages        = {737--760},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0923596598000411},
urldate      = {2019-04-05},
volume       = {14},
abstract     = {Apart from the existing technology on image compression represented by series of JPEG, MPEG and H.26x standards, new technology such as neural networks and genetic algorithms are being developed to explore the future of image coding. Successful applications of neural networks to vector quantization have now become well established, and other aspects of neural network involvement in this area are stepping up to play signi"cant roles in assisting with those traditional technologies. This paper presents an extensive survey on the development of neural networks for image compression which covers three categories: direct image compression by neural networks; neural network implementation of existing techniques, and neural network based technology which provide improvement over traditional algorithms. 1999 Elsevier Science B.V. All rights reserved.},
file         = {:done/1999JiangImage Image Compression with Neural Networks – a Survey.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015WangImage,
author       = {Wang, Bo and Gao, Yubin},
date         = {2015-03-01},
journaltitle = {TELKOMNIKA (Telecommunication Computing Electronics and Control)},
title        = {An Image Compression Scheme Based on Fuzzy Neural Network},
doi          = {10.12928/telkomnika.v13i1.1270},
issn         = {2302-9293, 1693-6930},
number       = {1},
pages        = {137},
url          = {http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
urldate      = {2019-04-05},
volume       = {13},
abstract     = {Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, selfadaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
file         = {:done/2015WangImage An Image Compression Scheme Based on Fuzzy Neural Network.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016TodericiFull,
author       = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
date         = {2016-08-17},
journaltitle = {arXiv:1608.05148 [cs]},
title        = {Full Resolution Image Compression with Recurrent Neural Networks},
eprint       = {1608.05148},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1608.05148},
urldate      = {2019-04-05},
abstract     = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3 /\\%–8.8 /\\% AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the ﬁrst neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
file         = {:done/2016TodericiFull Full Resolution Image Compression with Recurrent Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015TodericiVariable,
author       = {Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
date         = {2015-11-19},
journaltitle = {arXiv:1511.06085 [cs]},
title        = {Variable Rate Image Compression with Recurrent Neural Networks},
eprint       = {1511.06085},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1511.06085},
urldate      = {2019-04-05},
abstract     = {A large fraction of Internet trafﬁc is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will signiﬁcantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efﬁcient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32×32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10 /\\% or more.},
file         = {:done/2015TodericiVariable Variable Rate Image Compression with Recurrent Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017SanturkarGenerative,
author       = {Santurkar, Shibani and Budden, David and Shavit, Nir},
date         = {2017-03-04},
journaltitle = {arXiv:1703.01467 [cs]},
title        = {Generative Compression},
eprint       = {1703.01467},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1703.01467},
urldate      = {2019-04-05},
abstract     = {Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and suggest that it is a direction worth pursuing to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length coding schemes.},
file         = {:done/2017SanturkarGenerative Generative Compression.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017JohnstonImproved,
author       = {Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
date         = {2017-03-29},
journaltitle = {arXiv:1703.10114 [cs]},
title        = {Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks},
eprint       = {1703.10114},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1703.10114},
urldate      = {2019-04-05},
abstract     = {We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network’s hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efﬁciently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
langid       = {english},
}
@Article{2017TheisLossy,
author       = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Huszár, Ferenc},
date         = {2017-03-01},
journaltitle = {arXiv:1703.00395 [cs, stat]},
title        = {Lossy Image Compression with Compressive Autoencoders},
eprint       = {1703.00395},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1703.00395},
urldate      = {2019-04-05},
abstract     = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more ﬂexible than existing codecs. Autoencoders have the potential to address this need, but are difﬁcult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufﬁcient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efﬁcient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
file         = {:done/2017TheisLossy Lossy Image Compression with Compressive Autoencoders.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1994FritzkeGrowing,
author       = {Fritzke, Bernd},
date         = {1994-01},
journaltitle = {Neural Networks},
title        = {Growing cell structures—A self-organizing network for unsupervised and supervised learning},
doi          = {10.1016/0893-6080(94)90091-4},
issn         = {0893-6080},
number       = {9},
pages        = {1441--1460},
url          = {https://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
urldate      = {2019-04-05},
volume       = {7},
langid       = {english},
}
@InProceedings{2012WuStorage,
author     = {Wu, Yue and Hu, Jianqing and Wu, Wei and Zhou, Yong and Du, K. L.},
booktitle  = {2012 Fifth International Conference on Intelligent Computation Technology and Automation},
date       = {2012-01},
title      = {Storage Capacity of the Hopfield Network Associative Memory},
doi        = {10.1109/ICICTA.2012.89},
eventtitle = {2012 Fifth International Conference on Intelligent Computation Technology and Automation (ICICTA)},
isbn       = {978-1-4673-0470-2 978-0-7695-4637-7},
location   = {Zhangjiajie, Hunan, China},
pages      = {330--336},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/6150208/},
urldate    = {2019-04-05},
abstract   = {The Hopfield model is a well-known dynamic associative-memory model. In this paper, we investigate various aspects of the Hopfield model for associative memory. We conduct a systematic simulation investigation of several storage algorithms for Hopfield networks, and conclude that the perceptron learning based storage algorithms can achieve much better storage capacity than the Hebbian learning based algorithms.},
file       = {:done/2012WuStorage Storage Capacity of the Hopfield Network Associative Memory.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Article{2013CluneEvolutionary,
author       = {Clune, J. and Mouret, J.-B. and Lipson, H.},
date         = {2013-01-30},
journaltitle = {Proceedings of the Royal Society B: Biological Sciences},
title        = {The evolutionary origins of modularity},
doi          = {10.1098/rspb.2012.2863},
issn         = {0962-8452, 1471-2954},
number       = {1755},
pages        = {20122863--20122863},
urldate      = {2019-04-05},
volume       = {280},
file         = {:done/2013CluneEvolutionary The Evolutionary Origins of Modularity.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014KimConvolutional,
author       = {Kim, Yoon},
date         = {2014-08-25},
journaltitle = {arXiv:1408.5882 [cs]},
title        = {Convolutional Neural Networks for Sentence Classification},
eprint       = {1408.5882},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1408.5882},
urldate      = {2019-04-05},
abstract     = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classiﬁcation tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-speciﬁc vectors through ﬁne-tuning offers further gains in performance. We additionally propose a simple modiﬁcation to the architecture to allow for the use of both task-speciﬁc and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classiﬁcation.},
file         = {:done/2014KimConvolutional Convolutional Neural Networks for Sentence Classification.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015TangJoint,
author       = {Tang, Duyu and Qin, Bing and Wei, Furu and Dong, Li and Liu, Ting and Zhou, Ming},
date         = {2015-11},
journaltitle = {IEEE/ACM Transactions on Audio, Speech, and Language Processing},
title        = {A Joint Segmentation and Classification Framework for Sentence Level Sentiment Classification},
doi          = {10.1109/TASLP.2015.2449071},
issn         = {2329-9290},
number       = {11},
pages        = {1750--1761},
url          = {http://ieeexplore.ieee.org/document/7138591/},
urldate      = {2019-04-05},
volume       = {23},
abstract     = {In this paper, we propose a joint segmentation and classiﬁcation framework for sentence-level sentiment classiﬁcation. It is widely recognized that phrasal information is crucial for sentiment classiﬁcation. However, existing sentiment classiﬁcation algorithms typically split a sentence as a word sequence, which does not effectively handle the inconsistent sentiment polarity between a phrase and the words it contains, such as "not bad," "bad" and "a great deal of," "great". We address this issue by developing a joint framework for sentence-level sentiment classiﬁcation. It simultaneously generates useful segmentations and predicts sentence-level polarity based on the segmentation results. Speciﬁcally, we develop a candidate generation model to produce segmentation candidates of a sentence; a segmentation ranking model to score the usefulness of a segmentation candidate for sentiment classiﬁcation; and a classiﬁcation model for predicting the sentiment polarity of a segmentation. We train the joint framework directly from sentences annotated with only sentiment polarity, without using any syntactic or sentiment annotations in segmentation level. We conduct experiments for sentiment classiﬁcation on two benchmark datasets: a tweet dataset and a review dataset. Experimental results show that: 1) our method performs comparably with state-of-the-art methods on both datasets; 2) joint modeling segmentation and classiﬁcation outperforms pipelined baseline methods in various experimental settings.},
file         = {:done/2015TangJoint A Joint Segmentation and Classification Framework for Sentence Level Sentiment Classification.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017ChenImproving,
author       = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
date         = {2017-04},
journaltitle = {Expert Systems with Applications},
title        = {Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN},
doi          = {10.1016/j.eswa.2016.10.065},
issn         = {0957-4174},
pages        = {221--230},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0957417416305929},
urldate      = {2019-04-05},
volume       = {72},
abstract     = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classiﬁcation research focuses on one-technique-ﬁts-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which ﬁrst classiﬁes sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we ﬁnd that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to ﬁrst apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classiﬁcation. Our approach has been evaluated on four sentiment classiﬁcation datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classiﬁcation can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
langid       = {english},
}
@InProceedings{2016WangSam,
author     = {Wang, Zhenyang and Deng, Zhidong and Wang, Shiyao},
booktitle  = {2016 International Joint Conference on Neural Networks (IJCNN)},
date       = {2016-07},
title      = {SAM: A rethinking of prominent convolutional neural network architectures for visual object recognition},
doi        = {10.1109/IJCNN.2016.7727308},
eventtitle = {2016 International Joint Conference on Neural Networks (IJCNN)},
isbn       = {978-1-5090-0620-5},
location   = {Vancouver, BC, Canada},
pages      = {1008--1014},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/7727308/},
urldate    = {2019-04-05},
abstract   = {Convolutional neural networks play an increasingly important role in computer vision tasks, especially in the ﬁeld of visual object recognition. Many prominent models, such as Inception, Maxout, ResNet, and NIN, have been proposed to signiﬁcantly improve recognition performance. Inspired from those models, we propose a novel module called self-adaptive module (SAM). SAM consists of four passes and one selector. Speciﬁcally, the four passes include two direct passes with different receptive ﬁelds and depths, one residual pass, and one Maxout pass. Actually, the residual pass is used to speed up convergence, while we take advantage of the Maxout pass to enhance approximate capabilities of SAM. The selector is further designed to help choose reasonable output. Basically, SAM is intended to simplify design of any new deep learning architecture, since it no longer requires consideration of how to select receptive ﬁelds and depths. Our SAM is tested on the visual object recognition datasets including CIFAR-10, CIFAR100, MNIST, and SVHN. The experimental results demonstrate that the SAM-Net has superior recognition performances on the four benchmarks, which achieve test errors of 5.76///////\\%, 28.56///////\\%, 0.31///////\\%, and 1.98///////\\%, respectively.},
langid     = {english},
shorttitle = {SAM},
}
@Article{2017AraqueEnhancing,
author       = {Araque, Oscar and Corcuera-Platas, Ignacio and Sánchez-Rada, J. Fernando and Iglesias, Carlos A.},
date         = {2017-07},
journaltitle = {Expert Systems with Applications},
title        = {Enhancing deep learning sentiment analysis with ensemble techniques in social applications},
doi          = {10.1016/j.eswa.2017.02.002},
issn         = {0957-4174},
pages        = {236--246},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0957417417300751},
urldate      = {2019-04-05},
volume       = {77},
langid       = {english},
}
@Article{1982HopfieldNeural,
author       = {Hopfield, J. J.},
date         = {1982-04-01},
journaltitle = {Proceedings of the National Academy of Sciences},
title        = {Neural networks and physical systems with emergent collective computational abilities.},
doi          = {10.1073/pnas.79.8.2554},
issn         = {0027-8424, 1091-6490},
number       = {8},
pages        = {2554--2558},
urldate      = {2019-04-05},
volume       = {79},
abstract     = {Computational properties of use to biological organisms or to the construction of computers can emerge as collective properties of systems -having a large number of simple equivalent components (or neurons). The physical meaning ofcontent-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details ofthe modeling or the failure of individual devices.},
langid       = {english},
}
@InProceedings{2004HuangExtreme,
author     = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong},
booktitle  = {2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
date       = {2004},
title      = {Extreme learning machine: a new learning scheme of feedforward neural networks},
doi        = {10.1109/IJCNN.2004.1380068},
eventtitle = {2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
isbn       = {978-0-7803-8359-3},
location   = {Budapest, Hungary},
pages      = {985--990},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/1380068/},
urldate    = {2019-04-05},
volume     = {2},
abstract   = {It is clear that the learning speed of feedforward neural networks is in general far slower than required and it has been a major bottleneck in their applications for past decades. Two key reasons behind may be: 1) the slow gradientbased learning algorithms are extensively used to train neural networks, and 2) all the parameters of the networks are tuned iteratively by using such learning algorithms. Unlike these traditional implementations, this paper proposes a new learning algorithm called extreme learning machine (ELM) for singlehidden layer feedforward neural networks (SLFNs) which randomly chooses the input weights and analytically determines the output weights of SLFNs. In theory, this algorithm tends to provide the best generalization performance at extremely fast learning speed. The experimental results based on realworld benchmarking function approximation and classiﬁcation problems including large complex applications show that the new algorithm can produce best generalization performance in some cases and can learn much faster than traditional popular learning algorithms for feedforward neural networks.},
langid     = {english},
shorttitle = {Extreme learning machine},
}
@Article{2014GravesNeural,
author       = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
date         = {2014-10-20},
journaltitle = {arXiv:1410.5401 [cs]},
title        = {Neural Turing Machines},
eprint       = {1410.5401},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1410.5401},
urldate      = {2019-04-05},
abstract     = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-toend, allowing it to be efﬁciently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
file         = {:done/2014GravesNeural Neural Turing Machines.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{1992BoserTraining,
author    = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
booktitle = {Proceedings of the fifth annual workshop on Computational learning theory},
date      = {1992},
title     = {A training algorithm for optimal margin classifiers},
pages     = {144--152},
publisher = {ACM},
file      = {:done/1992BoserTraining A Training Algorithm for Optimal Margin Classifiers.pdf:application/pdf},
groups    = {tesse:5},
}
@Report{1985RumelhartLearning,
author      = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
date        = {1985},
institution = {California Univ San Diego La Jolla Inst for Cognitive Science},
title       = {Learning internal representations by error propagation},
type        = {resreport},
file        = {:done/1985RumelhartLearning Learning Internal Representations by Error Propagation.pdf:application/pdf},
groups      = {tesse:5},
year        = {1985},
}
@Article{2017DengDeep,
author       = {Deng, Shuiguang and Huang, Longtao and Xu, Guandong and Wu, Xindong and Wu, Zhaohui},
date         = {2017-05},
journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
title        = {On Deep Learning for Trust-Aware Recommendations in Social Networks},
doi          = {10.1109/TNNLS.2016.2514368},
issn         = {2162-237X, 2162-2388},
number       = {5},
pages        = {1164--1177},
url          = {http://ieeexplore.ieee.org/document/7414528/},
urldate      = {2019-04-05},
volume       = {28},
abstract     = {With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major beneﬁt of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user’s trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users’ interests and their trusted friends’ interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.},
langid       = {english},
}
@Misc{2012HintonLecture,
author = {Hinton, Geoffrey and Srivastava, Nitish and Swersky, Kevin},
date   = {2012},
title  = {Lecture 6a Overview of mini-batch gradient descent. Coursera.},
}
@Article{2015BennasarFeature,
author       = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
date         = {2015-12},
journaltitle = {Expert Systems with Applications},
title        = {Feature selection using Joint Mutual Information Maximisation},
doi          = {10.1016/j.eswa.2015.07.007},
issn         = {0957-4174},
number       = {22},
pages        = {8520--8532},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0957417415004674},
urldate      = {2019-04-05},
volume       = {42},
abstract     = {Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efﬁciency, scalability in terms of the dataset dimensionality, and independence from the classiﬁer. Common drawbacks of this approach are the lack of information about the interaction between the features and the classiﬁer, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature signiﬁcance.},
file         = {:done/2015BennasarFeature Feature Selection Using Joint Mutual Information Maximisation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015LiaoImportance,
author       = {Liao, Zhibin and Carneiro, Gustavo},
date         = {2015-08-03},
journaltitle = {arXiv:1508.00330 [cs]},
title        = {On the Importance of Normalisation Layers in Deep Learning with Piecewise Linear Activation Units},
eprint       = {1508.00330},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1508.00330},
urldate      = {2019-04-05},
abstract     = {Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets (e.g., CIFAR-10, CIFAR100, MNIST, and SVHN). The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classiﬁcation of similar input examples. During the training process, these subnetworks avoid overﬁtting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a signiﬁcant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classiﬁcation results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
langid       = {english},
}
@Article{2015HuangWhat,
author       = {Huang, Guang-Bin},
date         = {2015-06},
journaltitle = {Cognitive Computation},
title        = {What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt’s Dream and John von Neumann’s Puzzle},
doi          = {10.1007/s12559-015-9333-0},
issn         = {1866-9956, 1866-9964},
number       = {3},
pages        = {263--278},
urldate      = {2019-04-05},
volume       = {7},
abstract     = {The emergent machine learning technique—extreme learning machines (ELMs)—has become a hot area of research over the past years, which is attributed to the growing research activities and signiﬁcant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that (1) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes/neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes/ neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc.). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; (2) ELM is proposed for both generalized single-hidden-layer feedforward network and multi-hidden-layer feedforward networks (including biological neural networks); (3) homogeneous architecture-based ELM is proposed for feature learning, clustering, regression and (binary/multi-class) classiﬁcation. (4) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi-hidden-layer feedforward networks either.},
langid       = {english},
shorttitle   = {What are Extreme Learning Machines?},
}
@Article{2016TangExtreme,
author       = {Tang, Jiexiong and Deng, Chenwei and Huang, Guang-Bin},
date         = {2016-04},
journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
title        = {Extreme Learning Machine for Multilayer Perceptron},
doi          = {10.1109/TNNLS.2015.2424995},
issn         = {2162-237X, 2162-2388},
number       = {4},
pages        = {809--821},
url          = {http://ieeexplore.ieee.org/document/7103337/},
urldate      = {2019-04-05},
volume       = {27},
abstract     = {Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parameters are randomly generated and the output weights are analytically computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classiﬁcation and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before ﬁnal decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are ﬁxed without ﬁne-tuning. Therefore, it has much better learning efﬁciency than the DL. Extensive experiments on various widely used classiﬁcation data sets show that the proposed algorithm achieves better and faster convergence than the existing stateof-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further conﬁrm the generality and capability of the proposed learning scheme.},
file         = {:done/2016TangExtreme Extreme Learning Machine for Multilayer Perceptron.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017SongRobust,
author       = {Song, Qing and Zhao, Xu and Fan, Haijin and Wang, Danwei},
date         = {2017-05},
journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
title        = {Robust Recurrent Kernel Online Learning},
doi          = {10.1109/TNNLS.2016.2518223},
issn         = {2162-237X, 2162-2388},
number       = {5},
pages        = {1068--1081},
url          = {http://ieeexplore.ieee.org/document/7407373/},
urldate      = {2019-04-05},
volume       = {28},
abstract     = {We propose a robust recurrent kernel online learning (RRKOL) algorithm based on the celebrated real-time recurrent learning approach that exploits the kernel trick in a recurrent online training manner. The novel RRKOL algorithm guarantees weight convergence with regularized risk management through the use of adaptive recurrent hyperparameters for superior generalization performance. Based on a new concept of the structure update error with a variable parameter length, we are the ﬁrst one to propose the detailed structure update error, such that the weight convergence and robust stability proof can be integrated with a kernel sparsiﬁcation scheme based on a solid theoretical ground. The RRKOL algorithm automatically weighs the regularized term in the recurrent loss function, such that we not only minimize the estimation error but also improve the generalization performance through sparsiﬁcation with simulation support.},
file         = {:done/2017SongRobust Robust Recurrent Kernel Online Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2017OjhaMetaheuristic,
author       = {Ojha, Varun Kumar and Abraham, Ajith and Snášel, Václav},
date         = {2017-04},
journaltitle = {Engineering Applications of Artificial Intelligence},
title        = {Metaheuristic Design of Feedforward Neural Networks: A Review of Two Decades of Research},
doi          = {10.1016/j.engappai.2017.01.013},
eprint       = {1705.05584},
eprinttype   = {arxiv},
issn         = {0952-1976},
pages        = {97--116},
url          = {http://arxiv.org/abs/1705.05584},
urldate      = {2019-04-05},
volume       = {60},
abstract     = {Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such diﬀerent viewpoints mainly to improve the FNN’s generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimize the FNNs. Its success is evident from the FNN’s application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.},
langid       = {english},
shorttitle   = {Metaheuristic Design of Feedforward Neural Networks},
}
@Article{2005PalmesMutation,
author       = {Palmes, P. P. and Hayasaka, T. and Usui, S.},
date         = {2005-05},
journaltitle = {IEEE Transactions on Neural Networks},
title        = {Mutation-Based Genetic Neural Network},
doi          = {10.1109/TNN.2005.844858},
issn         = {1045-9227},
number       = {3},
pages        = {587--600},
url          = {http://ieeexplore.ieee.org/document/1427764/},
urldate      = {2019-04-05},
volume       = {16},
abstract     = {Evolving gradient-learning artiﬁcial neural networks (ANNs) using an evolutionary algorithm (EA) is a popular approach to address the local optima and design problems of ANN. The typical approach is to combine the strength of backpropagation (BP) in weight learning and EA’s capability of searching the architecture space. However, the BP’s "gradient descent" approach requires a highly computer-intensive operation that relatively restricts the search coverage of EA by compelling it to use a small population size. To address this problem, we utilized mutation-based genetic neural network (MGNN) to replace BP by using the mutation strategy of local adaptation of evolutionary programming (EP) to effect weight learning. The MGNN’s mutation enables the network to dynamically evolve its structure and adapt its weights at the same time. Moreover, MGNN’s EP-based encoding scheme allows for a ﬂexible and less restricted formulation of the ﬁtness function and makes ﬁtness computation fast and efﬁcient. This makes it feasible to use larger population sizes and allows MGNN to have a relatively wide search coverage of the architecture space. MGNN implements a stopping criterion where overﬁtness occurrences are monitored through "sliding-windows" to avoid premature learning and overlearning. Statistical analysis of its performance to some well-known classiﬁcation problems demonstrate its good generalization capability. It also reveals that locally adapting or scheduling the strategy parameters embedded in each individual network may provide a proper balance between the local and global searching capabilities of MGNN.},
file         = {:done/2005PalmesMutation Mutation Based Genetic Neural Network.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2008FloreanoNeuroevolution,
author       = {Floreano, Dario and Dürr, Peter and Mattiussi, Claudio},
date         = {2008-03},
journaltitle = {Evolutionary Intelligence},
title        = {Neuroevolution: from architectures to learning},
doi          = {10.1007/s12065-007-0002-4},
issn         = {1864-5909, 1864-5917},
number       = {1},
pages        = {47--62},
urldate      = {2019-04-05},
volume       = {1},
abstract     = {Artiﬁcial neural networks (ANNs) are applied to many real-world problems, ranging from pattern classiﬁcation to robot control. In order to design a neural network for a particular task, the choice of an architecture (including the choice of a neuron model), and the choice of a learning algorithm have to be addressed. Evolutionary search methods can provide an automatic solution to these problems. New insights in both neuroscience and evolutionary biology have led to the development of increasingly powerful neuroevolution techniques over the last decade. This paper gives an overview of the most prominent methods for evolving ANNs with a special focus on recent advances in the synthesis of learning architectures.},
file         = {:done/2008FloreanoNeuroevolution Neuroevolution\\_ from Architectures to Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Neuroevolution},
}
@Article{2015DengExtreme,
author       = {Deng, ChenWei and Huang, GuangBin and Xu, Jia and Tang, JieXiong},
date         = {2015-02},
journaltitle = {Science China Information Sciences},
title        = {Extreme learning machines: new trends and applications},
doi          = {10.1007/s11432-014-5269-3},
issn         = {1674-733X, 1869-1919},
number       = {2},
pages        = {1--16},
urldate      = {2019-04-05},
volume       = {58},
abstract     = {Extreme learning machine (ELM), as a new learning framework, draws increasing attractions in the areas of large-scale computing, high-speed signal processing, artiﬁcial intelligence, and so on. ELM aims to break the barriers between the conventional artiﬁcial learning techniques and biological learning mechanism and represents a suite of machine learning techniques in which hidden neurons need not to be tuned. ELM theories and algorithms argue that "random hidden neurons" capture the essence of some brain learning mechanisms as well as the intuitive sense that the eﬃciency of brain learning need not rely on computing power of neurons. Thus, compared with traditional neural networks and support vector machine, ELM oﬀers signiﬁcant advantages such as fast learning speed, ease of implementation, and minimal human intervention. Due to its remarkable generalization performance and implementation eﬃciency, ELM has been applied in various applications. In this paper, we ﬁrst provide an overview of newly derived ELM theories and approaches. On the other hand, with the ongoing development of multilayer feature representation, some new trends on ELM-based hierarchical learning are discussed. Moreover, we also present several interesting ELM applications to showcase the practical advances on this subject.},
file         = {:done/2015DengExtreme Extreme Learning Machines\\_ New Trends and Applications.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Extreme learning machines},
}
@Article{2015SatoApac,
author       = {Sato, Ikuro and Nishimura, Hiroki and Yokoi, Kensuke},
date         = {2015-05-12},
journaltitle = {arXiv:1505.03229 [cs]},
title        = {APAC: Augmented PAttern Classification with Neural Networks},
eprint       = {1505.03229},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1505.03229},
urldate      = {2019-04-05},
abstract     = {Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classiﬁcation problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classiﬁers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classiﬁcation, which is a way of classiﬁcation using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classiﬁers. Even our multilayer perceptron model beats some of the convolutional models with recentlyinvented stochastic regularization techniques on the CIFAR10 dataset.},
file         = {:done/2015SatoApac APAC\\_ Augmented PAttern Classification with Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {APAC},
}
@Article{2006LudermirOptimization,
author       = {Ludermir, T. B. and Yamazaki, A. and Zanchettin, C.},
date         = {2006-11},
journaltitle = {IEEE Transactions on Neural Networks},
title        = {An Optimization Methodology for Neural Network Weights and Architectures},
doi          = {10.1109/TNN.2006.881047},
issn         = {1045-9227, 1941-0093},
number       = {6},
pages        = {1452--1459},
url          = {http://ieeexplore.ieee.org/document/4012033/},
urldate      = {2019-04-05},
volume       = {17},
abstract     = {This paper introduces a methodology for neural network global optimization. The aim is the simultaneous optimization of multilayer perceptron (MLP) network weights and architectures, in order to generate topologies with few connections and high classiﬁcation performance for any data sets. The approach combines the advantages of simulated annealing, tabu search and the backpropagation training algorithm in order to generate an automatic process for producing networks with high classiﬁcation performance and low complexity. Experimental results obtained with four classiﬁcation problems and one prediction problem has shown to be better than those obtained by the most commonly used optimization techniques.},
file         = {:done/2006LudermirOptimization An Optimization Methodology for Neural Network Weights and Architectures.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2006TsaiTuning,
author       = {Tsai, J.-T. and Chou, J.-H. and Liu, T.-K.},
date         = {2006-01},
journaltitle = {IEEE Transactions on Neural Networks},
title        = {Tuning the Structure and Parameters of a Neural Network by Using Hybrid Taguchi-Genetic Algorithm},
doi          = {10.1109/TNN.2005.860885},
issn         = {1045-9227},
number       = {1},
pages        = {69--80},
url          = {http://ieeexplore.ieee.org/document/1593693/},
urldate      = {2019-04-05},
volume       = {17},
abstract     = {In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.},
langid       = {english},
}
@Article{2015SalamaLearning,
author       = {Salama, Khalid M. and Abdelbar, Ashraf M.},
date         = {2015-12},
journaltitle = {Swarm Intelligence},
title        = {Learning neural network structures with ant colony algorithms},
doi          = {10.1007/s11721-015-0112-z},
issn         = {1935-3812, 1935-3820},
number       = {4},
pages        = {229--265},
urldate      = {2019-04-05},
volume       = {9},
abstract     = {Ant colony optimization (ACO) has been successfully applied to classiﬁcation, where the aim is to build a model that captures the relationships between the input attributes and the target class in a given domain’s dataset. The constructed classiﬁcation model can then be used to predict the unknown class of a new pattern. While artiﬁcial neural networks are one of the most widely used models for pattern classiﬁcation, their application is commonly restricted to fully connected three-layer topologies. In this paper, we present a new algorithm, ANN-Miner, which uses ACO to learn the structure of feed-forward neural networks. We report computational results on 40 benchmark datasets for several variations of the algorithm. Performance is compared to the standard three-layer structure trained with two different weight-learning algorithms (back propagation, and the ACOR algorithm), and also to a greedy algorithm for learning NN structures. A nonparametric Friedman test is used to determine statistical signiﬁcance. In addition, we compare our proposed algorithm with NEAT, a prominent evolutionary algorithm for evolving neural networks, as well as three different well-known state-of-the-art classiﬁers, namely the C4.5 decision tree induction algorithm, the Ripper classiﬁcation rule induction algorithm, and support vector machines.},
file         = {:done/2015SalamaLearning Learning Neural Network Structures with Ant Colony Algorithms.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2007BenardosOptimizing,
author       = {Benardos, P. G. and Vosniakos, G.-C.},
date         = {2007-04},
journaltitle = {Engineering Applications of Artificial Intelligence},
title        = {Optimizing feedforward artificial neural network architecture},
doi          = {10.1016/j.engappai.2006.06.005},
issn         = {0952-1976},
number       = {3},
pages        = {365--382},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0952197606001072},
urldate      = {2019-04-05},
volume       = {20},
file         = {:done/2007BenardosOptimizing Optimizing Feedforward Artificial Neural Network Architecture.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016CortesAdanet,
author       = {Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
date         = {2016-07-04},
journaltitle = {arXiv:1607.01097 [cs]},
title        = {AdaNet: Adaptive Structural Learning of Artificial Neural Networks},
eprint       = {1607.01097},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1607.01097},
urldate      = {2019-04-05},
abstract     = {We present new algorithms for adaptively learning artiﬁcial neural networks. Our algorithms (ADANET) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classiﬁcation tasks extracted from the CIFAR-10 dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.},
langid       = {english},
shorttitle   = {AdaNet},
}
@Article{RoebelNeural,
author   = {Röbel, Alex},
title    = {Neural Network Modeling of Speech and Music Signals},
pages    = {7},
abstract = {Time series prediction is one of the major applications of neural networks. After a short introduction into the basic theoretical foundations we argue that the iterated prediction of a dynamical system may be interpreted as a model of the system dynamics. By means of RBF neural networks we describe a modeling approach and extend it to be able to model instationary systems. As a practical test for the capabilities of the method we investigate the modeling of musical and speech signals and demonstrate that the model may be used for synthesis of musical and speech signals.},
file     = {:done/RoebelNeural Neural Network Modeling of Speech and Music Signals.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2013RaczynskiDynamic,
author       = {Raczynski, Stanislaw A. and Vincent, Emmanuel and Sagayama, Shigeki},
date         = {2013-09},
journaltitle = {IEEE Transactions on Audio, Speech, and Language Processing},
title        = {Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling},
doi          = {10.1109/TASL.2013.2258012},
issn         = {1558-7916, 1558-7924},
number       = {9},
pages        = {1830--1840},
url          = {https://ieeexplore.ieee.org/document/6502207/},
urldate      = {2019-04-05},
volume       = {21},
abstract     = {Symbolic pitch modelling is a way of incorporating knowledge about relations between pitches into the process of analysing musical information or signals. In this paper, we propose a family of probabilistic symbolic polyphonic pitch models, which account for both the "horizontal" and the "vertical" pitch structure. These models are formulated as linear or loglinear interpolations of up to ﬁve sub-models, each of which is responsible for modelling a different type of relation.},
file         = {:done/2013RaczynskiDynamic Dynamic Bayesian Networks for Symbolic Polyphonic Pitch Modeling.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016MehriSamplernn,
author       = {Mehri, Soroush and Kumar, Kundan and Gulrajani, Ishaan and Kumar, Rithesh and Jain, Shubham and Sotelo, Jose and Courville, Aaron and Bengio, Yoshua},
date         = {2016-12-22},
journaltitle = {arXiv:1612.07837 [cs]},
title        = {SampleRNN: An Unconditional End-to-End Neural Audio Generation Model},
eprint       = {1612.07837},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1612.07837},
urldate      = {2019-04-05},
abstract     = {In this paper we propose a novel model for unconditional audio generation based on generating one audio sample at a time. We show that our model, which proﬁts from combining memory-less modules, namely autoregressive multilayer perceptrons, and stateful recurrent neural networks in a hierarchical structure is able to capture underlying sources of variations in the temporal sequences over very long time spans, on three datasets of different nature. Human evaluation on the generated samples indicate that our model is preferred over competing models. We also show how each component of the model contributes to the exhibited performance.},
file         = {:done/2016MehriSamplernn SampleRNN\\_ an Unconditional End to End Neural Audio Generation Model.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {SampleRNN},
}
@Article{2017WangTacotron,
author       = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
date         = {2017-03-29},
journaltitle = {arXiv:1703.10135 [cs]},
title        = {Tacotron: Towards End-to-End Speech Synthesis},
eprint       = {1703.10135},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1703.10135},
urldate      = {2019-04-05},
abstract     = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given <text, audio> pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-tosequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it’s substantially faster than sample-level autoregressive methods.},
file         = {:done/2017WangTacotron Tacotron\\_ Towards End to End Speech Synthesis.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Tacotron},
}
@Book{2010GrahameWind,
author    = {Grahame, Kenneth and Hunt, Peter},
date      = {2010},
title     = {The wind in the willows},
edition   = {New ed},
isbn      = {978-0-19-956756-0},
location  = {Oxford ; New York},
note      = {OCLC: ocn619552325},
pagetotal = {170},
publisher = {Oxford University Press},
file      = {:done/2010GrahameWind The Wind in the Willows.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Country life, England, Fiction, Friendship, Pastoral fiction, River life, Toad, of Toad Hall},
langid    = {english},
}
@Collection{2012AtkinsonManagement,
date       = {2012},
editor     = {Atkinson, Anthony A.},
title      = {Management accounting: information for decision-making and strategy execution},
edition    = {6th ed},
isbn       = {978-0-13-702497-1},
location   = {Upper Saddle River, N.J},
note       = {OCLC: ocn639166510},
pagetotal  = {526},
publisher  = {Pearson},
keywords   = {Managerial accounting},
langid     = {english},
shorttitle = {Management accounting},
}
@Book{2005BrownStrategic,
author    = {Brown, Steve},
date      = {2005},
title     = {Strategic operations management},
edition   = {2. ed},
editor    = {Brown, Steve},
isbn      = {978-0-7506-6319-9},
location  = {Amsterdam},
note      = {OCLC: 249641155},
pagetotal = {420},
publisher = {Elsevier, Butterworth-Heinemann},
file      = {:done/2005BrownStrategic Strategic Operations Management.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
year      = {2005},
}
@Article{2014LongFully,
author       = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
date         = {2014-11-14},
journaltitle = {arXiv:1411.4038 [cs]},
title        = {Fully Convolutional Networks for Semantic Segmentation},
eprint       = {1411.4038},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1411.4038},
urldate      = {2019-04-05},
abstract     = {Convolutional networks are powerful visual models that yield hierarchies of features. We show that convolutional networks by themselves, trained end-to-end, pixelsto-pixels, exceed the state-of-the-art in semantic segmentation. Our key insight is to build "fully convolutional" networks that take input of arbitrary size and produce correspondingly-sized output with efﬁcient inference and learning. We deﬁne and detail the space of fully convolutional networks, explain their application to spatially dense prediction tasks, and draw connections to prior models. We adapt contemporary classiﬁcation networks (AlexNet [19], the VGG net [31], and GoogLeNet [32]) into fully convolutional networks and transfer their learned representations by ﬁne-tuning [4] to the segmentation task. We then deﬁne a novel architecture that combines semantic information from a deep, coarse layer with appearance information from a shallow, ﬁne layer to produce accurate and detailed segmentations. Our fully convolutional network achieves state-of-the-art segmentation of PASCAL VOC (20 /\\% relative improvement to 62.2 /\\% mean IU on 2012), NYUDv2, and SIFT Flow, while inference takes less than one ﬁfth of a second for a typical image.},
file         = {:done/2014LongFully Fully Convolutional Networks for Semantic Segmentation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015RadfordUnsupervised,
author       = {Radford, Alec and Metz, Luke and Chintala, Soumith},
date         = {2015-11-19},
journaltitle = {arXiv:1511.06434 [cs]},
title        = {Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks},
eprint       = {1511.06434},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1511.06434},
urldate      = {2019-04-05},
abstract     = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
langid       = {english},
}
@Article{2017ZhuUnpaired,
author       = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
date         = {2017-03-30},
journaltitle = {arXiv:1703.10593 [cs]},
title        = {Unpaired Image-to-Image Translation using Cycle-Consistent Adversarial Networks},
eprint       = {1703.10593},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1703.10593},
urldate      = {2019-04-05},
abstract     = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to enforce F (G(X)) ≈ X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transﬁguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
file         = {:done/2017ZhuUnpaired Unpaired Image to Image Translation Using Cycle Consistent Adversarial Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2019EngelGansynth,
author       = {Engel, Jesse and Agrawal, Kumar Krishna and Chen, Shuo and Gulrajani, Ishaan and Donahue, Chris and Roberts, Adam},
date         = {2019-02-22},
journaltitle = {arXiv:1902.08710 [cs, eess, stat]},
title        = {GANSynth: Adversarial Neural Audio Synthesis},
eprint       = {1902.08710},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1902.08710},
urldate      = {2019-04-05},
abstract     = {Efficient audio synthesis is an inherently difficult machine learning task, as human perception is sensitive to both global structure and fine-scale waveform coherence. Autoregressive models, such as WaveNet, model local structure at the expense of global latent structure and slow iterative sampling, while Generative Adversarial Networks (GANs), have global latent conditioning and efficient parallel sampling, but struggle to generate locally-coherent audio waveforms. Herein, we demonstrate that GANs can in fact generate high-fidelity and locally-coherent audio by modeling log magnitudes and instantaneous frequencies with sufficient frequency resolution in the spectral domain. Through extensive empirical investigations on the NSynth dataset, we demonstrate that GANs are able to outperform strong WaveNet baselines on automated and human evaluation metrics, and efficiently generate audio several orders of magnitude faster than their autoregressive counterparts.},
langid       = {english},
shorttitle   = {GANSynth},
}
@Book{2018SpizhevoiOpencv,
author     = {Spizhevoi, Aleksei and Rybnikov, Aleksandr},
date       = {2018},
title      = {OpenCV 3 Computer Vision with Python Cookbook: Leverage the power of OpenCV 3 and Python to build computer vision applications.},
isbn       = {978-1-78847-875-5 978-1-78847-444-3},
location   = {Birmingham},
note       = {OCLC: 1030817073},
publisher  = {Packt Publishing},
url        = {http://public.eblib.com/choice/publicfullrecord.aspx?p=5332144},
urldate    = {2019-04-05},
abstract   = {OpenCV 3 is a native cross-platform library for computer vision, machine learning, and image processing. OpenCV's convenient high-level APIs hide very powerful internals designed for computational efficiency that can take advantage of multicore and GPU processing. This book will help you tackle increasingly challenging computer vision problems ...},
langid     = {english},
shorttitle = {OpenCV 3 Computer Vision with Python Cookbook},
}
@Book{1997LockeConduct,
author    = {Locke, John},
date      = {1997-06-01},
title     = {Of the Conduct of the Understanding},
edition   = {Facsimile edition},
isbn      = {978-1-85506-225-2},
location  = {Bristol},
pagetotal = {160},
publisher = {Thoemmes Pr},
abstract  = {This letter shows the importance Locke attached to the Conduct, and its intended place within the body of the Essay. Unfortunately he never completed the work, which was finally published only in the Posthumous Works of 1706. The big topic which Locke was only just beginning to open up was that of the 'Ethics of Belief'. Every man, he tells us, should regulate his ascent by the evidence alone - a maxim easier to state than to understand, and easier to understand that to put into practice.},
}
@Book{2010LuptonThinking,
author     = {Lupton, Ellen},
date       = {2010},
title      = {Thinking with type: a critical guide for designers, writers, editors, ////////\\\& students},
edition    = {2nd rev. and expanded ed},
isbn       = {978-1-56898-969-3},
location   = {New York},
note       = {OCLC: ocn528665832},
pagetotal  = {224},
publisher  = {Princeton Architectural Press},
keywords   = {Graphic design (Typography), Type and type-founding},
langid     = {english},
shorttitle = {Thinking with type},
}
@Book{2006DevoreModern,
author     = {Devore, Jay},
date       = {2006-03},
title      = {A Modern Introduction to Probability and Statistics: Understanding Why and How},
doi        = {10.1198/jasa.2006.s72},
urldate    = {2019-04-06},
volume     = {101},
langid     = {english},
shorttitle = {A Modern Introduction to Probability and Statistics},
}
@Book{2006LawsonHow,
author     = {Lawson, Bryan},
date       = {2006},
title      = {How designers think: the design process demystified},
edition    = {4. ed},
isbn       = {978-0-7506-6077-8},
location   = {Amsterdam},
note       = {OCLC: 254426229},
pagetotal  = {321},
publisher  = {Elsevier/Architectural Press},
langid     = {english},
shorttitle = {How designers think},
}
@Book{2015HortonAndroid,
author     = {Horton, John},
date       = {2015},
title      = {Android programming for beginners: learn all the Java and Android skills you need to start making powerful mobile applications},
edition    = {First published},
isbn       = {978-1-78588-326-2},
location   = {Birmingham},
note       = {OCLC: 1011380341},
pagetotal  = {660},
publisher  = {Packt Publishing},
langid     = {english},
shorttitle = {Android programming for beginners},
}
@Book{2004MccorduckMachines,
author     = {McCorduck, Pamela},
date       = {2004},
title      = {Machines who think: a personal inquiry into the history and prospects of artificial intelligence},
edition    = {25th anniversary update},
isbn       = {978-1-56881-205-2},
location   = {Natick, Mass},
pagetotal  = {565},
publisher  = {A.K. Peters},
keywords   = {Artificial intelligence, History},
langid     = {english},
shorttitle = {Machines who think},
}
@Book{2010RumseyStatistics,
author    = {Rumsey, Deborah J.},
date      = {2010},
title     = {Statistics essentials for dummies},
isbn      = {978-0-470-61839-4},
location  = {Indianapolis, IN},
pagetotal = {178},
publisher = {Wiley Pub., Inc},
series    = {For dummies},
file      = {:done/2010RumseyStatistics Statistics Essentials for Dummies.pdf:application/pdf},
groups    = {tesse:5, Probability},
keywords  = {Statistics},
langid    = {english},
}
@Book{2018LoderWeb,
author     = {Loder, Wolfgang},
date       = {2018},
title      = {Web Applications with Elm: Functional Programming for the Web},
doi        = {10.1007/978-1-4842-2610-0},
isbn       = {978-1-4842-2609-4 978-1-4842-2610-0},
location   = {Berkeley, CA},
publisher  = {Apress},
urldate    = {2019-04-06},
langid     = {english},
shorttitle = {Web Applications with Elm},
}
@Book{noauthor_portugues_nodate,
title  = {PORTUGUÊS Cesgranrio},
isbn   = {978-85-352-5911-7},
langid = {portuguese},
}
@Book{2011DefantClassical,
author    = {Defant, Andreas},
date      = {2011},
title     = {Classical summation in commutative and noncommutative Lp-spaces},
isbn      = {978-3-642-20437-1 978-3-642-20438-8},
location  = {Heidelberg ; New York},
note      = {OCLC: ocn746846973},
number    = {2021},
pagetotal = {171},
publisher = {Springer},
series    = {Lecture notes in mathematics},
keywords  = {Lp spaces},
langid    = {english},
}
@Book{2017GeronHands,
author     = {Géron, Aurélien},
date       = {2017},
title      = {Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems},
edition    = {First edition},
isbn       = {978-1-4919-6229-9},
location   = {Sebastopol, CA},
note       = {OCLC: ocn953432302},
pagetotal  = {551},
publisher  = {O'Reilly Media},
abstract   = {"Through a series of recent breakthroughs, deep learning has boosted the entire field of machine learning. Now, even programmers who know close to nothing about this technology can use simple, efficient tools to implement programs capable of learning from data. This practical book shows you how. By using concrete examples, minimal theory, and two production-ready Python frameworks--Scikit-Learn and TensorFlow--author Aurélien Géron helps you gain an intuitive understanding of the concepts and tools for building intelligent systems. You'll learn a range of techniques, starting with simple linear regression and progressing to deep neural networks. With exercises in each chapter to help you apply what you've learned, all you need is programming experience to get started" -- Back cover},
shorttitle = {Hands-on machine learning with Scikit-Learn and TensorFlow},
}
@Book{2015TallaridaPocket,
author   = {Tallarida, Ronald J.},
date     = {2015},
title    = {Pocket book of integrals and mathematical formulas},
isbn     = {978-1-4987-0476-2},
note     = {OCLC: 946827504},
abstract = {1. Elementary algebra and geometry -- 2. Determinants, matrices, and linear systems of equations -- 3. Trigonometry -- 4. Analytic geometry -- 5. Series, number facts, and theory -- 6. Differential calculus -- 7. Integral calculus -- 8. Vector analysis -- 9. Special functions -- 10. Differential equations -- 11. Statistics -- 12. Financial mathematics.},
file     = {:done/2015TallaridaPocket Pocket Book of Integrals and Mathematical Formulas.pdf:application/pdf},
groups   = {tesse:5, Calculus},
}
@Book{1995BaxterProduct,
author     = {Baxter, Mike},
date       = {1995-01-31},
title      = {Product Design: Practical Methods for the Systematic Development of New Products},
edition    = {1 edition},
isbn       = {978-0-7487-4197-7},
location   = {London},
pagetotal  = {324},
publisher  = {Routledge},
abstract   = {The discovery of market needs and the manufacture of a product to meet those needs are integral parts of the same process. Since most textbooks on new product development are written from either a marketing or an engineering perspective, it is important for students to encounter these two aspects of product development together in a single text.Product Design: Practical Methods for the Systematic Development of New Products covers the entire new product development process, from market research through concept design, embodiment design, design for manufacture, and product launch. Systematic and practical in its approach, the text offers both a structured management framework for product development and an extensive range of specific design methods. Chapters feature "Design Toolkits" that provide detailed guidance on systematic design methods, present examples with familiar products, and conclude with reviews of key concepts.This major text aims to turn the often haphazard and unstructured product design process into a quality-controlled, streamlined, and manageable procedure. It is ideal for students of engineering, design, and technology on their path to designing new products.},
shorttitle = {Product Design},
}
@Article{gandhi_central_nodate,
author = {Gandhi, Praça Mahatma},
title  = {Central de Concursos / Degrau Cultural},
pages  = {106},
langid = {portuguese},
}
@Book{2011JamesStudents,
author     = {James, J. F.},
date       = {2011},
title      = {A student's guide to Fourier transforms: with applications in physics and engineering},
edition    = {3rd ed},
isbn       = {978-0-521-17683-5},
location   = {Cambridge ; New York},
pagetotal  = {146},
publisher  = {Cambridge University Press},
shorttitle = {A student's guide to Fourier transforms},
}
@Book{2011TzengMultiple,
author     = {Tzeng, Gwo-hshiung and Huang, Jih-Jeng},
date       = {2011},
title      = {Multiple attribute decision making: methods and appliations},
isbn       = {978-1-4398-6157-8},
location   = {Boca Raton, Fla.},
note       = {OCLC: 746841344},
pagetotal  = {335},
publisher  = {CRC Press},
series     = {A Chapman ////////\\\& Hall book},
shorttitle = {Multiple attribute decision making},
}
@Book{martins_lingua_nodate,
author    = {Martins, Andréa},
title     = {LÍNGUA PORTUGUESA 1.000 Exercícios Questões simuladas e questões de concursos anteriores com os gabaritos},
publisher = {Central de Concursos / Degrau Cultural},
langid    = {portuguese},
}
@Book{2017HeizerOperations,
author     = {Heizer, Jay and Render, Barry and Munson, Chuck},
date       = {2017},
title      = {Operations management: sustainability and supply chain management},
edition    = {12},
isbn       = {978-0-13-413042-2},
location   = {Boston},
pagetotal  = {806},
publisher  = {Pearson},
file       = {:done/2017HeizerOperations Operations Management/\\_ Sustainability and Supply Chain Management.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Production management},
langid     = {english},
shorttitle = {Operations management},
}
@Book{2015FrazierOperations,
author    = {Frazier, Gaither},
date      = {2015-11-12},
title     = {Operations Management},
edition   = {9th edition},
isbn      = {978-81-315-0048-4},
location  = {Australia},
publisher = {Thomson Press},
abstract  = {Printed in Asia - Carries Same Contents as of US edition - Opt Expedited Shipping for 3 to 4 day delivery - - Includes CD-ROM - Printed in COLORS},
groups    = {tesse:4},
}
@Book{2016SlackOperations,
author    = {Slack, Prof Nigel and Brandon-Jones, Prof Alistair and Johnston, Prof Robert},
date      = {2016-06-24},
title     = {Operations Management},
edition   = {8},
isbn      = {978-1-292-09867-8},
location  = {Harlow, England London New York},
pagetotal = {752},
publisher = {Pearson},
abstract  = {Were you looking for the book with access to MyOMLab? This product is the book alone and does NOT come with access to MyOMLab. Buy Operations Management, 8th edition with MyOMLab access card (ISBN 9781292098777) if you need access to MyOMLab as well, and save money on this resource. You will also need a course ID from your instructor to access MyOMLab. Operations management is important, exciting, challenging … and everywhere you look! Important, because it enables organizations to provide services and products that we all need Exciting, because it is central to constant changes in customer preference, networks of supply and demand, and developments in technology Challenging, because solutions must be must be financially sound, resource-efficient, as well as environmentally and socially responsible And everywhere, because in our daily lives, whether at work or at home, we all experience and manage processes and operations. New features: There are now more than 110 of the popular ‘Operations in Practice’ examples throughout the book, over 40 /\\% of which are new. The importance of sustainability and Corporate Social Responsibility (CSR) has been emphasized further, and included throughout the book. We have even further strengthened the emphasis on the idea that ‘operations management’ is relevant to every type of business and all functional areas of the organization. Many new ideas in operations management have been incorporated, including the ‘three level’ approach to performance, the relationship between innovation, creativity and design, crowdsourcing, ideas management, business ecosystems, triadic relationships, office layout, telecommuting and organisational ‘ambidexterity’. However, we have retained the emphasis on the foundations of the subject. Six of the 19 cases at the end of the chapter are new (but the old ones are still available on the web site), and provide an up-to-date selection of operations issues. Operations Management focuses on the sustainable and socially responsible imperatives of operations management, using over 120 cases and illustrations of real-life operations around the world, including Apple, Médecins Sans Frontières, Amazon, Ecover, Dyson, Disneyland Paris, Google, The North Face, and many more. This is 24-carat excellence'Par Åhlström, Torsten and Ragnar Söderberg Chair of Business Administration, Stockholm School of Economics 'Operations Management is engaging and accessible, but it never dumbs-down. The book is comprehensive, but not overwhelming. Students hold on to this one; it’s a ‘keeper’.'Michael Shulver, Birmingham Business School 'This continues to be the definitive operations Management text … written by the masters of the field!'Dr Ross Ritchie, Lecturer in Operations Management, Loughborough University 'An essential text packed full of up-to-date examples that really bring the subject to life'Claire Moxham, University of Liverpool Management School 'An excellent book for those studying operations management. This book provides great illustrations to seamlessly link theory with practice' Frank Wiengarten, ESADE Business School Operations Management by Nigel Slack and Alistair Brandon-Jones is quite simply the best text on operations management. Comprehensive, engaging and insightful, I cannot recommend this book highly enough'Professor Andy Neely, Head, Institute for Manufacturing, Cambridge University Carrie Queenan, University of South Carolina Peter Race, Henley Business School, University of Reading},
file      = {:done/2016SlackOperations Operations Management.pdf:application/pdf},
groups    = {tesse:4},
}
@Book{2013OliveiraNocoes,
author = {Oliveira, Michelle Silva De and Moreira, Sherley Cabral},
date   = {2013},
title  = {Noções de contabilidade básica para cursos técnicos},
isbn   = {978-85-64124-17-2},
langid = {portuguese},
}
@Book{2017WalpoleProbability,
author     = {Walpole, Ronald E and Myers, Raymond H and Myers, Sharon L and Ye, Keying},
date       = {2017},
title      = {Probability ////////\\\& statistics for engineers ////////\\\& scientists: MyStatLab update},
isbn       = {978-1-292-16141-9},
note       = {OCLC: 1014366070},
url        = {http://www.myilibrary.com?id=947904},
urldate    = {2019-04-07},
langid     = {english},
shorttitle = {Probability ///////\\& statistics for engineers ///////\\& scientists},
}
@Collection{2008GowersPrinceton,
date      = {2008},
editor    = {Gowers, Timothy and Barrow-Green, June and Leader, Imre},
title     = {The Princeton companion to mathematics},
isbn      = {978-0-691-11880-2},
location  = {Princeton},
pagetotal = {1034},
publisher = {Princeton University Press},
file      = {:done/2008GowersPrinceton The Princeton Companion to Mathematics.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Mathematics},
langid    = {english},
year      = {2008},
}
@Book{2012GitmanPrinciples,
author    = {Gitman, Lawrence J. and Zutter, Chad J.},
date      = {2012},
title     = {Principles of managerial finance},
isbn      = {978-0-13-611946-3},
location  = {Boston (Estados Unidos},
note      = {OCLC: 991834493},
publisher = {Prentice Hall},
file      = {:done/2012GitmanPrinciples Principles of Managerial Finance.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2003BilaniukProblem,
author   = {Bilaniuk, Stefan},
date     = {2003},
title    = {A Problem Course in Mathematical Logic},
abstract = {This is a text for a problem-oriented course on mathematical logic and computability.},
file     = {:done/2003BilaniukProblem A Problem Course in Mathematical Logic.pdf:application/pdf},
groups   = {tesse:5},
langid   = {english},
}
@Book{2009FlajoletAnalytic,
author    = {Flajolet, Philippe and Sedgewick, Robert},
date      = {2009},
title     = {Analytic combinatorics},
isbn      = {978-0-521-89806-5},
location  = {Cambridge ; New York},
note      = {OCLC: ocn244767782},
pagetotal = {810},
publisher = {Cambridge University Press},
file      = {:done/2009FlajoletAnalytic Analytic Combinatorics.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Combinatieleer, Combinatorial analysis},
langid    = {english},
}
@Book{2016GastelHow,
author    = {Gastel, Barbara and Day, Robert A.},
date      = {2016},
title     = {How to write and publish a scientific paper},
edition   = {Eighth edition},
isbn      = {978-1-4408-4262-7 978-1-4408-4280-1},
location  = {Santa Barbara, California},
pagetotal = {326},
publisher = {Greenwood, an imprint of ABC-CLIO, LLC},
file      = {:done/2016GastelHow How to Write and Publish a Scientific Paper.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Technical writing},
langid    = {english},
}
@Book{2009DeKlerkIllustrated,
author    = {De Klerk, Judith and Green, Sally},
date      = {2009},
title     = {Illustrated maths dictionary},
isbn      = {978-0-7339-8661-1},
location  = {Port Melbourne},
note      = {OCLC: 937425073},
publisher = {Pearson},
abstract  = {Presenting the Illustrated Maths Dictionary 4th Edition, the revised and improved edition of Australia's best-selling mathematics dictionary by Judith De Klerk. New features in this fourth edition: new, cleaner design; computer terms with mathematical connotations; conversion tables; over fifty new entries; a bonus electronic version of the dictionary on CD-ROM. Together with the best features of previous editions: definitions written in simple language that children can understand but without losing accuracy; clear, precise and concise explanations of difficult terms; definitions that are supported by hundreds of examples and illustrations; a Useful Information section providing symbols, formulae and other supporting mathematical terms. The Illustrated Maths Dictionary 4th edition is an essential resource for primary and lower secondary students, teachers, student teachers and parents.},
file      = {:done/2009DeKlerkIllustrated Illustrated Maths Dictionary.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Book{2012GarrisonManagerial,
author    = {Garrison, Ray H. and Noreen, Eric W. and Brewer, Peter C.},
date      = {2012},
title     = {Managerial accounting},
edition   = {14th ed},
isbn      = {978-0-07-811100-6},
location  = {New York},
note      = {OCLC: ocn674938871},
pagetotal = {762},
publisher = {McGraw-Hill/Irwin},
file      = {:done/2012GarrisonManagerial Managerial Accounting.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Managerial accounting},
langid    = {english},
}
@Book{2009ClaphamConcise,
author    = {Clapham, Christopher and Nicholson, James},
date      = {2009},
title     = {The concise Oxford dictionary of mathematics},
edition   = {4},
isbn      = {978-0-19-923594-0},
location  = {Oxford ; New York},
note      = {OCLC: ocn299242316},
pagetotal = {510},
publisher = {Oxford University Press},
series    = {Oxford paperback reference},
file      = {:done/2009ClaphamConcise The Concise Oxford Dictionary of Mathematics.pdf:application/pdf},
groups    = {tesse:5},
}
@Collection{2012WieringReinforcement,
date       = {2012},
editor     = {Wiering, Marco and Otterlo, Martijn van},
title      = {Reinforcement learning: state-of-the-art},
isbn       = {978-3-642-27644-6 978-3-642-27645-3},
location   = {Heidelberg ; New York},
note       = {OCLC: ocn768170254},
pagetotal  = {638},
publisher  = {Springer},
series     = {Adaptation, learning, and optimization},
volume     = {12},
shorttitle = {Reinforcement learning},
}
@Article{2017RisiNeuroevolution,
author       = {Risi, Sebastian and Togelius, Julian},
date         = {2017-03},
journaltitle = {IEEE Transactions on Computational Intelligence and AI in Games},
title        = {Neuroevolution in Games: State of the Art and Open Challenges},
doi          = {10.1109/TCIAIG.2015.2494596},
issn         = {1943-068X, 1943-0698},
number       = {1},
pages        = {25--41},
url          = {http://ieeexplore.ieee.org/document/7307180/},
urldate      = {2019-04-07},
volume       = {9},
abstract     = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artiﬁcial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyze the application of NE in games along ﬁve different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the ﬁtness is determined and what type of input the network receives. The paper also highlights important open research challenges in the ﬁeld.},
file         = {:done/2017RisiNeuroevolution Neuroevolution in Games/\\_ State of the Art and Open Challenges.pdf:application/pdf},
groups       = {tesse:5},
keywords     = {neural networks, Artificial intelligence, artificial neural network training, Biological neural networks, computer game, computer games, evolutionary algorithm, Evolutionary algorithms, evolutionary computation, Evolutionary computation, Games, Genetic algorithms, learning (artificial intelligence), NE, Network topology, neural nets, neuroevolution},
langid       = {english},
shorttitle   = {Neuroevolution in Games},
}
@Article{2017FransOutline,
author       = {Frans, Kevin},
date         = {2017-04-28},
journaltitle = {arXiv:1704.08834 [cs]},
title        = {Outline Colorization through Tandem Adversarial Networks},
eprint       = {1704.08834},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1704.08834},
urldate      = {2019-04-07},
abstract     = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-deﬁned color scheme.},
file         = {:done/2017FransOutline Outline Colorization through Tandem Adversarial Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016IsolaImage,
author       = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
date         = {2016-11-21},
journaltitle = {arXiv:1611.07004 [cs]},
title        = {Image-to-Image Translation with Conditional Adversarial Networks},
eprint       = {1611.07004},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1611.07004},
urldate      = {2019-04-07},
abstract     = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
file         = {:done/2016IsolaImage Image to Image Translation with Conditional Adversarial Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{sigtia_end--end_nodate,
author   = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
title    = {An End-to-End Neural Network for Polyphonic Music Transcription},
pages    = {14},
abstract = {We present a neural network model for polyphonic music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony or the number or type of instruments. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We investigate various neural network architectures for the acoustic models and compare their performance to two popular state-of-the-art acoustic models. We also present an efﬁcient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications. We evaluate the model’s performance on the MAPS dataset and show that the proposed model outperforms state-ofthe-art transcription systems.},
langid   = {english},
}
@Article{2013ChadeganiComparison,
author       = {Chadegani, Arezoo Aghaei and Salehi, Hadi and Yunus, Melor Md and Farhadi, Hadi and Fooladi, Masood and Farhadi, Maryam and Ebrahim, Nader Ale},
date         = {2013-04-27},
journaltitle = {Asian Social Science},
title        = {A Comparison between Two Main Academic Literature Collections: Web of Science and Scopus Databases},
doi          = {10.5539/ass.v9n5p18},
issn         = {1911-2025, 1911-2017},
number       = {5},
url          = {http://www.ccsenet.org/journal/index.php/ass/article/view/26960},
urldate      = {2019-04-07},
volume       = {9},
abstract     = {Nowadays, the world’s scientific community has been publishing an enormous number of papers in different scientific fields. In such environment, it is essential to know which databases are equally efficient and objective for literature searches. It seems that two most extensive databases are Web of Science and Scopus. Besides searching the literature, these two databases used to rank journals in terms of their productivity and the total citations received to indicate the journals impact, prestige or influence. This article attempts to provide a comprehensive comparison of these databases to answer frequent questions which researchers ask, such as: How Web of Science and Scopus are different? In which aspects these two databases are similar? Or, if the researchers are forced to choose one of them, which one should they prefer? For answering these questions, these two databases will be compared based on their qualitative and quantitative characteristics.},
langid       = {english},
shorttitle   = {A Comparison between Two Main Academic Literature Collections},
}
@Article{2016SinghChawlaUnsung,
author       = {Singh Chawla, Dalmeet},
date         = {2016-01-04},
journaltitle = {Nature},
title        = {The unsung heroes of scientific software},
doi          = {10.1038/529115a},
issn         = {0028-0836, 1476-4687},
number       = {7584},
pages        = {115--116},
urldate      = {2019-04-07},
volume       = {529},
file         = {:done/2016SinghChawlaUnsung The Unsung Heroes of Scientific Software.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1972GarfieldCitation,
author       = {Garfield, Eugene},
date         = {1972},
journaltitle = {Essays of an Information Scientist},
title        = {Citation Analysis as a Tool in Journal Evaluation},
pages        = {24},
file         = {:done/1972GarfieldCitation Citation Analysis As a Tool in Journal Evaluation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2018KumarIs,
author       = {Kumar, Ashish},
date         = {2018},
journaltitle = {Journal of Indian Society of Periodontology},
title        = {Is "Impact" the "Factor" that matters…? (Part I)},
doi          = {10.4103/jisp.jisp///////\\_195///////\\_18},
issn         = {0972-124X},
number       = {2},
pages        = {95},
url          = {http://www.jisponline.com/text.asp?2018/22/2/95/230828},
urldate      = {2019-04-07},
volume       = {22},
langid       = {english},
shorttitle   = {Is "Impact" the "Factor" that matters…?},
}
@Article{2005JacsoAs,
author       = {Jacso, Peter},
date         = {2005},
journaltitle = {CURRENT SCIENCE},
title        = {As we may search – Comparison of major features of the Web of Science, Scopus, and Google Scholar citation-based and citation-enhanced databases},
number       = {9},
pages        = {11},
volume       = {89},
langid       = {english},
}
@Article{2006EditorsImpact,
author       = {Editors, The PLoS Medicine},
date         = {2006-06-06},
journaltitle = {PLoS Medicine},
title        = {The Impact Factor Game},
doi          = {10.1371/journal.pmed.0030291},
issn         = {1549-1676},
number       = {6},
pages        = {e291},
urldate      = {2019-04-07},
volume       = {3},
file         = {:done/2006EditorsImpact The Impact Factor Game.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{meho_impact_nodate,
author   = {Meho, Lokman I and Yang, Kiduk},
title    = {Impact of Data Sources on Citation Counts and Rankings of LIS Faculty: Web of Science vs. Scopus and Google Scholar},
pages    = {48},
abstract = {The Institute for Scientific Information’s (ISI) citation databases have been used for decades as a starting point and often as the only tools for locating citations and/or conducting citation analyses. ISI databases (or Web of Science [WoS]), however, may no longer be sufficient because new databases and tools that allow citation searching are now available. Using citations to the work of 25 library and information science faculty members as a case study, this paper examines the effects of using Scopus and Google Scholar (GS) on the citation counts and rankings of scholars as measured by WoS. Overall, more than 10,000 citing and purportedly citing documents were examined. Results show that Scopus significantly alters the relative ranking of those scholars that appear in the middle of the rankings and that GS stands out in its coverage of conference proceedings as well as international, non-English language journals. The use of Scopus and GS, in addition to WoS, helps reveal a more accurate and comprehensive picture of the scholarly impact of authors. WoS data took about 100 hours of collecting and processing time, Scopus consumed 200 hours, and GS a grueling 3,000 hours.},
langid   = {english},
}
@Article{2008FalagasComparison,
author       = {Falagas, Matthew E. and Pitsouni, Eleni I. and Malietzis, George A. and Pappas, Georgios},
date         = {2008-02},
journaltitle = {The FASEB Journal},
title        = {Comparison of PubMed, Scopus, Web of Science, and Google Scholar: strengths and weaknesses},
doi          = {10.1096/fj.07-9492LSF},
issn         = {0892-6638, 1530-6860},
number       = {2},
pages        = {338--342},
urldate      = {2019-04-07},
volume       = {22},
langid       = {english},
shorttitle   = {Comparison of PubMed, Scopus, Web of Science, and Google Scholar},
}
@Article{rossner_irreproducible_nodate,
author = {Rossner, Mike and Epps, Heather Van and Hill, Emma},
title  = {Irreproducible results: a response to Thomson Scientiﬁc},
pages  = {2},
langid = {english},
}
@Article{2009BrumbackImpact,
author       = {Brumback, Roger A.},
date         = {2009-03},
journaltitle = {Journal of Child Neurology},
title        = {Impact Factor Wars: Episode V—The Empire Strikes Back},
doi          = {10.1177/0883073808331366},
issn         = {0883-0738, 1708-8283},
number       = {3},
pages        = {260--262},
urldate      = {2019-04-07},
volume       = {24},
langid       = {english},
shorttitle   = {Impact Factor Wars},
}
@Article{2007RossnerShow,
author       = {Rossner, Mike and Van Epps, Heather and Hill, Emma},
date         = {2007-12-17},
journaltitle = {The Journal of Cell Biology},
title        = {Show me the data},
doi          = {10.1083/jcb.200711140},
issn         = {0021-9525, 1540-8140},
number       = {6},
pages        = {1091--1092},
urldate      = {2019-04-07},
volume       = {179},
file         = {:done/2007RossnerShow Show Me the Data.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2009AlvesPre,
author     = {Alves, Ivan and Aragao, Atila and Bastos, Braulio-Luis and Falcao, Jose and Fartes, E.},
booktitle  = {Proceedings of Offshore Technology Conference},
date       = {2009-05},
title      = {Pre-Salt Santos Basin—Well Construction Learning Curve Acceleration},
doi        = {10.4043/OTC-20177-MS},
eventtitle = {Offshore Technology Conference},
publisher  = {The Offshore Technology Conference},
url        = {http://www.onepetro.org/mslib/servlet/onepetropreview?id=OTC-20177-MS///////\\&soc=OTC},
urldate    = {2019-04-07},
abstract   = {Drilling and completing either exploratory or development wells in Pre-Salt prospects present several challenges. The wells are located in very deep waters, beyond 2,000 m WD and they are also deep wells, with more than 5,000 m TVD. Pressure and temperature is normal, but contaminants such as H2S and CO2 represent an additional difficulty.},
file       = {:done/2009AlvesPre Pre Salt Santos Basin—Well Construction Learning Curve Acceleration.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@Article{2010VaccaroProspective,
author       = {Vaccaro, Guilherme Luís Roehe and Pohlmann, Christopher and Lima, André Cirne and dos Santos, Manoela Silveira and de Souza, Cristina Botti and Azevedo, Debora},
date         = {2010-05},
journaltitle = {Renewable and Sustainable Energy Reviews},
title        = {Prospective scenarios for the biodiesel chain of a Brazilian state},
doi          = {10.1016/j.rser.2009.12.008},
issn         = {1364-0321},
number       = {4},
pages        = {1263--1272},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1364032109002883},
urldate      = {2019-04-07},
volume       = {14},
abstract     = {This paper presents a study based on the Systems Thinking and Scenario Planning (STSP) method, focusing the biodiesel production chain of Rio Grande do Sul State. The aim of the study was to identify key elements to comprehend the systemic structure of interaction among the ties of this chain. The study was held by a team of specialists over ﬁve months, including 15 meetings. Discussions were based on quantitative and qualitative data and a systemic map was constructed and reﬁned. Based on this modeling, four different prospective scenarios were comparatively analyzed in order to propose strategic actions to promote the sustainability and competitiveness of the chain. The results were then presented to two different groups of external specialists in order to validate the conclusions drawn and the proposals. Both groups agreed with the ideas presented. The paper is constructed as follows: a brief introduction focusing on contextual elements of the biodiesel production in Brazil and in Rio Grande do Sul; some background material regarding agroindustrial production chains and an overview of the biodiesel production chain of interest; a description of the method used to perform the research; main results and discussion; and conclusions. With this paper the authors also hope to contribute to the discussion regarding competitiveness and sustainability of biofuel chains in Brazil.},
langid       = {english},
}
@Article{gouveia_tecnologia_nodate,
author = {Gouveia, Flávia},
title  = {Tecnologia nacional para extrair petróleo e gás do pré-sal},
pages  = {6},
langid = {portuguese},
}
@Article{2013AmerReview,
author       = {Amer, Muhammad and Daim, Tugrul U. and Jetter, Antonie},
date         = {2013-02},
journaltitle = {Futures},
title        = {A review of scenario planning},
doi          = {10.1016/j.futures.2012.10.003},
issn         = {0016-3287},
pages        = {23--40},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0016328712001978},
urldate      = {2019-04-07},
volume       = {46},
abstract     = {This paper reviews the scenario planning literature looking for answers for the following questions: How do qualitative and quantitative scenario methods differ and what are the advantages and disadvantages? What methods exist for quantitative scenario planning? Particularly quantitative scenario methods often lead to a large number of so-called ‘‘raw’’ scenarios that need to be further reﬁned, discussed, and verbally described. How do scenario planners select raw scenarios for further exploration and how many should they choose? How is the problem of validation addressed in scenario studies? ß 2012 Elsevier Ltd. All rights reserved.},
file         = {:done/2013AmerReview A Review of Scenario Planning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2012PizarroChallenges,
author     = {Pizarro, Jorge Oscar De Santanna and Branco, Celso Cesar M.},
booktitle  = {SPE EOR Conference at Oil and Gas West Asia},
date       = {2012},
title      = {Challenges in Implementing an EOR Project in the Pre-Salt Province in Deep Offshore Brasil},
doi        = {10.2118/155665-MS},
eventtitle = {SPE EOR Conference at Oil and Gas West Asia},
location   = {Muscat, Oman},
publisher  = {Society of Petroleum Engineers},
urldate    = {2019-04-07},
abstract   = {Carbonate reservoirs contain expressive part of the world oil reserves. The exploitation of these resources, however, presents several challenges mainly associated with their complex pore geometry, large scale variation in permeability and sometimes unfavorable wettability. These challenges can become more significant when we move to a deep offshore environment. As offshore projects need to be planned well in advance, due to the lack of room in the platforms for future expansion, the pioneer application of EOR methods needs to be considered from the conceptual stage of the development.},
langid     = {english},
}
@Article{2012RiccominiPre,
author       = {Riccomini, Claudio and Sant´Anna, Lucy Gomes and Tassinari, Colombo Celso Gaeta},
date         = {2012-11-30},
journaltitle = {Revista USP},
title        = {Pré-sal: geologia e exploração},
doi          = {10.11606/issn.2316-9036.v0i95p33-42},
issn         = {2316-9036, 0103-9989},
number       = {95},
pages        = {33},
url          = {http://www.revistas.usp.br/revusp/article/view/52236},
urldate      = {2019-04-07},
volume       = {0},
abstract     = {Pre-salt is the world’s greatest oil-related finding of the past 50 years. Pre-salt oil lies in reserves located below large and thick salt layers found offshore Brazil, along the coast from the states of Espírito Santo to Santa Catarina, in deep and ultra-deep waters, and situated below 3-to-4-km-thick rock layers, under the seabed. The investigations carried out so far in some pre-salt areas have shown that giant and super-giant fields are likely to exist. Such fields could place Brazil among the leading oil producers as they are thought to hold up to 16 billion barrels of recoverable oil equivalent – boe (sum of oil and natural gas), and are likely to yield from 70 to 100 billion barrels of oil equivalent. This article deals with the geological conditions and the past scenario in which pre-salt reserves were formed, the characteristics of the oil system, estimated reserves, exploration and production.},
langid       = {portuguese},
shorttitle   = {Pré-sal},
}
@InProceedings{2012MoczydlowerDevelopment,
author     = {Moczydlower, Bruno and Salomao, Marcelo Curzio and Branco, Celso Cesar M. and Romeu, Regis Kruel and Homem, Tiago Da Rosa and De Freitas, Luiz Carlos and Lima, Helena Assaf T Souza},
booktitle  = {SPE Latin America and Caribbean Petroleum Engineering Conference},
date       = {2012},
title      = {Development of the Brazilian Pre-Salt Fields - When To Pay for Information and When To Pay for Flexibility},
doi        = {10.2118/152860-MS},
eventtitle = {SPE Latin America and Caribbean Petroleum Engineering Conference},
location   = {Mexico City, Mexico},
publisher  = {Society of Petroleum Engineers},
urldate    = {2019-04-07},
abstract   = {The Santos Basin Pre-Salt Cluster (SBPSC), Offshore Southeast Brazil, is a unique scenario, posing great development challenges. The microbial carbonate reservoir is unusual regarding its origin and petrophysical properties; the fluids have a variable CO2 content; the few analogue reservoirs around the world do not compare in terms of volumes, water depth and distance to the coast; and there are also flow assurance issues.},
langid     = {english},
}
@Article{2013PardoProspective,
author       = {Pardo, Nicolás and Moya, José Antonio},
date         = {2013-06},
journaltitle = {Energy},
title        = {Prospective scenarios on energy efficiency and CO2 emissions in the European Iron ////////\\\& Steel industry},
doi          = {10.1016/j.energy.2013.03.015},
issn         = {0360-5442},
pages        = {113--128},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0360544213001928},
urldate      = {2019-04-07},
volume       = {54},
abstract     = {The Iron ///////\\& Steel industry is one of the biggest industrial CO2 emitters in the European Union. The present work analyzes the potential for the improvement of the energy efﬁciency and CO2 emission reduction for this sector up to 2030. Three scenarios are analyzed: baseline scenario (BS) representing the current evolution of this sector and two alternative scenarios (AS1 and AS2) to study the sensitivity of fuel and resource prices and CO2 emission prices. In the integrated production route all the scenarios highlight the importance of breakthrough technologies, such as Top Gas Recycling, to obtain performance improvement in energy consumption and CO2 emissions. In the secondary production route conditions under BS scenario are enough for a suitable incorporation of the best technologies in this route making insensitive the possible incentives due to higher prices in fuel and resources or CO2 emissions.},
langid       = {english},
}
@InProceedings{2015CostaFragaBrazilian,
author     = {da Costa Fraga, Carlos Tadeu and Capeleiro Pinto, Antonio Carlos and Branco, Celso Cesar Moreira and de Sant´Anna Pizarro, Jorge Oscar and da Silva Paulo, Cezar Augusto},
booktitle  = {Offshore Technology Conference},
date       = {2015},
title      = {Brazilian Pre-Salt: An Impressive Journey from Plans and Challenges to Concrete Results},
doi        = {10.4043/25710-MS},
eventtitle = {Offshore Technology Conference},
location   = {Houston, Texas, USA},
publisher  = {Offshore Technology Conference},
urldate    = {2019-04-07},
abstract   = {In the early 2000s the context of the pre-salt formations was conceptual geological models of possible oil bearing reservoirs underneath a thick salt layer, many technical challenges, uncertainties and risks. Past only eight years from first discovery (2006), there are nine production systems, FPSOs, in operation, reaching an average oil rate of more than 700 thousand barrels per day and a cumulative production greater than 400 million barrels of oil, through 34 production wells. To optimize recovery, the first desulfated sea water and gas injectors were started. But these impressive numbers cannot be taken from granted: although nature has revealed prolific reservoirs, much experience, talent, planning and perseverance were necessary.},
langid     = {english},
shorttitle = {Brazilian Pre-Salt},
}
@InProceedings{2016CaoData,
author     = {Cao, Q. and Banerjee, R. and Gupta, S. and Li, J. and Zhou, W. and Jeyachandra, B.},
booktitle  = {SPE Argentina Exploration and Production of Unconventional Resources Symposium},
date       = {2016},
title      = {Data Driven Production Forecasting Using Machine Learning},
doi        = {10.2118/180984-MS},
eventtitle = {SPE Argentina Exploration and Production of Unconventional Resources Symposium},
location   = {Buenos Aires, Argentina},
publisher  = {Society of Petroleum Engineers},
urldate    = {2019-04-07},
abstract   = {Forecasting of production in unconventional prospects has gained a lot of attention in the recent years. The key challenges in unconventional reservoirs have been the requirement to put online a) a large number of wells in a short period of time, b) well productivity significantly driven by completion characteristics and that c) the physics of fluid flow in these prospects still remain uncertain. In this paper, machine learning algorithms are used to forecast production for existing and new wells in unconventional assets using inputs like geological maps, production history, pressure data and operational constraints. One of the most popular Machine Learning methods – Artificial Neural Network (ANN) is employed for this purpose. ANN can learn from large volume of data points without assuming a predetermined model and can adapt to newer data as and when it becomes available. The workflow involves using these data sets to train and optimize the ANN model which, subsequently, is used to predict the well production performance of both existing wells using their own history and new wells by using the history of nearby wells which were drilled in analogous geological locations. The proposed technique requires users to do less data conditioning and model building and focus more on analyzing what-if scenarios and determining the well performance.},
langid     = {english},
}
@InProceedings{2016MoraesCruzLula,
author     = {de Moraes Cruz, Rafael Oscar and Rosa, Marcelo Becher and Branco, Celso Cesar Moreira and de Sant'Anna Pizarro, Jorge Oscar and de Souza Silva, Celso Tarcisio},
booktitle  = {Offshore Technology Conference},
date       = {2016},
title      = {Lula NE Pilot Project - An Ultra-Deep Success in the Brazilian Pre-Salt},
doi        = {10.4043/27297-MS},
eventtitle = {Offshore Technology Conference},
location   = {Houston, Texas, USA},
publisher  = {Offshore Technology Conference},
urldate    = {2019-04-07},
abstract   = {This paper presents the successful history of Lula NE Pilot Project, a challenging megaproject with an aggressive time-driven schedule, faster than industry average, that demanded new technology development in a scenario of uncertainty. The area is part of the supergiant Lula field, located in the pre-salt region of Santos Basin, Southeast Brazil, 300 km off the coast of Rio de Janeiro state, in 2000 m water depth. It is a joint venture with Petrobras as the Operator, and BG E///////\\&P Brasil and Petrogal Brasil as partners. The project was designed as a Pilot aiming to test some new concepts for the production development in the pre-salt area. In terms of subsea gathering system, an innovative concept was deployed, combining flexible flowlines lying on sea floor, with rigid steel catenary risers (SCR) supported by a buoy positioned 250 m below sea level. The drainage plan considered eight oil producers, some of them with intelligent completion, one gas / CO2 injection well and five water alternating gas (WAG) injectors (two subsea WAG manifolds were also installed). A balanced approach between data acquisition and facilities flexibility made possible to face the many reservoir and production uncertainties. Details of the development concept will be discussed, as well as the main results obtained so far, highlighting the strategies adopted in order to mitigate risks and the influence of the acquired information to the following projects in the area.},
langid     = {english},
}
@Book{2016KowsmannGeology,
author     = {Kowsmann, Renato O},
date       = {2016},
title      = {Geology and geomorphology: regional environmental characterization of the Campos Basin, Southwest Atlantic},
isbn       = {978-85-352-8444-7},
note       = {OCLC: 965386433},
url        = {http://www.sciencedirect.com/science/book/9788535284447},
urldate    = {2019-04-07},
langid     = {english},
shorttitle = {Geology and geomorphology},
}
@Article{2016DiasSustainability,
author       = {Dias, Maria Amelia de Paula and Vianna, João Nildo de Souza and Felby, Claus},
date         = {2016-09},
journaltitle = {Futures},
title        = {Sustainability in the prospective scenarios methods: A case study of scenarios for biodiesel industry in Brazil, for 2030},
doi          = {10.1016/j.futures.2016.06.005},
issn         = {0016-3287},
pages        = {1--14},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0016328716300167},
urldate      = {2019-04-07},
volume       = {82},
abstract     = {In order to build prospective scenarios for biodiesel industry in Brazil, with a sustainable perspective, it was necessary to develop a cross-disciplinary work to include Sachs’ dimensions of sustainability into the scenarios method. This meant linking concepts from different disciplines, without transforming it in a new discipline. In order to support the proposition for the new method, a study case is presented, the framework for the biodiesel scenarios in Brazil, by 2030. An in-depth interview was used to test the proposition of having the sustainability dimensions as driving forces. The result was the identiﬁcation of a critical uncertainty composed of various aspects related the response to climate change and environmental conservation. The scenario storylines that were developed based on the critical uncertainties showed that sustainable options for the future are possible if the mental maps are enlarged to see beyond the business as usual.},
langid       = {english},
shorttitle   = {Sustainability in the prospective scenarios methods},
}
@Article{2017GomesSensibility,
author       = {Gomes, Carlos Francisco Simões and Costa, Helder Gomes and de Barros, Alexandre P},
date         = {2017-07-03},
journaltitle = {Journal of Modelling in Management},
title        = {Sensibility analysis of MCDA using prospective in Brazilian energy sector},
doi          = {10.1108/JM2-01-2016-0005},
editor       = {Huang, Zhimin and Shi, Xunpeng},
issn         = {1746-5664},
pages        = {00--00},
urldate      = {2019-04-07},
langid       = {english},
}
@InProceedings{2018PankajApplication,
author     = {Pankaj, Piyush and Geetan, Steve and MacDonald, Richard and Shukla, Priyavrat and Sharma, Abhishek and Menasria, Samir and Xue, Han and Judd, Tobias},
booktitle  = {Offshore Technology Conference},
date       = {2018},
title      = {Application of Data Science and Machine Learning for Well Completion Optimization},
doi        = {10.4043/28632-MS},
eventtitle = {Offshore Technology Conference},
location   = {Houston, Texas, USA},
publisher  = {Offshore Technology Conference},
urldate    = {2019-04-07},
abstract   = {In today's data-driven economy, operators that integrate vast stores of fundamental reservoir and production data with the highperformance predictive analytics solutions can emerge as winners in the contest of maximizing estimated ultimate recovery (EUR). The scope of this study is to demonstrate a new workflow coupling earth sciences with data analytics to operationalize well completion optimization. The workflow aims to build a robust predictive model that allows users to perform sensitivity analysis on completion designs within a few hours.},
langid     = {english},
}
@InProceedings{2018PollockMachine,
author     = {Pollock, Jacob and Stoecker-Sylvia, Zachary and Veedu, Vinod and Panchal, Neil and Elshahawi, Hani},
booktitle  = {Offshore Technology Conference},
date       = {2018},
title      = {Machine Learning for Improved Directional Drilling},
doi        = {10.4043/28633-MS},
eventtitle = {Offshore Technology Conference},
location   = {Houston, Texas, USA},
publisher  = {Offshore Technology Conference},
urldate    = {2019-04-07},
abstract   = {Directional drilling is a complex process involving the remote control of tool alignment and force application to a very long drill string subject to variable external forces. Controlling bit tool face orientation while ensuring adequate rate of penetration (ROP) is quite challenging, with aspects that have been described as more art than science. Improving this control helps preserve proper well trajectory and eliminate deviations that require corrective measures and add to well costs.},
file       = {:done/2018PollockMachine Machine Learning for Improved Directional Drilling.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@InProceedings{2018PankajNeed,
author     = {Pankaj, Piyush and Geetan, Steve and MacDonald, Richard and Shukla, Priyavrat and Sharma, Abhishek and Menasria, Samir and Judd, Tobias},
booktitle  = {SPE Canada Unconventional Resources Conference},
date       = {2018},
title      = {Need for Speed: Data Analytics Coupled to Reservoir Characterization Fast Tracks Well Completion Optimization},
doi        = {10.2118/189790-MS},
eventtitle = {SPE Canada Unconventional Resources Conference},
location   = {Calgary, Alberta, Canada},
publisher  = {Society of Petroleum Engineers},
urldate    = {2019-04-07},
abstract   = {In today's data-driven economy, operators that integrate vast stores of fundamental reservoir and production data with the high-performance predictive analytics solutions can emerge as winners in the contest of maximizing estimated ultimate recovery (EUR). The scope of this study is to demonstrate a new workflow coupling earth sciences with data analytics to operationalize well completion optimization. The workflow aims to build a robust predictive model that allows users to perform sensitivity analysis on completion designs within a few hours.},
langid     = {english},
shorttitle = {Need for Speed},
}
@InProceedings{2018NoshiRole,
author     = {Noshi, Christine I. and Schubert, Jerome J.},
booktitle  = {SPE/AAPG Eastern Regional Meeting},
date       = {2018},
title      = {The Role of Machine Learning in Drilling Operations; A Review},
doi        = {10.2118/191823-18ERM-MS},
eventtitle = {SPE/AAPG Eastern Regional Meeting},
location   = {Pittsburgh, Pennsylvania, USA},
publisher  = {Society of Petroleum Engineers},
urldate    = {2019-04-07},
abstract   = {Drilling problems such as stick slip vibration/hole cleaning, pipe failures, loss of circulation, BHA whirl, stuck pipe incidents, excessive torque and drag, low ROP, bit wear, formation damage and borehole instability, and the drilling of highly tortuous wells have only been tackled using physics-based models. Despite the mammoth generation of real-time metadata, there is a tremendous gap between statistical based models and empirical, mathematical, and physical-based models. Data mining techniques have made prominent contributions across a broad spectrum of industries. Its value is widely appreciated in a variety of applications, but its potential has not been fully tapped in the oil and gas industry. This paper presents a review compiling several years of Data Analytics applications in the drilling operations. This review discusses the benefits, deficiencies of the present practices, challenges, and novel applications under development to overcome industry deficiencies. This study encompasses a comprehensive compilation of data mining algorithms and industry applications from a predictive analytics standpoint using supervised and unsupervised advanced analytics algorithms to identify hidden patterns and help mitigate drilling challenges. Traditional data preparation and analysis methods are not sufficiently capable of rapid information extraction and clear visualization of big complicated data sets. Due to the petroleum industry's unfulfilled demand, Machine Learning (ML)-assisted industry workflow in the fields of drilling optimization and real time parameter analysis and mitigation is presented.},
langid     = {english},
}
@Article{1980EnzerInterax—an,
author       = {Enzer, Selwyn},
date         = {1980-06},
journaltitle = {Technological Forecasting and Social Change},
title        = {INTERAX—An interactive model for studying future business environments: Part I},
doi          = {10.1016/0040-1625(80)90049-9},
issn         = {0040-1625},
number       = {2},
pages        = {141--159},
url          = {http://linkinghub.elsevier.com/retrieve/pii/0040162580900499},
urldate      = {2019-04-07},
volume       = {17},
abstract     = {In exploring alternative futures, strategic planners are frequently forced into making simplifying assumptions because of time and methodological limitations. These assumptions typically reduce the interdisciplinary perspective, limit the range of policy choices, and restrict the investigation to only the "most likely" future changes. By so doing, many sources of future sutprise are eliminated from the analysis. This article describes INTERAX, a method for generating alternative future scenarios which requires very few simplifying assumptions. To accelerate the time required for the analysis, INTERAX includes a large multidisciplinary data base, which is of immediate use in a broad range of long-range strategic issues. INTERAX is described in two articles. This article describes the method and its philosophy and presents the data base. The second part, which will appear in the next issue of Technological Forecasting and Social Change, describes bow INTERAX generates scenarios and their use in exploring a variety of strategic issues.},
langid       = {english},
shorttitle   = {INTERAX—An interactive model for studying future business environments},
}
@Article{1980EnzerInterax—ana,
author       = {Enzer, Selwyn and Director, o̊Associate},
date         = {1980-07},
journaltitle = {Technological Forecasting and Social Change},
title        = {INTERAX—An interactive model for studying future business environments: Part II},
doi          = {10.1016/0040-1625(80)90064-5},
issn         = {0040-1625},
number       = {3},
pages        = {211--242},
url          = {http://linkinghub.elsevier.com/retrieve/pii/0040162580900645},
urldate      = {2019-04-07},
volume       = {17},
abstract     = {In exploring alternative futures, strategic planners are frequently forced into making simplifying assumptions because of time and methodological limitations. These assumptions typically reduce the interdisciplinary perspective, limit the range of policy choices, and restrict the investigation to only the "most likely" future changes. By so doing, many sources of future surprise am eliminated from the analysis. This article describes INTERAX, a method for generating alternative future scenarios which requires very few simplifying assumptions. To accelerate the time required for the analysis, INTERAX includes a large multidisciplinary data base, which is of immediate use in a broad range of long-range strategic issues. INTERAX is described in two parts. The previous issue of Technological Forecusring md Socinl Chnnge contained a description of the method and its philosophy and presented the data base. This article describes how INTERAX generates scenarios and uses them in exploring a variety of strategic issues.},
langid       = {english},
shorttitle   = {INTERAX—An interactive model for studying future business environments},
}
@Article{corsano_drilling_nodate,
author   = {Corsano, Alfred},
title    = {The Drilling Performance Curve: A Yardstick for Judging Drilling Performance},
pages    = {12},
abstract = {The Drilling Performance Curve (DPC) ia a simple yet powerful tool to assess the drilling performance in any given area where a c,;nsecutive series of similar wells have been drilled. All the information that is needed to perform the analysis is the sequence numbers of the well and the time it takes to reach a given depth.},
langid   = {english},
}
@Article{2005MietznerAdvantages,
author       = {Mietzner, Dana and Reger, Guido},
date         = {2005},
journaltitle = {International Journal of Technology Intelligence and Planning},
title        = {Advantages and disadvantages of scenario approaches for strategic foresight},
doi          = {10.1504/IJTIP.2005.006516},
issn         = {1740-2832, 1740-2840},
number       = {2},
pages        = {220},
url          = {http://www.inderscience.com/link.php?id=6516},
urldate      = {2019-04-07},
volume       = {1},
abstract     = {Scenarios, as a prime technique of future studies, have long been used by government planners, corporate managers and military analysts as powerful tools to aid in decision making in the face of uncertainty. The idea behind them is to establish thinking about possible futures which can minimise surprises and broaden the span of managers’ thinking about different possibilities. Today the question of what scenarios are is unclear except with regard to one point - they have become extremely popular. This paper attempts to shed light on differences in scenario approaches. It will describe the origin of scenarios and the development of different understandings and purposes for managers. Categories are developed to compare the different ways scenarios are performed. Finally, the advantages and disadvantages of scenario approaches are analysed.},
file         = {:done/2005MietznerAdvantages Advantages and Disadvantages of Scenario Approaches for Strategic Foresight.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{lima_os_nodate,
author = {Lima, Paulo César Ribeiro},
title  = {OS DESAFIOS, OS IMPACTOS E A GESTÃO DA EXPLORAÇÃO DO PRÉ-SAL},
pages  = {25},
langid = {portuguese},
}
@InProceedings{2009BeltraoChallenges,
author     = {Beltrao, Ricardo L. and Sombra, Cristiano and Lage, Antonio and Fagundes Netto, Jose and Henriques, Carlos},
booktitle  = {Proceedings of Offshore Technology Conference},
date       = {2009-05},
title      = {Challenges and New Technologies for the Development of the Pre-Salt Cluster, Santos Basin, Brazil},
doi        = {10.4043/OTC-19880-MS},
eventtitle = {Offshore Technology Conference},
publisher  = {The Offshore Technology Conference},
url        = {http://www.onepetro.org/mslib/servlet/onepetropreview?id=OTC-19880-MS///////\\&soc=OTC},
urldate    = {2019-04-07},
abstract   = {Pre-salt carbonate reservoirs from Santos Basin represent a great opportunity and probably the most important recent oil discovery. Tupi area (estimated to have recoverable volume of 5 to 8 bboe), which is the most known amongst several other leads in the cluster, is going to be a great insight for the production project.},
langid     = {english},
}
@Article{2002BerkhoutForesight,
author       = {Berkhout, Frans and Hertin, Julia},
date         = {2002},
journaltitle = {Greener Management International},
title        = {Foresight futures scenarios: developing and applying a participative strategic planning tool},
pages        = {37--53},
shorttitle   = {Foresight futures scenarios},
}
@Article{1995SchoemakerScenario,
author       = {Schoemaker, Paul JH},
date         = {1995},
journaltitle = {Sloan management review},
title        = {Scenario planning: a tool for strategic thinking},
number       = {2},
pages        = {25--50},
volume       = {36},
shorttitle   = {Scenario planning},
}
@Collection{2010ChenPlastics,
date       = {2010},
editor     = {Chen, Guo-Qiang},
title      = {Plastics from bacteria: natural functions and applications},
isbn       = {978-3-642-03286-8},
location   = {Heidelberg ; New York},
note       = {OCLC: ocn428029360},
pagetotal  = {450},
publisher  = {Springer},
series     = {Microbiology monographs ; v. 14},
keywords   = {Poly-beta-hydroxyalkanoates},
langid     = {english},
shorttitle = {Plastics from bacteria},
}
@Article{1993SwansonStarch,
author       = {Swanson, C. L. and Shogren, R. L. and Fanta, G. F. and Imam, S. H.},
date         = {1993-04},
journaltitle = {Journal of Environmental Polymer Degradation},
title        = {Starch-plastic materials-Preparation, physical properties, and biodegradability (a review of recent USDA research)},
doi          = {10.1007/BF01418208},
issn         = {1064-7564, 1572-8900},
number       = {2},
pages        = {155--166},
urldate      = {2019-04-07},
volume       = {1},
langid       = {english},
shorttitle   = {Starch-plastic materials?},
}
@Article{1997VanSoestInfluence,
author       = {Van Soest, J. J. G. and Knooren, N.},
date         = {1997-05-16},
journaltitle = {Journal of Applied Polymer Science},
title        = {Influence of glycerol and water content on the structure and properties of extruded starch plastic sheets during aging},
doi          = {10.1002/(SICI)1097-4628(19970516)64:7<1411::AID-APP21>3.0.CO;2-Y},
issn         = {0021-8995, 1097-4628},
number       = {7},
pages        = {1411--1422},
urldate      = {2019-04-07},
volume       = {64},
langid       = {english},
}
@Article{1998LuntLarge,
author       = {Lunt, James},
date         = {1998-01},
journaltitle = {Polymer Degradation and Stability},
title        = {Large-scale production, properties and commercial applications of polylactic acid polymers},
doi          = {10.1016/S0141-3910(97)00148-1},
issn         = {0141-3910},
number       = {1},
pages        = {145--152},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0141391097001481},
urldate      = {2019-04-07},
volume       = {59},
langid       = {english},
}
@Article{2000DrumrightPolylactic,
author       = {Drumright, R. E. and Gruber, P. R. and Henton, D. E.},
date         = {2000-12},
journaltitle = {Advanced Materials},
title        = {Polylactic Acid Technology},
doi          = {10.1002/1521-4095(200012)12:23<1841::AID-ADMA1841>3.0.CO;2-E},
issn         = {0935-9648, 1521-4095},
number       = {23},
pages        = {1841--1846},
urldate      = {2019-04-07},
volume       = {12},
file         = {:done/2000DrumrightPolylactic Polylactic Acid Technology.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2003LuengoBioplastics,
author       = {Luengo, José M. and Garcı́a, Belén and Sandoval, Angel and Naharro, Germán and Olivera, Elı́as R.},
date         = {2003-06},
journaltitle = {Current Opinion in Microbiology},
title        = {Bioplastics from microorganisms},
doi          = {10.1016/S1369-5274(03)00040-7},
issn         = {1369-5274},
number       = {3},
pages        = {251--260},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1369527403000407},
urldate      = {2019-04-07},
volume       = {6},
file         = {:done/2003LuengoBioplastics Bioplastics from Microorganisms.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2004DomenekBiodegradability,
author       = {Domenek, Sandra and Feuilloley, Pierre and Gratraud, Jean and Morel, Marie-Hélène and Guilbert, Stéphane},
date         = {2004-01},
journaltitle = {Chemosphere},
title        = {Biodegradability of wheat gluten based bioplastics},
doi          = {10.1016/S0045-6535(03)00760-4},
issn         = {0045-6535},
number       = {4},
pages        = {551--559},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0045653503007604},
urldate      = {2019-04-07},
volume       = {54},
abstract     = {A large variety of wheat gluten based bioplastics, which were plasticized with glycerol, were subjected to biodegradation. The materials covered the total range available for the biochemical control parameter Fi, which expresses the percentage of aggregated proteins. This quantity can be related to the density of covalent crosslinks in the wheat gluten network, which are induced by technological treatments. The biodegradability tests were performed in liquid medium (modiﬁed Sturm test) and in farmland soil. All gluten materials were fully degraded after 36 days in aerobic fermentation and within 50 days in farmland soil. No signiﬁcant diﬀerences were observed between the samples. The mineralization half-life time of 3.8 days in the modiﬁed Sturm test situated gluten materials among fast degrading polymers. The tests of microbial inhibition experiments revealed no toxic eﬀects of the modiﬁed gluten or of its metabolites. Thus, the protein bulk of wheat gluten materials is non-toxic and fully biodegradable, whatever the technological process applied.},
file         = {:done/2004DomenekBiodegradability Biodegradability of Wheat Gluten Based Bioplastics.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2008JacquelIsolation,
author       = {Jacquel, Nicolas and Lo, Chi-Wei and Wei, Yu-Hong and Wu, Ho-Shing and Wang, Shaw S.},
date         = {2008-04},
journaltitle = {Biochemical Engineering Journal},
title        = {Isolation and purification of bacterial poly(3-hydroxyalkanoates)},
doi          = {10.1016/j.bej.2007.11.029},
issn         = {1369-703X},
number       = {1},
pages        = {15--27},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1369703X07004433},
urldate      = {2019-04-07},
volume       = {39},
abstract     = {The isolation and the puriﬁcation of bacterial polyhydroxyalkanoates are the key step of the process proﬁtability in the fermentation system. That is why many scientists have studied this ﬁeld for the production of this biodegradable polymer. The ideal method should lead to a high purity and recovery level at a low production cost. This paper reviews four isolation methods, i.e. solvent extraction of halosolvent and nonhalosolvent, digestion of non-polyhydroxyalkanoate cell material involving surfactants, sodium hypochlorite or enzyme, mechanical cell disruption methods like using bead mills and high pressure homogenization, and new methods like spontaneous liberation of poly(3-hydroxybutyrate), dissolved air ﬂotation, air classiﬁcation, or by using supercritical CO2. The pretreatment of cell disruption and the puriﬁcation methods and analytical methods of polyhydroxyalkanoates are also presented.},
file         = {:done/2008JacquelIsolation Isolation and Purification of Bacterial Poly(3 Hydroxyalkanoates).pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2009SatyanarayanaBiodegradable,
author       = {Satyanarayana, Kestur G. and Arizaga, Gregorio G. C. and Wypych, Fernando},
date         = {2009-09},
journaltitle = {Progress in Polymer Science},
title        = {Biodegradable composites based on lignocellulosic fibers—An overview},
doi          = {10.1016/j.progpolymsci.2008.12.002},
issn         = {0079-6700},
number       = {9},
pages        = {982--1021},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0079670008001214},
urldate      = {2019-04-07},
volume       = {34},
abstract     = {The development of commercially viable "green products" based on natural resources for both matrices and reinforcements for a wide range of applications is on the rise. This effort includes new pathways to produce natural polymers with better mechanical properties and thermal stability using nanotechnology and use of natural polymers to make biodegradable plastics and their composites with lignocellulosic ﬁbers. This paper presents an overview of the developments made in the area of biodegradable composites, in terms of market, processing methods, matrix–reinforcement systems, morphology, properties and product development. Some critical issues and suggestions for future work are discussed, underscoring the roles of materials scientists and textile engineers for the future of these new "green" materials through value addition to enhance their use.},
file         = {:done/2009SatyanarayanaBiodegradable Biodegradable Composites Based on Lignocellulosic Fibers—An Overview.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{chee_bacterially_2010,
author = {Chee, Jiun-Yee and Yoga, Sugama-Salim and Lau, Nyok-Sean and Ling, Siew-Chen and Abed, Raeid M M and Sudesh, Kumar},
date   = {2010},
title  = {Bacterially Produced Polyhydroxyalkanoate (PHA): Converting Renewable Resources into Bioplastics},
pages  = {10},
langid = {english},
}
@Article{2010KeshavarzPolyhydroxyalkanoates,
author       = {Keshavarz, Tajalli and Roy, Ipsita},
date         = {2010-06},
journaltitle = {Current Opinion in Microbiology},
title        = {Polyhydroxyalkanoates: bioplastics with a green agenda},
doi          = {10.1016/j.mib.2010.02.006},
issn         = {1369-5274},
number       = {3},
pages        = {321--326},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1369527410000275},
urldate      = {2019-04-07},
volume       = {13},
langid       = {english},
shorttitle   = {Polyhydroxyalkanoates},
}
@Article{2010AkaraonyeProduction,
author       = {Akaraonye, Everest and Keshavarz, Tajalli and Roy, Ipsita},
date         = {2010-04-23},
journaltitle = {Journal of Chemical Technology ////////\\\& Biotechnology},
title        = {Production of polyhydroxyalkanoates: the future green materials of choice},
doi          = {10.1002/jctb.2392},
issn         = {02682575, 10974660},
number       = {6},
pages        = {732--743},
urldate      = {2019-04-07},
volume       = {85},
abstract     = {Polyhydroxyalkanoates (PHAs) have recently been the focus of attention as a biodegradable and biocompatible substitute for conventional non degradable plastics. The cost of large-scale production of these polymers has inhibited its widespread use. Thus, economical, large-scale production of PHAs is currently being studied intensively. Various bacterial strains, either wild-type or recombinant have been utilized with a wide spectrum of utilizable carbon sources. New fermentation strategies have been developed for the efﬁcient production of PHAs at high concentration and productivity. With the current advances, PHAs can now be produced to a concentration of 80 g L−1 with productivities greater than 4 g PHA L−1 h−1. These advances will further lower the production cost of PHAs and allow this family of polymers to become a leading biodegradable polymer in the near future. This review describes the properties of PHAs, their uses, the various attempts towards the production of PHAs, focusing on the utilization of cheap substrates and the development of different fermentation strategies for the production of these polymers, an essential step forward towards their widespread use.},
langid       = {english},
shorttitle   = {Production of polyhydroxyalkanoates},
}
@InCollection{2012KollerWhey,
author    = {Koller, Martin and Salerno, Anna and Muhr, Alexander and Reiterer, Angelika and Chiellini, Emo and Casella, Sergio and Horvat, Predrag and Braunegg, Gerhart},
booktitle = {Polyester},
date      = {2012-09-26},
title     = {Whey Lactose as a Raw Material for Microbial Production of Biodegradable Polyesters},
doi       = {10.5772/48737},
editor    = {Saleh, Hosam El-Din},
isbn      = {978-953-51-0770-5},
publisher = {InTech},
url       = {http://www.intechopen.com/books/polyester/whey-lactose-as-a-raw-material-for-microbial-production-of-biodegradable-polyesters},
urldate   = {2019-04-07},
langid    = {english},
}
@Article{2016Mozejko-ciesielskaBacterial,
author       = {Możejko-Ciesielska, Justyna and Kiewisz, Robert},
date         = {2016-11},
journaltitle = {Microbiological Research},
title        = {Bacterial polyhydroxyalkanoates: Still fabulous?},
doi          = {10.1016/j.micres.2016.07.010},
issn         = {0944-5013},
pages        = {271--282},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S094450131630043X},
urldate      = {2019-04-07},
volume       = {192},
abstract     = {Bacterial polyhydroxyalkanoates (PHA) are polyesters accumulated as carbon and energy storage materials under limited growth conditions in the presence of excess carbon sources. They have been developed as biomaterials with unique properties for the past many years being considered as a potential substitute for conventional non-degradable plastics. Due to the increasing concern towards global climate change, depleting petroleum resource and problems with an utilization of a growing number of synthetic plastics, PHAs have gained much more attention from industry and research. These environmentally friendly microbial polymers have great potential in biomedical, agricultural, and industrial applications. However, their production on a large scale is still limited. This paper describes the backgrounds of PHAs and discussed the current state of knowledge on the polyhydroxyalkanoates. Ability of bacteria to convert different carbon sources to PHAs, the opportunities and challenges of their introduction to global market as valuable renewable products have been also discussed.},
langid       = {english},
shorttitle   = {Bacterial polyhydroxyalkanoates},
}
@Article{2018RazaPolyhydroxyalkanoates,
author       = {Raza, Zulfiqar Ali and Abid, Sharjeel and Banat, Ibrahim M.},
date         = {2018-01},
journaltitle = {International Biodeterioration ////////\\\& Biodegradation},
title        = {Polyhydroxyalkanoates: Characteristics, production, recent developments and applications},
doi          = {10.1016/j.ibiod.2017.10.001},
issn         = {0964-8305},
pages        = {45--56},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0964830517300276},
urldate      = {2019-04-07},
volume       = {126},
abstract     = {Polyhydroxyalkanoates (PHAs) are biopolyesters, stored within cells as energy storage materials by various microorganisms. Due to their biocompatibility and biodegradability, PHAs have a wide range of applications in various industries such as biomedical sector including tissue engineering, bio-implant patches, drug delivery, surgery and wound dressing. PHAs are green plastics and they have positive social and environmental impact when compared with conventional plastics in terms of production and recycling. Moreover, PHAs do not possess acute and chronic health eﬀects when used in vivo. These bioplastics represent a renewable and sustainable resource to reduce landﬁll requirements without being persistence or causing pollution. A wide range of carbon sources, bacterial strains, fermentation conditions and recovery methods have been purposed by various researchers for better yield and economical perspectives. Recent advancements in synthetic biology and genetic engineering has led to the production of PHAs from non-PHAs producing strains with no toxins. Progression in recovery techniques has improved the extraction eﬃcacy from biomass with high purity. This review outlines production and characteristics of PHAs, developments in their production, and applications in various industries including nanotechnology.},
langid       = {english},
shorttitle   = {Polyhydroxyalkanoates},
}
@Misc{2017RosenheimFrequently,
author = {Rosenheim, HypoVereinsbank and De, Iban and Hyvedemm, Swift},
date   = {2017},
title  = {Frequently asked questions on bioplastics},
file   = {:done/2017RosenheimFrequently Frequently Asked Questions on Bioplastics.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
}
@Misc{2011BajpaiDurable,
author = {Bajpai, Pratima},
editor = {IntertechPira},
date   = {2011},
title  = {Durable Bioplastics},
file   = {:done/2011BajpaiDurable Durable Bioplastics.pdf:application/pdf},
groups = {tesse:5},
langid = {english},
pages  = {74},
year   = {2011},
}
@Article{2014AgustinBioplastic,
author       = {Agustin, Melissa B. and Ahmmad, Bashir and Alonzo, Shanna Marie M. and Patriana, Famille M.},
date         = {2014-12},
journaltitle = {Journal of Reinforced Plastics and Composites},
title        = {Bioplastic based on starch and cellulose nanocrystals from rice straw},
doi          = {10.1177/0731684414558325},
issn         = {0731-6844, 1530-7964},
number       = {24},
pages        = {2205--2213},
urldate      = {2019-04-07},
volume       = {33},
abstract     = {Bioplastic based on starch as the matrix and cellulose nanocrystals from rice straw as reinforcing filler were prepared in this study. The isolation of cellulose nanocrystal (CNC) followed a series of steps: delignification, sulfuric acid hydrolysis, and sonication. The process afforded short, rod-like CNCs with particle diameter ranging from 10 to 12 nm and crystallinity index of 76.1 /\\%. Fourier transform infrared analysis of the CNCs also confirmed absorption patterns typical of cellulose and the removal of silica. Bioplastic with different starch-to-CNC ratios were prepared by solution casting and evaporation method. Scanning electron micrographs of the films showed uniform dispersion of CNC in the starch matrix. Mechanical tests revealed that both tensile strength and modulus significantly increased with increasing CNC load while percent elongation decreased. The moisture uptake of the films reinforced with CNC also decreased an indication of improvement in water resistance. However, the thermal stability of the films decreased by the addition of CNC.},
file         = {:done/2014AgustinBioplastic Bioplastic Based on Starch and Cellulose Nanocrystals from Rice Straw.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015LiewEffect,
author       = {Liew, Kang Chiang and Khor, Lian Kim},
date         = {2015-07},
journaltitle = {Journal of King Saud University - Engineering Sciences},
title        = {Effect of different ratios of bioplastic to newspaper pulp fibres on the weight loss of bioplastic pot},
doi          = {10.1016/j.jksues.2013.08.001},
issn         = {1018-3639},
number       = {2},
pages        = {137--141},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1018363913000408},
urldate      = {2019-04-07},
volume       = {27},
abstract     = {Nowadays, industry is searching for an alternative to reduce the usage of petroleumbased non-degradable conventional seedling plant pots. In this study, three different types of bioplastic pots incorporated with newspaper pulp ﬁbres (the ratio of B75///////\\%:N25///////\\%, B50///////\\%:N50///////\\% and B25///////\\%:N75///////\\% denotes percentage of bioplastic to percentage of newspaper pulp ﬁbres) were produced while B0///////\\%:N100///////\\% acted as the control. All cylinder square shape moulded bioplastic pots with 100 mm height and 2 mm thickness were planted with Leucaena leucocephala seedlings for 60 days in two ground levels (below ground and above ground). Weight loss for bioplastic pots was evaluated. Results showed that bioplastic pots tested below ground had a higher percentage of weight loss than those planted above ground. For percentage of weight loss of bioplastic pots, most bioplastic pots that were tested in both ground levels only showed a signiﬁcant difference at p 6 0.05 after 30 days. Bioplastic pots B75///////\\%:N25///////\\% that were tested below ground have the highest percentage of weight loss with 77.93///////\\%. As conclusion, B50///////\\%:N50///////\\% is the most suitable ratio for the production of bioplastic pot.},
langid       = {english},
}
@Collection{2005MohantyNatural,
date      = {2005},
editor    = {Mohanty, Amar K. and Misra, Manjusri and Drzal, Lawrence T.},
title     = {Natural fibers, biopolymers, and biocomposites},
isbn      = {978-0-8493-1741-5},
location  = {Boca Raton, FL},
pagetotal = {875},
publisher = {Taylor //\\\& Francis},
file      = {:done/2005MohantyNatural Natural Fibers, Biopolymers, and Biocomposites.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Biopolymers, Fibers, Plant fibers, Polymeric composites},
langid    = {english},
}
@Misc{2017EuropePlastics,
author    = {Europe, Plastics},
date      = {2017},
title     = {Plastics – the Facts 2017},
file      = {:done/2017EuropePlastics Plastics – the Facts 2017.pdf:application/pdf},
groups    = {tesse:5},
publisher = {PlasticsEurope},
year      = {2017},
}
@Unpublished{2018BioplasticsBioplastics,
author    = {Bioplastics, European},
date      = {2018},
title     = {Bioplastics Fact Sheet},
file      = {:done/2018BioplasticsBioplastics Bioplastics Fact Sheet.pdf:application/pdf},
groups    = {tesse:5},
publisher = {European Bioplastics},
year      = {2018},
}
@Article{mohanty_sustainable_2002,
author = {Mohanty, A K and Misra, M and Drzal, L T},
date   = {2002},
title  = {Sustainable Bio-Composites from Renewable Resources: Opportunities and Challenges in the Green Materials World},
pages  = {8},
langid = {english},
}
@Article{2008SudeshSustainability,
author       = {Sudesh, Kumar and Iwata, Tadahisa},
date         = {2008-06},
journaltitle = {CLEAN - Soil, Air, Water},
title        = {Sustainability of Biobased and Biodegradable Plastics},
doi          = {10.1002/clen.200700183},
issn         = {18630650, 18630669},
number       = {5},
pages        = {433--442},
urldate      = {2019-04-07},
volume       = {36},
file         = {:done/2008SudeshSustainability Sustainability of Biobased and Biodegradable Plastics.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2009ThompsonPlastics,
author       = {Thompson, Richard C. and Moore, Charles J. and vom Saal, Frederick S. and Swan, Shanna H.},
date         = {2009-07-27},
journaltitle = {Philosophical Transactions of the Royal Society B: Biological Sciences},
title        = {Plastics, the environment and human health: current consensus and future trends},
doi          = {10.1098/rstb.2009.0053},
issn         = {0962-8436, 1471-2970},
number       = {1526},
pages        = {2153--2166},
urldate      = {2019-04-07},
volume       = {364},
langid       = {english},
shorttitle   = {Plastics, the environment and human health},
}
@Article{2010TaboneSustainability,
author       = {Tabone, Michaelangelo D. and Cregg, James J. and Beckman, Eric J. and Landis, Amy E.},
date         = {2010-11},
journaltitle = {Environmental Science ////////\\\& Technology},
title        = {Sustainability Metrics: Life Cycle Assessment and Green Design in Polymers},
doi          = {10.1021/es101640n},
issn         = {0013-936X, 1520-5851},
number       = {21},
pages        = {8264--8269},
urldate      = {2019-04-07},
volume       = {44},
langid       = {english},
shorttitle   = {Sustainability Metrics},
}
@Article{2012KaranaCharacterization,
author       = {Karana, Elvin},
date         = {2012-12},
journaltitle = {Journal of Cleaner Production},
title        = {Characterization of ‘natural’ and ‘high-quality’ materials to improve perception of bio-plastics},
doi          = {10.1016/j.jclepro.2012.07.034},
issn         = {0959-6526},
pages        = {316--325},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0959652612003721},
urldate      = {2019-04-07},
volume       = {37},
abstract     = {Over the past decade, the deployment of sustainable product design has led to a dramatic increase in the use of bio-plastics as an environmentally sensitive substitute to regular petroleum-based ones. Published literature has explored the environmental performance and their suitability as an alternative for regular plastics. However, the reception of these materials by users, who come into contact with these materials embodied in consumer products, has not been researched and published. Even though the principle of using such materials with improved environmental credentials is sound, it is down to the users’ appreciation of those materials that ultimately determine their commercial success. A signiﬁcant challenge faced by material developers and product designers is to facilitate the appraisal of bio-plastics as a natural alternative to regular plastics, whilst at the same time meeting users’ perceptions of quality.},
langid       = {english},
}
@Article{2013PhilpBioplastics,
author       = {Philp, Jim C. and Bartsev, Alexandre and Ritchie, Rachael J. and Baucher, Marie-Ange and Guy, K.},
date         = {2013-09},
journaltitle = {New Biotechnology},
title        = {Bioplastics science from a policy vantage point},
doi          = {10.1016/j.nbt.2012.11.021},
issn         = {1871-6784},
number       = {6},
pages        = {635--646},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1871678412008783},
urldate      = {2019-04-07},
volume       = {30},
file         = {:done/2013PhilpBioplastics Bioplastics Science from a Policy Vantage Point.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014VanCauwenbergheMicroplastics,
author       = {Van Cauwenberghe, Lisbeth and Janssen, Colin R.},
date         = {2014-10},
journaltitle = {Environmental Pollution},
title        = {Microplastics in bivalves cultured for human consumption},
doi          = {10.1016/j.envpol.2014.06.010},
issn         = {0269-7491},
pages        = {65--70},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0269749114002425},
urldate      = {2019-04-07},
volume       = {193},
abstract     = {Microplastics are present throughout the marine environment and ingestion of these plastic particles (<1 mm) has been demonstrated in a laboratory setting for a wide array of marine organisms. Here, we investigate the presence of microplastics in two species of commercially grown bivalves: Mytilus edulis and Crassostrea gigas. Microplastics were recovered from the soft tissues of both species. At time of human consumption, M. edulis contains on average 0.36 ± 0.07 particles gÀ1 (wet weight), while a plastic load of 0.47 ± 0.16 particles gÀ1 ww was detected in C. gigas. As a result, the annual dietary exposure for European shellﬁsh consumers can amount to 11,000 microplastics per year. The presence of marine microplastics in seafood could pose a threat to food safety, however, due to the complexity of estimating microplastic toxicity, estimations of the potential risks for human health posed by microplastics in food stuffs is not (yet) possible.},
file         = {:done/2014VanCauwenbergheMicroplastics Microplastics in Bivalves Cultured for Human Consumption.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014LawMicroplastics,
author       = {Law, K. L. and Thompson, R. C.},
date         = {2014-07-11},
journaltitle = {Science},
title        = {Microplastics in the seas},
doi          = {10.1126/science.1254065},
issn         = {0036-8075, 1095-9203},
number       = {6193},
pages        = {144--145},
urldate      = {2019-04-07},
volume       = {345},
file         = {:done/2014LawMicroplastics Microplastics in the Seas.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015LiMicroplastics,
author       = {Li, Jiana and Yang, Dongqi and Li, Lan and Jabeen, Khalida and Shi, Huahong},
date         = {2015-12},
journaltitle = {Environmental Pollution},
title        = {Microplastics in commercial bivalves from China},
doi          = {10.1016/j.envpol.2015.09.018},
issn         = {0269-7491},
pages        = {190--195},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0269749115300658},
urldate      = {2019-04-07},
volume       = {207},
abstract     = {We investigated microplastic pollution in 9 commercial bivalves from a ﬁshery market in China. Multiple types of microplastics, including ﬁbers, fragments and pellets, occurred in the tissue of all bivalves. The number of total microplastics varied from 2.1 to 10.5 items/g and from 4.3 to 57.2 items/individual for bivalves. Scapharca subcrenata contained on average 10.5 items/g and exhibited the highest levels of microplastics by weight. Fibers were the most common microplastics and consisted of more than half of the total microplastics in each of the 8 species. In Alectryonella plicatula, pellets accounted for 60 /\\% of the total microplastics. The most common size class was less than 250 mm and accounted for 33e84 /\\% of the total microplastics calculated by species. Our results suggest that microplastic pollution was widespread and exhibited a relatively high level in commercial bivalves from China. More intensive investigations on microplastics should be conducted in seafood.},
file         = {:done/2015LiMicroplastics Microplastics in Commercial Bivalves from China.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016Costananoplastics,
author       = {da Costa, João Pinto and Santos, Patrícia S. M. and Duarte, Armando C. and Rocha-Santos, Teresa},
date         = {2016-10},
journaltitle = {Science of The Total Environment},
title        = {(Nano)plastics in the environment – Sources, fates and effects},
doi          = {10.1016/j.scitotenv.2016.05.041},
issn         = {0048-9697},
pages        = {15--26},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0048969716309731},
urldate      = {2019-04-07},
volume       = {566-567},
abstract     = {There has been a considerable increase on research of the ecological consequences of microplastics released into the environment, but only a handful of works have focused on the nano-sized particles of polymer-based materials. Though their presence has been difﬁcult to adequately ascertain, due to the inherent technical difﬁculties for isolating and quantifying them, there is an overall consensus that these are not only present in the environment –either directly released or as the result of weathering of larger fragments – but that they also pose a signiﬁcant threat to the environment and human health, as well. The reduced size of these particulates (b 1 μm) makes them susceptible of ingestion by organisms that are at the base of the food-chain. Moreover, the characteristic high surface area-to-volume ratio of nanoparticles may add to their potential hazardous effects, as other contaminants, such as persistent organic pollutants, could be adsorbed and undergo bioaccumulation and bioampliﬁcation phenomena.},
file         = {:done/2016Costananoplastics (Nano)plastics in the Environment – Sources, Fates and Effects.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016BrockhausCrossroads,
author       = {Brockhaus, Sebastian and Petersen, Moritz and Kersten, Wolfgang},
date         = {2016-07},
journaltitle = {Journal of Cleaner Production},
title        = {A crossroads for bioplastics: exploring product developers' challenges to move beyond petroleum-based plastics},
doi          = {10.1016/j.jclepro.2016.04.003},
issn         = {0959-6526},
pages        = {84--95},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0959652616302414},
urldate      = {2019-04-07},
volume       = {127},
abstract     = {Bioplastics play an increasingly important role for consumer products. These new materials might increase product sustainability but they are currently confined to niche markets. While research has gained important insight into the technical challenges, few studies to date explore the behavioral aspects for product developers as they move to employ bioplastics in their development efforts. This manuscript reports the findings of a grounded inductive study based on interview data with 32 product developers in the consumer goods industry. The Theory of Planned Behavior is employed to guide the research and provide a theoretical background to derive implications. The study finds that behavioral challenges impede the increased use of bioplastics. Product developers experience a lack of perceived behavioral control and struggle with doubts about the environmental benefits and incurring trade-offs of bioplastics with respect to the Triple Bottom Line. While product developers are intrinsically motivated to make more use of bioplastics, they often refrain from bringing products to the mass market due to uncertainties of customer receptiveness and fears of greenwashing allegations. Implications for industry and research are detailed.},
langid       = {english},
shorttitle   = {A crossroads for bioplastics},
}
@Article{2016SijtsemaConsumer,
author       = {Sijtsema, Siet J. and Onwezen, Marleen C. and Reinders, Machiel J. and Dagevos, Hans and Partanen, Asta and Meeusen, Marieke},
date         = {2016-06},
journaltitle = {NJAS - Wageningen Journal of Life Sciences},
title        = {Consumer perception of bio-based products—An exploratory study in 5 European countries},
doi          = {10.1016/j.njas.2016.03.007},
issn         = {1573-5214},
pages        = {61--69},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1573521416300070},
urldate      = {2019-04-07},
volume       = {77},
abstract     = {This study explores people’s perceptions (i.e., positive and negative associations, mixed feelings) regarding the concept of ‘bio-based’ in general and speciﬁc bio-based products. This exploratory study is one of the ﬁrst consumer studies in the ﬁeld of bio-based research. Three focus group discussions were organized in the Czech Republic, Denmark, Germany, Italy, and The Netherlands (with 89 participants in total) in which projective techniques were applied.},
langid       = {english},
}
@Article{2017CecchiniBioplastics,
author       = {Cecchini, Cecilia},
date         = {2017-07-28},
journaltitle = {The Design Journal},
title        = {Bioplastics made from upcycled food waste. Prospects for their use in the field of design},
doi          = {10.1080/14606925.2017.1352684},
issn         = {1460-6925, 1756-3062},
issue        = {sup1},
pages        = {S1596--S1610},
urldate      = {2019-04-07},
volume       = {20},
abstract     = {The negative effects on the environment of the intensive use of synthetic, oil-derived plastics to make products have given renewed impetus to the search for biopolymers derived from vegetable, animal or microbial matter that could prove to be a sound alternative in a number of applications. However, the real challenge is to create new materials from food waste and not from specially grown crops, whose production in any case comes at an environmental cost.},
langid       = {english},
}
@Article{2018LebretonEvidence,
author       = {Lebreton, L. and Slat, B. and Ferrari, F. and Sainte-Rose, B. and Aitken, J. and Marthouse, R. and Hajbane, S. and Cunsolo, S. and Schwarz, A. and Levivier, A. and Noble, K. and Debeljak, P. and Maral, H. and Schoeneich-Argent, R. and Brambini, R. and Reisser, J.},
date         = {2018-12},
journaltitle = {Scientific Reports},
title        = {Evidence that the Great Pacific Garbage Patch is rapidly accumulating plastic},
doi          = {10.1038/s41598-018-22939-w},
issn         = {2045-2322},
number       = {1},
url          = {http://www.nature.com/articles/s41598-018-22939-w},
urldate      = {2019-04-07},
volume       = {8},
file         = {:done/2018LebretonEvidence Evidence That the Great Pacific Garbage Patch Is Rapidly Accumulating Plastic.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015RognoliDiy,
author       = {Rognoli, Valentina and Bianchini, Massimo and Maffei, Stefano and Karana, Elvin},
date         = {2015-12},
journaltitle = {Materials //\\\& Design},
title        = {DIY materials},
doi          = {10.1016/j.matdes.2015.07.020},
issn         = {0264-1275},
pages        = {692--702},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0264127515300964},
urldate      = {2019-04-07},
volume       = {86},
abstract     = {The democratization of personal fabrication technologies in parallel to the rising desire of individuals for personalizing their products offers great opportunities to experiment with advanced, distributed and shared production processes as well as design new materials. In this article, we introduce the notion of Do-It-Yourself (DIY) Materials, which are created through individual or collective self-production practices, often by techniques and processes of the designer‟s own invention. They can be totally new materials, modified, or further developed versions of existing materials. In order to provide an operational vocabulary to discuss DIY materials, we have collected 27 DIY material cases developed in the last five years. We group the collected cases under two main categories: (1) DIY new materials: which focus on creative material ingredients (e.g. a material made of dried, blended waste citrus peel combined with natural binders); and (2) DIY new identities for conventional materials: which focus on new production techniques, giving new expressions to existing materials (i.e. they do not necessarily contain new ingredients, such as 3D printed metal). Grounded on the commonalities of collected cases, we discuss the design opportunities, including new aesthetic impressions offered through DIY material design practices.},
file         = {:done/2015RognoliDiy DIY Materials.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2015KaranaMethod,
author       = {Karana, Elvin and Barati, Bahareh and Rognoli, Valentina},
date         = {2015},
journaltitle = {International Journal Of Design},
title        = {A Method to Design for Material Experiences},
number       = {2},
pages        = {20},
volume       = {9},
file         = {:done/2015KaranaMethod A Method to Design for Material Experiences.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016RognoliMaterial,
author   = {Rognoli, Valentina and Garcia, Camilo Ayala and Parisi, Stefano},
date     = {2016},
title    = {The material experiences as DIY-Materials:},
pages    = {9},
volume   = {4},
abstract = {This paper presents the growing phenomena of self-produced materials, namely ‘Do-It-Yourself (DIY) Materials’, and illustrates the pattern of conditions and circumstances which led to the emergence of this approach to materials. The DIY Materials phenomena is derived from a broader concept defined as ‘emerging materials experiences’ and through their definition we think in new and meaningful interaction between users and these new materials qualities derived by a self-production process. Through the presentation of a case study, NeWool, a bio composite made of starch and wool fibres, we outline a design practice based on direct experimentation, tinkering and envisioning for designer.},
file     = {:done/2016RognoliMaterial The Material Experiences As DIY Materials/_.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2017KaranaGrowing,
author   = {Karana, Elvin},
date     = {2017},
title    = {Growing materials for product design},
pages    = {16},
abstract = {The possibility to fabricate materials from living organisms offers appealing advantages for product design, such as higher sustainability and an interesting novel aesthetics. Several designers are now ‘growing’ their own materials. Despite the large interest shown, this emerging material practice is still scarcely understood in design literature. The aim of this paper is to shed light on what it means to design with growing organisms as collaborators, identifying the defining traits of this novel, designerly way of ‘doing materials’. To do so, we first compare this specific approach to the approaches of others working in the intersections of biology and design. In this way, we outline the boundaries of Growing Design, defining its unique characteristics. We then provide detailed descriptions of three classes of Growing Materials: fungal, bacterial and algal materials. For each class, we bring two examples of designers utilizing these materials for industrial design purposes. This helps to further explain what truly distinguishes Growing Materials from other conventional materials and to understand the challenges in working with them. Finally, this discussion enables us to set out a research agenda for Growing Design, supporting the development of these materials for industrial production.},
file     = {:done/2017KaranaGrowing Growing Materials for Product Design.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2018CamereFabricating,
author       = {Camere, Serena and Karana, Elvin},
date         = {2018-06},
journaltitle = {Journal of Cleaner Production},
title        = {Fabricating materials from living organisms: An emerging design practice},
doi          = {10.1016/j.jclepro.2018.03.081},
issn         = {0959-6526},
pages        = {570--584},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0959652618307388},
urldate      = {2019-04-07},
volume       = {186},
abstract     = {Biotechnology offers exciting opportunities for novel and more sustainable alternatives for the design and manufacturing of products. One of the most promising approaches is the fabrication of materials from living organisms, such as fungi and bacteria. An increasing number of designers are engaging in this "Growing Design" practice, exploring the unique potentials of the grown materials for product design. In Growing Design, designers operate in interdisciplinary contexts, engaging in early stage material developments. Despite the widespread interest towards Growing Design, no systematic study has been conducted so far to understand how this practice unfolds and its contribution to the progression towards cleaner production. To this end, eight recognized professionals in the field were interviewed. The results illustrate how the conception of materials in design evolves when designers co-perform with biological organisms. This alters how the design process unfolds and the mindset adopted in design practice, shaping a novel, systemic vision on production and consumption practices. The paper further discusses the need for developing new sensibilities to face complex interdisciplinary problems in Growing Design and highlights the role designers can take in developing new materials for sustainable production.},
langid       = {english},
shorttitle   = {Fabricating materials from living organisms},
}
@Article{2017BridgensDesign,
author       = {Bridgens, Ben and Lilley, Debra},
date         = {2017-07-28},
journaltitle = {The Design Journal},
title        = {Design for Next… Year. The Challenge of Designing for Material Change},
doi          = {10.1080/14606925.2017.1352715},
issn         = {1460-6925, 1756-3062},
issue        = {sup1},
pages        = {S160--S171},
urldate      = {2019-04-07},
volume       = {20},
abstract     = {From the moment of purchase, pristine objects are subjected to an array of stimuli including wear, impact, heat, light, water and air which alter their tactile and aesthetic properties. Material change is often regarded as ‘damage’ or ‘degradation’, but has potential to be used as a tool to engender emotional engagement to an object. We present a framework for designers to better understand how materials change with use, and in turn how people respond to materials as they change. Key challenges are identified which must be overcome to use this framework in design practice – people’s physical interaction with objects is poorly understood, it is difficult to simulate material change, materials resources for designers do not provide information about material change, and people’s responses to aged materials depend on a complex web of interacting factors.},
langid       = {english},
}
@Article{2008JohnBiofibres,
author       = {John, M. and Thomas, S.},
date         = {2008-02-08},
journaltitle = {Carbohydrate Polymers},
title        = {Biofibres and biocomposites},
doi          = {10.1016/j.carbpol.2007.05.040},
issn         = {0144-8617},
number       = {3},
pages        = {343--364},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0144861707002974},
urldate      = {2019-04-07},
volume       = {71},
abstract     = {This review deals with a recent study of the literature on the various aspects of cellulosic ﬁbres and biocomposites. Cellulosic ﬁbre reinforced polymeric composites are ﬁnding applications in many ﬁelds ranging from construction industry to automotive industry. The pros and cons of using these ﬁbres are enumerated in this review. The classiﬁcation of composites into green composites, hybrid biocomposites and textile biocomposites are discussed. New developments dealing with cellulose based nanocomposites and electrospinning of nanoﬁbres have also been presented. Recent studies pertaining to the above topics have also been cited. Finally, the applications of cellulosic ﬁbre reinforced polymeric composites have been highlighted.},
file         = {:done/2008JohnBiofibres Biofibres and Biocomposites.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014ThakurReview,
author       = {Thakur, Vijay Kumar and Thakur, Manju Kumari and Gupta, Raju Kumar},
date         = {2014-04-03},
journaltitle = {International Journal of Polymer Analysis and Characterization},
title        = {Review: Raw Natural Fiber–Based Polymer Composites},
doi          = {10.1080/1023666X.2014.880016},
issn         = {1023-666X, 1563-5341},
number       = {3},
pages        = {256--271},
urldate      = {2019-04-07},
volume       = {19},
langid       = {english},
shorttitle   = {Review},
}
@Article{2016PickeringReview,
author       = {Pickering, K.L. and Efendy, M.G. Aruan and Le, T.M.},
date         = {2016-04},
journaltitle = {Composites Part A: Applied Science and Manufacturing},
title        = {A review of recent developments in natural fibre composites and their mechanical performance},
doi          = {10.1016/j.compositesa.2015.08.038},
issn         = {1359-835X},
pages        = {98--112},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S1359835X15003115},
urldate      = {2019-04-07},
volume       = {83},
abstract     = {Recently, there has been a rapid growth in research and innovation in the natural ﬁbre composite (NFC) area. Interest is warranted due to the advantages of these materials compared to others, such as synthetic ﬁbre composites, including low environmental impact and low cost and support their potential across a wide range of applications. Much effort has gone into increasing their mechanical performance to extend the capabilities and applications of this group of materials. This review aims to provide an overview of the factors that affect the mechanical performance of NFCs and details achievements made with them.},
langid       = {english},
}
@Article{2016OmraniState,
author       = {Omrani, Emad and Menezes, Pradeep L. and Rohatgi, Pradeep K.},
date         = {2016-06},
journaltitle = {Engineering Science and Technology, an International Journal},
title        = {State of the art on tribological behavior of polymer matrix composites reinforced with natural fibers in the green materials world},
doi          = {10.1016/j.jestch.2015.10.007},
issn         = {2215-0986},
number       = {2},
pages        = {717--736},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S221509861500172X},
urldate      = {2019-04-07},
volume       = {19},
abstract     = {Natural ﬁber reinforced polymer composites have emerged as a potential environmentally friendly and cost-effective alternative to synthetic ﬁber reinforced composites. Therefore, in the past decade, a number of major industries, such as the automotive, construction and packaging industries, have shown a considerable interest in the progress of new natural ﬁber reinforced composite materials. The availability of natural ﬁbers and the ease of manufacturing have tempted researchers to study their feasibility of their application as reinforcement and the extent to which they satisfy the required speciﬁcations in tribological applications. However, less information concerning the tribological performance of natural ﬁber reinforced composite material is available in the literature. Hence, the aim of this bibliographic review is to demonstrate the tribological behavior of natural ﬁber reinforced composites and ﬁnd a knowledge about their usability for various applications that tribology plays a dominant role. This review presents the reported work on natural ﬁber reinforced composites with special reference to the type of ﬁbers, matrix polymers, treatment of ﬁbers and test parameters. The results show that composites reinforced with natural ﬁbers have an improvement in tribological properties and their properties are comparable with conventional ﬁbers. In addition, ﬁber treatment and ﬁber orientation are two important factors can affect tribological properties where treated ﬁbers and normal oriented ﬁbers exhibit better friction and wear behavior. This review is trying to evaluate the effect of test parameter including normal load and sliding speed on tribological properties, and the results vary based on type of reinforcement. Generally, due to their positive economic and environmental aspects, as well as their good tribological properties, natural composites are showing a good potential for employing in several applications.},
langid       = {english},
}
@Article{2018SanjayCharacterization,
author       = {Sanjay, M.R. and Madhu, P. and Jawaid, Mohammad and Senthamaraikannan, P. and Senthil, S. and Pradeep, S.},
date         = {2018-01},
journaltitle = {Journal of Cleaner Production},
title        = {Characterization and properties of natural fiber polymer composites: A comprehensive review},
doi          = {10.1016/j.jclepro.2017.10.101},
issn         = {0959-6526},
pages        = {566--581},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0959652617323946},
urldate      = {2019-04-07},
volume       = {172},
abstract     = {The world is in need of more eco-friendly material, therefore researchers around the globe focus on developing new materials that would improve the environmental quality of products. This need for new green materials has led to the utilization of composites made from raw natural fibers and polymer matrices, and this has become one of the most widely investigated research topics in recent times. Natural fiber composites are an alternative for replacing environmentally harmful synthetic materials and help control pollution problems. In addition, they are low cost, have better mechanical properties and require low production energy consumption. Also, using such materials in construction works, it is possible to improve the sustainability by eliminating construction wastes. Keeping in view all the benefits of natural fiber reinforced polymer composites, this paper first discusses various fabrication techniques employed for the production of these composites and then presents a detailed review of the research devoted to the analysis of their structure and properties by a variety of characterization techniques.},
langid       = {english},
shorttitle   = {Characterization and properties of natural fiber polymer composites},
}
@Article{2018ElanchezhianReview,
author       = {Elanchezhian, C. and Ramnath, B. Vijaya and Ramakrishnan, G. and Rajendrakumar, M. and Naveenkumar, V. and Saravanakumar, M. K.},
date         = {2018},
journaltitle = {Materials Today: Proceedings},
title        = {Review on mechanical properties of natural fiber composites.},
doi          = {10.1016/j.matpr.2017.11.276},
issn         = {2214-7853},
number       = {1},
pages        = {1785--1790},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S2214785317325440},
urldate      = {2019-04-07},
volume       = {5},
abstract     = {The present experimental study aims at learning the mechanical behavior of natural fiber composites. Natural fibers have an attracting the interest to engineers, researchers, professionals and scientists all over the world as an alternative reinforcement, because of its superior properties such as high specific strength, low weight, low cost, fairly good mechanical properties, nonabrasive, eco-friendly and bio-degradable characteristics. A brief review has been carried out to make use of natural fibers (such as abaca, jute, sisal, banana, cotton, coir, hemp, etc) abundantly available in India. This paper presents a review on the mechanical properties of Abaca, Jute, Sisal.},
file         = {:done/2018ElanchezhianReview Review on Mechanical Properties of Natural Fiber Composites..pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2007NeillHow,
author       = {Neill, Ushma S.},
date         = {2007-12-03},
journaltitle = {Journal of Clinical Investigation},
title        = {How to write a scientific masterpiece},
doi          = {10.1172/JCI34288},
issn         = {0021-9738},
number       = {12},
pages        = {3599--3602},
url          = {http://www.jci.org/articles/view/34288},
urldate      = {2019-04-07},
volume       = {117},
abstract     = {The abstract is your hook, the most important information readers use when deciding whether to abandon your article or read it more in depth. It pays to make it easy to understand and broadly appealing; informative but not too detailed. What is the formula for such a masterpiece? Here is one that we try to follow as best we can: first, start with a sentence or two that frames the work. Introduce the disease and system you are studying and indicate what was previously unknown — framing why we are here now. Move on to the major finding, then spend a few sentences detailing the steps and mechanisms uncovered. Make sure to indicate the species studied, especially when this changes with different experiments. End the abstract with a sentence or two indicating the implications of the work without inflating the relevance.},
file         = {:done/2007NeillHow How to Write a Scientific Masterpiece.pdf:application/pdf},
groups       = {tesse:5, Writing},
langid       = {english},
}
@Article{2011AndradeHow,
author       = {Andrade, Chittaranjan},
date         = {2011},
journaltitle = {Indian Journal of Psychiatry},
title        = {How to write a good abstract for a scientific paper or conference presentation},
doi          = {10.4103/0019-5545.82558},
issn         = {0019-5545},
number       = {2},
pages        = {172},
url          = {http://www.indianjpsychiatry.org/text.asp?2011/53/2/172/82558},
urldate      = {2019-04-07},
volume       = {53},
abstract     = {Abstracts of scientific papers are sometimes poorly written, often lack important information, and occasionally convey a biased picture. This paper provides detailed suggestions, with examples, for writing the background, methods, results, and conclusions sections of a good abstract. The primary target of this paper is the young researcher; however, authors with all levels of experience may find useful ideas in the paper.},
file         = {:done/2011AndradeHow How to Write a Good Abstract for a Scientific Paper or Conference Presentation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014DerntlBasics,
author       = {Derntl, Michael},
date         = {2014},
journaltitle = {International Journal of Technology Enhanced Learning},
title        = {Basics of research paper writing and publishing},
doi          = {10.1504/IJTEL.2014.066856},
issn         = {1753-5255, 1753-5263},
number       = {2},
pages        = {105},
url          = {http://www.inderscience.com/link.php?id=66856},
urldate      = {2019-04-07},
volume       = {6},
abstract     = {Publishing research results is an integral part of a researcher’s professional life. However, writing is not every researcher’s favourite activity, and getting a paper published can be a very tedious and time-consuming process. Fortunately, many of the obstacles along the writing and publishing path can be avoided by following some simple guidelines and practices. This paper presents a synthesis of guidelines found in literature about structuring and writing scientific papers. The paper outlines the process of publishing research papers in journals and conference proceedings, aiming to provide early-stage researchers with a handy introduction to essential issues. The paper takes an interdisciplinary stance by giving examples from technology-enhanced learning research and borrowing from literature in social, natural and computing sciences.},
file         = {:done/2014DerntlBasics Basics of Research Paper Writing and Publishing.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{hanauer_one_nodate,
author = {Hanauer, Sue},
title  = {One picture is worth a thousand words.},
pages  = {4},
langid = {english},
}
@Misc{2018ElsevierElsevier,
author = {Elsevier},
date   = {2018},
title  = {Elsevier How to publish in scholarly journals},
file   = {:done/2018ElsevierElsevier Elsevier How to Publish in Scholarly Journals.pdf:application/pdf},
groups = {tesse:5},
}
@Book{2013TurabianManual,
author     = {Turabian, Kate L.},
date       = {2013-03-28},
title      = {A Manual for Writers of Research Papers, Theses, and Dissertations},
edition    = {8},
editor     = {Booth, Wayne C. and Colomb, Gregory G. and Williams, Joseph M. and Staff, University of Chicago Press},
isbn       = {978-0-226-81638-8},
location   = {Chicago},
pagetotal  = {464},
publisher  = {University of Chicago Press},
abstract   = {A little more than seventy-five years ago, Kate L. Turabian drafted a set of guidelines to help students understand how to write, cite, and formally submit research writing. Seven editions and more than nine million copies later, the name Turabian has become synonymous with best practices in research writing and style. Her Manual for Writers continues to be the gold standard for generations of college and graduate students in virtually all academic disciplines. Now in its eighth edition, A Manual for Writers of Research Papers, Theses, and Dissertations has been fully revised to meet the needs of today’s writers and researchers.The Manual retains its familiar three-part structure, beginning with an overview of the steps in the research and writing process, including formulating questions, reading critically, building arguments, and revising drafts. Part II provides an overview of citation practices with detailed information on the two main scholarly citation styles (notes-bibliography and author-date), an array of source types with contemporary examples, and detailed guidance on citing online resources.The final section treats all matters of editorial style, with advice on punctuation, capitalization, spelling, abbreviations, table formatting, and the use of quotations. Style and citation recommendations have been revised throughout to reflect the sixteenth edition of The Chicago Manual of Style. With an appendix on paper format and submission that has been vetted by dissertation officials from across the country and a bibliography with the most up-to-date listing of critical resources available, A Manual for Writers remains the essential resource for students and their teachers.},
file       = {:done/2013TurabianManual A Manual for Writers of Research Papers, Theses, and Dissertations.epub:ePUB},
groups     = {tesse:5},
shorttitle = {A Manual for Writers of Research Papers, Theses, and Dissertations, Eighth Edition},
}
@Book{2013O′learyEssential,
author    = {O′Leary, Zina},
date      = {2013-12-20},
title     = {The Essential Guide to Doing Your Research Project},
edition   = {Second edition},
isbn      = {978-1-4462-5897-2},
location  = {Los Angeles},
pagetotal = {384},
publisher = {SAGE Publications Ltd},
abstract  = {The Essential Guide to Doing Your Research Project 2e is the ultimate companion to successfully completing your research project. Warm and pragmatic, it gives you the skills and the confidence needed to succeed no matter what happens along the way. The book guides you through every step of your research project, from getting started to analysing data and writing up. Each stage is clearly set out, highlighting best practice and providing practical tips and down-to-earth advice for actually doing research. Key features include: Fully developed companion website including podcasts, worksheets, examples of real projects and links to journal articles Chapter summaries Boxed definitions of key terms Full glossary Suggestions for further reading Bursting with real world examples and multidisciplinary case studies, this book addresses the key questions posed by anyone hoping to complete a research project. It is the must-have textbook every student needs. Available with Perusall―an eBook that makes it easier to prepare for class Perusall is an award-winning eBook platform featuring social annotation tools that allow students and instructors to collaboratively mark up and discuss their SAGE textbook. Backed by research and supported by technological innovations developed at Harvard University, this process of learning through collaborative annotation keeps your students engaged and makes teaching easier and more effective. Learn more.},
file      = {:done/2013O′learyEssential The Essential Guide to Doing Your Research Project.epub:ePUB},
groups    = {tesse:5},
}
@Article{2015SanyangEffect,
author       = {Sanyang, Muhammed and Sapuan, Salit and Jawaid, Mohammad and Ishak, Mohamad and Sahari, Japar},
date         = {2015-06-18},
journaltitle = {Polymers},
title        = {Effect of Plasticizer Type and Concentration on Tensile, Thermal and Barrier Properties of Biodegradable Films Based on Sugar Palm (Arenga pinnata) Starch},
doi          = {10.3390/polym7061106},
issn         = {2073-4360},
number       = {6},
pages        = {1106--1124},
url          = {http://www.mdpi.com/2073-4360/7/6/1106},
urldate      = {2019-04-07},
volume       = {7},
langid       = {english},
}
@Article{2016MendesBiodegradable,
author       = {Mendes, J.F. and Paschoalin, R.T and Carmona, V.B. and Sena Neto, Alfredo R and Marques, A.C.P. and Marconcini, J.M. and Mattoso, L.H.C. and Medeiros, E.S. and Oliveira, J.E.},
date         = {2016-02},
journaltitle = {Carbohydrate Polymers},
title        = {Biodegradable polymer blends based on corn starch and thermoplastic chitosan processed by extrusion},
doi          = {10.1016/j.carbpol.2015.10.093},
issn         = {0144-8617},
pages        = {452--458},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0144861715010735},
urldate      = {2019-04-07},
volume       = {137},
abstract     = {Blends of thermoplastic cornstarch (TPS) and chitosan (TPC) were obtained by melt extrusion. The effect of TPC incorporation in TPS matrix and polymer interaction on morphology and thermal and mechanical properties were investigated. Possible interactions between the starch molecules and thermoplastic chitosan were assessed by XRD and FTIR techniques. Scanning Electron Microscopy (SEM) analyses showed a homogeneous fracture surface without the presence of starch granules or chitosan aggregates. Although the incorporation of thermoplastic chitosan caused a decrease in both tensile strength and stiffness, ﬁlms with better extensibility and thermal stability were produced.},
langid       = {english},
}
@Article{2018WahyuningtiyasProperties,
author       = {Wahyuningtiyas, Nanang Eko and Suryanto, Heru},
date         = {2018-06-15},
journaltitle = {Journal of Mechanical Engineering Science and Technology},
title        = {Properties of Cassava Starch based Bioplastic Reinforced by Nanoclay},
doi          = {10.17977/um016v2i12018p020},
issn         = {25800817, 25802402},
number       = {1},
pages        = {20--26},
url          = {http://journal2.um.ac.id/index.php/jmest/article/view/5066},
urldate      = {2019-04-07},
volume       = {2},
abstract     = {Synthetic Synthetic plastic is chemical materials which cause severe environmental problems. Incinerating plastic waste leads to release of hazardous gases, which is not good for humans. Bioplastic can help reduce the dependence on fossil fuels and petroleum, that bioplastic can solve the problem of synthetic plastic use. This research aims to define the properties of the cassava starch-based bioplastic reinforced by nanoclay. Methods were experimental with bioplastic component of cassava starch, glycerol as plasticizer and nanoclay as reinforcement. The bioplastic was analyzed using XRD, tensile test, moisture absorption, biodegradability, and compared with another bioplastic. The results show that the addition of nanoclay into bioplastic results increasing the tensile strength of bioplastic also increases from 5.2 MPa to 6.3 MPa. This research revealed that complete degradation of nanoclay reinforced bioplastic could be achieved on the 6th day.},
file         = {:done/2018WahyuningtiyasProperties Properties of Cassava Starch Based Bioplastic Reinforced by Nanoclay.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2006HuangHigh,
author       = {Huang, M and Yu, J and Ma, X},
date         = {2006-03-03},
journaltitle = {Carbohydrate Polymers},
title        = {High mechanical performance MMT-urea and formamide-plasticized thermoplastic cornstarch biodegradable nanocomposites},
doi          = {10.1016/j.carbpol.2005.09.006},
issn         = {0144-8617},
number       = {3},
pages        = {393--399},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0144861705004406},
urldate      = {2019-04-07},
volume       = {63},
abstract     = {Biodegradable nanocomposites have been successfully fabricated from the thermoplastic cornstarch (TPCS) and activated-montmorillonite (MMT) by melt-intercalation. TPCS was plasticized with novel plasticizers urea and formamide, and the activated-montmorillonites were obtained using citric acid as the activated solvent. Compared with urea and formamide-plasticized thermoplastic cornstarch (UFTPCS), the mechanical properties of nanocomposites were very good. The thermal analysis was investigated by Differential Scanning Calorimetry (DSC). The effect of water content on the mechanical properties of nanocomposites was studied. Dynamic Mechanical Thermal Analysis (DMTA) was also carried out. The structure and morphology of biodegradable nanocomposites were characterized by wide-angle X-ray diffraction (WAXD), scanning electron microscope (SEM) and transmission electron microscope (TEM). It was revealed that UFTPCS were intercalated into the layers of MMT successfully, and layers of MMT were fully exfoliated and so formed the exfoliated nanocomposites with MMT. This manufacturing process is simple and environmentally friendly.},
langid       = {english},
}
@Article{2011VieiraNatural,
author       = {Vieira, Melissa Gurgel Adeodato and da Silva, Mariana Altenhofen and dos Santos, Lucielen Oliveira and Beppu, Marisa Masumi},
date         = {2011-03},
journaltitle = {European Polymer Journal},
title        = {Natural-based plasticizers and biopolymer films: A review},
doi          = {10.1016/j.eurpolymj.2010.12.011},
issn         = {0014-3057},
number       = {3},
pages        = {254--263},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0014305710004763},
urldate      = {2019-04-07},
volume       = {47},
abstract     = {In recent years, much attention has been focused on research to replace petroleum-based commodity plastics, in a cost-effective manner, with biodegradable materials offering competitive mechanical properties. Biopolymers have been considered as the most promising materials for this purpose. However, they generally present poor mechanical properties regarding processability and end-use application, since the fragility and brittleness exhibited during thermoformation can limit their potential for application. In order to overcome this problem, plasticizers are added to provide the necessary workability to biopolymers. This class of products became more visible when biodegradable additives and plasticizers also became the focus of material scientists. The use of natural and/or biodegradable plasticizers, with low toxicity and good compatibility with several plastics, resins, rubber and elastomers in substitution of conventional plasticizers, such as phthalates and other synthetic conventional plasticizers attracted the market along with the increasing worldwide trend towards use of biopolymers. Here we discuss the main results and developments in natural plasticizer/synthetic and biopolymer-based ﬁlms during the last decades.},
langid       = {english},
shorttitle   = {Natural-based plasticizers and biopolymer films},
}
@Article{rahmatiah_al_faruqy_properties_2016,
author = {Rahmatiah Al Faruqy, M. Sujuthi and Liew, Kang Chang},
date   = {2016},
title  = {Properties of Bioplastic Sheets Made from Different Types of Starch Incorporated With Recycled Newspaper Pulp},
}
@Article{2011YunosEffect,
author       = {Yunos, M. Z. B. and Rahman, WAWA},
date         = {2011},
journaltitle = {J. Appl. Sci},
title        = {Effect of glycerol on performance rice straw/starch based polymer},
doi          = {10.3923/jas.2011.2456.2459},
number       = {13},
pages        = {2456--2459},
volume       = {11},
}
@InCollection{2004VilpouxStarch,
author    = {Vilpoux, Olivier and Averous, Luc},
booktitle = {Technology, use and potentialities of Latin American starchy tubers},
date      = {2004},
title     = {Starch-based plastics},
pages     = {33},
file      = {:done/2004VilpouxStarch Starch Based Plastics.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Thesis{2014NazriProduction,
author      = {Nazri, Nur Syazana Bt},
date        = {2014},
institution = {UNIVERSITI MALAYSIA PAHANG},
title       = {Production Of Bio Resin From Palm Oil},
type        = {candthesis},
file        = {:done/2014NazriProduction Production of Bio Resin from Palm Oil.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
year        = {2014},
}
@Article{2017XiongMicrosoft,
author       = {Xiong, W. and Wu, L. and Alleva, F. and Droppo, J. and Huang, X. and Stolcke, A.},
date         = {2017-08-20},
journaltitle = {arXiv:1708.06073 [cs]},
title        = {The Microsoft 2017 Conversational Speech Recognition System},
eprint       = {1708.06073},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1708.06073},
urldate      = {2019-04-07},
abstract     = {We describe the 2017 version of Microsoft’s conversational speech recognition system, in which we update our 2016 system with recent developments in neural-network-based acoustic and language modeling to further advance the state of the art on the Switchboard speech recognition task. The system adds a CNN-BLSTM acoustic model to the set of model architectures we combined previously, and includes character-based and dialog session aware LSTM language models in rescoring. For system combination we adopt a twostage approach, whereby subsets of acoustic models are ﬁrst combined at the senone/frame level, followed by a word-level voting via confusion networks. We also added a confusion network rescoring step after system combination. The resulting system yields a 5.1 /\\% word error rate on the 2000 Switchboard evaluation set.},
file         = {:done/2017XiongMicrosoft The Microsoft 2017 Conversational Speech Recognition System.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2017LiAcoustic,
author     = {Li, Bo and Sainath, Tara N. and Narayanan, Arun and Caroselli, Joe and Bacchiani, Michiel and Misra, Ananya and Shafran, Izhak and Sak, Haşim and Pundak, Golan and Chin, Kean and Sim, Khe Chai and Weiss, Ron J. and Wilson, Kevin W. and Variani, Ehsan and Kim, Chanwoo and Siohan, Olivier and Weintraub, Mitchel and McDermott, Erik and Rose, Richard and Shannon, Matt},
booktitle  = {Interspeech 2017},
date       = {2017-08-20},
title      = {Acoustic Modeling for Google Home},
doi        = {10.21437/Interspeech.2017-234},
eventtitle = {Interspeech 2017},
pages      = {399--403},
publisher  = {ISCA},
url        = {http://www.isca-speech.org/archive/Interspeech///////\\_2017/abstracts/0234.html},
urldate    = {2019-04-07},
abstract   = {This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-LSTMs to model frequency variations. On the system level, improvements include adapting the model using Google Home speciﬁc data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of WER of 8-28 /\\% relative compared to the current production system.},
file       = {:done/2017LiAcoustic Acoustic Modeling for Google Home.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
}
@InProceedings{2017DhamdhereAnalyza,
author     = {Dhamdhere, Kedar and McCurley, Kevin S. and Nahmias, Ralfi and Sundararajan, Mukund and Yan, Qiqi},
booktitle  = {Proceedings of the 22nd International Conference on Intelligent User Interfaces - IUI '17},
date       = {2017},
title      = {Analyza: Exploring Data with Conversation},
doi        = {10.1145/3025171.3025227},
eventtitle = {the 22nd International Conference},
isbn       = {978-1-4503-4348-0},
location   = {Limassol, Cyprus},
pages      = {493--504},
publisher  = {ACM Press},
url        = {http://dl.acm.org/citation.cfm?doid=3025171.3025227},
urldate    = {2019-04-07},
abstract   = {We describe Analyza, a system that helps lay users explore data. Analyza has been used within two large real world systems. The ﬁrst is a question-and-answer feature in a spreadsheet product. The second provides convenient access to a revenue/inventory database for a large sales force. Both user bases consist of users who do not necessarily have coding skills, demonstrating Analyza’s ability to democratize access to data.},
langid     = {english},
shorttitle = {Analyza},
}
@Article{2017SeeGet,
author       = {See, Abigail and Liu, Peter J. and Manning, Christopher D.},
date         = {2017-04-14},
journaltitle = {arXiv:1704.04368 [cs]},
title        = {Get To The Point: Summarization with Pointer-Generator Networks},
eprint       = {1704.04368},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1704.04368},
urldate      = {2019-04-07},
abstract     = {Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.},
langid       = {english},
shorttitle   = {Get To The Point},
}
@Article{2017YuLearning,
author       = {Yu, Adams Wei and Lee, Hongrae and Le, Quoc V.},
date         = {2017-04-22},
journaltitle = {arXiv:1704.06877 [cs]},
title        = {Learning to Skim Text},
eprint       = {1704.06877},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1704.06877},
urldate      = {2019-04-07},
abstract     = {Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classiﬁcation to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difﬁcult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classiﬁcation and automatic Q /\\&A, our proposed model, a modiﬁed LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.},
file         = {:done/2017YuLearning Learning to Skim Text.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016NallapatiAbstractive,
author       = {Nallapati, Ramesh and Zhou, Bowen and santos, Cicero Nogueira dos and Gulcehre, Caglar and Xiang, Bing},
date         = {2016-02-18},
journaltitle = {arXiv:1602.06023 [cs]},
title        = {Abstractive Text Summarization Using Sequence-to-Sequence RNNs and Beyond},
eprint       = {1602.06023},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1602.06023},
urldate      = {2019-04-07},
abstract     = {In this work, we model abstractive text summarization using Attentional EncoderDecoder Recurrent Neural Networks, and show that they achieve state-of-the-art performance on two different corpora. We propose several novel models that address critical problems in summarization that are not adequately modeled by the basic architecture, such as modeling key-words, capturing the hierarchy of sentence-toword structure, and emitting words that are rare or unseen at training time. Our work shows that many of our proposed models contribute to further improvement in performance. We also propose a new dataset consisting of multi-sentence summaries, and establish performance benchmarks for further research.},
file         = {:done/2016NallapatiAbstractive Abstractive Text Summarization Using Sequence to Sequence RNNs and beyond.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2016MiyatoAdversarial,
author       = {Miyato, Takeru and Dai, Andrew M. and Goodfellow, Ian},
date         = {2016-05-25},
journaltitle = {arXiv:1605.07725 [cs, stat]},
title        = {Adversarial Training Methods for Semi-Supervised Text Classification},
eprint       = {1605.07725},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1605.07725},
urldate      = {2019-04-07},
abstract     = {Adversarial training provides a means of regularizing supervised learning algorithms while virtual adversarial training is able to extend supervised learning algorithms to the semi-supervised setting. However, both methods require making small perturbations to numerous entries of the input vector, which is inappropriate for sparse high-dimensional inputs such as one-hot word representations. We extend adversarial and virtual adversarial training to the text domain by applying perturbations to the word embeddings in a recurrent neural network rather than to the original input itself. The proposed method achieves state of the art results on multiple benchmark semi-supervised and purely supervised tasks. We provide visualizations and analysis showing that the learned word embeddings have improved in quality and that while training, the model is less prone to overﬁtting.},
file         = {:done/2016MiyatoAdversarial Adversarial Training Methods for Semi Supervised Text Classification.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{lu_best_2017,
author = {Lu, Jiasen and Kannan, Anitha and Yang, Jianwei and Parikh, Devi and Batra, Dhruv},
date   = {2017},
title  = {Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model},
pages  = {11},
langid = {english},
}
@Online{noauthor_deep_2017,
date       = {2017},
title      = {Deep Learning for Siri’s Voice: On-device Deep Mixture Density Networks for Hybrid Unit Selection Synthesis - Apple},
url        = {https://machinelearning.apple.com/2017/08/06/siri-voices.html},
titleaddon = {Apple Machine Learning Journal},
urldate    = {2019-04-07},
abstract   = {Apple Machine Learning Journal publishes posts written by Apple engineers about their work using machine learning technologies to help build innovative products for millions of people around the world.},
langid     = {american},
shorttitle = {Deep Learning for Siri’s Voice},
}
@Online{2017Improving,
date       = {2017},
title      = {Improving the Realism of Synthetic Images - Apple},
url        = {https://machinelearning.apple.com/2017/07/07/GAN.html},
titleaddon = {Apple Machine Learning Journal},
urldate    = {2019-04-07},
abstract   = {Apple Machine Learning Journal publishes posts written by Apple engineers about their work using machine learning technologies to help build innovative products for millions of people around the world.},
file       = {:done/2017Improving Improving the Realism of Synthetic Images Apple.pdf:application/pdf},
groups     = {tesse:2},
langid     = {american},
}
@Article{2014HussainAffective,
author       = {Hussain, Amir and Cambria, Erik and Schuller, Björn and Howard, Newton},
date         = {2014-10},
journaltitle = {Neural Networks},
title        = {Affective neural networks and cognitive learning systems for big data analysis},
doi          = {10.1016/j.neunet.2014.07.010},
issn         = {0893-6080},
pages        = {1--3},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0893608014001762},
urldate      = {2019-04-07},
volume       = {58},
file         = {:done/2014HussainAffective Affective Neural Networks and Cognitive Learning Systems for Big Data Analysis.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{raza_disjunctive_2018,
author     = {Raza, Mohammad and Gulwani, Sumit},
date       = {2018},
title      = {Disjunctive Program Synthesis: a Robust Approach to Programming by Example},
eventtitle = {AAAI 2018},
pages      = {10},
abstract   = {Programming by example (PBE) systems allow end users to easily create programs by providing a few input-output examples to specify their intended task. The system attempts to generate a program in a domain speciﬁc language (DSL) that satisﬁes the given examples. However, a key challenge faced by existing PBE techniques is to ensure the robustness of the programs that are synthesized from a small number of examples, as these programs often fail when applied to new inputs. This is because there can be many possible programs satisfying a small number of examples, and the PBE system has to somehow rank between these candidates and choose the correct one without any further information from the user. In this work we present a different approach to PBE in which the system avoids making a ranking decision at the synthesis stage, by instead synthesizing a disjunctive program that includes the many possible top-ranked programs as possible alternatives and selects between these different choices upon execution on a new input. This delayed choice brings the important beneﬁt of comparing the possible outputs produced by the different disjuncts on a given input at execution time. We present a generic framework for synthesizing such disjunctive programs in arbitrary DSLs, and describe two concrete implementations of disjunctive synthesis in the practical domains of data extraction from plain text and HTML documents. We present an evaluation showing the signiﬁcant increase in robustness achieved with our disjunctive approach, as illustrated by an increase from 59///////\\% to 93///////\\% of tasks for which correct programs can be learnt from a single example.},
langid     = {english},
}
@InProceedings{wang_show_2018,
author     = {Wang, Jing and Fu, Jianlong and Tang, Jinhui and Li, Zechao and Mei, Tao},
date       = {2018},
title      = {Show, Reward and Tell: Automatic Generation of Narrative Paragraph from Photo Stream by Adversarial Training},
eventtitle = {AAAI 2018},
pages      = {8},
abstract   = {Impressive image captioning results (i.e., an objective description for an image) are achieved with plenty of training pairs. In this paper, we take one step further to investigate the creation of narrative paragraph for a photo stream. This task is even more challenging due to the difﬁculty in modeling an ordered photo sequence and in generating a relevant paragraph with expressive language style for storytelling. The difﬁculty can even be exacerbated by the limited training data, so that existing approaches almost focus on searchbased solutions. To deal with these challenges, we propose a sequence-to-sequence modeling approach with reinforcement learning and adversarial training. First, to model the ordered photo stream, we propose a hierarchical recurrent neural network as story generator, which is optimized by reinforcement learning with rewards. Second, to generate relevant and story-style paragraphs, we design the rewards with two critic networks, including a multi-modal and a languagestyle discriminator. Third, we further consider the story generator and reward critics as adversaries. The generator aims to create indistinguishable paragraphs to human-level stories, whereas the critics aim at distinguishing them and further improving the generator by policy gradient. Experiments on three widely-used datasets show the effectiveness, against state-of-the-art methods with relative increase of 20.2///////\\% by METEOR. We also show the subjective preference for the proposed approach over the baselines through a user study with 30 human subjects.},
langid     = {english},
}
@Article{2014GraefeMemory,
author       = {Graefe, Goetz and Volos, Haris and Kimura, Hideaki and Kuno, Harumi and Tucek, Joseph and Lillibridge, Mark and Veitch, Alistair},
date         = {2014-09-01},
journaltitle = {Proceedings of the VLDB Endowment},
title        = {In-memory performance for big data},
doi          = {10.14778/2735461.2735465},
issn         = {2150-8097},
number       = {1},
pages        = {37--48},
url          = {http://dl.acm.org/citation.cfm?doid=2735461.2735465},
urldate      = {2019-04-07},
volume       = {8},
file         = {:done/2014GraefeMemory In Memory Performance for Big Data.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2014GiannakisSignal,
author       = {Giannakis, Georgios B. and Bach, Francis and Cendrillon, Raphael and Mahoney, Michael and Neville, Jennifer},
date         = {2014-09},
journaltitle = {IEEE Signal Processing Magazine},
title        = {Signal Processing for Big Data [From the Guest Editors]},
doi          = {10.1109/MSP.2014.2330054},
issn         = {1053-5888},
number       = {5},
pages        = {15--16},
url          = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6879633},
urldate      = {2019-04-07},
volume       = {31},
file         = {:done/2014GiannakisSignal Signal Processing for Big Data [From the Guest Editors].pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Book{2011WittenData,
author     = {Witten, Ian H. and Frank, Eibe and Hall, Mark A.},
date       = {2011-01-20},
title      = {Data Mining: Practical Machine Learning Tools and Techniques},
edition    = {3 edition},
isbn       = {978-0-12-374856-0},
location   = {Burlington, MA},
pagetotal  = {664},
publisher  = {Morgan Kaufmann},
abstract   = {Data Mining: Practical Machine Learning Tools and Techniques, Third Edition, offers a thorough grounding in machine learning concepts as well as practical advice on applying machine learning tools and techniques in real-world data mining situations. This highly anticipated third edition of the most acclaimed work on data mining and machine learning will teach you everything you need to know about preparing inputs, interpreting outputs, evaluating results, and the algorithmic methods at the heart of successful data mining. Thorough updates reflect the technical changes and modernizations that have taken place in the field since the last edition, including new material on Data Transformations, Ensemble Learning, Massive Data Sets, Multi-instance Learning, plus a new version of the popular Weka machine learning software developed by the authors. Witten, Frank, and Hall include both tried-and-true techniques of today as well as methods at the leading edge of contemporary research. The book is targeted at information systems practitioners, programmers, consultants, developers, information technology managers, specification writers, data analysts, data modelers, database R///////\\&D professionals, data warehouse engineers, data mining professionals. The book will also be useful for professors and students of upper-level undergraduate and graduate-level data mining and machine learning courses who want to incorporate data mining as part of their data management knowledge base and expertise.Provides a thorough grounding in machine learning concepts as well as practical advice on applying the tools and techniques to your data mining projectsOffers concrete tips and techniques for performance improvement that work by transforming the input or output in machine learning methodsIncludes downloadable Weka software toolkit, a collection of machine learning algorithms for data mining tasks―in an updated, interactive interface. Algorithms in toolkit cover: data pre-processing, classification, regression, clustering, association rules, visualization},
shorttitle = {Data Mining},
}
@Article{2011ArnoldNefarious,
author       = {Arnold, Douglas N. and Fowler, Kristine K.},
date         = {2011},
journaltitle = {Notices of the AMS},
title        = {Nefarious numbers},
number       = {3},
pages        = {434--437},
volume       = {58},
file         = {:done/2011ArnoldNefarious Nefarious Numbers.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{2018OliveiraProspective,
author       = {Oliveira, Altina Silva and de Barros, Marta Duarte and de Carvalho Pereira, Fernanda and Gomes, Carlos Francisco Simões and da Costa, Helder Gomes},
date         = {2018-06},
journaltitle = {Futures},
title        = {Prospective scenarios: A literature review on the Scopus database},
doi          = {10.1016/j.futures.2018.03.005},
issn         = {0016-3287},
pages        = {20--33},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0016328716302476},
urldate      = {2019-04-07},
volume       = {100},
abstract     = {The use of prospective scenarios has been discussed by companies in diﬀerent sectors. As such, this work seeks to study the literature on the prospection of scenarios, permitting diﬀerent types of analyses and applications. A bibliometric study was performed on the Scopus database, accessed from the CAPES portal in April 2015 and updated in June 2017 in order to identify how the articles about the term prospective scenarios are presented in the literature. 87 articles on the subject were found indexed on Scopus, of which only 17 were from Brazil. It is expected, therefore, that this work will contribute to the construction of an overview of the existing literature on prospective scenarios in order to stimulate the interest of more researchers for the subject.},
langid       = {english},
shorttitle   = {Prospective scenarios},
}
@Book{2017KaehlerLearning,
author     = {Kaehler, Adrian and Bradski, Gary},
date       = {2017-01-08},
title      = {Learning OpenCV 3: Computer Vision in C++ with the OpenCV Library},
edition    = {1 edition},
isbn       = {978-1-4919-3799-0},
location   = {Sebastopol, CA},
pagetotal  = {1024},
publisher  = {O'Reilly Media},
abstract   = {Get started in the rapidly expanding field of computer vision with this practical guide. Written by Adrian Kaehler and Gary Bradski, creator of the open source OpenCV library, this book provides a thorough introduction for developers, academics, roboticists, and hobbyists. You’ll learn what it takes to build applications that enable computers to "see" and make decisions based on that data.With over 500 functions that span many areas in vision, OpenCV is used for commercial applications such as security, medical imaging, pattern and face recognition, robotics, and factory product inspection. This book gives you a firm grounding in computer vision and OpenCV for building simple or sophisticated vision applications. Hands-on exercises in each chapter help you apply what you’ve learned.This volume covers the entire library, in its modern C++ implementation, including machine learning tools for computer vision.Learn OpenCV data types, array types, and array operationsCapture and store still and video images with HighGUITransform images to stretch, shrink, warp, remap, and repairExplore pattern recognition, including face detectionTrack objects and motion through the visual fieldReconstruct 3D images from stereo visionDiscover basic and advanced machine learning techniques in OpenCV},
shorttitle = {Learning OpenCV 3},
}
@Book{2018TazehkandiComputer,
author     = {Tazehkandi, Amin Ahmadi},
date       = {2018-01-02},
title      = {Computer Vision with OpenCV 3 and Qt5: Build visually appealing, multithreaded, cross-platform computer vision applications},
edition    = {Edição: 1},
pagetotal  = {488},
publisher  = {Packt Publishing},
abstract   = {Blend the power of Qt with OpenCV to build cross-platform computer vision applications Key Features ● Start creating robust applications with the power of OpenCV and Qt combined ● Learn from scratch how to develop cross-platform computer vision applications ● Accentuate your OpenCV applications by developing them with Qt Book Description Developers have been using OpenCV library to develop computer vision applications for a long time. However, they now need a more effective tool to get the job done and in a much better and modern way. Qt is one of the major frameworks available for this task at the moment. This book will teach you to develop applications with the combination of OpenCV 3 and Qt5, and how to create cross-platform computer vision applications. We’ll begin by introducing Qt, its IDE, and its SDK. Next you’ll learn how to use the OpenCV API to integrate both tools, and see how to configure Qt to use OpenCV. You’ll go on to build a full-fledged computer vision application throughout the book. Later, you’ll create a stunning UI application using the Qt widgets technology, where you’ll display the images after they are processed in an efficient way. At the end of the book, you’ll learn how to convert OpenCV Mat to Qt QImage. You’ll also see how to efficiently process images to filter them, transform them, detect or track objects as well as analyze video. You’ll become better at developing OpenCV applications. What you will learn ● Get an introduction to Qt IDE and SDK ● Be introduced to OpenCV and see how to communicate between OpenCV and Qt ● Understand how to create UI using Qt Widgets ● Learn to develop cross-platform applications using OpenCV 3 and Qt 5 ● Explore the multithreaded application development features of Qt5 ● Improve OpenCV 3 application development using Qt5 ● Build, test, and deploy Qt and OpenCV apps, either dynamically or statically ● See Computer Vision technologies such as filtering and transformation of images, detecting and matching objects, template matching, object tracking, video and motion analysis, and much more ● Be introduced to QML and Qt Quick for iOS and Android application development Who this book is for This book is for readers interested in building computer vision applications. Intermediate knowledge of C++ programming is expected. Even though no knowledge of Qt5 and OpenCV 3 is assumed, if you’re familiar with these frameworks, you’ll benefit. Table of Contents Introduction to Qt and OpenCV Creating our first Qt and OpenCV project Creating a comprehensive Qt+OpenCV project Mat and Qimage The Graphics View Framework Image Processing in OpenCV Features and Descriptors Multi-Threading Video Analysis Debugging and Testing Static Linking and Deployment Computer Vision Apps for Android and iOS},
shorttitle = {Computer Vision with OpenCV 3 and Qt5},
}
@Book{2018JohnsonHands,
author     = {Johnson, Andrew},
date       = {2018-05-31},
title      = {Hands-On Functional Programming in Rust: Build modular and reactive applications with functional programming techniques in Rust 2018},
isbn       = {978-1-78883-935-8},
pagetotal  = {292},
publisher  = {Packt Publishing},
abstract   = {Explore the support Rust offers for creating functional applications in Rust. Learn about various design patterns, implementing concurrency, metaprogramming, and so on in the processKey FeaturesLearn generics, organization, and design patterns in functional programmingModularize your applications and make them highly reusable and testable using functional design patternsGet familiar with complex concepts such as metaprogramming, concurrency, and immutabilityBook DescriptionFunctional programming allows developers to divide programs into smaller, reusable components that ease the creation, testing, and maintenance of software as a whole. Combined with the power of Rust, you can develop robust and scalable applications that fulfill modern day software requirements. This book will help you discover all the Rust features that can be used to build software in a functional way.We begin with a brief comparison of the functional and object-oriented approach to different problems and patterns. We then quickly look at the patterns of control flow, data the abstractions of these unique to functional programming. The next part covers how to create functional apps in Rust; mutability and ownership, which are exclusive to Rust, are also discussed. Pure functions are examined next and you'll master closures, their various types, and currying. We also look at implementing concurrency through functional design principles and metaprogramming using macros. Finally, we look at best practices for debugging and optimization.By the end of the book, you will be familiar with the functional approach of programming and will be able to use these techniques on a daily basis.What you will learnHow Rust supports the use of basic functional programming principlesUse functional programming to handle concurrency with eleganceRead and interpret complex type signatures for types and functionsImplement powerful abstractions using meta programming in RustCreate quality code formulaically using Rust's functional design patternsMaster Rust's complex ownership mechanisms particularly for mutabilityWho This Book Is ForThis book is for Rust developers who are comfortable with the language and now want to improve their coding abilities by learning advanced functional techniques to enhance their skillset and create robust and testable apps.Table of ContentsFunctional Programming - a comparisonFunctional Control FlowFunctional Data StructuresGenerics and PolymorphismCode Organization and Application ArchitectureMutability, Ownership, and Pure FunctionsDesign PatternsImplementing ConcurrencyMetaprogramming,Debugging, and Performance},
shorttitle = {Hands-On Functional Programming in Rust},
}
@Book{noauthor_proceedings_2008,
date  = {2008},
title = {Proceedings of the 8th International Conference on New Interfaces for Musical Expression},
isbn  = {13-978-88-901344-6-3},
}
@Book{2015AllenGetting,
author     = {Allen, David and Fallows, James},
date       = {2015-03-17},
title      = {Getting Things Done: The Art of Stress-Free Productivity},
edition    = {Revised edition},
pagetotal  = {352},
publisher  = {Penguin Books},
abstract   = {The book Lifehack calls "The Bible of business and personal productivity.""A completely revised and updated edition of the blockbuster bestseller from 'the personal productivity guru'"—Fast CompanySince it was first published almost fifteen years ago, David Allen’s Getting Things Done has become one of the most influential business books of its era, and the ultimate book on personal organization. "GTD" is now shorthand for an entire way of approaching professional and personal tasks, and has spawned an entire culture of websites, organizational tools, seminars, and offshoots. Allen has rewritten the book from start to finish, tweaking his classic text with important perspectives on the new workplace, and adding material that will make the book fresh and relevant for years to come. This new edition of Getting Things Done will be welcomed not only by its hundreds of thousands of existing fans but also by a whole new generation eager to adopt its proven principles.},
shorttitle = {Getting Things Done},
}
@MvBook{2016HallidayFundamentos,
author    = {Halliday, David and Resnick, Robert and Walker, Jearl},
date      = {2016-06-30},
title     = {Fundamentos de Fìsica - Vol. 2 - Gravitação, Ondas e Termodinâmica},
edition   = {Edição: 10},
pagetotal = {1045},
publisher = {LTC},
volumes   = {4},
abstract  = {A nova edição do maior clássico de Física traz o melhor para você! Raspe o código promocional que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-vol2 e ganhe o e-book de Fundamentos de Física - Vol. 2 - Gravitação, Ondas e Termodinâmica + vídeos exclusivos de Física Experimental. Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10ª edição: • Módulos e Objetivos de Aprendizado - Os capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. • Capítulos Reformulados - Para facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. • Novos Exemplos, Perguntas e Problemas - 250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-vol2 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
volume    = {2},
}
@MvBook{2016HallidayFundamentosa,
author    = {Halliday, David and Resnick, Robert and Walker, Jearl},
date      = {2016-06-30},
title     = {Fundamentos de Física - Vol. 1 - Mecânica},
edition   = {Edição: 10},
pagetotal = {1336},
publisher = {LTC},
volumes   = {4},
abstract  = {A nova edição do maior clássico de Física traz o melhor para você! Raspe o código promocional que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-vol1 e ganhe o e-book de Fundamentos de Física - Vol. 1 - Mecânica + vídeos exclusivos de Física Experimental. Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10ª edição: • Módulos e Objetivos de Aprendizado - Os capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. • Capítulos Reformulados - Para facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. • Novos Exemplos, Perguntas e Problemas - 250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-vol1 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
volume    = {1},
}
@MvBook{2016HallidayFundamentosb,
author    = {Halliday, David and Resnick, Robert and Walker, Jearl},
date      = {2016-06-30},
title     = {Fundamentos de Física - Vol. 4 - Óptica e Física Moderna},
edition   = {Edição: 10},
pagetotal = {1379},
publisher = {LTC},
volumes   = {4},
abstract  = {A nova edição do maior clássico de Física traz o melhor para você! Raspe o código promocional que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-vol1 e ganhe o e-book de Fundamentos de Física - Vol. 4 - Óptica e Física Moderna + vídeos exclusivos de Física Experimental. Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10ª edição: • Módulos e Objetivos de Aprendizado - Os capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. • Capítulos Reformulados - Para facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. • Novos Exemplos, Perguntas e Problemas - 250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-vol4 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
volume    = {4},
}
@MvBook{2016ResnickFundamentos,
author    = {Resnick, Robert and Walker, Jearl and Halliday, David},
date      = {2016-06-30},
title     = {Fundamentos de Física - Vol. 3 - Eletromagnetismo},
edition   = {Edição: 10ª, Nova Edição},
isbn      = {978-85-216-3037-1},
publisher = {LTC},
volumes   = {4},
abstract  = {A nova edição do maior clássico de Física traz o melhor para você!Na compra do exemplar impresso, raspe o código promocional (PIN) da etiqueta que aparece na "orelha" do livro, entre no site www.grupogen.com.br/halliday-volume-3 e acesse 30 vídeos exclusivos de Física Experimental. E com o mesmo código PIN, oferecemos uma oferta exclusiva! Por apenas R$ 20,00, você poderá adquirir o e-book do volume de "Fundamentos de Física - Vol. 3 - Eletromagnetismo" e mais os vídeos completos (140 vídeos) de Física Experimental que acompanham o livro! Saiba mais em www.halliday.com.br ---------- Sucesso há mais de quatro décadas em todo o mundo, Fundamentos de Física continua cumprindo o desafio de apresentar a Física de maneira clara, unindo a teoria e os exercícios às aplicações práticas do mundo real. Novidades da 10a edição:- Módulos e Objetivos de AprendizadoOs capítulos vêm agora divididos em módulos conceituais, dedicados a temas básicos, com uma lista de objetivos do aprendizado para que o estudante identifique, de antemão, todos os conceitos e as definições que verá naquele módulo. - Capítulos ReformuladosPara facilitar o aprendizado, alguns capítulos foram reformulados, como o que aborda a lei de Gauss e o potencial elétrico. Houve também a preocupação de estabelecer uma ligação mais clara e direta com os conceitos-chave apresentados. - Novos Exemplos, Perguntas e Problemas250 novos problemas, 50 perguntas inéditas e 16 novos exemplos foram acrescentados a esta edição. Permanecem como destaques desta 10a edição os materiais suplementares, todos traduzidos e disponíveis no site www.grupogen.com.br/halliday-volume-3 (LTC Editora – GEN | Grupo Editorial Nacional), mediante cadastro.},
volume    = {3},
}
@Book{1979AlexanderTimeless,
author    = {Alexander, Christopher},
date      = {1979},
title     = {The Timeless Way of Building},
isbn      = {978-0-19-502402-9},
location  = {New York, NY},
pagetotal = {552},
publisher = {Oxford University Press},
abstract  = {The theory of architecture implicit in our world today, Christopher Alexander believes, is bankrupt. More and more people are aware that something is deeply wrong. Yet the power of present-day ideas is so great that many feel uncomfortable, even afraid, to say openly that they dislike what is happening, because they are afraid to seem foolish, afraid perhaps that they will be laughed at. Now, at last, there is a coherent theory which describes in modern terms an architecture as ancient as human society itself. The Timeless Way of Building is the introductory volume in the Center for Environmental Structure series, Christopher Alexander presents in it a new theory of architecture, building, and planning which has at its core that age-old process by which the people of a society have always pulled the order of their world from their own being. Alexander writes, "There is one timeless way of building. It is thousands of years old, and the same today as it has always been. The great traditional buildings of the past, the villages and tents and temples in which man feels at home, have always been made by people who were very close to the center of this way. And as you will see, this way will lead anyone who looks for it to buildings which are themselves as ancient in their form as the trees and hills, and as our faces are."},
file      = {:done/1979AlexanderTimeless The Timeless Way of Building.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2016SeverinoMetodologia,
author    = {Severino, Antônio Joaquim},
date      = {2016},
title     = {Metodologia do Trabalho Cientifico},
isbn      = {978-85-249-2448-4},
location  = {São Paulo - SP},
publisher = {Cortez},
abstract  = {Este livro tornou-se referência entre alunos e professores universitários, sendo uma das obras mais conceituadas sobre o tema. Amplamente indicado como leitura em instituições de ensino e cursos de todo país, trata-se de uma iniciação teórica, metodológica e prática ao trabalho científico, auxiliando desde a organização dos estudos e pesquisas científicas, até na elaboração de trabalhos acadêmicos. Em sua nova edição, traz noções básicas do uso do computador como ferramenta de elaboração de textos, de intercâmbio entre pesquisadores e de busca de referências, além de um capítulo sobre a contribuição da Internet à pesquisa.},
}
@Misc{2006HillerCarbon,
author = {Hiller, Jonathan},
date   = {2006},
title  = {Carbon fiber vs wood as an acoustic guitar soundboard},
file   = {:done/2006HillerCarbon Carbon Fiber Vs Wood As an Acoustic Guitar Soundboard.pdf:application/pdf},
groups = {tesse:5},
}
@Misc{2006HillerComposite,
author = {Hiller, Jonathan},
date   = {2006},
title  = {Composite acoustic guitar project},
file   = {:done/2006HillerComposite Composite Acoustic Guitar Project.pdf:application/pdf},
groups = {tesse:5},
}
@Book{2016StrangIntroduction,
author    = {Strang, Gilbert},
date      = {2016-06-10},
title     = {Introduction to Linear Algebra, Fifth Edition},
edition   = {Fifth Edition edition},
isbn      = {978-0-9802327-7-6},
location  = {Wellesley, MA},
pagetotal = {584},
publisher = {Wellesley-Cambridge Press},
abstract  = {Gilbert Strang's textbooks have changed the entire approach to learning linear algebra -- away from abstract vector spaces to specific examples of the four fundamental subspaces: the column space and nullspace of A and A'. This new fifth edition has become more than a textbook for the basic linear algebra course. That is its first purpose and always will be. The new chapters about applications of the SVD, probability and statistics, and Principal Component Analysis in finance and genetics, make it also a textbook for a second course, plus a resource at work. Linear algebra has become central in modern applied mathematics. This book supports the value of understanding linear algebra. Introduction to Linear Algebra, Fifth Edition includes challenge problems to complement the review problems that have been highly praised in previous editions. The basic course is followed by eight applications: differential equations in engineering, graphs and networks, statistics, Fourier methods and the FFT, linear programming, computer graphics, cryptography, Principal Component Analysis, and singular values. Audience: Thousands of teachers in colleges and universities and now high schools are using this book, which truly explains this crucial subject. This text is for readers everywhere, with support from the websites and video lectures. Every chapter begins with a summary for efficient review. Contents: Chap. 1: Introduction to Vectors; Chap. 2: Solving Linear Equations; Chap. 3: Vector Spaces and Subspaces; Chap. 4: Orthogonality; Chap. 5: Determinants; Chap. 6: Eigenvalues and Eigenvectors; Chap. 7: Singular Value Decomposition; Chap. 8: Linear Transformations; Chap. 9: Complex Vectors and Matrices; Chap. 10: Applications; Chap. 11: Numerical Linear Algebra; Chap. 12: Linear Algebra in Probability and Statistics; Matrix Factorizations; Index; Six Great Theorems.},
}
@Book{2013HallidayFundamentals,
author    = {Halliday, David and Resnick, Robert and Walker, Jearl},
date      = {2013-08-13},
title     = {Fundamentals of Physics Extended},
edition   = {10 edition},
isbn      = {978-1-118-23072-5},
location  = {Hoboken, NJ},
pagetotal = {1448},
publisher = {Wiley},
abstract  = {The 10th edition of Halliday's Fundamentals of Physics, Extended building upon previous issues by offering several new features and additions. The new edition offers most accurate, extensive and varied set of assessment questions of any course management program in addition to all questions including some form of question assistance including answer specific feedback to facilitate success. The text also offers multimedia presentations (videos and animations) of much of the material that provide an alternative pathway through the material for those who struggle with reading scientific exposition. Furthermore, the book includes math review content in both a self-study module for more in-depth review and also in just-in-time math videos for a quick refresher on a specific topic. The Halliday content is widely accepted as clear, correct, and complete. The end-of-chapters problems are without peer. The new design, which was introduced in 9e continues with 10e, making this new edition of Halliday the most accessible and reader-friendly book on the market. WileyPLUS sold separately from text.},
file      = {:done/2013HallidayFundamentals Fundamentals of Physics Extended.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2010BurnetteHello,
author     = {Burnette, Ed},
date       = {2010-08-07},
title      = {Hello, Android: Introducing Google's Mobile Development Platform},
edition    = {Third edition},
isbn       = {978-1-934356-56-2},
location   = {Raleigh, N.C},
pagetotal  = {280},
publisher  = {Pragmatic Bookshelf},
abstract   = {Google's Android is shaking up the mobile market in a big way. With Android, you can write programs that run on any compatible cell phone or tablet in the world. It's a mobile platform you can't afford not to learn, and this book gets you started. Hello, Android has been updated to Android 2.3.3, with revised code throughout to reflect this updated version. That means that the book is now up-to-date for tablets such as the Kindle Fire. All examples were tested for forwards and backwards compatibility on a variety of devices and versions of Android from 1.5 to 4.0. (Note: the Kindle Fire does not support home screen widgets or wallpaper, so those samples couldn't be tested on the Fire.)Android is an operating system for mobile phones and tablets. It's inside millions of cell phones and other devices, including the hugely popular Amazon Kindle Fire, making Android the foremost platform for mobile application developers. That could be your own program running on all those devices.Within minutes, Hello, Android will get you started creating your first working application: Android's version of "Hello, World." From there, you'll build up a more substantial example: an Android Sudoku game. By gradually adding features to the game, you'll learn the basics of Android programming. You'll also see how to build in audio and video support, add graphics using 2D and 3D OpenGL, network with web pages and web services, and store data with SQLite. You'll also learn how to publish your applications to the Android Market.The ///////\\#1 book for learning Android is now in its third edition. Every page and example was reviewed and updated for compatibility with the latest versions. Freshly added material covers installing applications to the SD card, supporting multi-touch, and creating live wallpaper. You'll also find plenty of real-world advice on how to support all major Android versions in use today.If you'd rather be coding than reading about coding, this book is for you.},
shorttitle = {Hello, Android},
}
@Book{2009KothariResearch,
author    = {Kothari, C. R.},
date      = {2009},
title     = {Research Methodology Methods and Techniques},
edition   = {2nd revised edition edition},
publisher = {New Age International Publishers},
file      = {:done/2009KothariResearch Research Methodology Methods and Techniques.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2000DudaPattern,
author    = {Duda, Richard O. and Hart, Peter E. and Stork, David G.},
date      = {2000-11-09},
title     = {Pattern Classification},
edition   = {2 edition},
isbn      = {978-0-471-05669-0},
location  = {New York},
pagetotal = {688},
publisher = {Wiley-Interscience},
abstract  = {The first edition, published in 1973, has become a classic reference in the field. Now with the second edition, readers will find information on key new topics such as neural networks and statistical pattern recognition, the theory of machine learning, and the theory of invariances. Also included are worked examples, comparisons between different methods, extensive graphics, expanded exercises and computer project topics.},
file      = {:done/2000DudaPattern Pattern Classification.pdf:application/pdf},
groups    = {tesse:5},
}
@Book{2018GriffithsIntroduction,
author    = {Griffiths, David J. and Schroeter, Darrell F.},
date      = {2018-08-16},
title     = {Introduction to Quantum Mechanics},
edition   = {3 edition},
isbn      = {978-1-107-18963-8},
location  = {Cambridge},
pagetotal = {508},
publisher = {Cambridge University Press},
abstract  = {Changes and additions to the new edition of this classic textbook include a new chapter on symmetries, new problems and examples, improved explanations, more numerical problems to be worked on a computer, new applications to solid state physics, and consolidated treatment of time-dependent potentials.},
file      = {:done/2018GriffithsIntroduction Introduction to Quantum Mechanics.pdf:application/pdf},
groups    = {tesse:5, Quantum Mechanics},
}
@Book{2012AndersonStatistics,
author    = {Anderson, Thomas A. Williams David R., Dennis J. Sweeney},
date      = {2012},
title     = {Statistics for Business and Economics},
isbn      = {978-1-285-02755-5},
publisher = {Cengage Learning},
file      = {:done/2012AndersonStatistics Statistics for Business and Economics.pdf:application/pdf},
groups    = {tesse:5, Probability},
}
@Book{2014WhitePhysics,
author     = {White, Harvey E. and White, Donald H.},
date       = {2014-06-18},
title      = {Physics and Music: The Science of Musical Sound},
edition    = {Reprint edition},
isbn       = {978-0-486-77934-8},
location   = {Mineola, New York},
pagetotal  = {448},
publisher  = {Dover Publications},
abstract   = {This foundational text is written for students who want to go beyond the perceptual stage of music to learn how musical sound is created and perceived. It surveys a wide range of topics related to acoustics, beginning with a brief history of the art and science of music. Succeeding chapters explore the general principles of sound, musical scales, the primary ways in which sound can be generated, the characteristics of instruments, the use of mechanical and electronic recording devices, hi-fi stereophonic and quadraphonic sound, the design of electronic musical instruments, and architectural acoustics.Comprehensive yet accessible, Physics and Music includes over 300 diagrams, photographs, and tables. Each chapter concludes with questions, problems, and projects, in addition to references for further study. 1980 edition.},
shorttitle = {Physics and Music},
}
@Book{2005SurowieckiWisdom,
author    = {Surowiecki, James},
date      = {2005-08-16},
title     = {The Wisdom of Crowds},
edition   = {Reprint edition},
isbn      = {978-0-385-72170-7},
location  = {New York, NY},
pagetotal = {336},
publisher = {Anchor},
abstract  = {In this fascinating book, New Yorker business columnist James Surowiecki explores a deceptively simple idea: Large groups of people are smarter than an elite few, no matter how brilliant—better at solving problems, fostering innovation, coming to wise decisions, even predicting the future. With boundless erudition and in delightfully clear prose, Surowiecki ranges across fields as diverse as popular culture, psychology, ant biology, behavioral economics, artificial intelligence, military history, and politics to show how this simple idea offers important lessons for how we live our lives, select our leaders, run our companies, and think about our world.},
file      = {:done/2005SurowieckiWisdom The Wisdom of Crowds.epub:ePUB},
groups    = {tesse:5},
}
@Book{2011KotlerPrinciples,
author    = {Kotler, Philip and Armstrong, Gary},
date      = {2011-02-06},
title     = {Principles of Marketing},
edition   = {14 edition},
isbn      = {978-0-13-216712-3},
location  = {Boston},
pagetotal = {744},
publisher = {Prentice Hall},
abstract  = {Learn how to create value and gain loyal customers. Today’s marketing challenge is to create vibrant, interactive communities of consumers who make products and brands a part of their daily lives. To help readers understand how to create value and gain loyal customers, Principles of Marketing presents fundamental marketing information in a comprehensive format, organized around an innovative customer-value framework. The fourteenth edition includes coverage on sustainability and a focus on marketing in today's challenging economic climate.},
file      = {:done/2011KotlerPrinciples Principles of Marketing.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{norman_innovation_2003,
author     = {Norman, E. W. L.},
date       = {2003},
title      = {Innovation in design and technology: the polymer acoustic guitar and the case for the relegation of'the design process'},
shorttitle = {Innovation in design and technology},
}
@Book{1999KotlerMarketing,
author     = {Kotler, Philip},
date       = {1999-07-19},
title      = {Marketing Management: Millennium Edition},
edition    = {10 edition},
isbn       = {978-0-13-012217-9},
location   = {Upper Saddle River, N.J},
pagetotal  = {784},
publisher  = {Prentice Hall},
abstract   = {This world-wide best-selling book highlights the most recent trends and developments in global marketingwith an emphasis on the importance of teamwork between marketing and all the other functions of the business. It introduces new perspectives in successful strategic market planning, and presents additional company examples of creative, market-focused, and customer-driven action. Coverage includes a focus on marketing in the 21st Century that introduces the new ideas, tools and practices companies will need to successfully operate in the New Millenium. Chapter topics discuss building customer satisfaction, market-oriented strategic planning, analyzing consumer markets and buyer behavior, dealing with the competition, designing pricing strategies and programs, and managing the sales force. For marketing managers who want to increase their understanding of the major issues of strategic, tactical, and administrative marketingalong with the opportunities and needs of the marketplace in the years ahead.},
shorttitle = {Marketing Management},
}
@Book{JolleySummation,
author    = {Jolley, L. B. W.},
title     = {Summation of Series},
edition   = {Second Revised Edition edition},
isbn      = {978-0-486-60023-9},
location  = {New York},
pagetotal = {251},
publisher = {Dover Publications Inc.},
abstract  = {Over 1,100 common series, all grouped for easy reference. Arranged by category, these series include arithmetical and geometrical progressions, powers and products of natural numbers, figurate and polygonal numbers, inverse natural numbers, exponential and logarithmic series, binomials, simple inverse products, factorials, trigonometrical and hyperbolic expansions, and additional series. 1961 edition.},
file      = {:done/JolleySummation Summation of Series.pdf:application/pdf},
groups    = {tesse:2, Mathematics},
}
@Article{arcanjo_o_2013,
author   = {Arcanjo, Loque},
date     = {2013},
title    = {O Violao de Heitor Villa-Lobos entre a Belle Epoque carioca e as rodas de choro.},
url      = {https://www.academia.edu/4924894/O///////\\_Violao///////\\_de///////\\_Heitor///////\\_Villa-Lobos///////\\_entre///////\\_a///////\\_Belle///////\\_Epoque///////\\_e///////\\_as///////\\_rodas///////\\_de///////\\_choro},
urldate  = {2019-04-08},
abstract = {O Violao de Heitor Villa-Lobos entre a Belle Epoque e as rodas de choro.},
langid   = {english},
}
@InProceedings{2013RibeiroNumerical,
author     = {Ribeiro, Roseli},
booktitle  = {22nd International Congress of Mechanical Engineering},
date       = {2013},
title      = {Numerical Analysis of Acoustic Guitars Soundboards},
eventtitle = {22nd International Congress of Mechanical Engineering},
file       = {:done/2013RibeiroNumerical Numerical Analysis of Acoustic Guitars Soundboards.pdf:application/pdf},
groups     = {tesse:5},
}
@Article{2016LeeMathematical,
author       = {Lee, Meng Koon and Hosseini Fouladi, Mohammad and Namasivayam, Satesh Narayana},
date         = {2016},
journaltitle = {Advances in Acoustics and Vibration},
title        = {Mathematical Modelling and Acoustical Analysis of Classical Guitars and Their Soundboards},
doi          = {10.1155/2016/6084230},
issn         = {1687-6261, 1687-627X},
pages        = {1--10},
url          = {https://www.hindawi.com/journals/aav/2016/6084230/},
urldate      = {2019-04-08},
volume       = {2016},
langid       = {english},
}
@Collection{2007BottouLarge,
date      = {2007},
editor    = {Bottou, Léon},
title     = {Large-scale kernel machines},
isbn      = {978-0-262-02625-3},
location  = {Cambridge, Mass},
note      = {OCLC: ocm79002103},
pagetotal = {396},
publisher = {The MIT Press},
series    = {Neural information processing series},
file      = {:done/2007BottouLarge Large Scale Kernel Machines.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{yau_is_2013,
author   = {Yau, Alex and Murphy, Christian},
date     = {2013},
title    = {Is a Rigorous Agile Methodology the Best Development Strategy for Small Scale Tech Startups?},
pages    = {10},
abstract = {Recently, Agile development processes have become popular in the software development community, and have been shown to be effective in large organizations. However, given that the communication and cooperation dynamics in startup companies are very different from that of larger, more established companies, and the fact that the initial focus of a startup might be significantly different from its ultimate goal, it is questionable whether a rigid process model that works for larger companies is appropriate in tackling the problems faced by a startup. When we scale down even further and observe the small scale startup with only a few members, many of the same problems that Agile methodology sets out to solve do not even exist. Then, for a small scale startup, is it still worth putting the resources into establishing a process model? Do the benefits of adopting an Agile methodology outweigh the opportunity cost of spending the resources elsewhere? This paper examines the advantages and disadvantages of adopting an Agile methodology in a small scale tech startup and compares it to other process models, such as the Waterfall model and Lean Startup. In determining whether a rigorous agile methodology is the best development strategy for small scale tech startups, we consider the metrics of cost, time, quality, and scope in light of the particular needs of small startup organizations, and present a case study of a company that has needed to answer this very question.},
langid   = {english},
}
@Article{2018GhezziAgile,
author       = {Ghezzi, Antonio and Cavallo, Angelo},
date         = {2018-06},
journaltitle = {Journal of Business Research},
title        = {Agile Business Model Innovation in Digital Entrepreneurship: Lean Startup Approaches},
doi          = {10.1016/j.jbusres.2018.06.013},
issn         = {0148-2963},
pages        = {S014829631830300X},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S014829631830300X},
urldate      = {2019-04-08},
abstract     = {Digital startups in the early stages of their development frequently undergo innovation to their value architecture and Business Model. A set of pragmatic methods drawing on lean and agile principles has recently been proposed to support digital entrepreneurs facing Business Model Innovation (BMI), known as Lean Startup Approaches (LSAs). However, the theoretical and practical relationship between BMI and LSAs in dynamic digital environments has seldom been investigated. To fill this gap, our study draws on an exploratory multiple-case study based on three digital multisided platform startups to craft a unified framework that can disclose the relationship between BMI, LSAs and Agile Development (AD), within the context of Strategic Agility. Our findings, which emerge from the unified framework, show that LSAs can be employed as agile methods to enable Business Model Innovation in Digital Entrepreneurship. These findings are then organized around a set of propositions, with the aim of developing a research agenda directed towards integrating BMI, LSAs and AD processes and methods.},
langid       = {english},
shorttitle   = {Agile Business Model Innovation in Digital Entrepreneurship},
}
@InCollection{2016DucMinimum,
author     = {Duc, Anh Nguyen and Abrahamsson, Pekka},
booktitle  = {Agile Processes, in Software Engineering, and Extreme Programming},
date       = {2016},
title      = {Minimum Viable Product or Multiple Facet Product? The Role of MVP in Software Startups},
doi        = {10.1007/978-3-319-33515-5///////\\_10},
editor     = {Sharp, Helen and Hall, Tracy},
isbn       = {978-3-319-33514-8 978-3-319-33515-5},
location   = {Cham},
pages      = {118--130},
publisher  = {Springer International Publishing},
urldate    = {2019-04-08},
volume     = {251},
shorttitle = {Minimum Viable Product or Multiple Facet Product?},
}
@Article{2014FlyvbjergWhat,
author       = {Flyvbjerg, Bent},
date         = {2014},
journaltitle = {Project Management Journal},
title        = {What You Should Know About Megaprojects and Why: An Overview},
doi          = {10.1002/pmj.21409},
issn         = {1938-9507},
number       = {2},
pages        = {6--19},
urldate      = {2019-04-08},
volume       = {45},
abstract     = {This paper takes stock of megaproject management, an emerging and hugely costly field of study, by first answering the question of how large megaprojects are by measuring them in the units of mega, giga, and tera, and concluding with how we are presently entering a new "tera era" of trillion-dollar projects. Second, total global megaproject spending is assessed, at US$6 to US$9 trillion annually, or 8///////\\% of the total global gross domestic product (GDP), which denotes the biggest investment boom in human history. Third, four "sublimes"—political, technological, economic, and aesthetic—are identified and used to explain the increased size and frequency of megaprojects. Fourth, the "iron law of megaprojects" is laid out and documented: Over budget, over time, over and over again. Moreover, the "break–fix model" of megaproject management is introduced as an explanation of the iron law. Fifth, Albert O. Hirschman's theory of the "Hiding Hand" is revisited and critiqued as unfounded and corrupting for megaproject thinking in both the academy and policy. Sixth, it is shown how megaprojects are systematically subject to "survival of the unfittest," which explains why the worst projects get built rather than the best. Finally, it is argued that the conventional way of managing megaprojects has reached a "tension point," in which tradition is being challenged and reform is emerging.},
langid       = {english},
rights       = {© 2014 by the Project Management Institute},
shorttitle   = {What You Should Know About Megaprojects and Why},
}
@Article{2008OruetaNew,
author       = {Orueta, Fernando Diaz and Fainstein, Susan S.},
date         = {2008},
journaltitle = {International Journal of Urban and Regional Research},
title        = {The New Mega-Projects: Genesis and Impacts},
doi          = {10.1111/j.1468-2427.2008.00829.x},
issn         = {1468-2427},
number       = {4},
pages        = {759--767},
urldate      = {2019-04-08},
volume       = {32},
abstract     = {Critiques of urban renewal and large-scale developments were prominent in the period 1960–80. In particular, they emphasized the negative environmental and social consequences of these schemes and especially attacked them for displacing low-income and ethnically different populations. In the 1980s and 1990s, we saw a decline in such projects in many places, responding to popular protest and intellectual dissent, along with a new emphasis on preservation. More recently, however, we see the revival of mega-projects, often connected with tourism and sports development and incorporating the designs of world-famous architects. Frequently these are on landfill or abandoned industrial sites. The symposium for which this is an introduction shows the growing convergence of North American and European projects. This convergence is visible in their physical form, their financing, and in the role played by the state in a world marked by neoliberalism. At the same time, the new projects do display a greater environmental sensitivity and commitment to urbanity than the modernist schemes of an earlier epoch. Résumé Dans la période 1960–1980, les critiques sur les aménagements à grande échelle et les grandes rénovations urbaines étaient fréquentes. Elles soulignaient notamment les conséquences environnementales et sociales néfastes de ces programmes, en leur reprochant en particulier de déplacer les populations à faible revenu ou d'appartenance ethnique différente. Dans les années 1980 et 1990, ces projets se sont faits plus rares dans bien des endroits, répondant à la contestation populaire et au désaccord des intellectuels, parallèlement à une préoccupation nouvelle pour la préservation. Dernièrement, pourtant, les mégaprojets ont réapparu, souvent associés à un aménagement touristique ou sportif et intégrant des créations d'architectes de renommée mondiale. Ils se situent fréquemment sur le site d'anciennes décharges ou usines abandonnées. Le symposium dont ce texte sert d'introduction montre la convergence croissante des projets nord-américains et européens, convergence que l'on constate dans leur forme physique, leur financement et dans le rôle que joue l'État dans un monde empreint de néolibéralisme. En même temps, les nouveaux projets affichent une sensibilitéà l'environnement et un engagement vis-à-vis de l'urbanité plus marqués que les programmes modernistes antérieurs.},
langid       = {english},
rights       = {© 2009 The Authors. Journal Compilation © 2009 Joint Editors and Blackwell Publishing Ltd},
shorttitle   = {The New Mega-Projects},
}
@Book{2013GreimanMegaproject,
author     = {Greiman, Virginia A.},
date       = {2013-06-13},
title      = {Megaproject Management: Lessons on Risk and Project Management from the Big Dig},
doi        = {10.1002/9781118671092},
isbn       = {978-1-118-67109-2 978-1-118-11547-3},
location   = {Hoboken, NJ, USA},
publisher  = {John Wiley ////////\\\& Sons, Inc.},
urldate    = {2019-04-08},
langid     = {english},
shorttitle = {Megaproject Management},
}
@Online{2008DoriaO,
author     = {Doria, Pedro},
date       = {2008},
title      = {O verdadeiro preço de Itaipu},
url        = {https://alias.estadao.com.br/noticias/geral,o-verdadeiro-preco-de-itaipu,163784},
titleaddon = {Estadão},
urldate    = {2019-04-08},
abstract   = {Uma negociação a portas fechadas, no tempo dos militares, volta para assombrar o Brasil},
langid     = {brazil},
}
@Online{2013JeronimoPonte,
author     = {Jeronimo, Josie},
date       = {2013-02-07},
title      = {A ponte de R$ 7 bilhões},
url        = {https://istoe.com.br/274208///////\\_A+PONTE+DE+R+7+BILHOES/},
titleaddon = {ISTOÉ Independente},
urldate    = {2019-04-08},
abstract   = {Há muitas dúvidas sobre a necessidade da obra que ligará Salvador a Itaparica. A certeza é que o valor pago pelo governo da Bahia está superdimensionado},
langid     = {brazil},
}
@Online{2014OtavioO,
author     = {Otávio, Chico and Góes, Bruno},
date       = {2014},
title      = {O Globo - Ponte Rio Niterói},
url        = {http://infograficos.oglobo.globo.com/pais/ponte-rio-niteroi.html},
titleaddon = {O Globo},
urldate    = {2019-04-08},
}
@Online{2018PrestesPrestes,
author     = {Prestes, Monica},
date       = {2018},
title      = {Prestes a ser concluída, Belo Monte é criticada por atingidos e especialistas},
url        = {https://temas.folha.uol.com.br/projeto-amazonia/hidreletricas/prestes-a-ser-concluida-belo-monte-e-criticada-por-atingidos-e-especialistas.shtml},
titleaddon = {Folha de São Paulo},
urldate    = {2019-04-08},
abstract   = {Mudança na vazão dos rios para abastecer Belo Monte muda fauna e hábitos///////\\&},
langid     = {brazil},
}
@Online{noauthor_em_2010,
date       = {2010-04-09},
title      = {Em dinheiro de hoje, Brasília custaria US$ 83 bilhões - Brasília 50 anos - iG},
url        = {https://ultimosegundo.ig.com.br/brasilia50anos/em-dinheiro-de-hoje-brasilia-custaria-us-83-bilhoes/n1237588758783.html},
titleaddon = {Último Segundo},
urldate    = {2019-04-08},
abstract   = {Valor equivale a quase seis vezes o que o Brasil pretende investir nos Jogos Olímpicos de 2016},
langid     = {brazil},
}
@InReference{noauthor_estadio_2019,
booktitle = {Wikipédia, a enciclopédia livre},
date      = {2019-04-05},
title     = {Estádio do Maracanã},
note      = {Page Version ID: 54720810},
url       = {https://pt.wikipedia.org/w/index.php?title=Est///////\\%C3///////\\%A1dio///////\\_do///////\\_Maracan///////\\%C3///////\\%A3///////\\&oldid=54720810},
urldate   = {2019-04-08},
abstract  = {Estádio Jornalista Mário Filho, mais conhecido como Maracanã, o popular Maraca ("semelhante a um chocalho" em tupi-guarani, devido ao som de pássaros que viviam por ali), é um estádio de futebol localizado na Zona Norte do Rio de Janeiro e inaugurado em 1950, durante o mandato do então General de Divisão e Prefeito do Distrito Federal do Rio de Janeiro Marechal Ângelo Mendes de Moraes, tendo sido utilizado na Copa do Mundo de Futebol daquele ano. Desde então, o Maracanã foi palco de grandes momentos do futebol brasileiro e mundial, como o milésimo gol de Pelé, finais do Campeonato Brasileiro, Carioca de Futebol, Taça Libertadores da América e do primeiro Campeonato Mundial de Clubes da FIFA, além de competições internacionais e partidas da Seleção Brasileira. O estádio foi um dos locais de competição dos Jogos Pan-Americanos de 2007, recebendo o futebol, as cerimônias de abertura e de encerramento. Sediou futebol e as cerimônias de abertura e encerramento dos Jogos Olímpicos de 2016, que foram realizados na cidade do Rio de Janeiro. Foi também o palco da partida final da Copa das Confederações de 2013 e da Copa do Mundo FIFA de 2014Ao longo do tempo, no entanto, o estádio passou a assumir caráter de espaço multiúso ao receber outros eventos como espetáculos e partidas de outros esportes, como o voleibol em uma oportunidade. Após diversas obras de modernização, a capacidade do estádio é de 78 838 espectadores, sendo o maior estádio do Brasil.},
langid    = {portuguese},
rights    = {Creative Commons Attribution-ShareAlike License},
}
@Online{noauthor_maracana:_2013,
date       = {2013},
title      = {Maracanã: ‘preço final’ é R$ 1,19 bi, 69///////\\% acima do previsto},
url        = {https://veja.abril.com.br/esporte/maracana-preco-final-e-r-119-bi-69-acima-do-previsto/},
titleaddon = {VEJA.com},
urldate    = {2019-04-08},
abstract   = {Governo afirma que não haverá outras surpresas indesejadas para contribuinte},
langid     = {brazil},
shorttitle = {Maracanã},
}
@Online{2018OttaTransposicao,
author     = {Otta, Lu},
date       = {2018},
title      = {Transposição custará R$ 800 milhões ao ano - Economia},
url        = {https://economia.estadao.com.br/noticias/geral,transposicao-custara-r-800-milhoes-ao-ano,70002268817},
titleaddon = {Estadão},
urldate    = {2019-04-08},
abstract   = {Ministério da Transparência diz que se projeto de transposição do Rio São Francisco não se autossustentar conta será paga pelo Tesouro},
langid     = {brazil},
}
@Online{noauthor_projeto_2018,
date       = {2018-12-18},
title      = {Projeto de Integração do Rio São Francisco: uma obra que já entrou para a história},
url        = {https://www.huffpostbrasil.com/2018/12/18/projeto-de-integracao-do-rio-sao-francisco-uma-obra-que-ja-entrou-para-a-historia///////\\_a///////\\_23621697/},
titleaddon = {HuffPost Brasil},
urldate    = {2019-04-08},
abstract   = {O projeto de integração do Rio São Franscisco é a mais importante iniciativa do governo federal em temos da Política Nacional de Recursos Hídricos.},
langid     = {portuguese},
shorttitle = {Projeto de Integração do Rio São Francisco},
}
@Online{noauthor_brasil_2015,
date       = {2015-01-20},
title      = {Brasil tem 6 das 100 obras mais importantes do mundo},
url        = {//www.brasil247.com/pt/247/brasil/167233/Brasil-tem-6-das-100-obras-mais-importantes-do-mundo.htm},
titleaddon = {Brasil 247},
urldate    = {2019-04-08},
abstract   = {Seis grandes obras de infraestrutura do Brasil estão entre as 100 mais importantes do mundo, de acordo com lista feita pela consultoria internacional KPMG; quatro delas têm o carimbo do Programa de Aceleração do Crescimento (PAC), do governo federal, Seis grandes obras de infraestrutura do Brasil estão entre as 100 mais importantes do mundo, de acordo com lista feita pela consultoria internacional KPMG; quatro delas têm o carimbo do Programa de Aceleração do Crescimento (PAC), do governo federal},
langid     = {brazil},
}
@InReference{noauthor_ef-354_2019,
booktitle = {Wikipédia, a enciclopédia livre},
date      = {2019-03-03},
title     = {EF-354},
note      = {Page Version ID: 54416427},
url       = {https://pt.wikipedia.org/w/index.php?title=EF-354///////\\&oldid=54416427},
urldate   = {2019-04-08},
abstract  = {A Ferrovia Transcontinental - EF-354 (também referida como Ferrovia Transoceânica), é o projeto de uma ferrovia firmado entre os governos do Brasil e Peru, que busca conectar o Oceano Atlântico, no litoral brasileiro ao Oceano Pacífico no litoral peruano, atravessando de Leste a Oeste o continente Sul-americano. A extensão total é estimada em 4.400 km em solo brasileiro.},
langid    = {portuguese},
rights    = {Creative Commons Attribution-ShareAlike License},
}
@Online{2018MaiaFerrovia,
author     = {Maia, Tião},
date       = {2018},
title      = {Ferrovia Transoceânica começa a sair do papel},
url        = {https://www.expressoamazonia.com.br/index.php/economia/413-ferrovia-transoceanica-comeca-a-sair-do-papel.html},
titleaddon = {Expresso Amazônia},
urldate    = {2019-04-08},
}
@Online{2013NederCusto,
author     = {Neder, Vinícius},
date       = {2013},
title      = {Custo para explorar pré-sal de Libra pode chegar a US$ 400 bilhões - Economia},
url        = {https://economia.estadao.com.br/noticias/geral,custo-para-explorar-pre-sal-de-libra-pode-chegar-a-us-400-bilhoes,169408e},
titleaddon = {Estadão},
urldate    = {2019-04-09},
abstract   = {Valor foi estimado por consultoria internacional e é quatro vezes superior à projeção da ANP},
langid     = {brazil},
}
@Online{noauthor_angra_2000,
date       = {2000},
title      = {Angra 2 começa a funcionar após gastar R$ 12 bilhões},
url        = {https://www1.folha.uol.com.br/fsp/dinheiro/fi2307200009.htm},
titleaddon = {Folha de São Paulo},
urldate    = {2019-04-09},
}
@InReference{noauthor_usina_2019,
booktitle = {Wikipédia, a enciclopédia livre},
date      = {2019-03-12},
title     = {Usina Hidrelétrica Santo Antônio},
note      = {Page Version ID: 54490770},
url       = {https://pt.wikipedia.org/w/index.php?title=Usina///////\\_Hidrel///////\\%C3///////\\%A9trica///////\\_Santo///////\\_Ant///////\\%C3///////\\%B4nio///////\\&oldid=54490770},
urldate   = {2019-04-09},
abstract  = {A Hidrelétrica Santo Antônio está localizada no Rio Madeira, na cidade de Porto Velho, capital de Rondônia. Possui 50 turbinas do tipo Bulbo para geração de energia elétrica com potência de cerca de 71,6 megawatts (MW) cada uma, totalizando 3.568,3 MW de potência instalada e 2.424 MW de energia assegurada. É a quarta maior hidrelétrica em operação no Brasil e uma das maiores do mundo. A concessionária responsável pela hidrelétrica é a Santo Antônio Energia, atualmente quarta maior geradora hídrica do país, formada pelas empresas Odebrecht Energia do Brasil, SAAG Investimentos, Furnas Centrais Elétricas, Cemig e Caixa FIP Amazônia Energia. A hidrelétrica, juntamente com a de Jirau, no mesmo rio, são consideradas fundamentais para o suprimento de energia elétrica no Brasil e estiveram entre as obras mais importantes do Governo Federal entre 2008 e 2016.O leilão de concessão foi realizado em dezembro de 2007. Os estudos de inventário e viabilidade aconteceram previamente entre os anos de 2001 e 2006. Em 2008 as obras foram iniciadas. Em 30 de março a hidrelétrica recebeu autorização da Agência Nacional de Energia Elétrica (Aneel) para iniciar sua operação. As obras de construção foram concluías em dezembro de 2016.},
langid    = {portuguese},
rights    = {Creative Commons Attribution-ShareAlike License},
}
@Article{2012Wang"leagile",
author       = {Wang, Xiaofeng and Conboy, Kieran and Cawley, Oisin},
date         = {2012-06},
journaltitle = {Journal of Systems and Software},
title        = {"Leagile" software development: An experience report analysis of the application of lean approaches in agile software development},
doi          = {10.1016/j.jss.2012.01.061},
issn         = {0164-1212},
number       = {6},
pages        = {1287--1299},
url          = {https://linkinghub.elsevier.com/retrieve/pii/S0164121212000404},
urldate      = {2019-04-09},
volume       = {85},
abstract     = {In recent years there has been a noticeable shift in attention from those who use agile software development toward lean software development, often labelled as a shift "from agile to lean". However, the reality may not be as simple or linear as this label implies. To provide a better understanding of lean software development approaches and how they are applied in agile software development, we have examined 30 experience reports published in past agile software conferences in which experiences of applying lean approaches in agile software development were reported. The analysis identified six types of lean application. The results of our study show that lean can be applied in agile processes in different manners for different purposes. Lean concepts, principles and practices are most often used for continuous agile process improvement, with the most recent introduction being the kanban approach, introducing a continuous, flow-based substitute to time-boxed agile processes.},
langid       = {english},
shorttitle   = {"Leagile" software development},
}
@Misc{2013SirkiaeLean,
author  = {Sirkiä, Rami and Laanti, Maarit},
date    = {2013},
title   = {Lean and agile financial planning},
url     = {http://www.scrummaster.dk/lib/AgileLeanLibrary/Topics////////\\_ScalingScrumAgile/Whitepaper///////\\_///////\\%20Lean-Agile///////\\%20Financial///////\\%20Planning///////\\%20(Dec///////\\%202013).pdf},
urldate = {2019-04-09},
}
@Book{1994DudequeHistoria,
author = {Dudeque, Norton},
date   = {1994},
title  = {História do violão},
isbn   = {85-85132-85-X},
file   = {:done/1994DudequeHistoria História Do Violão.pdf:application/pdf},
groups = {tesse:5},
}
@Book{2012CostaIntroducao,
author    = {Costa, S. F.},
date      = {2012},
title     = {Introdução Ilustrada à Estatística},
edition   = {Edição: 1ª},
isbn      = {978-85-294-0419-6},
publisher = {Harbra},
abstract  = {No Brasil, poucos livros, principalmente de Estatística, passam da 2ª ou 3ª edição. O sucesso desta obra, agora em 5.ª edição, totalmente revista, deve-se, indiscutivelmente, à preocupação do autor com a organização e apresentação da matéria e à calorosa acolhida que a obra vem recebendo por parte dos usuários desde seu lançamento em 1988. Continua disponível, aos usuários que contatem a Editora por meio de formulário incluso no livro, o gabarito da Curva Normal, instrumento grandemente facilitador na resolução de alguns exercícios e problemas que dependam da visualização do gráfico, além das respostas aos exercícios propostos no livro. Aos professores estão disponíveis as soluções dos exercícios propostos no livro do aluno, além de exercícios e problemas adicionais. O gabarito com a Curva Normal também faz parte desse material. Sem dúvida, uma obra que prioriza a compreensão por parte dos alunos.},
}
@InProceedings{2018KarrasProgressive,
author    = {Karras, Tero and Aila, Timo and Laine, Samuli and Lehtinen, Jaakko},
booktitle = {ICLR 2018},
date      = {2018},
title     = {Progressive Growing Of Gans For Improved Quality, Stability, And Variation},
pages     = {26},
abstract  = {We describe a new training methodology for generative adversarial networks. The key idea is to grow both the generator and discriminator progressively: starting from a low resolution, we add new layers that model increasingly ﬁne details as training progresses. This both speeds the training up and greatly stabilizes it, allowing us to produce images of unprecedented quality, e.g., CELEBA images at 10242. We also propose a simple way to increase the variation in generated images, and achieve a record inception score of 8.80 in unsupervised CIFAR10. Additionally, we describe several implementation details that are important for discouraging unhealthy competition between the generator and discriminator. Finally, we suggest a new metric for evaluating GAN results, both in terms of image quality and variation. As an additional contribution, we construct a higher-quality version of the CELEBA dataset.},
file      = {:done/2018KarrasProgressive Progressive Growing of Gans for Improved Quality, Stability, and Variation.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
year      = {2018},
}
@Article{2018EslamiNeural,
author       = {Eslami, S. M. Ali and Jimenez Rezende, Danilo and Besse, Frederic and Viola, Fabio and Morcos, Ari S. and Garnelo, Marta and Ruderman, Avraham and Rusu, Andrei A. and Danihelka, Ivo and Gregor, Karol and Reichert, David P. and Buesing, Lars and Weber, Theophane and Vinyals, Oriol and Rosenbaum, Dan and Rabinowitz, Neil and King, Helen and Hillier, Chloe and Botvinick, Matt and Wierstra, Daan and Kavukcuoglu, Koray and Hassabis, Demis},
date         = {2018-06-15},
journaltitle = {Science},
title        = {Neural scene representation and rendering},
doi          = {10.1126/science.aar6170},
issn         = {0036-8075, 1095-9203},
number       = {6394},
pages        = {1204--1210},
urldate      = {2019-04-16},
volume       = {360},
abstract     = {Scene representation—the process of converting visual sensory data into concise descriptions—is a requirement for intelligent behavior. Recent work has shown that neural networks excel at this task when provided with large, labeled datasets. However, removing the reliance on human labeling remains an important open problem. To this end, we introduce the Generative Query Network (GQN), a framework within which machines learn to represent scenes using only their own sensors. The GQN takes as input images of a scene taken from different viewpoints, constructs an internal representation, and uses this representation to predict the appearance of that scene from previously unobserved viewpoints. The GQN demonstrates representation learning without human labels or domain knowledge, paving the way toward machines that autonomously learn to understand the world around them.},
file         = {:done/2018EslamiNeural Neural Scene Representation and Rendering.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2017ChivukulaAdversarial,
author     = {Chivukula, Aneesh Sreevallabh and Liu, Wei},
booktitle  = {2017 International Joint Conference on Neural Networks (IJCNN)},
date       = {2017-05},
title      = {Adversarial learning games with deep learning models},
doi        = {10.1109/IJCNN.2017.7966196},
eventtitle = {2017 International Joint Conference on Neural Networks (IJCNN)},
isbn       = {978-1-5090-6182-2},
location   = {Anchorage, AK, USA},
pages      = {2758--2767},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/7966196/},
urldate    = {2019-04-16},
}
@Article{2018TuylsSymmetric,
author       = {Tuyls, Karl and Pérolat, Julien and Lanctot, Marc and Ostrovski, Georg and Savani, Rahul and Leibo, Joel Z. and Ord, Toby and Graepel, Thore and Legg, Shane},
date         = {2018-12},
journaltitle = {Scientific Reports},
title        = {Symmetric Decomposition of Asymmetric Games},
doi          = {10.1038/s41598-018-19194-4},
issn         = {2045-2322},
number       = {1},
pages        = {1015},
url          = {http://www.nature.com/articles/s41598-018-19194-4},
urldate      = {2019-04-16},
volume       = {8},
file         = {:done/2018TuylsSymmetric Symmetric Decomposition of Asymmetric Games.pdf:application/pdf},
groups       = {tesse:5, Game Theory},
langid       = {english},
}
@Article{2019ParkSemantic,
author       = {Park, Taesung and Liu, Ming-Yu and Wang, Ting-Chun and Zhu, Jun-Yan},
date         = {2019-03-18},
journaltitle = {arXiv:1903.07291 [cs]},
title        = {Semantic Image Synthesis with Spatially-Adaptive Normalization},
eprint       = {1903.07291},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1903.07291},
urldate      = {2019-04-16},
abstract     = {We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style as synthesizing images. Code will be available at https://github.com/NVlabs/SPADE .},
file         = {:done/2019ParkSemantic Semantic Image Synthesis with Spatially Adaptive Normalization.pdf:application/pdf},
groups       = {tesse:5},
}
@Book{1984CremerPhysics,
author     = {Cremer, Lothar},
date       = {1984-11-14},
title      = {The Physics of the Violin},
edition    = {UK ed. edition},
isbn       = {978-0-262-52707-1},
pagetotal  = {474},
publisher  = {The MIT Press},
translator = {Allen, John S.},
abstract   = {This major work covers almost all that has been learned about the acoustics of stringed instruments from Helmholtz's 19th-century theoretical elaborations to recent electroacoustic and holographic measurements. Many of the results presented here were uncovered by the author himself (and by his associates and students) over a 20-year period of research on the physics of instruments in the violin family. Lothar Cremer is one of the world's most respected authorities on architectural acoustics and, not incidentally, an avid avocational violinist and violist.The book―which was published in German in 1981―first of all meets the rigorous technical standards of specialists in musical acoustics. But it also serves the needs and interests of two broader groups: makers and players of stringed instruments are expressly addressed, since the implications of the mathematical formulations are fully outlined and explained; and acousticians in general will find that the work represents a textbook illustration of the application of fundamental principles and up-to-date techniques to a specific problem. The first―and longest―of the book's three parts investigates the oscillatory responses of bowed (and plucked) strings. The natural nonlinearities that derive from considerations of string torsion and bending stiffness are deftly handled and concisely modeled. The second part deals with the body of the instrument. Special attention is given to the bridge, which transmits the oscillations of the strings to the wooden body and its air cavity. In this case, linear modeling proves serviceable for the most part―a simplification that would not be possible with lute―like instruments such as the guitar. The radiation of sound from the body into the listener's space, which is treated as an extension of the instrument itself, is the subject of the book's final part.},
}
@Article{2016FriedPerspective,
author       = {Fried, Ohad and Shechtman, Eli and Goldman, Dan B. and Finkelstein, Adam},
date         = {2016-07-11},
journaltitle = {ACM Transactions on Graphics},
title        = {Perspective-aware manipulation of portrait photos},
doi          = {10.1145/2897824.2925933},
issn         = {0730-0301},
number       = {4},
pages        = {1--10},
url          = {http://dl.acm.org/citation.cfm?doid=2897824.2925933},
urldate      = {2019-04-18},
volume       = {35},
abstract     = {This paper introduces a method to modify the apparent relative pose and distance between camera and subject given a single portrait photo. Our approach ﬁts a full perspective camera and a parametric 3D head model to the portrait, and then builds a 2D warp in the image plane to approximate the effect of a desired change in 3D. We show that this model is capable of correcting objectionable artifacts such as the large noses sometimes seen in "selﬁes," or to deliberately bring a distant camera closer to the subject. This framework can also be used to re-pose the subject, as well as to create stereo pairs from an input portrait. We show convincing results on both an existing dataset as well as a new dataset we captured to validate our method.},
file         = {:done/2016FriedPerspective Perspective Aware Manipulation of Portrait Photos.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{tahvanainen_modelling_nodate,
author   = {Tahvanainen, Henna},
title    = {Modelling body vibration and sound radiation of a modiﬁed kantele},
pages    = {73},
abstract = {In this thesis, it is shown that the modiﬁed kantele includes vibrational modes of both the freely vibrating top plate and the enclosed air. Thus, it has a higher mode density than the traditional kanteles. Because of the coupling of the enclosed air modes to the body, the modiﬁed kantele radiates more omni-directionally than the traditional kanteles. Consequently, the modiﬁed kantele has a higher radiation eﬃciency than the traditional kanteles when the size of the air gap is small (1-3 mm).},
langid   = {english},
}
@Software{2019VdumoulinTechnical,
author     = {vdumoulin},
date       = {2019-04-18},
title      = {A technical report on convolution arithmetic in the context of deep learning: vdumoulin/convarithmetic},
note       = {original-date: 2016-02-24T15:18:33Z},
url        = {https://github.com/vdumoulin/conv///////\\_arithmetic},
urldate    = {2019-04-18},
rights     = {MIT},
shorttitle = {A technical report on convolution arithmetic in the context of deep learning},
}
@InProceedings{2014WangEnergy,
author     = {Wang, Y. and Li, B. and Luo, R. and Chen, Y. and Xu, N. and Yang, H.},
booktitle  = {2014 Design, Automation Test in Europe Conference Exhibition (DATE)},
date       = {2014-03},
title      = {Energy efficient neural networks for big data analytics},
doi        = {10.7873/DATE.2014.358},
eventtitle = {2014 Design, Automation Test in Europe Conference Exhibition (DATE)},
pages      = {1--2},
abstract   = {The world is experiencing a data revolution to discover knowledge in big data. Large scale neural networks are one of the mainstream tools of big data analytics. Processing big data with large scale neural networks includes two phases: the training phase and the operation phase. Huge computing power is required to support the training phase. And the energy efficiency (power efficiency) is one of the major considerations of the operation phase. We first explore the computing power of GPUs for big data analytics and demonstrate an efficient GPU implementation of the training phase of large scale recurrent neural networks (RNNs). We then introduce a promising ultrahigh energy efficient implementation of neural networks' operation phase by taking advantage of the emerging memristor technique. Experiment results show that the proposed GPU implementation of RNNs is able to achieve 2 11× speed-up compared with the basic CPU implementation. And the scaled-up recurrent neural network trained with GPUs realizes an accuracy of 47 /\\% on the Microsoft Research Sentence Completion Challenge, the best result achieved by a single RNN on the same dataset. In addition, the proposed memristor-based implementation of neural networks demonstrates power efficiency of > 400 GFLOPS/W and achieves energy savings of 22× on the HMAX model compared with its pure digital implementation counterpart.},
file       = {:done/2014WangEnergy Energy Efficient Neural Networks for Big Data Analytics.pdf:application/pdf},
groups     = {tesse:5},
}
@InProceedings{2014KapralovaBig,
author    = {Kapralova, Olga and Alex, John and Weinstein, Eugene and Moreno, Pedro and Siohan, Olivier},
booktitle = {Conference of the International Speech Communication Association (Interspeech)},
date      = {2014},
title     = {A big data approach to acoustic model training corpus selection},
file      = {:done/2014KapralovaBig A Big Data Approach to Acoustic Model Training Corpus Selection.pdf:application/pdf},
groups    = {tesse:5},
}
@InProceedings{2018LiptonBbq,
author     = {Lipton, Zachary and Li, Xiujun and Gao, Jianfeng and Li, Lihong and Ahmed, Faisal and Deng, Li},
booktitle  = {Thirty-Second AAAI Conference on Artificial Intelligence},
date       = {2018},
title      = {Bbq-networks: Efficient exploration in deep reinforcement learning for task-oriented dialogue systems},
shorttitle = {Bbq-networks},
}
@InProceedings{2018KimKorean,
author    = {Kim, Juntae and Choi, Heejin and Park, Jinuk and Kim, Sangjin and Kim, Jongjin and Hahn, Minsoo},
booktitle = {INTERSPEECH 2018},
date      = {2018},
title     = {Korean Singing Voice Synthesis System based on an LSTM Recurrent Neural Network},
publisher = {International Speech Communication Association},
file      = {:done/2018KimKorean Korean Singing Voice Synthesis System Based on an LSTM Recurrent Neural Network.pdf:application/pdf},
groups    = {tesse:5},
}
@InProceedings{2016NishimuraSinging,
author    = {Nishimura, Masanari and Hashimoto, Kei and Oura, Keiichiro and Nankaku, Yoshihiko and Tokuda, Keiichi},
booktitle = {Interspeech},
date      = {2016},
title     = {Singing Voice Synthesis Based on Deep Neural Networks.},
pages     = {2478--2482},
}
@InProceedings{2014ZenDeep,
author     = {Zen, Heiga and Senior, Andrew},
booktitle  = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date       = {2014-05},
title      = {Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis},
doi        = {10.1109/ICASSP.2014.6854321},
eventtitle = {ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn       = {978-1-4799-2893-4},
location   = {Florence, Italy},
pages      = {3844--3848},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/6854321/},
urldate    = {2019-04-18},
abstract   = {Statistical parametric speech synthesis (SPSS) using deep neural networks (DNNs) has shown its potential to produce naturally-sounding synthesized speech. However, there are limitations in the current implementation of DNN-based acoustic modeling for speech synthesis, such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations, this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech.},
langid     = {english},
}
@Article{2017BlaauwNeurala,
author       = {Blaauw, Merlijn and Bonada, Jordi},
date         = {2017},
journaltitle = {Applied Sciences},
title        = {A neural parametric singing synthesizer modeling timbre and expression from natural songs},
number       = {12},
pages        = {1313},
volume       = {7},
}
@Article{fosler-lussier_markov_nodate,
author   = {Fosler-Lussier, Eric},
title    = {Markov Models and Hidden Markov Models: A Brief Tutorial},
pages    = {9},
abstract = {This tutorial gives a gentle introduction to Markov models and Hidden Markov models as mathematical abstractions, and relates them to their use in automatic speech recognition. This material was developed for the Fall 1995 semester of CS188: Introduction to Arti cial Intelligence at the University of California, Berkeley. It is targeted for introductory AI courses basic knowledge of probability theory (e.g. Bayes' Rule) is assumed. This version is slightly updated from the original, including a few minor error corrections, a short /Further Reading" section, and exercises that were given as a homework in the Fall 1995 class.},
langid   = {english},
}
@Article{2019GafniVid2game,
author       = {Gafni, Oran and Wolf, Lior and Taigman, Yaniv},
date         = {2019-04-17},
journaltitle = {arXiv:1904.08379 [cs, stat]},
title        = {Vid2Game: Controllable Characters Extracted from Real-World Videos},
eprint       = {1904.08379},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1904.08379},
urldate      = {2019-04-21},
abstract     = {We are given a video of a person performing a certain activity, from which we extract a controllable model. The model generates novel image sequences of that person, according to arbitrary user-defined control signals, typically marking the displacement of the moving body. The generated video can have an arbitrary background, and effectively capture both the dynamics and appearance of the person. The method is based on two networks. The first network maps a current pose, and a single-instance control signal to the next pose. The second network maps the current pose, the new pose, and a given background, to an output frame. Both networks include multiple novelties that enable high-quality performance. This is demonstrated on multiple characters extracted from various videos of dancers and athletes.},
file         = {:done/2019GafniVid2game Vid2Game/\\_ Controllable Characters Extracted from Real World Videos.pdf:application/pdf},
groups       = {tesse:5},
shorttitle   = {Vid2Game},
}
@Book{2018SubramanianDeep,
author     = {Subramanian, Vishnu},
date       = {2018},
title      = {Deep learning with PyTorch a practical approach to building neural network models using PyTorch},
isbn       = {978-1-78862-607-1},
location   = {Birmingham, UK},
note       = {OCLC: 1078352321},
publisher  = {Packt Publishing},
url        = {http://proxy2.hec.ca/login?url=http://proquestcombo.safaribooksonline.com/?uiCode=hecmontreal///////\\&xmlId=9781788624336},
urldate    = {2019-03-26},
langid     = {english},
shorttitle = {Deep learning with PyTorch},
}
@Book{2003LakatosFundamentos,
author    = {Lakatos, Eva Maria and Marconi, Marina de Andrade},
date      = {2003},
title     = {Fundamentos de metodologia científica},
isbn      = {978-85-224-3397-1},
location  = {São Paulo},
note      = {OCLC: 53849497},
publisher = {Atlas São Paulo},
file      = {:done/2003LakatosFundamentos Fundamentos De Metodologia Científica.pdf:application/pdf},
groups    = {tesse:5},
langid    = {portuguese},
}
@Electronic{2014Berkeley,
date         = {2014},
title        = {The Berkeley Artificial Intelligence Research Blog},
url          = {http://bair.berkeley.edu/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
}
@Electronic{2018Caffe,
date         = {2018},
title        = {Caffe Deep Learning Framework},
url          = {http://caffe.berkeleyvision.org/},
urldate      = {2018-06-30},
howpublished = {online},
}
@Electronic{2018Making,
date         = {2018},
title        = {Making music using new sounds generated with machine learning},
url          = {https://www.blog.google/technology/ai/making-music-using-new-sounds-generated-machine-learning/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
location     = {Disponvel em},
publisher    = {<},
}
@Electronic{2015Ivy,
date         = {2015},
title        = {Ivy Audio},
url          = {http://www.ivyaudio.com/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
}
@Electronic{2018Keras,
date         = {2018},
title        = {Keras: The Python Deep Learning library},
url          = {https://keras.io/},
urldate      = {2018-06-30},
howpublished = {online},
location     = {Keras Documentation, [s.d.]. Disponvel em},
publisher    = {<},
}
@Electronic{2018Magenta,
date         = {2018},
title        = {Magenta},
url          = {https://magenta.tensorflow.org/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
location     = {Magenta Disponvel em},
publisher    = {<},
}
@Electronic{2011University,
date         = {2011},
title        = {University of Iowa Electronic Music Studios},
url          = {http://theremin.music.uiowa.edu/MIS.html},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
unidentified = {Disponvel em: <},
volume       = {2018},
}
@Electronic{2018Neon,
date         = {2018},
title        = {Neon},
url          = {https://ai.intel.com/neon/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
}
@Electronic{2018Nsynthsuper,
date         = {2018},
title        = {NSynthSuper},
url          = {https://nsynthsuper.withgoogle.com/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
location     = {GoogleGoogle Disponvel em},
publisher    = {<},
}
@Article{1986RumelhartLearning,
author       = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
date         = {1986-10-09},
journaltitle = {Nature},
title        = {Learning representations by back-propagating errors},
doi          = {10.1038/323533a0},
pages        = {533 EP -},
volume       = {323},
day          = {09},
publisher    = {Nature Publishing Group SN -},
}
@Electronic{2018Torch,
date         = {2018},
title        = {Torch, a scientific computing framework for LuaJIT},
url          = {http://torch.ch/},
urldate      = {2018-06-30},
howpublished = {online},
}
@Electronic{2018Philharmonia,
date         = {2018},
title        = {Philharmonia Orchestra},
url          = {http://www.philharmonia.co.uk/explore/sound///////\\_samples},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
unidentified = {Disponvel em: <},
volume       = {2018},
}
@Electronic{2018Tensorflow,
date         = {2018},
title        = {TensorFlow},
url          = {https://www.tensorflow.org/},
urldate      = {2018-06-30},
groups       = {tesse:4},
howpublished = {online},
location     = {TensorFlow Disponvel em},
publisher    = {<},
}
@Electronic{2018Repositorio,
date         = {2018},
title        = {Repositório Dissertação},
url          = {https://github.com/tesserato/tesserato.github.io},
urldate      = {2018-06-30},
howpublished = {online},
}
@Electronic{2018Theano,
date         = {2018},
title        = {Theano 1.0.0 documentation},
url          = {http://deeplearning.net/software/theano/},
urldate      = {2018-06-30},
howpublished = {online},
}
@Electronic{2012Rmsprop,
date         = {2012},
title        = {Rmsprop: Divide the gradient by a running average of its recent magnitude.},
url          = {https://www.coursera.org/lecture/neural-networks/rmsprop-divide-the-gradient-by-a-running-average-of-its-recent-magnitude-YQHki},
urldate      = {2018-06-30},
howpublished = {online},
journaltitle = {COURSERA: Neural networks for machine learning},
volume       = {4},
}
@Article{2017HutchingsTalking,
author        = {Hutchings, P.},
date          = {2017},
journaltitle  = {arXiv preprint arXiv:1706.09558},
title         = {Talking Drums: Generating drum grooves with neural networks},
eprint        = {1706.09558},
eprinttype    = {arXiv},
number        = {1},
pages         = {43--47},
url           = {http://arxiv.org/abs/1706.09558},
volume        = {1},
abstract      = {Presented is a method of generating a full drum kit part for a provided kick-drum sequence. A sequence to sequence neural network model used in natural language translation was adopted to encode multiple musical styles and an online survey was developed to test different techniques for sampling the output of the softmax function. The strongest results were found using a sampling technique that drew from the three most probable outputs at each subdivision of the drum pattern but the consistency of output was found to be heavily dependent on style.},
arxivid       = {1706.09558},
keywords      = {generative music,music generation,percussion,rnn,state of the art,translation},
mendeley-tags = {music generation,state of the art},
}
@Article{1999JiangImagea,
author       = {Jiang, J},
date         = {1999},
journaltitle = {Signal Processing: Image Communication},
title        = {Image compression with neural networks--a survey},
number       = {9},
pages        = {737--760},
volume       = {14},
publisher    = {Elsevier},
}
@Article{2007BengioScaling,
author        = {Bengio, Yoshua and LeCun, Yann and Lecun, Yann},
date          = {2007},
journaltitle  = {Large Scale Kernel Machines},
title         = {Scaling Learning Algorithms towards AI},
doi           = {10.1.1.72.4580},
issn          = {0009-9104},
number        = {1},
pages         = {321--360},
abstract      = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
file          = {:done/2007BengioScaling Scaling Learning Algorithms Towards AI.pdf:application/pdf},
groups        = {tesse:5},
isbn          = {1002620262},
keywords      = {theory},
mendeley-tags = {theory},
pmid          = {11359439},
}
@Article{1999YaoEvolvinga,
author        = {Yao, Xin},
date          = {1999},
journaltitle  = {Proceedings of the IEEE},
title         = {Evolving artificial neural networks},
doi           = {10.1109/5.784219},
eprint        = {1108.1530},
eprinttype    = {arXiv},
issn          = {0018-9219},
number        = {9},
pages         = {1423--1447},
volume        = {87},
abstract      = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
arxivid       = {1108.1530},
isbn          = {9780470287194},
keywords      = {evolutionary computation,evolutive,intelligent systems,neu-,seminal},
mendeley-tags = {evolutive,seminal},
pmid          = {9821520},
publisher     = {IEEE},
}
@Article{2016KrishnaEyeriss,
author        = {Krishna, Tushar and Emer, Joel and Sze, Vivienne and Conference, International Solid-state Circuits and Francisco, San and Chen, Yu-hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
date          = {2016},
title         = {Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks The MIT Faculty has made this article openly available . Please share Citation " Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Publisher Version},
keywords      = {hardware},
mendeley-tags = {hardware},
}
@InProceedings{2012KasabovNeucube,
author       = {Kasabov, Nikola},
booktitle    = {ANNPR},
date         = {2012},
title        = {NeuCube EvoSpike Architecture for Spatio-temporal Modelling and Pattern Recognition of Brain Signals.},
organization = {Springer},
pages        = {225--243},
}
@Article{2013StilgoeDeveloping,
author       = {Stilgoe, Jack and Owen, Richard and Macnaghten, Phil},
date         = {2013-11},
journaltitle = {Research Policy},
title        = {Developing a framework for responsible innovation},
doi          = {10.1016/j.respol.2013.05.008},
issn         = {0048-7333},
number       = {9},
pages        = {1568--1580},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0048733313000930},
volume       = {42},
abstract     = {The governance of emerging science and innovation is a major challenge for contemporary democracies. In this paper we present a framework for understanding and supporting efforts aimed at 'responsible innovation'. The framework was developed in part through work with one of the first major research projects in the controversial area of geoengineering, funded by the UK Research Councils. We describe this case study, and how this became a location to articulate and explore four integrated dimensions of responsible innovation: anticipation, reflexivity, inclusion and responsiveness. Although the framework for responsible innovation was designed for use by the UK Research Councils and the scientific communities they support, we argue that it has more general application and relevance. ?? 2013 Elsevier B.V. All rights reserved.},
keywords     = {Emerging technologies,Ethics,Geoengineering,Governance,Responsible innovation},
publisher    = {Elsevier B.V.},
}
@Article{MorseMagnitude,
author = {Morse, Bryan},
title  = {Magnitude and Phase The Fourier Transform : Examples , Properties , Common Pairs CS 450 : Introduction to Digital Signal and Image Processing Example : Fourier Transform of a Cosine Example : Fourier Transform of a Cosine Odd and Even Functions Sinusoids},
}
@Article{2017LouizosL,
author     = {Louizos, Christos and Welling, Max},
date       = {2017},
title      = {L EARNING S PARSE N EURAL N ETWORKS THROUGH L 0 R EGULARIZATION},
eprint     = {arXiv:1712.01312v1},
eprinttype = {arXiv},
pages      = {1--13},
arxivid    = {arXiv:1712.01312v1},
file       = {:done/2017LouizosL L EARNING S PARSE N EURAL N ETWORKS tHROUGH L 0 R EGULARIZATION.pdf:application/pdf},
groups     = {tesse:2},
}
@Misc{Unknown,
title = {Unknown - Unknown - Leaps and bounds.pdf.pdf},
}
@Article{2014DonahueSynthesizing,
author     = {Donahue, Chris and Mcauley, Julian and Puckette, Miller},
date       = {2014},
title      = {Synthesizing Audio with Generative Adversarial Networks},
eprint     = {arXiv:1802.04208v1},
eprinttype = {arXiv},
arxivid    = {arXiv:1802.04208v1},
}
@Article{1942No,
date   = {1942},
title  = {No Title},
pages  = {1--9},
groups = {tesse:4},
}
@Article{2014WeinmanTop,
author = {Weinman, Jaime J},
date   = {2014},
title  = {Top of page},
number = {May 2012},
pages  = {261277},
groups = {tesse:4},
}
@Article{2013LedfordStart,
author = {Ledford, B Y Heidi},
date   = {2013},
title  = {START-UP},
}
@Article{1998SamplerRedefining,
author       = {Sampler, J.},
date         = {1998},
journaltitle = {Strategic Management Journal},
title        = {Redefining industry structure for the information age},
number       = {4},
pages        = {343--355},
url          = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0266(199804)19:4{////////\\%}3C343::AID-SMJ975{////////\\%}3E3.0.CO;2-G/abstract},
volume       = {19},
abstract     = {We are entering a new competitive age in which the basis of competition is being fundamentally altered through the introduction of advanced information technologies and public communication infrastructures, such as the Internet. In these environments, the nature and locus of competition will radically alter, as information becomes an increasingly important resource. This paper develops ideas around the strategic characteristics of information-information separability and industry concentration, related diversification, and innovation for firms competing in the Information Age.},
keywords     = {industry boundary,industry structure,information,information age,information separability},
publisher    = {Wiley Online Library},
}
@Article{XuSupertagging,
author       = {Xu, W and Auli, M and Clark, S Ccg},
journaltitle = {CL (},
title        = {Supertagging with a Recurrent Neural Network},
volume       = {2},
}
@Book{TheNo,
author = {The, Rossing},
title  = {No Title},
isbn   = {0060189878},
groups = {tesse:4},
}
@Article{GrinsteinNo,
author     = {Grinstein, Eric and Duong, Ngoc Q K and Ozerov, Alexey and Patrick, P},
title      = {No Title},
eprint     = {arXiv:1710.11385v1},
eprinttype = {arXiv},
arxivid    = {arXiv:1710.11385v1},
groups     = {tesse:4},
}
@Misc{1962,
title = {1962 On Estimation of a Probability Density Function and Mode.pdf},
}
@Misc{1988,
title = {1988 A theoretical framework for back-propagation.pdf},
}
@InProceedings{2016WuInvestigatinga,
author       = {Wu, Zhizheng and King, Simon},
booktitle    = {Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
date         = {2016},
title        = {Investigating gated recurrent networks for speech synthesis},
organization = {IEEE},
pages        = {5140--5144},
}
@Unpublished{No,
title  = {No Title},
url    = {https://github.com/tesserato/tesserato.github.io{////////\\%}3E},
groups = {tesse:4},
}
@Article{TheDead,
author = {The, I S},
title  = {DEAD ?},
number = {March 2002},
}
@Article{2016LeonetiUtility,
author       = {Leoneti, Alexandre Bevilacqua},
date         = {2016},
journaltitle = {Operations Research Perspectives},
title        = {Utility Function for modeling Group Multicriteria Decision Making problems as games},
doi          = {10.1016/j.orp.2016.04.001},
issn         = {2214-7160},
pages        = {21--26},
volume       = {3},
keywords     = {Utility function,Group Multicriteria Decision,MAUT},
publisher    = {Elsevier Ltd},
}
@Article{2006DodgsonRole,
author = {Dodgson, Mark and Gann, David and Salter, Ammon},
date   = {2006},
title  = {The role of technology in the shift towards open innovation : the case},
pages  = {333--346},
}
@Misc{Estelles,
title = {Estelles-Arolas, Gonzalez-Ladron-de-Guevara - 2012 - Towards an integrated crowdsourcing definition.pdf},
}
@Article{2014MerriProperties,
author     = {Merri, Bart Van},
date       = {2014},
title      = {On the Properties of Neural Machine Translation: Encoder–Decoder Approaches},
eprint     = {arXiv:1409.1259v2},
eprinttype = {arXiv},
arxivid    = {arXiv:1409.1259v2},
}
@Article{Noa,
title      = {No Title},
eprint     = {arXiv:1709.08243v2},
eprinttype = {arXiv},
arxivid    = {arXiv:1709.08243v2},
groups     = {tesse:4},
}
@Article{1994Lee-mortimerStrategic,
author = {Lee-mortimer, Andrew},
date   = {1994},
title  = {Strategic Design},
number = {2},
pages  = {31--34},
volume = {1},
}
@Article{Modelling,
title  = {MODELLING THE DECAY OF PIANO SOUNDS Tian Cheng , Simon Dixon , Matthias Mauch Centre for Digital Music , Queen Mary University of London , London , United Kingdom},
volume = {1},
}
@Article{2008Digital,
date   = {2008},
title  = {Digital Synthesis of and Plucked-String Timbres},
number = {2},
pages  = {43--55},
volume = {7},
}
@Article{2016SalihSecond,
author = {Salih, A},
date   = {2016},
title  = {Second-Order Wave Equation d ' Alembert ' s Solution in Infinite Domain},
number = {December},
pages  = {1--24},
}
@Article{2014DippolitoImportance,
author       = {D'Ippolito, Beatrice},
date         = {2014-02},
journaltitle = {Technovation},
title        = {The importance of design for firms׳ competitiveness: A review of the literature},
doi          = {10.1016/j.technovation.2014.01.007},
issn         = {0166-4972},
pages        = {1--15},
url          = {http://www.sciencedirect.com/science/article/pii/S016649721400008X},
abstract     = {Scholars dedicated increasing attention towards appreciating how design has changed individuals׳ perception of new products, firms׳ understanding and formulation of strategy, or other relevant actors׳ approach to innovation and technology management. By emphasising the importance of design for the definition of consumers׳ needs, the restructuring of firms׳ organisational structures and strategies, and the evolution of firms׳ value creation processes, this review paper identifies relevant research gaps and questions that would benefit from future scholarly attention. In particular, it is suggested that such effort should address the analysis of how design consumption can help better comprehend consumers׳ needs; what are the implications of design thinking on the skill sets of design professionals; the organisational structure of firms, including the reconfiguration of other business functions, and their strategy; and whether and how design thinking can shape firms׳ value creation processes and contribute to the formalisation of design tasks.},
keywords     = {Literature Review Process},
publisher    = {Elsevier},
}
@Article{2012FosterCreative,
author   = {Foster, Richard N},
date     = {2012},
title    = {Creative Destruction Whips through Corporate America},
keywords = {Kodak, Radio Shack, Bear Stearns},
}
@Book{2018Nsynthsupera,
date      = {2018},
title     = {NSynthSuper},
location  = {GoogleGoogle Disponvel em},
publisher = {<},
url       = {https://nsynthsuper.withgoogle.com/{////////\\%}3E},
groups    = {tesse:4},
}
@Article{MasriUsing,
author = {Masri, P},
title  = {USING DIGITAL WAVEGUIDES},
number = {0},
volume = {44},
}
@Article{2012Bettis-outlandDecision,
author       = {Bettis-Outland, H},
date         = {2012-06},
journaltitle = {Journal of Business Research},
title        = {Decision-making's impact on organizational learning and information overload},
doi          = {10.1016/j.jbusres.2010.12.021},
issn         = {0148-2963},
number       = {6},
pages        = {814--820},
url          = {http://www.sciencedirect.com/science/article/pii/S0148296310002845},
volume       = {65},
abstract     = {Although an abundance of academic literature positions organizational information processing as antecedent to decision making, little attention is paid to the possibility that decision making can be antecedent to certain elements of organizational information processing. Specifically, does the decision making process impact the type of organizational learning that takes place? Do different approaches to decision making alter the amount and variety of information made available to the organization, that is, the level of information overload? This paper examines incremental and comprehensive decision making to understand the effects of different decision making types on organizational learning and information overload. Incrementalism suggests that decision making should take place in small steps or increments. This approach analyzes only a few scenarios to make decisions resulting in few, if any, major organizational changes. However, comprehensive decision making requires the consideration of all possible scenarios and potential outcomes, resulting in a major overhaul of traditions and procedures within the organization. Consequently, each decision making approach has a different impact on organizational learning and information overload. (C) 2011 Elsevier Inc. All rights reserved.},
keywords     = {Decision making,Incrementalism,Information overload,Organizational learning},
publisher    = {Elsevier Inc.},
}
@Article{2007BengioScalinga,
author = {Bengio, Yoshua and Lecun, Yann},
date   = {2007},
title  = {Scaling Learning Algorithms towards AI To appear in " Large-Scale Kernel Machines ",},
number = {1},
pages  = {1--41},
}
@Article{GOVINDAN20161,
author   = {Govindan, Kannan and Jepsen, Martin Brandt},
title    = {ELECTRE: A comprehensive literature review on methodologies and applications},
doi      = {https://doi.org/10.1016/j.ejor.2015.07.019},
issn     = {0377-2217},
number   = {1},
pages    = {1 - 29},
url      = {http://www.sciencedirect.com/science/article/pii/S0377221715006529},
volume   = {250},
abstract = {Multi-criteria decision analysis (MCDA) is a valuable resource within operations research and management science. Various MCDA methods have been developed over the years and applied to decision problems in many different areas. The outranking approach, and in particular the family of ELECTRE methods, continues to be a popular research field within MCDA, despite its more than 40 years of existence. In this paper, a comprehensive literature review of English scholarly papers on ELECTRE and ELECTRE-based methods is performed. Our aim is to investigate how ELECTRE and ELECTRE-based methods have been considered in various areas. This includes area of applications, modifications to the methods, comparisons with other methods, and general studies of the ELECTRE methods. Although a significant amount of literature on ELECTRE is in a language different from English, we focus only on English articles, because many researchers may not be able to perform a study in some of the other languages. Each paper is categorized according to its main focus with respect to ELECTRE, i.e. if it considers an application, performs a review, considers ELECTRE with respect to the problem of selecting an MCDA method or considers some methodological aspects of ELECTRE. A total of 686 papers are included in the review. The group of papers considering an application of ELECTRE consists of 544 papers, and these are further categorized into 13 application areas and a number of sub-areas. In addition, all papers are classified according to the country of author affiliation, journal of publication, and year of publication. For the group of applied papers, the distribution by ELECTRE version vs. application area and ELECTRE version vs. year of publication are provided. We believe that this paper can be a valuable source of information for researchers and practitioners in the field of MCDA and ELECTRE in particular.},
journal  = {European Journal of Operational Research},
keywords = {Multiple criteria decision aiding (MCDA), Outranking, ELECTRE, Literature review},
year     = {2016},
}
@Book{2018GaziUnderstanding,
author    = {Gazi, O.},
date      = {2018},
title     = {Understanding digital signal processing},
location  = {Traducao. [s.l.]},
publisher = {Springer},
file      = {:done/2018GaziUnderstanding Understanding Digital Signal Processing.epub:ePUB},
groups    = {tesse:5, DSP},
}
@Article{2015CourtneyArxiv,
author     = {Courtney, Elya and Courtney, Michael},
date       = {2015},
title      = {arXiv : 1507 . 01832v1 [ physics . data-an ] 6 Jul 2015},
eprint     = {arXiv:1507.01832v1},
eprinttype = {arXiv},
arxivid    = {arXiv:1507.01832v1},
}
@Article{1995Unclassified,
date  = {1995},
title = {UNCLASSIFIED LIMITATION CHANGES TO : FROM :},
}
@Article{2018Theanoa,
date         = {2018},
journaltitle = {Multilayer Perceptron - DeepLearning 0.},
title        = {Theano},
url          = {http://deeplearning.net/software/theano/{////////\\%}3E},
volume       = {1},
}
@Book{2018Magentaa,
date      = {2018},
title     = {Magenta},
location  = {Magenta Disponvel em},
publisher = {<},
url       = {https://magenta.tensorflow.org/{////////\\%}3E},
groups    = {tesse:4},
}
@Article{SamplesUniversity,
author = {Samples, Musical Instrument},
title  = {University of Iowa Electronic Music Studios},
url    = {http://theremin.music.uiowa.edu/MIS.html{////////\\%}3E},
volume = {2018},
groups = {tesse:4},
}
@Article{Literature,
journaltitle = {2016},
title        = {A literature review is a description of the literature relevant to a particular field or topic. It gives an overview of:},
abstract     = {literature review},
}
@Article{RumelhartLearning,
author       = {Rumelhart, D E and Hinton, G E and Williams, R J},
journaltitle = {[s.l.] California Univ San Diego La Jolla Inst for Cognitive Science},
title        = {Learning internal representations by error propagation.},
volume       = {1985},
}
@Article{2017EisenachN,
author = {Eisenach, Carson and Wang, Zhaoran},
date   = {2017},
title  = {N ONPARAMETRICALLY L EARNING A CTIVATION F UNCTIONS IN D EEP N EURAL N ETS},
pages  = {1--23},
}
@Book{PapertPerceptrons,
author = {Papert, Seymour A},
title  = {Perceptrons},
isbn   = {0262631113},
}
@Misc{2002DewickTechnological,
author   = {Dewick, Paul and Green, Ken and Miozzo, Marcela},
date     = {2002},
title    = {Technological Change, Industry Structure and the Environment},
abstract = {This paper seeks to contribute towards the construction and application of a method to assess the long-term impact of the development of their technological technologies on the environment. The paper identifies the effect of three pervasive technologies – biotechnology, information technology and nanotechnology – on the production of a range of sectors and their consequent environmental effects. The sectors are selected according to taxonomies of characteristics. The technological impact is assessed qualitatively in terms of changes in production scale and resource intensity and their resulting impact on industrial greenhouse gas emissions},
number   = {January},
pages    = {1--31},
}
@Misc{Wang,
title = {Wang et al. - 2011 - Rapid parametric design methods for shoe-last customization.pdf},
}
@Article{MARTTUNEN20171,
author   = {Marttunen, Mika and Lienert, Judit and Belton, Valerie},
title    = {Structuring problems for Multi-Criteria Decision Analysis in practice: A literature review of method combinations},
doi      = {https://doi.org/10.1016/j.ejor.2017.04.041},
issn     = {0377-2217},
number   = {1},
pages    = {1 - 17},
url      = {http://www.sciencedirect.com/science/article/pii/S0377221717303880},
volume   = {263},
abstract = {Structuring problems for Multi-Criteria Decision Analysis (MCDA) has attracted increasing attention over the past 20 years from both a conceptual and a practical perspective. This is reflected in a significant growth in the number of published applications which use a formal approach to problem structuring in combination with an analytic method for multi-criteria analysis. The problem structuring approaches (PSMs) include general methodologies such as Checkland's Soft Systems Method (SSM), Eden and Ackermann's Strategic Options Design and Analysis (SODA) and other methods that focus on a particular aspect. We carried out a literature review that covers eight PSMs (Cognitive and Causal Maps, DPSIR, Scenario Planning, SSM, Stakeholder Analysis, Strategic Choice Approach, SODA and SWOT) and seven MCDA methods (AHP, ANP, ELECTRE, MAUT, MAVT, PROMETHEE and TOPSIS). We first identified and analysed 333 articles published during 2000-2015, then selected 68 articles covering all PSM-MCDA combinations, which were studied in detail to understand the associated processes, benefits and challenges. The three PSMs most commonly combined with MCDA are SWOT, Scenario Planning and DPSIR. AHP was by far the most commonly applied MCDA method. Combining PSMs with MCDA produces a richer view of the decision situation and enables more effective support for different phases of the decision-making process. Some limitations and challenges in combining PSMs and MCDA are also identified, most importantly relating to building a value tree and assigning criteria weights.},
journal  = {European Journal of Operational Research},
keywords = {Problem structuring, Multiple Criteria Decision Analysis, Multi-methodology, Multi-stakeholder decision-making},
year     = {2017},
}
@Article{2008BrabhamCrowdsourcing,
author       = {Brabham, D. C.},
date         = {2008-02},
journaltitle = {Convergence: The International Journal of Research into New Media Technologies},
title        = {Crowdsourcing as a Model for Problem Solving: An Introduction and Cases},
doi          = {10.1177/1354856507084420},
issn         = {1354-8565},
number       = {1},
pages        = {75--90},
volume       = {14},
abstract     = {Crowdsourcing is an online, distributed problem-solving and production model that has emerged in recent years. Notable examples of the model include Threadless, iStockphoto, Inno- Centive, the Goldcorp Challenge, and user-generated advertising contests. This article provides an introduction to crowdsourcing, both its theoretical grounding and exemplar cases, taking care to distinguish crowdsourcing from open source production. This article also explores the possibilities for the model, its potential to exploit a crowd of innovators, and its potential for use beyond for- profit sectors. Finally, this article proposes an agenda for research into crowdsourcing.},
isbn         = {1354856507084},
keywords     = {about human ingenuity,challenge,collective intelligence,crowdsourcing,designer,distributed problem solving,goldcorp,innocentive,is going on,istockphoto,its unfolding is to,of client,open source,reject the binary notion,right now,story to be told,the first step to,the next step is,there is an incredible,threadless,to look to what,wisdom of crowds},
}
@Article{2015StefanoState,
author = {Stefano, Nara Medianeira and Stefano, N M and Filho, N Casarotto and Vergara, L G L and Rocha, R U G},
date   = {2015},
title  = {State of the Art Research and its Applications COPRAS ( Complex Proportional Assessment ): State of the Art Research and its Applications},
doi    = {10.1109/TLA.2015.7404925},
number = {January 2016},
}
@Article{AudioIvy,
author = {Audio, Ivy},
title  = {Ivy Audio},
url    = {http://www.ivyaudio.com/{////////\\%}3E},
volume = {2018},
groups = {tesse:4},
}
@Article{2014SathyanarayanaGentlea,
author = {Sathyanarayana, Shashi and Ph, D},
date   = {2014},
title  = {A Gentle Introduction to Backpropagation What is so difficult about designing a neural},
pages  = {1--15},
}
@Book{EckMaking,
author    = {Eck, D},
title     = {Making music using new sounds generated with machine learning},
location  = {Disponvel em},
publisher = {<},
url       = {https://www.blog.google/technology/ai/making-music-using-new-sounds-generated-machine-learning/{////////\\%}3E},
groups    = {tesse:4},
}
@Article{1987ScottFive,
author = {Scott, Mel and Bruce, Richard},
date   = {1987},
title  = {Five Stages of Growth Business in Small},
number = {3},
volume = {20},
}
@Misc{Poverty,
title = {Poverty and profits in the information age.pdf},
}
@Article{2015HarrisonEdinburgh,
author = {Harrison, Reginald L and Bilbao, Stefan},
date   = {2015},
title  = {Edinburgh Research Explorer An algorithm for a valved brass instrument synthesis environment using finite-difference time-domain methods with performance optimisation AN ALGORITHM FOR A VALVED BRASS INSTRUMENT SYNTHESIS ENVIRONMENT USING FINITE-DIFFERENCE},
}
@Article{2006DewickModelling,
author       = {Dewick, Paul and Green, Ken and Fleetwood, Toby and Miozzo, Marcela},
date         = {2006-11},
journaltitle = {Technological Forecasting and Social Change},
title        = {Modelling creative destruction: Technological diffusion and industrial structure change to 2050},
doi          = {10.1016/j.techfore.2006.04.002},
issn         = {0040-1625},
number       = {9},
pages        = {1084--1106},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0040162506000862},
volume       = {73},
abstract     = {Future disruptive, pervasive technologies will have important consequences for industrial structure, economic growth and the environment. Drawing on theories of technological diffusion, industrial evolution and long-term technological change this paper explores the effect of the development and diffusion of two future pervasive technologies on five industrial sectors in three regions during the 21st century in terms of their effect on economic structural change. Through semi-structured interviews with over 100 experts in the two technologies, the paper quantifies the effects of future biotechnologies and nanotechnologies on the industrial structure of the EU, USA and China in 2020 and 2050. The paper finds that as a result of the development and diffusion of future biotechnologies and nanotechnologies, some industries grow whilst others decline and some new ones emerge. The evidence suggests that the effect is different across countries and time; whereas the experts commonly believe that effect of the technologies on the industrial structure of the EU and US is likely to be similar, the effect in China is considered to be less by 2020 but the same as in the EU and US by 2050. This finding has important implications for the location of production, economic growth and energy demand in the future. ?? 2006 Elsevier Inc. All rights reserved.},
keywords     = {Biotechnologies,Energy,Industrial structure,Nanotechnologies,Technological diffusion},
}
@Unpublished{Torch,
title = {Torch},
url   = {http://torch.ch/{////////\\%}3E},
}
@Article{2013BernroiderEpub,
author = {Bernroider, Edward and Schmöllerl, Patrick},
date   = {2013},
title  = {ePub WU Institutional Repository A technological , organisational , and environmental analysis},
}
@Article{2000MontiNo,
author = {Monti, Giuliano and Sandler, Mark},
date   = {2000},
title  = {No Title},
number = {1},
pages  = {7--10},
groups = {tesse:4},
}
@Book{2018Tensorflowa,
date      = {2018},
title     = {TensorFlow},
location  = {TensorFlow Disponvel em},
publisher = {<},
url       = {https://www.tensorflow.org/{////////\\%}3E},
groups    = {tesse:4},
}
@Article{SteinPrinceton,
author       = {Stein, E M and Shakarchi, R},
journaltitle = {Traducao. [s.l.] Princeton University Press},
title        = {Princeton lectures in analysis.},
volume       = {2003},
}
@Misc{Unknowna,
title = {Unknown - Unknown - Democratizing Innovation.pdf.pdf},
}
@Book{2006SmithBasic,
author    = {Smith, J A},
date      = {2006},
title     = {basic introduction to digital waveguide synthesis (for the technically inclined)},
location  = {Stanford University. stanford. edu/ jos/swgt},
publisher = {Center for Computer Research in Music and Acoustics (CCRMA)},
url       = {http://ccrma},
}
@Misc{Democratizing,
title = {Democratizing innovation.pdf},
}
@Misc{1983,
title = {Extensions of the Karplus-Strong Plucked String Algorithm},
}
@Article{2009BrunetteReviewa,
author = {Brunette, E S and Flemmer, R C and Flemmer, C L A},
date   = {2009},
title  = {review of artificial intelligence},
pages  = {4},
volume = {2009},
}
@Article{2001Jacquet-lagrPreference,
author   = {Jacquet-lagr, Eric},
date     = {2001},
title    = {Preference disaggregation : 20 years of MCDA experience},
pages    = {233--245},
volume   = {130},
keywords = {1,criteria,general philosophy,goal programming,in decision-making involving multiple,introduction and background,multicriteria analysis,preference disaggregation,regression},
}
@Book{AdlerDesign,
author = {Adler, Isabel K},
title  = {Design Thinking Design Thinking Inovação em negócios},
isbn   = {9788565424004},
}
@Book{AnalysisNo,
author = {Analysis, Multi-criteria Decision},
title  = {No Title},
isbn   = {9781119974079},
groups = {tesse:4},
}
@Article{1994SchumpeterThomas,
author = {Schumpeter, Joseph A and Mccraw, Thomas and Mirowski, Phillip},
date   = {1994},
title  = {Thomas K . McCraw , Cambridge : Harvard University Press , 719 pages ,},
pages  = {1--8},
}
@Article{ZwickerPsychoacoustics,
author       = {Zwicker, E and Fastl, H},
journaltitle = {Traducao. [s.l.] Springer Science ////////\\\& Business Media},
title        = {Psychoacoustics: Facts and models.},
volume       = {2013},
}
@Article{VincentEfficient,
author     = {Vincent, Pascal and Bouthillier, Xavier},
title      = {Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets [ Technical report ]},
eprint     = {arXiv:1412.7091v3},
eprinttype = {arXiv},
pages      = {1--15},
arxivid    = {arXiv:1412.7091v3},
}
@Article{2012Estelles-arolasTowards,
author       = {Estelles-Arolas, E. and Gonzalez-Ladron-de-Guevara, F.},
date         = {2012},
journaltitle = {Journal of Information Science},
title        = {Towards an integrated crowdsourcing definition},
doi          = {10.1177/016555150000000},
number       = {2},
pages        = {189--200},
volume       = {38},
abstract     = {Crowdsourcing is a relatively recent concept that encompasses many practices. This diversity leads to the blurring of the limits of crowdsourcing that may be identified virtually with any type of Internet-based collaborative activity, such as co-creation or user innovation. Varying definitions of crowdsourcing exist and therefore, some authors present certain specific examples of crowdsourcing as paradigmatic, while others present the same examples as the opposite. In this paper, existing definitions of crowdsourcing are analyzed to extract common elements and to establish the basic characteristics of any crowdsourcing initiative. Based on these existing definitions, an exhaustive and consistent definition for crowdsourcing is presented and contrasted in eleven cases.},
isbn         = {0165551500000},
keywords     = {crowdsourcing,definition,innovation},
}
@Article{1997DavidcacchioneAmerican,
author = {Cacchione, David A.},
date   = {1997},
title  = {American Scientist},
doi    = {10.1511/2011.89.106},
issn   = {0003-0996},
number = {2},
pages  = {108--112},
volume = {85},
isbn   = {2136240900},
}
@Article{2008PisanoWhich,
author       = {Pisano, Gary P. and Verganti, Roberto},
date         = {2008},
journaltitle = {Harvard Business Review},
title        = {Which Kind of Collaboration Is Right for You?},
number       = {12},
pages        = {78--86},
url          = {http://search.ebscohost.com/login.aspx?direct=true{////////\\&}db=buh{////////\\&}AN=35387060{////////\\&}site=ehost-live{////////\\%}5Cnhttp://content.ebscohost.com/ContentServer.asp?T=P{////////\\&}P=AN{////////\\&}K=35387060{////////\\&}S=R{////////\\&}D=buh{////////\\&}EbscoContent=dGJyMMvl7ESep7Q4wtvhOLCmr0uep65Ssqu4TLGWxWXS{////////\\&}ContentCustomer=dGJyMPGotk{////////\\%}2BxrLZQuePfgeyx44Dt6fIA},
volume       = {86},
abstract     = {Nowadays, virtually no companies innovate alone. Firms team up with a variety of partners, in a wide number of ways, to create new technologies, products, and services. But what is the best way to leverage the power of outsiders? To help executives answer that question, Pisano, of Harvard Business School, and Verganti, of Politecnico di Milano, developed a simple framework focused on two questions: Given your strategy, how open or closed should your network of collaborators be? And who should decide which problems to tackle and which solutions to adopt? There are four basic modes of collaboration, say the authors. An elite circle is a closed network with a hierarchical governance: One company selects the participants, defines the problem, and chooses the solution. For instance, Alessi, an Italian home-products company, invited 200 outside experts in postmodern architecture to contribute ideas for new home-product designs. An innovation mall is hierarchical but open: Anyone can post a problem or propose solutions in it, but the company posting the problem chooses the solution. An example is InnoCentive.com, an eBay-like site where companies post scientific challenges. An innovation community is open and decentralized: Anyone can propose problems, offer solutions, and decide which ideas to use -- as happens in the Linux open-source software community. A consortium is a private group of participants that operate as equals and jointly select problems, decide how to conduct work, and choose solutions. IBM has set up a number of consortia with other companies to develop next-generation semiconductor technologies. No one approach is superior; each involves strategic trade-offs. When choosing among modes, firms must weigh their advantages and challenges, and assess which will work best with their strategy, capabilities, structure, and assets. INSETS: The Four Ways to Collaborate;How to Choose the Best Mode of Collaboration.},
}
@Article{2000InformationChapter,
author = {Information, Background and Of, Description and Mcdm, Some},
date   = {2000},
title  = {Chapter 2 MULTI-CRITERIA DECISION MAKING METHODS 2.1},
}
@Article{2001LairdPhysicalb,
author = {Laird, Joel Augustus},
date   = {2001},
title  = {THE PHYSICAL MODELLING OF DRUMS USING DIGITAL},
number = {November},
}
@Article{2017MixReliability,
author   = {Mix, Power and Dester, Mauricio and Dester, M},
date     = {2017},
title    = {Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Power Mix},
doi      = {10.1109/TLA.2016.7459613},
number   = {March 2016},
keywords = {energy storage systems,intermittent sources,mix,multi-criteria decision analysis,power,renewable sources},
}
@Book{Licensed,
title = {Licensed to : CengageBrain User Licensed to : CengageBrain User},
isbn  = {9781111221294},
}
@Article{2006ReiffensteinPatents,
author       = {Reiffenstein, T Codification},
date         = {2006},
journaltitle = {The Canadian Geographer/Le Gographe canadien, v.},
title        = {patents and the geography of knowledge transfer in the electronic musical instrument industry},
volume       = {50},
}
@Article{2007BerthonWhen,
author       = {Berthon, Pierre R. and Pitt, Leyland F. and McCarthy, Ian and Kates, Steven M.},
date         = {2007-01},
journaltitle = {Business Horizons},
title        = {When customers get clever: Managerial approaches to dealing with creative consumers},
doi          = {10.1016/j.bushor.2006.05.005},
issn         = {0007-6813},
number       = {1},
pages        = {39--47},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0007681306000796},
volume       = {50},
abstract     = {Creative consumers (defined as customers who adapt, modify, or transform a proprietary offering) represent an intriguing paradox for business. On one hand, they can signify a black hole for future revenue, with breach of copyright and intellectual property. On the other hand, they represent a gold mine of ideas and business opportunities. Central to business is the need to create and capture value, and creative consumers demand a shift in the mindsets and business models of how firms accomplish both. Based upon their attitude and action toward customer innovation, we develop a typology of firms' stances toward creative consumers. We then consider the implications of the stances model for corporate strategy and examine a three-step approach to dealing with creative consumers: awareness, analysis, and response. ?? 2006 Kelley School of Business, Indiana University.},
keywords     = {Creative customers,Diagnostics,Firm stance,Strategic response},
}
@Article{2016LeiteProcess,
author       = {Leite, M.L.G. and Purcidonio, P.M. and Tarjano, C.},
date         = {2016},
journaltitle = {Espacios},
title        = {The process of product development for startups based on creative innovation},
issn         = {0798-1015},
number       = {7},
volume       = {37},
abstract     = {The systematic innovation has been considered one of the most important practices in the business environment, increasingly uncertain and changeable. Understanding how changes they are occurring in society motivated by advances in information technology have impacted the innovation process, through the lens of startups. it is essential for the economic growth of a country, since most new products come these processes. The aim of this study is to develop a new model of development products in startups and small organizations seeking to develop physical products from innovations based on creativity.},
keywords     = {Innovation,Product development,Startups},
}
@Article{1998SolomonMulti,
author   = {Solomon, Anthony and Wishart, Nicole and Dublish, Sandipa},
date     = {1998},
title    = {Multi-attribute decision making : A simulation comparison of select methods},
number   = {97},
volume   = {2217},
keywords = {multiple criteria analysis},
}
@Article{2017ElectronicWave,
author   = {Electronic, A N and Of, Journal and Catalana, Societat and Atiques, D E Matem},
date     = {2017},
title    = {The wave equation for stiff strings and piano tuning},
doi      = {10.2436/20.2002.02.11.1},
pages    = {1--16},
volume   = {3},
groups   = {Acoustics},
keywords = {00a65,2010,35g16,35l05,dissonance,inharmonic spectrum,msc,musical scale,stiffness,string,vibrating,wave equation},
}
@Article{1994ChaigneNumerical,
author = {Chaigne, Antoine and Cedex, Paris},
date   = {1994},
title  = {Numerical simulations of piano strings . I . A physical model for a struck string using finite difference methods},
number = {February},
pages  = {1112--1118},
volume = {95},
}
@Book{2015HeMultimodal,
author    = {He, L and Others},
date      = {2015},
title     = {Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks},
edition   = {Proceeding},
publisher = {AnaisACM},
groups    = {tesse:4},
}
@Article{2000Copyright,
date  = {2000},
title = {Copyright 2000. All Rights Reserved.},
}
@Article{2016PhumrattanaprapinMachine,
author   = {Phumrattanaprapin, Khanittha},
date     = {2016},
title    = {Machine with Multilayer Perceptron},
number   = {2},
pages    = {196--204},
volume   = {10},
keywords = {chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
}
@Article{AngellWinners,
author = {Angell, Ian},
title  = {Winners and Losers in the Information Age},
}
@Misc{Leaps,
title = {Leaps and bounds},
}
@Article{2011WangRapid,
author       = {Wang, Jin and Zhang, Haining and Lu, Guodong and Liu, Zheng},
date         = {2011-01},
journaltitle = {The International Journal of Advanced Manufacturing Technology},
title        = {Rapid parametric design methods for shoe-last customization},
doi          = {10.1007/s00170-010-3144-y},
issn         = {0268-3768},
number       = {1-4},
pages        = {173--186},
volume       = {54},
abstract     = {With the development of computer-aided design (CAD) technology and increasing demands of customized footwear, shoe-lasts are requested to be designed rapidly so as to speed-up the process of footwear manufacturing. Thus, this study presents a CAD system for shoe-last rapid customized design based on the piecewise reconstruction to realize the interactive deformation and separate/global shoe-last form reuse. First, piecewise remodeling method is proposed based on the multi-layer parametric definition and contour curves are extracted from the mesh. Then, five types of proper constraints to support surface manipulation are proposed, and the draft-driven deformation by the contour curve bending can realize the interactive local surface design in free angle of view. Finally, shoe-last styles can be saved and reused globally or separately to share design results between different shoe-lasts. Experimental examples show that customized shoe-lasts can be easily and rapidly generated by adopting the parametric design methods.},
keywords     = {deformation,form reuse,interactive,parametric design,shoe-last},
}
@Article{1993PoczosPerceptron,
author = {Poczos, Lecturer Barnabas},
date   = {1993},
title  = {Perceptron History of Artificial Neural Networks The Neuron},
number = {1982},
pages  = {1--10},
}
@Article{RobelSignals,
author = {Robel, Axel},
title  = {Signals},
}
@Article{1998GuitouniTentative,
author   = {Guitouni, Adel and Martel, Jean-marc},
date     = {1998},
title    = {Tentative guidelines to help choosing an appropriate MCDA method},
pages    = {501--521},
volume   = {109},
keywords = {aggregation procedure,behavioural considerations,comparative analysis,decision making situation,multicriteria analysis,multicriterion,multicriterion decision aid method,preferences modelling},
}
@Article{Boulanger-lewandowskiPhone,
author = {Boulanger-lewandowski, Nicolas},
title  = {PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS Universit ´ e de Montr ´ Montr ´ Jasha Droppo Mike Seltzer Dong Yu One Microsoft Way},
number = {5},
}
@Article{2013HutchinsTop,
author = {Hutchins, Aaron},
date   = {2013},
title  = {Top of page},
pages  = {649206},
groups = {tesse:4},
}
@Article{2015DeheExpert,
author       = {Dehe, Benjamin and Bamford, David},
date         = {2015},
journaltitle = {Expert Systems With Applications},
title        = {Expert Systems with Applications Development , test and comparison of two Multiple Criteria Decision Analysis ( MCDA ) models : A case of healthcare infrastructure location},
doi          = {10.1016/j.eswa.2015.04.059},
issn         = {0957-4174},
number       = {19},
pages        = {6717--6727},
volume       = {42},
keywords     = {ahp,analytical hierarchy process,er,evidential reasoning,mcda,multiple criteria decision analysis},
publisher    = {Elsevier Ltd},
}
@Article{2011NobleElevating,
author       = {Noble, Charles H},
date         = {2011},
journaltitle = {Journal of Product Innovation Management},
title        = {On Elevating Strategic Design Research*},
doi          = {10.1111/j.1540-5885.2011.00808.x},
number       = {3},
pages        = {389--393},
volume       = {28},
abstract     = {While the popular understanding of the influence of design is growing, academic research has largely been restricted to considering consumer-level responses to design elements. This paper reviews this past work and proposes a more strategic research agenda for the field, with the potential to explicate linkages between design elements and strategies and outcomes related to innovation and corporate performance.},
publisher    = {Blackwell Publishing Inc},
}
@Article{KlambauerSelf,
author       = {Klambauer, G and Others},
journaltitle = {Anais},
title        = {Self-normalizing neural networksAdvances in Neural Information Processing Systems},
volume       = {2017},
}
@Article{2006PillerToolkits,
author       = {Piller, Frank T. and Walcher, Dominik},
date         = {2006},
journaltitle = {R////////\\\&D Management},
title        = {Toolkits for idea competitions: a novel method to integrate users in new product development},
doi          = {10.1111/j.1467-9310.2006.00432.x},
number       = {3},
pages        = {307--318},
volume       = {36},
abstract     = {Research has shown that many innovations originate not in the manufacturer but the user domain. Internet-based toolkits for idea competitions (TIC) are a novel way for manufacturers to access innovative ideas and solutions from users. Idea competitions build on the nature of competition as a means to encourage users to participate at an open innovation process, to inspire their creativity, and to increase the quality of the submissions. When the contest ends, submissions are evaluated by an expert panel. Users whose submissions score highest receive an award from the manufacturer, which is often granted in exchange for the right to exploit the solution in its domain. Following the idea of evolutionary prototyping, we developed a TIC in cooperation with a manufacturer of sports goods. The TIC was launched as a pilot in one of the company's markets. Submissions were evaluated using the consensual assessment technique. The evaluation of this study provides suggestions for further research, but also implications for managers willing to explore TIC in their organization.},
}
@Article{2012AndersonCompetition,
author       = {Anderson, Simon P. and de Palma, André},
date         = {2012},
journaltitle = {The RAND Journal of Economics},
title        = {Competition for attention in the Information (overload) Age},
doi          = {10.1111/j.1756-2171.2011.00155.x},
number       = {1},
pages        = {1--25},
volume       = {43},
abstract     = {The Information Age has a surfeit of information received relative to what is processed. We model multiple sectors competing for consumer attention, with competition in price within each sector. Sector advertising levels follow a constant elasticity of substitution (CES) form, and within-sector prices are dispersed with a truncated Pareto distribution. The "information hump" shows highest ad levels for intermediate attention levels. Overall, advertising is excessive, although the allocation across sectors is optimal. The blame for information overload falls most on product categories with low information transmission costs and low profits.},
}
@Article{2003___,
date         = {2003},
journaltitle = {The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
volume       = {114},
}
@Book{2018Caffea,
date      = {2018},
title     = {Caffe},
location  = {Caffe | Model Zoo Disponvel em},
publisher = {<},
url       = {http://caffe.berkeleyvision.org/{////////\\%}3E},
}
@Article{2011OrdaniniCrowd,
author       = {Ordanini, Andrea and Miceli, Lucia and Pizzetti, Marta and Parasuraman, a.},
date         = {2011},
journaltitle = {Journal of Service Management},
title        = {Crowd-funding: transforming customers into investors through innovative service platforms},
doi          = {10.1108/09564231111155079},
issn         = {1757-5818},
number       = {4},
pages        = {443--470},
volume       = {22},
abstract     = {Purpose – The purpose of this paper is to analyze the emerging crowd-funding phenomenon, that is a collective effort by consumers who network and pool their money together, usually via the internet, in order to invest in and support efforts initiated by other people or organizations. Successful service businesses that organize crowd-funding and act as intermediaries are emerging, attesting to the viability of this means of attracting investment. Design/methodology/approach – The research employs a "grounded theory" approach, performing an in-depth qualitative analysis of three cases involving crowd-funding initiatives: SellaBand in the music business, Trampoline in financial services, and Kapipal in non-profit services. These cases were selected to represent a diverse set of crowd-funding operations that vary in terms of risk/return for the investor and the type of payoff associated to the investment. Findings – The research addresses two research questions: how and why do consumers turn into crowd-funding participants? and how and why do service providers set up a crowd-funding initiative? Concerning the first research question, the authors' findings reveal purposes, characteristics, roles and tasks, and investment size of crowd-funding activity from the consumer's point of view. Regarding the second research question, the authors' analysis reveals purposes, service roles, and network effects of crowd-funding activity investigated from the point of view of the service organization that set up the initiative. Practical implications – The findings also have implications for service managers interested in launching and/or managing crowd-funding initiatives. Originality/value – The paper addresses an emerging phenomenon and contributes to service theory in terms of extending the consumer's role from co-production and co-creation to investment.},
keywords     = {crowd-funding,customer-investors,customers,investments,paper type research paper,service innovation},
}
@Book{2018Neona,
date      = {2018},
title     = {Neon},
location  = {Intel AI Disponvel em},
publisher = {<},
url       = {https://ai.intel.com/neon/{////////\\%}3E},
groups    = {tesse:4},
}
@Misc{Matthieu,
title = {Matthieu Ricard, Trinh Xuan Thuan-The quantum and the lotus a journey to the frontiers where science and Buddhism meet-Three Rivers Press (2004).pdf},
}
@Article{2012TielemanRmsprop,
author       = {Tieleman, T and Hinton, G Lecture 6.5-rmsprop:},
date         = {2012},
journaltitle = {COURSERA: Neural networks for machine learning, v.},
title        = {rmsprop: Divide the gradient by a running average of its recent magnitude},
volume       = {4},
}
@Article{1875ArthurIs,
author = {Arthur, By W Brian},
date   = {1875},
title  = {Is the Information Revolution Dead ?},
}
@Article{2010FiniOutside,
author       = {Fini, Riccardo and Lacetera, Nicola and Shane, Scott},
date         = {2010},
journaltitle = {Research Policy},
title        = {Inside or outside the IP system? Business creation in academia},
doi          = {10.1016/j.respol.2010.05.014},
issn         = {0048-7333},
number       = {8},
pages        = {1060--1069},
volume       = {39},
abstract     = {Research and public policy on academic entrepreneurship are largely based on the assumption that faculty members start businesses to commercialize inventions that have been disclosed to university administrators and have been patented. In this paper, we analyze a sample of 11,572 professors and find that much academic entrepreneurship occurs outside the university intellectual property system. Specifically, about 2/3 of businesses started by academics are not based on disclosed and patented inventions. Moreover, we show that individual characteristics, departmental and organizational affiliations, and time allocation of academics that have started business outside the IP system are different from those of academics that have started businesses to exploit disclosed and patented inventions. We discuss the implications for research on and the practice of academic entrepreneurship. 2010 Elsevier B.V. All rights reserved.},
isbn         = {0048-7333},
keywords     = {Academic entrepreneurship,Business creation,Knowledge transfer},
}
@Article{2017RisiNeuroevolutiona,
author       = {Risi, S and Togelius, J},
date         = {2017},
journaltitle = {IEEE Transactions on Computational Intelligence and AI in Games, v.},
title        = {Neuroevolution in games:State of the art and open challenges},
volume       = {9},
}
@Article{1996HaganNeural,
author       = {Hagan, M T and Others},
date         = {1996},
journaltitle = {Traducao. [s.l.] Pws Pub},
title        = {Neural network design},
volume       = {20},
groups       = {tesse:4},
}
@Book{1969MinskiIntroduction,
author    = {Minski, M. L. and Papert, S. A. Perceptrons:},
date      = {1969},
title     = {an introduction to computational geometry},
location  = {MA},
publisher = {MIT Press, Cambridge},
file      = {:done/1969MinskiIntroduction An Introduction to Computational Geometry.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{JesusMulti,
author   = {Jesus, T O De and Soares, M S},
title    = {A Multi-Criteria Analysis of Techniques and Tools for Tracing Software Requirements},
keywords = {management,requirements engineering,requirements traceability},
}
@Article{2012PoetzValue,
author       = {Poetz, Marion K. and Schreier, Martin},
date         = {2012-03},
journaltitle = {Journal of Product Innovation Management},
title        = {The Value of Crowdsourcing: Can Users Really Compete with Professionals in Generating New Product Ideas?},
doi          = {10.1111/j.1540-5885.2011.00893.x},
issn         = {0737-6782},
number       = {2},
pages        = {245--256},
volume       = {29},
abstract     = {Generating ideas for new products used to be the exclusive domain of marketers, engineers, and/or designers. Users have only recently been recognized as an alternative source of new product ideas. Whereas some have attributed great potential to outsourcing idea generation to the "crowd" of users ("crowdsourcing"), others have clearly been more skeptical. The authors join this debate by presenting a real-world comparison of ideas actually generated by a firm's professionals with those generated by users in the course of an idea generation contest. Both professionals and users provided ideas to solve an effective and relevant problem in the consumer goods market for baby products. Executives from the underlying company evaluated all ideas (blind to their source) in terms of key quality dimensions including novelty, customer benefit, and feasibility. The study reveals that the crowdsourcing process generated user ideas that score significantly higher in terms of novelty and customer benefit, and somewhat lower in terms of feasibility. However, the average values for feasibility—in sharp contrast to novelty and customer benefit—tended to be relatively high overall, meaning that feasibility did not constitute a narrow bottleneck in this study. Even more interestingly, it is found that user ideas are placed more frequently than expected among the very best in terms of novelty and customer benefit. These findings, which are quite counterintuitive from the perspective of classic new product development (NPD) literature, suggest that, at least under certain conditions, crowdsourcing might constitute a promising method to gather user ideas that can complement those of a firm's professionals at the idea generation stage in NPD.},
}
@Article{DoctoralNo,
author = {Doctoral, Helping and Write, Students},
title  = {No Title},
groups = {tesse:4},
}
@Article{2016Zamani-sabziStatistical,
author       = {Zamani-sabzi, Hamed and Phillip, James and Gard, Charlotte C and Abudu, Shalamu},
date         = {2016},
journaltitle = {Operations Research Perspectives},
title        = {Statistical and analytical comparison of multi-criteria decision-making techniques under fuzzy environment},
doi          = {10.1016/j.orp.2016.11.001},
issn         = {2214-7160},
pages        = {92--117},
volume       = {3},
keywords     = {Statistical analysis of ranking methods,Fuzzy envi,methods,statistical analysis of ranking},
publisher    = {Elsevier Ltd},
}
@Article{2010BehzadianPromethee,
author       = {Behzadian, Majid and Kazemzadeh, R B and Albadvi, A and Aghdasi, M},
date         = {2010},
journaltitle = {European Journal of Operational Research},
title        = {PROMETHEE : A comprehensive literature review on methodologies and applications},
doi          = {10.1016/j.ejor.2009.01.021},
issn         = {0377-2217},
number       = {1},
pages        = {198--215},
volume       = {200},
publisher    = {Elsevier B.V.},
}
@Article{CorporationL,
author = {Corporation, Westinghouse Electric and Pittsburgh, East and Arbor, Ann},
title  = {notitle},
number = {4},
pages  = {978--986},
}
@Article{SamplesPhilharmonia,
author = {Samples, Sound},
title  = {Philharmonia Orchestra},
url    = {http://www.philharmonia.co.uk/explore/sound{////////\\_}samples{////////\\%}3E},
volume = {2018},
groups = {tesse:4},
}
@Article{2016BahrampourC,
author = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
date   = {2016},
title  = {C OMPARATIVE S TUDY OF C AFFE , N EON , T HEANO , AND T ORCH FOR D EEP L EARNING},
pages  = {1--11},
}
@Article{2007DonosoF,
author   = {Donoso, Pedro and Tann, Alberto and Guimar, Francisco},
date     = {2007},
title    = {A f ´ ısica do violino},
number   = {December},
keywords = {acoustics,helmholtz,musical instruments,resonance,violin},
}
@Article{1985Diego862,
author = {Diego, S A N},
date   = {1985},
title  = {862 18 120,},
number = {V},
}
@Article{2015ReyesSupersymmetric,
author     = {Reyes, Marco A and Arcos-olalla, Rafael},
date       = {2015},
title      = {Supersymmetric features of the Error and Dawson ' s functions arXiv : 1510 . 03735v1 [ math-ph ] 13 Oct 2015},
eprint     = {arXiv:1510.03735v1},
eprinttype = {arXiv},
number     = {2},
pages      = {1--13},
arxivid    = {arXiv:1510.03735v1},
keywords   = {02,05,1,10,11,30,dawson,error function,gp,ln,mv,pacs numbers,pb,s function,supersymmetry,the error function,the integral,which is defined by},
}
@Article{2005SawhneyCollaborating,
author       = {Sawhney, Mohanbir and Verona, Gianmario and Prandelli, Emanuela},
date         = {2005-01},
journaltitle = {Journal of Interactive Marketing},
title        = {Collaborating to create: The Internet as a platform for customer engagement in product innovation},
doi          = {10.1002/dir.20046},
issn         = {1094-9968},
number       = {4},
pages        = {4--17},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S1094996805700785},
volume       = {19},
abstract     = {In the networked world, firms are recognizing the power of the Internet as a platform for co-creating value with customers.We focus on how the Internet has impacted the process of collaborative innovation—a key process in value co-creation.We outline the distinctive capabilities of the Internet as a platform for customer engagement, including interactivity, enhanced reach, persistence, speed, and flexibility, and suggest that firms can use these capabilities to engage customers in collaborative product innovation through a variety of Internet-based mechanisms.We discuss how these mechanisms can facilitate collaborative innovation at different stages of the New Product Development process (back end vs. front end stages) and for differing levels of customer involvement (high reach vs. high richness).We present two detailed exploratory case studies to illustrate the integrated and systematic usage of Internetbased collaborative innovation mechanisms—Ducati from the motorbike industry and Eli Lilly from the pharmaceutical industry.We derive implications for managerial practice and academic research on collaborative innovation.},
publisher    = {Elsevier},
}
@Article{1996SerafinVirtual,
author = {Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C and Nordahl, Rolf and Gables, Coral},
date   = {1996},
title  = {Virtual Reality Musical Instruments : State of the Art , Design Principles , and Future Directions},
doi    = {10.1162/COMJ},
}
@Article{2017EphratLooking,
author     = {Ephrat, Ariel and Hebrew, The and Freeman, William T and Rubinstein, Michael and Jon, Only and Rory, Only},
date       = {2017},
title      = {Looking to Listen at the Cocktail Party : A Speaker-Independent Audio-Visual Model for Speech Separation},
eprint     = {arXiv:1804.03619v1},
eprinttype = {arXiv},
arxivid    = {arXiv:1804.03619v1},
}
@Article{2013ElliotCross,
author       = {Elliot, E.a. and Nakata, C.},
date         = {2013-12},
journaltitle = {Journal of Product Innovation Management},
title        = {Cross-Cultural Creativity: Conceptualization and propositions for global new product development},
doi          = {10.1111/jpim.12066},
issn         = {0737-6782},
pages        = {110--125},
volume       = {30},
abstract     = {In today's global business environment, where multinational companies are pressed to increase revenues in order to survive, creativity may hold the key to ensuring their new product development (NPD) efforts lead to innovations with worldwide appeal, such as Apple's iPad and Gillette's Fusion Razor. To leverage creativity for effective global NPD, businesses want to know how cultures differ in their concepts of creativity and the impact of those differences on approaches to developing new products. Because global new products are increasingly developed in, by, and for multiple cultures, a particular need is for a culturally reflective understanding, or conceptualization, of creativity. While creativity is believed to be culturally tied, the dominant framework of creativity used in business and management assumes that creativity is culturally indifferent or insensitive. This knowledge gap is addressed by studying the role of creativity in NPD practices in a cross-cultural or global context. The study begins by first developing a culturally anchored conceptualization of creativity. Called cross-cultural creativity, the concept draws on creativity insights from the field of art and aesthetics. The concept specifies two modes of creativity, neither of which is superior to the other, called the spontaneous or S route and the divergent or D route. The S route emphasizes adaptiveness, processes, intuitiveness, and metamorphism, while the D route focuses on disruptiveness, results, rationality, and literalism. Next, this new concept is applied to NPD by positing how creativity in distinct cultures may shape NPD practices, as illustrated by Japanese and U.S. firms. Research propositions are formulated to capture these patterns, and thereafter, theoretical and practical implications of the framework and propositions are discussed. The implications center on global NPD, which is a complex enterprise involving typically more than one culture to design and develop new products for several geographic markets. The study is of interest to researchers needing a globally situated, culturally attached framework of creativity for international NPD studies, and managers seeking to exploit creativity in multinational and multicultural innovation projects.},
}
@Misc{Revisao,
title = {A revisão da bibliografia em teses e dissertações.pdf},
}
@Article{Tao,
title  = {tão simples e compacta quanto possível , adotando-se , para tanto , a notação matricial . Embora matematicamente equivalente às derivações apresentadas em ( XXX ), optou-se por uma abordagem direta com a intenção de tornar mais intuitivo o entendimento do},
number = {Xxx},
}
@Article{2002ZopounidisMulticriteria,
author   = {Zopounidis, Constantin and Doumpos, Michael},
date     = {2002},
title    = {Multicriteria classification and sorting methods : A literature review},
pages    = {229--246},
volume   = {138},
keywords = {classification,decision rules,multiple criteria analysis,outranking relations,preference,sorting,utility functions},
}
@Book{2011UlrichDesign,
author    = {Ulrich, Karl},
title     = {Design: Creation of Artifacts in Society},
isbn      = {9780983648703},
publisher = {University of Pennsylvania},
url       = {https://www.amazon.com/Design-Creation-Artifacts-Karl-Ulrich-ebook/dp/B005S4EO1Y?SubscriptionId=AKIAIOBINVZYXZQZ2U3A///////\\&tag=chimbori05-20///////\\&linkCode=xm2///////\\&camp=2025///////\\&creative=165953///////\\&creativeASIN=B005S4EO1Y},
year      = {2011},
}
@Article{Percussion,
title = {Percussion Instrument Modelling In 3D : Sound Synthesis Through Time Domain Numerical Simulation University of Edinburgh},
}
@Article{schaffner2017towards,
author    = {Schaffner, Michael and Scheidegger, Florian and Cavigelli, Lukas and Kaeslin, Hubert and Benini, Luca and Smolic, Aljosa},
title     = {Towards Edge-Aware Spatio-Temporal Filtering in Real-Time},
number    = {1},
pages     = {265--280},
volume    = {27},
journal   = {IEEE Transactions on Image Processing},
publisher = {IEEE},
year      = {2017},
}
@Article{Institute,
title = {Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. ® www.jstor.org},
}
@Article{2015HighlanderArxiv,
author     = {Highlander, Tyler},
date       = {2015},
title      = {arXiv : 1601 . 06815v1 [ cs . NE ] 25 Jan 2016 Very Efficient Training of Convolutional Neural Networks using Fast Fourier},
eprint     = {arXiv:1601.06815v1},
eprinttype = {arXiv},
pages      = {1--9},
arxivid    = {arXiv:1601.06815v1},
}
@Article{2017IshizakaAre,
author       = {Ishizaka, Alessio and Siraj, Sajid},
date         = {2017},
journaltitle = {European Journal of Operational Research},
title        = {Are multi-criteria decision-making tools useful? An experimental comparative study of three methods},
doi          = {10.1016/j.ejor.2017.05.041},
issn         = {0377-2217},
keywords     = {AHP,Decision analysis,Experimental evaluation,MACBETH,SMART},
publisher    = {Elsevier},
}
@Unpublished{1985ParkerLearninga,
author = {Parker, D B},
date   = {1985},
title  = {Learning logic},
}
@Article{OliveiraSmart,
author   = {Oliveira, G A Q S M and Seleme, R and Zattar, I C},
title    = {Smart Grid Performance Assessment Via Approach Method},
keywords = {multicriteria,project management,roadmap,smart grids},
}
@Article{2002DewickTyndall,
author = {Dewick, Paul and Green, Ken and Miozzo, Marcela},
date   = {2002},
title  = {Tyndall ˚ Centre and the Environment Technological Change , Industry Structure and the Environment},
number = {January},
}
@Book{NormanEveryday,
author = {Norman, Don},
title  = {of EVERYDAY THINGS THE DESIGN OF EVERYDAY},
isbn   = {9780465050659},
}
@Article{BahrampourComparative,
author     = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
title      = {Comparative Study of Deep Learning Software Frameworks},
eprint     = {arXiv:1511.06435v3},
eprinttype = {arXiv},
arxivid    = {arXiv:1511.06435v3},
}
@Article{2017ZappiShader,
author = {Zappi, Victor and Allen, Andrew and Fels, Sidney},
date   = {2017},
title  = {Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments},
pages  = {145--150},
groups = {tesse:4},
}
@Article{JaderbergDecoupled,
author     = {Jaderberg, Max and Graves, Alex},
title      = {Decoupled Neural Interfaces using Synthetic},
eprint     = {arXiv:1608.05343v2},
eprinttype = {arXiv},
volume     = {1},
arxivid    = {arXiv:1608.05343v2},
}
@Article{2014Polyphonic,
date   = {2014},
title  = {POLYPHONIC PIANO TRANSCRIPTION USING NON-NEGATIVE MATRIX FACTORISATION WITH GROUP SPARSITY Ken O ' Hanlon and Mark D . Plumbley Queen Mary University of London},
number = {May},
pages  = {3136--3140},
volume = {1},
}
@Article{1997AlbertsInformation,
author = {Alberts, David S and Papp, Daniel S},
date   = {1997},
title  = {The Information Age : An Anthology on Its Impact and Consequences Table of Contents},
}
@Book{ThePython,
author    = {The, Keras:},
title     = {Python Deep Learning library},
location  = {Keras Documentation, [s.d.]. Disponvel em},
publisher = {<},
url       = {https://keras.io/{////////\\%}3E},
}
@Misc{._sawhney,
title = {.Sawhney, Verona e Prandelli2005(JIM)COLLABORATING TO CREATE- THE INTERNET AS A PLATFORM FOR CUSTOMER ENGAGEMENT IN PRODUCT INNOVATION.pdf},
}
@Article{2018CarayannisTechnological,
author       = {Carayannis, Elias G and Ferreira, João J M and Jalali, Marjan S and Ferreira, Fernando A F},
date         = {2018},
journaltitle = {Technological Forecasting ////////\\\& Social Change},
title        = {Technological Forecasting ////////\\\& Social Change MCDA in knowledge-based economies : Methodological developments and real world applications},
doi          = {10.1016/j.techfore.2018.01.028},
issn         = {0040-1625},
number       = {xxxx},
pages        = {0--1},
publisher    = {Elsevier},
}
@Article{2008ArtsPhysical,
author = {Arts, Sonic},
date   = {2008},
title  = {Physical modelling of the piano : An investigation into the e ff ect of string sti ff ness on the hammer-string interaction},
number = {September},
}
@Article{2013IndexTechnology,
author = {Index, Industrial},
date   = {2013},
title  = {Technology Is Wiping Out Companies Faster than Ever},
pages  = {2013--2014},
}
@Article{LabBerkeley,
author = {Lab, Berkeley Artificial Intelligence Research},
title  = {The Berkeley Artificial Intelligence Research Blog},
url    = {http://bair.berkeley.edu/{////////\\%}3E},
volume = {2018},
groups = {tesse:4},
}
@Book{maeda2006laws,
author    = {Maeda, John},
title     = {The laws of simplicity},
publisher = {MIT press},
year      = {2006},
}
@Article{NayebiGruv,
author = {Nayebi, Aran and Vitelli, Matt},
title  = {GRUV : Algorithmic Music Generation using Recurrent Neural Networks},
pages  = {1--6},
}
@InProceedings{1990Widrow30,
author       = {Widrow, Bernard and Lehr, Michael A.},
booktitle    = {Proceedings of the IEEE},
date         = {1990-09},
title        = {30 Years of Adaptive Neural Networks: Perceptron, Madaline, and Backpropagation},
doi          = {10.1109/5.58323},
location     = {n. 9, p. 14151442},
number       = {9},
pages        = {1415--1442},
publisher    = {v. 78},
url          = {http://ieeexplore.ieee.org/document/58323/},
urldate      = {2019-04-07},
volume       = {78},
issn         = {0018-9219},
journaltitle = {Proceedings of the IEEE},
langid       = {english},
shorttitle   = {30 years of adaptive neural networks},
year         = {1990},
}
@Article{2018RobertsHierarchical,
author       = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
date         = {2018},
journaltitle = {CoRR},
title        = {A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music},
eprint       = {1803.05428},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1803.05428},
volume       = {abs/1803.05428},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1803-05428},
file         = {:done/2018RobertsHierarchical A Hierarchical Latent Vector Model for Learning Long Term Structure in Music.pdf:application/pdf},
groups       = {tesse:5},
journal      = {CoRR},
timestamp    = {Mon, 13 Aug 2018 16:47:28 +0200},
year         = {2018},
}
@Article{2012ZeilerAdadelta,
author       = {Zeiler, Matthew D.},
date         = {2012},
journaltitle = {CoRR},
title        = {ADADELTA: An Adaptive Learning Rate Method},
eprint       = {1212.5701},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1212.5701},
volume       = {abs/1212.5701},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1212-5701},
journal      = {CoRR},
timestamp    = {Mon, 13 Aug 2018 16:45:57 +0200},
year         = {2012},
}
@Article{2014KingmaAdam,
author       = {Kingma, Diederik P. and Ba, Jimmy},
date         = {2014},
journaltitle = {CoRR},
title        = {Adam: A Method for Stochastic Optimization},
eprint       = {1412.6980},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1412.6980},
volume       = {abs/1412.6980},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/KingmaB14},
journal      = {CoRR},
timestamp    = {Mon, 13 Aug 2018 16:47:35 +0200},
year         = {2014},
}
@InProceedings{2000FontanaUsing,
author       = {Fontana, Federico and Rocchesso, Davide and Apollonio, Enzo},
booktitle    = {Proceedings of the International Conference on Digital Audio Effects (DAFx)},
date         = {2000-12},
title        = {Using the waveguide mesh in modelling 3D resonators},
pages        = {229--232},
volume       = {2000},
file         = {:done/2000FontanaUsing Using the Waveguide Mesh in Modelling 3D Resonators.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Anais},
}
@Article{1991HornikApproximation,
author       = {Hornik, Kurt},
date         = {1991},
journaltitle = {Neural Networks},
title        = {Approximation capabilities of multilayer feedforward networks},
doi          = {10.1016/0893-6080(91)90009-T},
eprint       = {arXiv:1011.1669v3},
eprinttype   = {arXiv},
issn         = {0893-6080},
number       = {2},
pages        = {251--257},
volume       = {4},
abstract     = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
arxivid      = {arXiv:1011.1669v3},
file         = {:done/1991HornikApproximation Approximation Capabilities of Multilayer Feedforward Networks.pdf:application/pdf},
groups       = {tesse:5},
isbn         = {0893-6080},
pmid         = {25246403},
publisher    = {Elsevier},
}
@InProceedings{2015NguyenDeep,
author       = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
booktitle    = {Proceedings of the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
date         = {2015},
title        = {Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
organization = {IEEE},
publisher    = {IEEE},
url          = {http://www.evolvingai.org/fooling},
}
@InProceedings{2018DonahueSynthesizing,
author       = {Donahue, Chris and McAuley, Julian and Puckette, Miller},
booktitle    = {Proceedings of the Sixth International Conference on Learning Representations},
date         = {2018},
title        = {Synthesizing Audio with GANs},
note         = {Published: Workshop},
url          = {https://openreview.net/forum?id=r1RwYIJPM},
file         = {:done/2018DonahueSynthesizing Synthesizing Audio with GANs.pdf:application/pdf},
groups       = {tesse:5},
howpublished = {Workshop},
}
@Article{2011DuchiAdaptive,
author       = {Duchi, John and Hazan, Elad and Singer, Yoram},
date         = {2011-07},
journaltitle = {Journal of Machine Learning Research},
title        = {Adaptive Subgradient Methods for Online Learning and Stochastic Optimization},
issn         = {1532-4435},
pages        = {2121--2159},
url          = {http://dl.acm.org/citation.cfm?id=1953048.2021068},
volume       = {12},
acmid        = {2021068},
file         = {:done/2011DuchiAdaptive Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.pdf:application/pdf},
groups       = {tesse:5},
numpages     = {39},
publisher    = {JMLR.org},
}
@Article{1990ElmanFinding,
author       = {Elman, Jeffrey L.},
date         = {1990},
journaltitle = {Cognitive Science},
title        = {Finding Structure in Time},
doi          = {10.1207/s15516709cog1402///////\\_1},
eprint       = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402 /\\_1},
number       = {2},
pages        = {179--211},
volume       = {14},
abstract     = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
file         = {:done/1990ElmanFinding Finding Structure in Time.pdf:application/pdf},
groups       = {tesse:5},
}
@Article{2017GrinsteinAudio,
author       = {Grinstein, Eric and Duong, Ngoc Q. K. and Ozerov, Alexey and Pérez, Patrick},
date         = {2017},
journaltitle = {CoRR},
title        = {Audio style transfer},
eprint       = {1710.11385},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1710.11385},
volume       = {abs/1710.11385},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1710-11385},
file         = {:done/2017GrinsteinAudio Audio Style Transfer.pdf:application/pdf},
groups       = {tesse:5},
timestamp    = {Mon, 13 Aug 2018 16:48:00 +0200},
}
@InProceedings{2015HeMultimodala,
author       = {He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
booktitle    = {Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge - AVEC /textquotesingle15},
date         = {2015},
title        = {Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks},
doi          = {10.1145/2808196.2811641},
isbn         = {978-1-4503-3743-4},
location     = {New York, NY, USA},
note         = {event-place: Brisbane, Australia},
organization = {ACM},
pages        = {73--80},
publisher    = {ACM Press},
series       = {AVEC '15},
groups       = {tesse:4},
keywords     = {audio and video features, DBLSTM-RNN, multimodal fusion, offset, physiological feature,},
}
@Article{1993LeshnoMultilayer,
author       = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
date         = {1993},
journaltitle = {Neural Networks},
title        = {Multilayer feedforward networks with a nonpolynomial activation function can approximate any function},
doi          = {10.1016/S0893-6080(05)80131-5},
issn         = {0893-6080},
number       = {6},
pages        = {861--867},
url          = {http://www.sciencedirect.com/science/article/pii/S0893608005801315},
volume       = {6},
keywords     = {(μ) approximation, Activation functions, Multilayer feedforward networks, Role of threshold, Universal approximation capabilities},
publisher    = {Elsevier},
}
@Article{2013RigaudParametric,
author       = {Rigaud, François and David, Bertrand and Daudet, Laurent},
date         = {2013-05},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {A parametric model and estimation techniques for the inharmonicity and tuning of the piano},
doi          = {10.1121/1.4799806},
number       = {5},
pages        = {3107--3118},
volume       = {133},
}
@Book{1999SmithScientist,
author = {Smith, Steven W.},
date   = {1999},
title  = {The Scientist and Engineer's Guide to Digital Signal Processing},
isbn   = {0-9660176-4-1},
url    = {http://www.dspguide.com/},
file   = {:done/1999SmithScientist The Scientist and Engineer's Guide to Digital Signal Processing.pdf:application/pdf},
groups = {tesse:5, DSP},
}
@Article{2000VaughnMusic,
author       = {Vaughn, Kathryn},
date         = {2000},
journaltitle = {Journal of Aesthetic Education},
title        = {Music and Mathematics: Modest Support for the Oft-Claimed Relationship},
issn         = {00218510, 15437809},
number       = {3},
pages        = {149--166},
url          = {http://www.jstor.org/stable/3333641},
volume       = {34},
publisher    = {University of Illinois Press},
}
@Book{2015YadavIntroduction,
author    = {Yadav, Neha and Yadav, Anupam and Kumar, Manoj},
date      = {2015-02-26},
title     = {An Introduction to Neural Network Methods for Differential Equations},
doi       = {10.1007/978-94-017-9816-7},
publisher = {Springer Netherlands},
file      = {:done/2015YadavIntroduction An Introduction to Neural Network Methods for Differential Equations.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{2017ZhangTowards,
author       = {Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon and Zhang, Saizheng and Laurent, César and Bengio, Yoshua and Courville, Aaron C.},
date         = {2017},
journaltitle = {CoRR},
title        = {Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks},
eprint       = {1701.02720},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1701.02720},
volume       = {abs/1701.02720},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/ZhangPBZLBC17},
file         = {:done/2017ZhangTowards Towards End to End Speech Recognition with Deep Convolutional Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
timestamp    = {Mon, 13 Aug 2018 16:49:16 +0200},
}
@Article{2017CaoSpatial,
author       = {Cao, Jianan and Farnham, David J. and Lall, Upmanu},
date         = {2017},
journaltitle = {CoRR},
title        = {Spatial-temporal wind field prediction by Artificial Neural Networks},
eprint       = {1712.05293},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1712.05293},
volume       = {abs/1712.05293},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1712-05293},
file         = {:done/2017CaoSpatial Spatial Temporal Wind Field Prediction by Artificial Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
timestamp    = {Mon, 13 Aug 2018 16:48:46 +0200},
}
@Article{1995TabeiSimple,
author       = {Tabei, Makoto and Musicus, Bruce R.},
date         = {1995},
journaltitle = {IEEE transactions on signal processing},
title        = {A simple estimator for frequency and decay rate},
doi          = {10.1109/78.506615},
issn         = {1053-587X},
number       = {6},
pages        = {1504--1511},
volume       = {44},
file         = {:done/1995TabeiSimple A Simple Estimator for Frequency and Decay Rate.pdf:application/pdf},
groups       = {tesse:5},
publisher    = {IEEE},
}
@MastersThesis{2005GaerderPhysical,
author      = {Gärder, Anders},
date        = {2005},
institution = {Chalmers University of Technology},
title       = {Physical modeling of percussion instruments},
pages       = {80},
file        = {:done/2005GaerderPhysical Physical Modeling of Percussion Instruments.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
year        = {2005},
}
@Book{2010PetreUnwritten,
author    = {Petre, Marian and Rugg, Gordon},
date      = {2010},
title     = {The unwritten rules of PhD research},
doi       = {10.1049/em:20040508},
edition   = {Second edition},
isbn      = {978-0-335-23702-9},
location  = {Maidenhead},
note      = {OCLC: 934394471},
pages     = {320},
pagetotal = {272},
publisher = {Open University Press},
series    = {Open up study skills},
url       = {http://books.google.com/books?id={////////\\_}DDwCqx6wpcC{////////\\&}printsec=frontcover{////////\\&}dq=unwritten+rules+of+phd+research{////////\\&}hl={////////\\&}cd=1{////////\\&}source=gbs{////////\\_}api{////////\\%}255Cnpapers2://publication/uuid/48967E01-55F9-4397-B941-310D9C5405FA{////////\\%}255Cnhttp://medcontent.metapress.com/index/A65RM03P4874243N.p},
booktitle = {Vasa},
file      = {:done/2010PetreUnwritten The Unwritten Rules of PhD Research.pdf:application/pdf},
groups    = {tesse:5},
issn      = {1472-4677},
langid    = {english},
pmid      = {1275585},
}
@Article{2014RehmanImage,
author       = {Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
date         = {2014-01-27},
journaltitle = {Research Journal of Applied Sciences, Engineering and Technology},
title        = {Image Compression: A Survey},
doi          = {10.19026/rjaset.7.303},
issn         = {20407459, 20407467},
number       = {4},
pages        = {656--672},
url          = {http://maxwellsci.com/jp/mspabstract.php?jid=RJASET///////\\&doi=rjaset.7.303},
urldate      = {2019-03-28},
volume       = {7},
abstract     = {Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction. Maxwell Scientific Organization, 2014.},
file         = {:done/2014RehmanImage Image Compression/\\_ a Survey.pdf:application/pdf},
groups       = {tesse:5},
isbn         = {9233351788872},
keywords     = {Compression,Image,Lossless,Lossy,Review,image compression- lossy},
langid       = {english},
publisher    = {Maxwell Science Publishing},
shorttitle   = {Image Compression},
}
@Article{2002Egmont-petersenImage,
author       = {Egmont-Petersen, M. and de Ridder, D. and Handels, H.},
date         = {2002-10},
journaltitle = {Pattern Recognition},
title        = {Image processing with neural networks—a review},
doi          = {10.1016/S0031-3203(01)00178-9},
issn         = {0031-3203},
number       = {10},
pages        = {2279--2301},
url          = {http://linkinghub.elsevier.com/retrieve/pii/S0031320301001789},
urldate      = {2019-04-05},
volume       = {35},
abstract     = {We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopÿeld neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension speciÿes the type of task performed by the algorithm: preprocessing, data reduction=feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses speciÿc constraints to a neural-based approach. These speciÿc conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and speciÿcally to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments. ? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
file         = {:done/2002Egmont-petersenImage Image Processing with Neural Networks—a Review.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
publisher    = {Elsevier},
}
@InProceedings{2004CollobertLinks,
author       = {Collobert, Ronan and Bengio, Samy},
booktitle    = {Twenty-first international conference on Machine learning - ICML '04},
date         = {2004},
title        = {Links between perceptrons, MLPs and SVMs},
doi          = {10.1145/1015330.1015415},
eventtitle   = {Twenty-first international conference},
location     = {Banff, Alberta, Canada},
organization = {ACM},
pages        = {23},
publisher    = {ACM Press},
url          = {http://portal.acm.org/citation.cfm?doid=1015330.1015415},
urldate      = {2019-04-05},
abstract     = {We propose to study links between three important classiﬁcation algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs). We ﬁrst study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs. After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function. We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin. These ideas are extended afterward to the case of MLPs. Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space. Finally, we present a very simple MLP based on the previous ﬁndings, which yields better performances in generalization and speed than the other models.},
file         = {:done/2004CollobertLinks Links between Perceptrons, MLPs and SVMs.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2013HayesFirst,
author       = {Hayes, Brian},
date         = {2013},
journaltitle = {American Scientist},
title        = {First Links in the Markov Chain},
number       = {April},
pages        = {7},
file         = {:done/2013HayesFirst First Links in the Markov Chain.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2012HintonDeep,
author       = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara and Kingsbury, Brian},
date         = {2012-11},
journaltitle = {IEEE Signal Processing Magazine},
title        = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
doi          = {10.1109/MSP.2012.2205597},
issn         = {1053-5888},
number       = {6},
pages        = {82--97},
url          = {http://ieeexplore.ieee.org/document/6296526/},
urldate      = {2019-04-07},
volume       = {29},
langid       = {english},
publisher    = {IEEE},
shorttitle   = {Deep Neural Networks for Acoustic Modeling in Speech Recognition},
}
@Book{2006BishopPattern,
author    = {Bishop, Christopher M.},
date      = {2006-08-17},
title     = {Pattern Recognition and Machine Learning},
edition   = {Corrected at 8th printing 2009},
isbn      = {0-387-31073-8},
location  = {New York, NY},
note      = {OCLC: 845772798},
pagetotal = {738},
publisher = {Springer-Verlag New York Inc.},
series    = {Information science and statistics},
url       = {https://www.ebook.de/de/product/5324937/christopher///////\\_m///////\\_bishop///////\\_pattern///////\\_recognition///////\\_and///////\\_machine///////\\_learning.html},
file      = {:done/2006BishopPattern Pattern Recognition and Machine Learning.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{2017OmarPredicting,
author       = {Omar, Normah and Johari, Zulaikha `Amirah and Smith, Malcolm},
date         = {2017},
journaltitle = {Journal of Financial Crime},
title        = {Predicting fraudulent financial reporting using artificial neural network},
doi          = {10.1108/jfc-11-2015-0061},
issn         = {1359-0790},
number       = {2},
pages        = {362--387},
url          = {http://dx.doi.org/10.1108/eb025814{////////\\%}5Cnhttp://},
volume       = {24},
abstract     = {This study explores the effectiveness of an Artificial Neural Network (ANN) in predicting fraudulent financial reporting in small market capitalization companies in Malaysia. Design/methodology/approach Based on the concepts of ANN, a mathematical model is developed to compare non-fraud and fraud companies selected from among small market capitalization companies in Malaysia; the fraud companies had already been charged by the Securities Commission for the falsification of financial statements. Ten financial ratios are used as fraud risk indicators to predict fraudulent financial reporting using ANN. Findings Indicate that the proposed ANN methodology outperforms other statistical techniques widely used for predicting fraudulent financial reporting. Originality/value The study is one of few to adopt the ANN approach to the prediction of financial reporting fraud.},
file         = {:done/2017OmarPredicting Predicting Fraudulent Financial Reporting Using Artificial Neural Network.pdf:application/pdf},
groups       = {tesse:5},
isbn         = {1359079051062},
keywords     = {security},
}
@InProceedings{2018ReddiConvergence,
author    = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
booktitle = {Proceedings of the International Conference on Learning Representations},
date      = {2018},
title     = {On the Convergence of Adam and Beyond},
url       = {https://openreview.net/forum?id=ryQu7f-RZ},
file      = {:done/2018ReddiConvergence On the Convergence of Adam and beyond.pdf:application/pdf},
groups    = {tesse:5},
}
@Article{2018PangConvolution,
author       = {Pang, Y. and Sun, M. and Jiang, X. and Li, X.},
date         = {2018-05},
journaltitle = {IEEE Transactions on Neural Networks and Learning Systems},
title        = {Convolution in Convolution for Network in Network},
doi          = {10.1109/TNNLS.2017.2676130},
issn         = {2162-237X},
number       = {5},
pages        = {1587--1597},
volume       = {29},
file         = {:done/2018PangConvolution Convolution in Convolution for Network in Network.pdf:application/pdf},
groups       = {tesse:5},
publisher    = {IEEE Transactions on Neural Networks and Learning Systems},
}
@Article{2016GoldbergPrimer,
author       = {Goldberg, Yoav},
date         = {2016},
journaltitle = {J. Artif. Intell. Res.(JAIR)},
title        = {A Primer on Neural Network Models for Natural Language Processing.},
eprint       = {1510.00726},
eprinttype   = {arXiv},
pages        = {345--420},
url          = {http://arxiv.org/abs/1510.00726},
volume       = {57},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/Goldberg15c},
file         = {:done/2016GoldbergPrimer A Primer on Neural Network Models for Natural Language Processing..pdf:application/pdf},
groups       = {tesse:5},
timestamp    = {Mon, 13 Aug 2018 16:48:41 +0200},
}
@Article{2016RuderOverview,
author       = {Ruder, Sebastian},
date         = {2016},
journaltitle = {CoRR},
title        = {An overview of gradient descent optimization algorithms},
eprint       = {arXiv:1609.04747v2},
eprinttype   = {arXiv},
pages        = {1--14},
url          = {http://arxiv.org/abs/1609.04747},
volume       = {abs/1609.04747},
arxivid      = {arXiv:1609.04747v2},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/Ruder16},
file         = {:done/2016RuderOverview An Overview of Gradient Descent Optimization Algorithms.pdf:application/pdf},
groups       = {tesse:5},
timestamp    = {Mon, 13 Aug 2018 16:48:10 +0200},
}
@Article{2017MitalTime,
author       = {Mital, Parag K.},
date         = {2017},
journaltitle = {CoRR},
title        = {Time Domain Neural Audio Style Transfer},
eprint       = {arXiv:1711.11160v1},
eprinttype   = {arXiv},
number       = {Nips},
url          = {http://arxiv.org/abs/1711.11160},
volume       = {abs/1711.11160},
arxivid      = {arXiv:1711.11160v1},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/abs-1711-11160},
file         = {:done/2017MitalTime Time Domain Neural Audio Style Transfer.pdf:application/pdf},
groups       = {tesse:5},
timestamp    = {Mon, 13 Aug 2018 16:47:15 +0200},
}
@Article{2016AljumahNovel,
author       = {Aljumah, A. and Ahamad, T. A.},
date         = {2016},
journaltitle = {International Journal of Computer Science and Network Security},
title        = {A novel approach for detecting DDoS using artificial neural networks},
pages        = {132--138},
volume       = {16},
file         = {:done/2016AljumahNovel A Novel Approach for Detecting DDoS Using Artificial Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{2018HintonMatrix,
author    = {Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
booktitle = {Proceedings of the International Conference on Learning Representations},
date      = {2018},
title     = {Matrix capsules with EM routing},
pages     = {1--15},
url       = {https://openreview.net/forum?id=HJWLfGWRb},
file      = {:done/2018HintonMatrix Matrix Capsules with EM Routing.pdf:application/pdf},
groups    = {tesse:5},
year      = {2018},
}
@Book{2010LaiIntroduction,
author    = {Lai, W. Michael and Rubin, David and Krempl, Erhard},
date      = {2010},
title     = {Introduction to continuum mechanics},
edition   = {3. ed., reprint},
isbn      = {978-0-7506-2894-5},
location  = {Amsterdam},
note      = {OCLC: 832831309},
pagetotal = {556},
publisher = {Elsevier [u.a.]},
file      = {:done/2010LaiIntroduction Introduction to Continuum Mechanics.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{2016ShiImproving,
author        = {Shi, Weiwei and Gong, Yihong and Wang, Jinjun},
date          = {2016},
journaltitle  = {IJCAI International Joint Conference on Artificial Intelligence},
title         = {Improving CNN Performance with Min-Max Objective},
issn          = {1045-0823},
pages         = {7},
volume        = {2016-Janua},
abstract      = {In this paper, we propose a novel method to improve object recognition accuracies of convolutional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The MinMax objective explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin between different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computation cost. Experiments with shallow and deep models on four benchmark datasets including CIFAR10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max objective achieve remarkable performance improvements compared to the corresponding baseline models.},
file          = {:done/2016ShiImproving Improving CNN Performance with Min Max Objective.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {Machine Learning,object detection},
langid        = {english},
mendeley-tags = {object detection},
}
@Article{2016ZhangImproving,
author        = {Zhang, Shizhou and Gong, Yihong and Wang, Jinjun},
date          = {2016},
journaltitle  = {IJCAI International Joint Conference on Artificial Intelligence},
title         = {Improving DCNN Performance with Sparse Category-Selective Objective Function},
issn          = {1045-0823},
pages         = {7},
volume        = {2016-Janua},
abstract      = {In this paper, we choose to learn useful cues from object recognition mechanisms of the human visual cortex, and propose a DCNN performance improvement method without the need for increasing the network complexity. Inspired by the categoryselective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As experimental results show, when applying the proposed method to the "Quick" model and NIN models, image classiﬁcation performances are remarkably improved on four widely used benchmark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
file          = {:done/2016ZhangImproving Improving DCNN Performance with Sparse Category Selective Objective Function.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {Machine Learning,image synthesis},
langid        = {english},
mendeley-tags = {image synthesis},
}
@Article{2014YosinskiHow,
author        = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
date          = {2014},
title         = {How transferable are features in deep neural networks?},
eprint        = {1411.1792},
eprinttype    = {arXiv},
issn          = {1049-5258},
pages         = {9},
url           = {http://arxiv.org/abs/1411.1792},
abstract      = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the ﬁrst layer they learn features similar to Gabor ﬁlters and color blobs. Such ﬁrst-layer features appear not to be speciﬁc to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to speciﬁc by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus speciﬁcity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difﬁculties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A ﬁnal surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after ﬁne-tuning to the target dataset.},
arxivid       = {1411.1792},
file          = {:done/2014YosinskiHow How Transferable Are Features in Deep Neural Networks/_.pdf:application/pdf},
groups        = {tesse:2},
keywords      = {theory},
langid        = {english},
mendeley-tags = {theory},
}
@InProceedings{2013Boulanger-lewandowskiHigh,
author       = {Boulanger-Lewandowski, N. and Bengio, Y. and Vincent, P.},
booktitle    = {Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
date         = {2013-05},
title        = {High-dimensional sequence transduction},
doi          = {10.1109/ICASSP.2013.6638244},
organization = {IEEE},
pages        = {3178--3182},
publisher    = {AnaisIEEE},
file         = {:done/2013Boulanger-lewandowskiHigh High Dimensional Sequence Transduction.pdf:application/pdf},
groups       = {tesse:5},
keywords     = {Accuracy, global distribution mode, Hidden Markov models, high dimensional output sequence, high dimensional sequence transduction, musically plausible transcription, Noise, polyphonic audio music, polyphonic transcription, probabilistic model, realistic output distribution, recurrent neural network, Recurrent neural networks, restricted Boltzmann machine, Sequence transduction, Smoothing methods, symbolic notation, test error rate, Training, Vectors},
}
@Article{2015LeeGeneralizing,
author        = {Lee, Chen-Yu and Gallagher, Patrick W and Tu, Zhuowen},
date          = {2015},
journaltitle  = {Artificial Intelligence and Statistics},
title         = {Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree},
doi           = {10.1109/TPAMI.2017.2703082},
eprint        = {1509.08985},
eprinttype    = {arXiv},
issn          = {0162-8828},
pages         = {9},
url           = {http://arxiv.org/abs/1509.08985},
volume        = {51},
abstract      = {We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling ﬁlters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These beneﬁts come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.},
arxivid       = {1509.08985},
booktitle     = {Artificial Intelligence and Statistics},
keywords      = {optimization},
langid        = {english},
mendeley-tags = {optimization},
pmid          = {67101},
}
@InProceedings{2015SakFast,
author        = {Sak, Hasim and Senior, Andrew W. and Rao, Kanishka and Beaufays, Françoise},
booktitle     = {Interspeech 2015},
date          = {2015},
title         = {Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition},
eprint        = {1507.06947},
eprinttype    = {arXiv},
pages         = {1468--1472},
url           = {https://www.isca-speech.org/archive/interspeech///////\\_2015/i15///////\\_1468.html},
abstract      = {We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.},
arxivid       = {1507.06947},
journaltitle  = {arXiv preprint arXiv:1507.06947},
keywords      = {speech recognition},
mendeley-tags = {speech recognition},
pmid          = {1000285842},
}
@InProceedings{2016PhurattanaprapinExtended,
author        = {Phurattanaprapin, Khanittha and Horata, Punyaphol},
booktitle     = {2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
date          = {2016-07},
title         = {Extended Hierarchical Extreme Learning Machine with Multilayer Perceptron},
doi           = {10.1109/JCSSE.2016.7748874},
eventtitle    = {2016 13th International Joint Conference on Computer Science and Software Engineering (JCSSE)},
isbn          = {978-1-5090-2033-1},
location      = {Khon Kaen, Thailand},
number        = {2},
pages         = {1--5},
publisher     = {IEEE},
url           = {http://ieeexplore.ieee.org/document/7748874/},
urldate       = {2019-04-05},
volume        = {10},
abstract      = {The Deep Learning approach provides a high performance of classiﬁcation, especially when invoking image classiﬁcation problems. However, a shortcomming of the traditional Deep Learning method is the large time scale of training. The hierarchical extreme learning machine (H-ELM) framework was based on the hierarchical learning architecture of multilayer perceptron to address the problem. H-ELM is composed of two parts; the ﬁrst entails unsupervised multilayer encoding, and the second is the supervised feature classiﬁcation. H-ELM can give a higher accuracy rate than the traditional ELM. However, there still remains room to enhance its classiﬁcation performance. This paper therefore proposes a new method termed the extending hierarchical extreme learning machine (EH-ELM), which extends the number of layers in the supervised portion of the H-ELM from a single layer to multiple layers. To evaluate the performance of the EH-ELM, the various classiﬁcation datasets were studied and compared with the H-ELM and the multilayer ELM, as well as various state-of-the-art such deep architecture methods. The experimental results show that the EH-ELM improved the accuracy rates over most other methods.},
file          = {:done/2016PhurattanaprapinExtended Extended Hierarchical Extreme Learning Machine with Multilayer Perceptron.pdf:application/pdf},
groups        = {tesse:5},
journaltitle  = {ECTI Transactions on Computer and Information Technology},
keywords      = {ELM,chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
langid        = {english},
mendeley-tags = {ELM},
}
@InProceedings{bonada_expressive_2016,
author     = {Bonada, Jordi and Umbert, Martí and Blaauw, Merlijn},
date       = {2016-09-08},
title      = {Expressive Singing Synthesis Based on Unit Selection for the Singing Synthesis Challenge 2016},
doi        = {10.21437/Interspeech.2016-872},
eventtitle = {Interspeech 2016},
pages      = {1230--1234},
url        = {http://www.isca-speech.org/archive/Interspeech///////\\_2016/abstracts/0872.html},
urldate    = {2019-03-26},
abstract   = {Sample and statistically based singing synthesizers typically require a large amount of data for automatically generating expressive synthetic performances. In this paper we present a singing synthesizer that using two rather small databases is able to generate expressive synthesis from an input consisting of notes and lyrics. The system is based on unit selection and uses the Wide-Band Harmonic Sinusoidal Model for transforming samples. The ﬁrst database focuses on expression and consists of less than 2 minutes of free expressive singing using solely vowels. The second one is the timbre database which for the English case consists of roughly 35 minutes of monotonic singing of a set of sentences, one syllable per beat. The synthesis is divided in two steps. First, an expressive vowel singing performance of the target song is generated using the expression database. Next, this performance is used as input control of the synthesis using the timbre database and the target lyrics. A selection of synthetic performances have been submitted to the Interspeech Singing Synthesis Challenge 2016, in which they are compared to other competing systems.},
keywords   = {[Electronic Manuscript]},
langid     = {english},
}
@Article{2015WuDrum,
author        = {Wu, Chih Wei and Lerch, Alexander},
date          = {2015},
journaltitle  = {2015 23rd European Signal Processing Conference, EUSIPCO 2015},
title         = {Drum transcription using partially fixed non-negative matrix factorization},
doi           = {10.1109/EUSIPCO.2015.7362590},
pages         = {1281--1285},
abstract      = {In this paper, a drum transcription algorithm using partially fixed non-negative matrix factorization is presented. The proposed method allows users to identify percussive events in complex mixtures with a minimal training set. The algorithm decomposes the music signal into two parts: percussive part with pre-defined drum templates and harmonic part with undefined entries. The harmonic part is able to adapt to the music content, allowing the algorithm to work in polyphonic mixtures. Drum event times can be simply picked from the percussive activation matrix with onset detection. The system is efficient and robust even with a minimal training set. The recognition rates for the ENST dataset vary from 56.7 to 78.9 /\\% for three percussive instruments extracted from polyphonic music.},
file          = {:done/2015WuDrum Drum Transcription Using Partially Fixed Non Negative Matrix Factorization.pdf:application/pdf},
groups        = {tesse:5},
isbn          = {9780992862633},
keywords      = {Automatic Music Transcription,Drum Transcription,MIR,NMF,music transcription,tesis},
langid        = {english},
mendeley-tags = {music transcription,tesis},
}
@Electronic{2018WikiDrumgizmo,
author       = {Wiki, DrumGizmo},
date         = {2018},
title        = {DrumGizmo Wiki},
url          = {https://www.drumgizmo.org/wiki/doku.php},
urldate      = {30.06.2018},
howpublished = {online},
volume       = {2018},
}
@Article{2014SrivastavaDropout,
author        = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
date          = {2014},
journaltitle  = {Journal of Machine Learning Research},
title         = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
doi           = {10.1214/12-AOS1000},
eprint        = {1102.4807},
eprinttype    = {arXiv},
issn          = {1533-7928},
pages         = {1929--1958},
volume        = {15},
abstract      = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
arxivid       = {1102.4807},
isbn          = {1532-4435},
keywords      = {deep learning,model combination,neural networks,regularization,seminal},
langid        = {english},
mendeley-tags = {seminal},
}
@Article{2004KarjalainenDigital,
author       = {Matti Karjalainen and Cumhur Erkut},
date         = {2004},
journaltitle = {Journal on Advances in Signal Processing (EURASIP)},
title        = {Digital Waveguides versus Finite Difference Structures: Equivalence and Mixed Modeling},
doi          = {10.1155/s1110865704401176},
number       = {7},
pages        = {978--989},
volume       = {2004},
journal      = {{EURASIP} Journal on Advances in Signal Processing},
keywords     = {acoustic signal processing,and phrases,digital waveguides,fdtd model structures,hybrid models,scattering},
month        = {jun},
publisher    = {Springer Nature},
year         = {2004},
}
@Article{2011DudaDft,
author       = {Duda, Krzysztof and Magalas, Leszek B. and Majewski, Mariusz and Zielinski, Tomasz P.},
date         = {2011-11},
journaltitle = {IEEE Transactions on Instrumentation and Measurement},
title        = {DFT-based Estimation of Damped Oscillation Parameters in Low-Frequency Mechanical Spectroscopy},
doi          = {10.1109/TIM.2011.2113124},
issn         = {0018-9456, 1557-9662},
number       = {11},
pages        = {3608--3618},
url          = {http://ieeexplore.ieee.org/document/6022793/},
urldate      = {2019-03-28},
volume       = {60},
abstract     = {In this paper, we analyze and compare the properties of different well-known and also new nonparametric discrete Fourier transform (DFT)-based methods for resonant frequency and logarithmic decrement estimation in application to mechanical spectroscopy. We derive a new DFT interpolation algorithm for a signal analyzed with Rife–Vincent class-I windows and also propose new formulas that extend Bertocco and Yoshida methods. We study errors of the resonant frequency and logarithmic decrement estimation in realistic conditions that include measurement noise and a zero-point drift. We also investigate the systematic errors of the estimation methods of interest. A nonlinear least squares time-domain parametric signal ﬁtting is used to determine the boundaries of statistical efﬁciency in all tests.},
langid       = {english},
}
@Misc{2000FeldmanDerivation,
author = {Feldman, Joel},
date   = {2000},
title  = {Derivation of the Wave Equation},
url    = {http://www.math.ubc.ca//textasciitilde feldman/m256/wave.pdf},
file   = {:done/2000FeldmanDerivation Derivation of the Wave Equation.pdf:application/pdf},
groups = {tesse:5},
pages  = {1--2},
}
@Article{2017PengDeepmimic,
author       = {Peng, Xue Bin and Abbeel, Pieter and Levine, Sergey and van de Panne, Michiel},
date         = {2017},
journaltitle = {ACM Transactions on Graphics},
title        = {DeepMimic : Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills},
doi          = {10.1145/3197517.3201311},
eprint       = {arXiv:1804.02717v1},
eprinttype   = {arXiv},
issn         = {0730-0301},
number       = {4},
pages        = {1--14},
url          = {http://arxiv.org/abs/1804.02717},
urldate      = {2019-03-26},
volume       = {37},
abstract     = {A longstanding goal in character animation is to combine data-driven specification of behavior with a system that can execute a similar behavior in a physical simulation, thus enabling realistic responses to perturbations and environmental variation. We show that well-known reinforcement learning (RL) methods can be adapted to learn robust control policies capable of imitating a broad range of example motion clips, while also learning complex recoveries, adapting to changes in morphology, and accomplishing user-specified goals. Our method handles keyframed motions, highly-dynamic actions such as motion-captured flips and spins, and retargeted motions. By combining a motion-imitation objective with a task objective, we can train characters that react intelligently in interactive settings, e.g., by walking in a desired direction or throwing a ball at a user-specified target. This approach thus combines the convenience and motion quality of using motion clips to define the desired style and appearance, with the flexibility and generality afforded by RL methods and physics-based animation. We further explore a number of methods for integrating multiple clips into the learning process to develop multi-skilled agents capable of performing a rich repertoire of diverse skills. We demonstrate results using multiple characters (human, Atlas robot, bipedal dinosaur, dragon) and a large variety of skills, including locomotion, acrobatics, and martial arts.},
arxivid      = {arXiv:1804.02717v1},
langid       = {english},
shorttitle   = {DeepMimic},
}
@Article{2016HadjeresDeepbach,
author       = {Hadjeres, Gaëtan and Pachet, François and Nielsen, Frank},
date         = {2016-12-03},
journaltitle = {arXiv:1612.01010 [cs]},
title        = {DeepBach: a Steerable Model for Bach Chorales Generation},
eprint       = {1612.01010},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1612.01010},
urldate      = {2019-03-28},
abstract     = {This paper introduces DeepBach, a graphical model aimed at modeling polyphonic music and speciﬁcally hymn-like pieces. We claim that, after being trained on the chorale harmonizations by Johann Sebastian Bach, our model is capable of generating highly convincing chorales in the style of Bach. DeepBach’s strength comes from the use of pseudo-Gibbs sampling coupled with an adapted representation of musical data. This is in contrast with many automatic music composition approaches which tend to compose music sequentially. Our model is also steerable in the sense that a user can constrain the generation by imposing positional constraints such as notes, rhythms or cadences in the generated score. We also provide a plugin on top of the MuseScore music editor making the interaction with DeepBach easy to use.},
arxivid      = {arXiv:1612.01010v2},
file         = {:done/2016HadjeresDeepbach DeepBach/\\_ a Steerable Model for Bach Chorales Generation.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {DeepBach},
}
@Article{2017ArikDeep,
author       = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
date         = {2017-02-24},
journaltitle = {arXiv:1702.07825 [cs]},
title        = {Deep Voice: Real-time Neural Text-to-Speech},
eprint       = {1702.07825},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1702.07825},
urldate      = {2019-03-26},
abstract     = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises ﬁve major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-tophoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classiﬁcation (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more ﬂexible than traditional text-tospeech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
langid       = {english},
shorttitle   = {Deep Voice},
}
@Book{2017PattersonDeep,
author    = {Patterson, Josh and Gibson, Adam},
date      = {2017-08-11},
title     = {Deep Learning: A Practitioner's Approach},
edition   = {1},
isbn      = {978-1-4919-1425-0},
publisher = {O'Reilly Media, Inc.},
url       = {https://www.ebook.de/de/product/23640784/adam///////\\_gibson///////\\_josh///////\\_patterson///////\\_deep///////\\_learning///////\\_the///////\\_definitive///////\\_guide.html},
volume    = {2017},
}
@Article{2015SainathDeep,
author       = {Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and Mohamed, Abdel-rahman and Dahl, George and Ramabhadran, Bhuvana},
date         = {2015-04},
journaltitle = {Neural Networks},
title        = {Deep Convolutional Neural Networks for Large-scale Speech Tasks},
doi          = {10.1016/j.neunet.2014.08.005},
pages        = {39--48},
volume       = {64},
file         = {:done/2015SainathDeep Deep Convolutional Neural Networks for Large Scale Speech Tasks.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{xu_deep_2017,
author       = {Xu, Lamei and Lin, Jin and Wang, Lina and Yin, Chunyong and Wang, Jin},
date         = {2017-02-12},
title        = {Deep Convolutional Neural Network based Approach for Aspect-based Sentiment Analysis},
doi          = {10.14257/astl.2017.143.41},
eventtitle   = {Advanced Science and Technology 2017},
number       = {Ast},
pages        = {199--204},
url          = {http://onlinepresent.org/proceedings/vol143///////\\_2017/41.pdf},
urldate      = {2019-04-05},
volume       = {143},
abstract     = {Sentiment analysis is an important task in natural language processing and has a wide range of applications. This paper describes our deep learning approach to multilingual aspect-based sentiment analysis. Our model use a deep convolutional neural network for both aspect extraction and aspect-based sentiment analysis. We take aspect extraction as a multi-label classification problem, outputting probabilities over aspects parameterized by a threshold. For the sentiment towards an aspect, we concatenate an aspect vector with every word embedding and apply a convolution over it. Experiments result shows that our system performs comparably well on the Yelp reviews.},
journaltitle = {Advanced Science and Technology Letters},
keywords     = {aspect-based sentiment,convolution neural network,sentiment analysis,word2vec},
langid       = {english},
}
@InProceedings{2008CecottiConvolutional,
author       = {Cecotti, Hubert and Graeser, Axel},
booktitle    = {2008 19th International Conference on Pattern Recognition},
date         = {2008-12},
title        = {Convolutional Neural Network with embedded Fourier Transform for EEG classification},
doi          = {10.1109/ICPR.2008.4761638},
eventtitle   = {2008 19th International Conference on Pattern Recognition (ICPR)},
isbn         = {978-1-4244-2174-9},
location     = {Tampa, FL, USA},
pages        = {1--4},
publisher    = {IEEE},
url          = {http://ieeexplore.ieee.org/document/4761638/},
urldate      = {2019-03-26},
abstract     = {In BCI (Brain - Computer Interface) systems, brain signals must be processed to identify distinct activities that convey different mental states. We propose a new technique for the classiﬁcation of electroencephalographic (EEG) Steady-State Visual Evoked Potential (SSVEP) activity for non-invasive BCI. The proposed method is based on a Convolutional Neural Network that includes a Fourier transform between hidden layers in order to switch from the time domain to the frequency domain analysis in the network. The ﬁrst step allows the creation of different channels. The second step is dedicated to the transformation of the signal in the frequency domain. The last step is the classiﬁcation. It uses a hybrid rejection strategy that uses a junk class for the mental transition states and thresholds for the conﬁdence values. The presented results with ofﬂine processing are obtained with 6 electrodes on 2 subjects with a time segment of 1s. The system is reliable for both subjects over 95///////\\%, with rejection criterion.},
journaltitle = {2008 19th International Conference on Pattern Recognition (ICPR)},
langid       = {english},
}
@Article{2016EsserConvolutional,
author       = {Esser, Steven K. and Merolla, Paul A. and Arthur, John V. and Cassidy, Andrew S. and Appuswamy, Rathinakumar and Berg, David J. and Mckinstry, Jeffrey L. and Melano, Timothy and Barch, Davis R. and Nolfo, Carmelo and Amir, Arnon and Taba, Brian and Flickner, Myron D. and Modha, Dharmendra S.},
date         = {2016-10-11},
journaltitle = {Proceedings of the National Academy of Sciences},
title        = {Convolutional Networks for Fast , Energy-Efficient Neuromorphic Computing},
doi          = {10.1073/pnas.1604850113},
eprint       = {arXiv:1603.08270v2},
eprinttype   = {arXiv},
issn         = {0027-8424, 1091-6490},
number       = {Figure 1},
pages        = {1--7},
url          = {http://arxiv.org/abs/1603.08270},
urldate      = {2019-03-26},
volume       = {113},
abstract     = {Deep networks are now able to achieve human-level performance on a broad spectrum of recognition tasks. Independently, neuromorphic computing has now demonstrated unprecedented energy-eﬃciency through a new chip architecture based on spiking neurons, low precision synapses, and a scalable communication network. Here, we demonstrate that neuromorphic computing, despite its novel architectural primitives, can implement deep convolution networks that i) approach state-of-the-art classiﬁcation accuracy across 8 standard datasets, encompassing vision and speech, ii) perform inference while preserving the hardware’s underlying energy-eﬃciency and high throughput, running on the aforementioned datasets at between 1200 and 2600 frames per second and using between 25 and 275 mW (effectively > 6000 frames / sec / W) and iii) can be speciﬁed and trained using backpropagation with the same ease-of-use as contemporary deep learning. For the ﬁrst time, the algorithmic power of deep learning can be merged with the eﬃciency of neuromorphic processors, bringing the promise of embedded, intelligent, brain-inspired computing one step closer.},
arxivid      = {arXiv:1603.08270v2},
file         = {:done/2016EsserConvolutional Convolutional Networks for Fast , Energy Efficient Neuromorphic Computing.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2005BensaComputational,
author       = {Bensa, Julien and Bilbao, Stefan and Kronland-Martinet, Richard and Smith, Julius and Voinier, Thierry},
date         = {2005},
journaltitle = {Acta Acustica united with Acustica},
title        = {Computational modeling of stiff piano strings using digital waveguides and finite difference},
pages        = {289--298},
url          = {https://hal.archives-ouvertes.fr/hal-00088061},
volume       = {91},
journal      = {Acta Acustica united with Acustica},
keywords     = {digital waveguides, physical modeling, finite difference},
publisher    = {Hirzel Verlag},
year         = {2005},
}
@Article{2017SoteloChar2wav,
author        = {Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Santos, Joao Felipe and Kastner, Kyle and Courville, Aaron and Bengio, Yoshua},
date          = {2017},
journaltitle  = {Iclr},
title         = {Char2Wav: End-to-End Speech Synthesis},
number        = {October},
pages         = {6},
url           = {https://openreview.net/pdf?id=B1VWyySKx},
abstract      = {We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder. The reader is an encoderdecoder model with attention. The encoder is a bidirectional recurrent neural network that accepts text or phonemes as inputs, while the decoder is a recurrent neural network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
file          = {:done/2017SoteloChar2wav Char2Wav\\_ End to End Speech Synthesis.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {speech synthesis},
langid        = {english},
mendeley-tags = {speech synthesis},
}
@InProceedings{2014DalgleishBlurring,
author       = {Dalgleish, Mat and Foster, Chris and Spencer, Steve},
booktitle    = {Proceedings of the 9th Conference on Interdisciplinary Musicology – CIM14},
date         = {2014},
title        = {Blurring the Lines: An Integrated Compositional Model for Digital Music Instrument Design},
pages        = {6},
publisher    = {Proceedings of the 9th Conference on Interdisciplinary Musicology. CIM14. Berlin, Germany},
url          = {http://www.academia.edu/9198495/BLURRING///////\\_THE///////\\_LINES///////\\_AN///////\\_INTEGRATED///////\\_COMPOSITIONAL///////\\_MODEL///////\\_FOR///////\\_DIGITAL///////\\_MUSICAL///////\\_INSTRUMENT///////\\_DESIGN},
volume       = {14},
abstract     = {Computer-based interactive music systems date back as far as the late 1960s, but increasingly accessible technologies have prompted significant growth in interest in digital musical instruments (DMIs) over the last decade. To date, the designers of DMIs have generally borrowed paradigms from acoustic instruments or the field of Human-Computer Interaction (HCI). However, it can be argued that DMIs are a fundamentally different case and the suitability of these paradigms is debatable at best. For instance, DMIs lack the haptic feedback of acoustic instruments. Musical instruments are also highly specialized rather than general-purpose tools, and musical performance is not typically task-based. Additionally, Jordà notes that their designers have tended to focus on isolated parts of the problem, to the detriment of instrumental cohesion and character. While a few authors have considered DMIs as more fully rounded constructions, and the term ‘composed instruments’ has been used to describe the specification of the input-output relationship as an intentional act of composition, we argue that this is insufficient. Drawing on theories of affordances and ecological music creation, we describe an alternative model that considers DMI design as part of a broader compositional process that also includes text and hybrid acoustic-digital space. The traditionally distinct roles of designer, composer and performer are seen to blur, and the notion of composition-specific instruments is discussed. As an example of the model in practice, the interdisciplinary collaborative piece Desire Lines is described. This serves to aid an initial assessment of the model and its implementation, and informs some remarks around its limitations and future possibilities.},
journaltitle = {. Berlin},
langid       = {english},
year         = {2014},
}
@PhdThesis{1974WerbosRegression,
author      = {Werbos, Paul John},
date        = {1974},
institution = {Harvard University},
title       = {Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences},
type        = {phdthesis},
url         = {https://www.researchgate.net/publication/35055330///////\\_Beyond///////\\_regression///////\\_new///////\\_tools///////\\_for///////\\_prediction///////\\_and///////\\_analysis///////\\_in///////\\_the///////\\_behavior///////\\_sciences///////\\_microform},
month       = {aug},
year        = {1974},
}
@InProceedings{2016ShiBenchmarking,
author     = {Shi, S. and Wang, Q. and Xu, P. and Chu, X.},
booktitle  = {Proceedings of the 2016 7th International Conference on Cloud Computing and Big Data (CCBD)},
date       = {2016-11},
title      = {Benchmarking State-of-the-Art Deep Learning Software Tools},
doi        = {10.1109/CCBD.2016.029},
eprint     = {arXiv:1608.07249v7},
eprinttype = {arXiv},
pages      = {99--104},
publisher  = {AnaisIEEE},
arxivid    = {arXiv:1608.07249v7},
edition    = {Cloud Comp},
keywords   = {Training, Neural networks, Convolutional Neural Networks, Deep Learning, Recurrent Neural Networks, Machine learning, Graphics processing units, Benchmark testing, Caffe, CNTK, CPU platform, deep network training, Feed-forward Neural Networks, GPU, GPU-accelerated deep learning software tools, hardware platforms, Instruction sets, machine learning method, open-source deep learning software tool benchmarking, running time performance evaluation, TensorFlow, Tools, Torch},
}
@Article{2015IoffeBatch,
author       = {Ioffe, Sergey and Szegedy, Christian},
date         = {2015-02-10},
journaltitle = {arXiv:1502.03167 [cs]},
title        = {Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift},
eprint       = {arXiv:1502.03167v3},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1502.03167},
urldate      = {2019-03-26},
abstract     = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classiﬁcation model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a signiﬁcant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classiﬁcation: reaching 4.9///////\\% top-5 validation error (and 4.8///////\\% test error), exceeding the accuracy of human raters.},
arxivid      = {arXiv:1502.03167v3},
langid       = {english},
shorttitle   = {Batch Normalization},
}
@Misc{2006MakinBackpropagation,
author = {Makin, J. G.},
date   = {2006},
title  = {Backpropagation},
url    = {http://www.cs.cornell.edu/courses/cs5740/2016sp/resources/backprop.pdf},
file   = {:done/2006MakinBackpropagation Backpropagation.pdf:application/pdf},
groups = {tesse:5},
pages  = {1--8},
}
@InProceedings{2016SouthallAutomatic,
author        = {Southall, Carl and Stables, Ryan and Hockman, Jason},
booktitle     = {Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016, New York City, United States, August 7-11, 2016},
date          = {2016},
title         = {Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks},
isbn          = {978-0-692-75506-8},
pages         = {591--597},
url           = {https://wp.nyu.edu/ismir2016/wp-content/uploads/sites/2294/2016/07/217///////\\_Paper.pdf},
volume        = {2016},
abstract      = {Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in-struments in audio recordings. Neural networks have al-ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro-pose the use of neural networks for ADT in order to ex-ploit their ability to capture a complex configuration of fea-tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu-ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit-able for online operation. In both systems, a separate net-work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis-ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec-tively. The results demonstrate the effectiveness of the pre-sented methods for solo percussion and a capacity for iden-tifying snare drums, which are historically the most diffi-cult drum class to detect.},
file          = {:done/2016SouthallAutomatic Automatic Drum Transcription Using Bi Directional Recurrent Neural Networks.pdf:application/pdf},
groups        = {tesse:5},
journaltitle  = {Proceedings of International Society for Music Information Retrieval Conference (ISMIR)},
keywords      = {music transcription,tesis},
mendeley-tags = {music transcription,tesis},
}
@Article{2016PersioArtiﬁcial,
author        = {Persio, Luca Di and Honchar, Oleksandr},
date          = {2016},
journaltitle  = {International Journal of Circuits, Systems and Signal Processing},
title         = {Artiﬁcial Neural Networks architectures for stock price prediction: comparisons and applications},
issn          = {1998-4464},
pages         = {403--413},
volume        = {10},
abstract      = {We present an Artiﬁcial Neural Network (ANN) approach to predict stock market indices, particularly with respect to the forecast of their trend movements up or down. Exploiting different Neural Networks architectures, we provide numerical analysis of concrete ﬁnancial time series. In particular, after a brief re´sume´ of the existing literature on the subject, we consider the Multi-layer Perceptron (MLP), the Convolutional Neural Networks (CNN), and the Long Short-Term Memory (LSTM) recurrent neural networks techniques. We focus on the importance of choosing the correct input features, along with their preprocessing, for the speciﬁc learning algorithm one wants to use. Eventually, we consider the S///////\\&P500 historical time series, predicting trend on the basis of data from the past days, and proposing a novel approach based on combination of wavelets and CNN, which outperforms the basic neural networks ones. We show, that neural networks are able to predict ﬁnancial time series movements even trained only on plain time series data and propose more ways to improve results.},
keywords      = {Artificial neural networks,Convolutional neural network,Deep Learning,Financial forecasting,Long shortterm memory,Multi-layer neural network,Recurrent neural network,Stock markets analysis,Time series analysis,forecasting},
langid        = {english},
mendeley-tags = {forecasting},
}
@Book{2016RussellArtificial,
author     = {Russell, Stuart J. and Norvig, Peter and Davis, Ernest},
date       = {2016},
title      = {Artificial intelligence: a modern approach},
edition    = {3rd ed},
isbn       = {9781292153964},
location   = {Upper Saddle River},
pagetotal  = {1132},
publisher  = {Prentice Hall},
series     = {Prentice Hall series in artificial intelligence},
url        = {https://www.ebook.de/de/product/25939961/stuart///////\\_russell///////\\_peter///////\\_norvig///////\\_artificial///////\\_intelligence///////\\_a///////\\_modern///////\\_approach///////\\_global///////\\_edition.html},
volume     = {2016},
ean        = {9781292153964},
file       = {:done/2016RussellArtificial Artificial Intelligence/\\_ a Modern Approach.pdf:application/pdf},
groups     = {tesse:5},
keywords   = {Artificial intelligence},
langid     = {english},
shorttitle = {Artificial intelligence},
year       = {2016},
}
@InProceedings{2017GullyArticulatory,
author    = {Amelia J. Gully and Takenori Yoshimura and Damian T. Murphy and Kei Hashimoto and Yoshihiko Nankaku and Keiichi Tokuda},
booktitle = {Proceedings of the Interspeech 2017},
date      = {2017-08},
title     = {Articulatory Text-to-Speech Synthesis Using the Digital Waveguide Mesh Driven by a Deep Neural Network},
doi       = {10.21437/Interspeech.2017-900},
pages     = {234--238},
publisher = {ISCA},
keywords  = {[Electronic Manuscript]},
month     = {aug},
year      = {2017},
}
@InProceedings{2017ParkAnalysis,
author       = {Park, Sungheon and Kwak, Nojun},
booktitle    = {Computer Vision – ACCV 2016},
date         = {2017},
title        = {Analysis on the Dropout Effect in Convolutional Neural Networks},
doi          = {10.1007/978-3-319-54184-6///////\\_12},
editor       = {Lai, Shang-Hong and Lepetit, Vincent and Nishino, Ko and Sato, Yoichi},
isbn         = {978-3-319-54183-9 978-3-319-54184-6},
location     = {Cham},
organization = {Springer},
pages        = {189--204},
publisher    = {Springer International Publishing},
urldate      = {2019-04-05},
volume       = {10112},
abstract     = {Regularizing neural networks is an important task to reduce overﬁtting. Dropout [1] has been a widely-used regularization trick for neural networks. In convolutional neural networks (CNNs), dropout is usually applied to the fully connected layers. Meanwhile, the regularization eﬀect of dropout in the convolutional layers has not been thoroughly analyzed in the literature. In this paper, we analyze the eﬀect of dropout in the convolutional layers, which is indeed proved as a powerful generalization method. We observed that dropout in CNNs regularizes the networks by adding noise to the output feature maps of each layer, yielding robustness to variations of images. Based on this observation, we propose a stochastic dropout whose drop ratio varies for each iteration. Furthermore, we propose a new regularization method which is inspired by behaviors of image ﬁlters. Rather than randomly drop the activation, we selectively drop the activations which have high values across the feature map or across the channels. Experimental results validate the regularization performance of selective max-drop and stochastic dropout is competitive to the dropout or spatial dropout [2].},
file         = {:done/2017ParkAnalysis Analysis on the Dropout Effect in Convolutional Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{2001MuellerIntroduction,
author        = {Müller, Klaus-Robert and Mika, Sebastian and Rätsch, Gunnar and Tsuda, Koji and Schölkopf, Bernhard},
date          = {2001},
journaltitle  = {IEEE TRANSACTIONS ON NEURAL NETWORKS},
title         = {An Introduction to Kernel-Based Learning Algorithms},
doi           = {10.1109/72.914517},
issn          = {1045-9227},
number        = {2},
pages         = {21},
volume        = {12},
abstract      = {This paper provides an introduction to support vector machines (SVMs), kernel Fisher discriminant analysis, and kernel principal component analysis (PCA), as examples for successful kernel-based learning methods. We first give a short background about Vapnik–Chervonenkis (VC) theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by finally discussing applications such as optical character recognition (OCR) and DNA analysis.},
file          = {:done/2001MuellerIntroduction An Introduction to Kernel Based Learning Algorithms.pdf:application/pdf},
groups        = {tesse:5},
isbn          = {1045-9227},
keywords      = {Boosting,Fisher's discriminant,Kernel PCA,Kernel methods,Mathematical programming machines,Mercer kernels,Principal component analysis (PCA),Single-class classification,Support vector machines (SVMs),seminal},
langid        = {english},
mendeley-tags = {seminal},
pmid          = {18244377},
}
@InProceedings{2006TuohyEvolved,
author        = {Tuohy, D. R. and Potter, W. D.},
booktitle     = {ICMC},
date          = {2006},
title         = {An Evolved Neural Network/HC Hybrid for Tablature Creation in GA- based Guitar Arranging},
isbn          = {{////////\\%}(},
number        = {January 2006},
pages         = {576--579},
url           = {http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2006.119},
volume        = {2006},
abstract      = {In this paper we describe a technique for creating guitar tablature using a neural network. Training data was parsed from an online repository of human-created tablatures. The contents of both the input layer and the set of training data have been optimized through genetic search in order to maximize the accuracy of the network. The output of the network is im- proved upon with a local heuristic hill-climber (HC). We implement this model in an existing system for generating guitar arrangements via genetic algorithm (GA). When compared to the original system for generating tablature, we note modest improvement in tablature quality and drastic improvements in execution time.},
journaltitle  = {CMC},
keywords      = {GA,fingeringprediction,guitar,music transcription},
mendeley-tags = {GA,music transcription},
year          = {2006},
}
@InProceedings{2017ParvatSurvey,
author    = {Parvat, A. and Chavan, J. and Kadam, S. and Dev, S. and Pathak, V.},
booktitle = {Proceedings of the 2017 International Conference on Inventive Systems and Control (ICISC)},
date      = {2017-01},
title     = {A survey of deep-learning frameworks},
doi       = {10.1109/ICISC.2017.8068684},
pages     = {1--7},
publisher = {AnaisIEEE},
edition   = {Inventive},
file      = {:done/2017ParvatSurvey A Survey of Deep Learning Frameworks.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Training, Mathematical model, Neural networks, Computational modeling, Machine learning, artificial neural network, Artificial neural networks, Deep learning, deep-learning frameworks, Graphics processing units, Libraries, machine learning, Software libraries},
}
@Article{2008HuangReview,
author       = {Huang, Norden E and Wu, Zhaohua},
date         = {2008},
journaltitle = {Reviews of Geophysics},
title        = {A review on Hilbert-Huang transform: Method and its applications to geophysical studies},
doi          = {10.1029/2007RG000228.1.INTRODUCTION},
issn         = {8755-1209},
number       = {2007},
pages        = {1--23},
urldate      = {2019-03-28},
volume       = {46},
langid       = {english},
shorttitle   = {A review on Hilbert-Huang transform},
}
@InProceedings{he_praat-based_2016,
author     = {He, Lei and Dellwo, Volker},
date       = {2016-09-08},
title      = {A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform},
doi        = {10.21437/Interspeech.2016-1447},
eventtitle = {Interspeech 2016},
number     = {September},
pages      = {530--534},
url        = {http://www.isca-speech.org/archive/Interspeech///////\\_2016/abstracts/1447.html},
urldate    = {2019-03-26},
abstract   = {A speech signal can be viewed as a high frequency carrier signal containing the temporal fine structure (TFS) that is modulated by a low frequency envelope (ENV). A widely used method to decompose a speech signal into the TFS and ENV is the Hilbert transform. Although this method has been available for about one century and is widely applied in various kinds of speech processing tasks (e.g. speech chimeras), there are only very few speech processing packages that contain readily available functions for the Hilbert transform, and there is very little textbook type literature tailored for speech scientists to explain the processes behind the transform. With this paper we provide the code for carrying out the Hilbert operation to obtain the TFS and ENV in the widely used speech processing software Praat, and explain the basics of the procedure. To verify our code, we compare the Hilbert transform in Praat with a widely applied function for the same purpose in MATLAB ("hilbert(...)"). We can confirm that both methods arrive at identical outputs.},
langid     = {english},
}
@Article{2003IiiBasic,
author    = {Iii, Julius O Smith},
date      = {2006},
title     = {A Basic Introduction to Digital Waveguide Synthesis (for the Technically Inclined)},
pages     = {1--5},
url       = {https://ccrma.stanford.edu/ jos/swgt/},
address   = {Stanford University. stanford. edu/ jos/swgt},
location  = {Stanford University. stanford. edu/ jos/swgt},
publisher = {Center for Computer Research in Music and Acoustics (CCRMA)},
year      = {2006},
}
@Article{2015HouBlind,
author        = {Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong and Member, Senior and Tao, Dacheng and Member, Senior},
date          = {2015},
journaltitle  = {IEEE transactions on neural networks and learning systems},
title         = {Blind Image Quality Assessment via Deep Learning},
doi           = {10.1109/TNNLS.2014.2336852},
issn          = {2162-237X, 2162-2388},
number        = {6},
pages         = {1275--1286},
url           = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6872541},
urldate       = {2019-04-05},
volume        = {26},
abstract      = {This paper investigates how to blindly evaluate the visual quality of an image by learning rules from linguistic descriptions. Extensive psychological evidence shows that humans prefer to conduct evaluations qualitatively rather than numerically. The qualitative evaluations are then converted into the numerical scores to fairly benchmark objective image quality assessment (IQA) metrics. Recently, lots of learning-based IQA models are proposed by analyzing the mapping from the images to numerical ratings. However, the learnt mapping can hardly be accurate enough because some information has been lost in such an irreversible conversion from the linguistic descriptions to numerical scores. In this paper, we propose a blind IQA model, which learns qualitative evaluations directly and outputs numerical scores for general utilization and fair comparison. Images are represented by natural scene statistics features. A discriminative deep model is trained to classify the features into ﬁve grades, corresponding to ﬁve explicit mental concepts, i.e., excellent, good, fair, poor, and bad. A newly designed quality pooling is then applied to convert the qualitative labels into scores. The classiﬁcation framework is not only much more natural than the regression-based models, but also robust to the small sample size problem. Thorough experiments are conducted on popular databases to verify the model’s effectiveness, efﬁciency, and robustness.},
file          = {:done/2015HouBlind Blind Image Quality Assessment Via Deep Learning.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {image classification,sentiment analysis},
langid        = {english},
mendeley-tags = {image classification,sentiment analysis},
publisher     = {IEEE},
}
@Article{2015KulkarniDeep,
author        = {Kulkarni, Tejas D. and Whitney, William F. and Kohli, Pushmeet and Tenenbaum, Josh},
date          = {2015},
journaltitle  = {Advances in Neural Information Processing Systems 28},
title         = {Deep Convolutional Inverse Graphics Network},
doi           = {10.1063/1.4914407},
editor        = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
eprint        = {1503.03167},
eprinttype    = {arXiv},
issn          = {1089-7550},
pages         = {2539--2547},
url           = {http://papers.nips.cc/paper/5851-deep-convolutional-inverse-graphics-network.pdf},
volume        = {2015},
abstract      = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
arxivid       = {1503.03167},
booktitle     = {Advances in Neural Information Processing Systems 28},
file          = {:done/2015KulkarniDeep Deep Convolutional Inverse Graphics Network.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {image synthesis,tesis},
mendeley-tags = {image synthesis,tesis},
publisher     = {Curran Associates, Inc.},
year          = {2015},
}
@InProceedings{2017SabourDynamic,
author       = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E.},
booktitle    = {Advances in Neural Information Processing Systems 30},
date         = {2017},
title        = {Dynamic Routing Between Capsules},
editor       = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
eprint       = {arXiv:1710.09829v1},
eprinttype   = {arXiv},
number       = {Nips},
pages        = {3856--3866},
publisher    = {Curran Associates, Inc.},
url          = {http://papers.nips.cc/paper/6975-dynamic-routing-between-capsules.pdf},
volume       = {2017},
arxivid      = {arXiv:1710.09829v1},
file         = {:done/2017SabourDynamic Dynamic Routing between Capsules.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Advances in Neural Information Processing Systems 30},
year         = {2017},
}
@Book{2004CoppinArtificial,
author    = {Coppin, Ben},
date      = {2004-04-11},
title     = {Artificial intelligence illuminated},
edition   = {1st ed},
isbn      = {0-7637-3230-3},
location  = {Boston},
pagetotal = {740},
publisher = {JONES //\\\& BARTLETT PUB INC},
url       = {https://www.ebook.de/de/product/4345313/ben///////\\_coppin///////\\_artificial///////\\_intelligence///////\\_illuminated.html},
volume    = {2004},
ean       = {9780763732301},
file      = {:done/2004CoppinArtificial Artificial Intelligence Illuminated.pdf:application/pdf},
groups    = {tesse:5},
keywords  = {Artificial intelligence},
langid    = {english},
year      = {2004},
}
@InProceedings{2015XuCcg,
author       = {Xu, Wenduan and Auli, Michael and Clark, Stephen},
booktitle    = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
date         = {2015},
title        = {CCG Supertagging with a Recurrent Neural Network},
doi          = {10.3115/v1/p15-2041},
isbn         = {9781941643730},
note         = {event-place: Beijing, China},
number       = {2014},
pages        = {250--255},
publisher    = {Association for Computational Linguistics},
url          = {http://www.aclweb.org/anthology/P15-2041},
file         = {:done/2015XuCcg CCG Supertagging with a Recurrent Neural Network.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
keywords     = {sentence classification},
}
@InProceedings{2016ZhangColorful,
author       = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
booktitle    = {European Conference on Computer Vision},
date         = {2016},
title        = {Colorful Image Colorization},
eprint       = {1603.08511},
eprinttype   = {arXiv},
location     = {Anais},
organization = {Springer},
pages        = {649--666},
publisher    = {Springer},
url          = {http://arxiv.org/abs/1603.08511},
volume       = {abs/1603.08511},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/ZhangIE16},
file         = {:done/2016ZhangColorful Colorful Image Colorization.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {CoRR},
timestamp    = {Mon, 13 Aug 2018 16:46:30 +0200},
}
@Book{2016GoodfellowDeep,
author    = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
date      = {2016},
title     = {Deep Learning},
note      = {http://www.deeplearningbook.org},
publisher = {MIT Press},
volume    = {1},
file      = {:done/2016GoodfellowDeep Deep Learning.pdf:application/pdf},
groups    = {tesse:5},
}
@InProceedings{1999JensenEnvelope,
author       = {Jensen, Kristoffer},
booktitle    = {PROCEEDINGS OF THE DAFX},
date         = {1999},
title        = {Envelope Model Of Isolated Musical Sounds},
doi          = {10.1109/MMSP.2001.962718},
number       = {1},
pages        = {9--12},
abstract     = {This paper presents a model of the envelope of the additive parameters of isolated musical sounds, along with a new method for the estimation of the important envelope split- point times. The model consists of start, attack, sustain, release, and end segments with variable split-point amplitude and time. The estimation of the times is done using smoothed derivatives of the envelopes. The estimated split-point values can be used together with a curve-form model introduced in this paper in the analysis/synthesis of musical sounds. The envelope model can recreate noise-less musical sounds with good fidelity, and the method for the estimation of the envelope times performs significantly better than the classical percentage- based method.},
file         = {:done/1999JensenEnvelope Envelope Model of Isolated Musical Sounds.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Audio},
}
@InProceedings{2013GravesSpeech,
author    = {Graves, A. and Mohamed, A. and Hinton, G.},
booktitle = {Proceedings of the 2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
date      = {2013-05},
title     = {Speech recognition with deep recurrent neural networks},
doi       = {10.1109/ICASSP.2013.6638947},
pages     = {6645--6649},
publisher = {AnaisIEEE},
edition   = {Acoustics,},
keywords  = {Noise, Recurrent neural networks, Training, Vectors, Acoustics, recurrent neural networks, speech recognition, Speech recognition, connectionist temporal classification, deep neural networks, deep recurrent neural networks, end-to-end training methods, long short-term memory RNN architecture, sequential data},
}
@InCollection{2012LecunEfficient,
author    = {Lecun, Y. A. and Others},
booktitle = {Neural networks: Tricks of the trade},
date      = {2012},
title     = {Efficient backprop},
doi       = {10.1007/978-3-642-35289-8///////\\_3},
location  = {Traducao. [s.l.] p. 948},
pages     = {9--48},
publisher = {Springer},
file      = {:done/2012LecunEfficient Efficient Backprop.pdf:application/pdf},
groups    = {tesse:5},
}
@PhdThesis{2014SocherRecursive,
author      = {Socher, Richard},
date        = {2014},
institution = {Stanford University},
title       = {Recursive deep learning for natural language processing and computer vision},
type        = {phdthesis},
url         = {https://nlp.stanford.edu//textasciitilde socherr/thesis.pdf},
file        = {:done/2014SocherRecursive Recursive Deep Learning for Natural Language Processing and Computer Vision.pdf:application/pdf},
groups      = {tesse:5},
publisher   = {Citeseer},
school      = {Stanford University},
year        = {2014},
}
@InProceedings{2016ChoiAutomatic,
author        = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
booktitle     = {Proceedings of the 17th International Society for Music Information Retrieval Conference, ISMIR 2016},
date          = {2016},
title         = {Automatic tagging using deep convolutional neural networks},
eprint        = {1606.00298},
eprinttype    = {arXiv},
location      = {New York City, United States},
pages         = {805--811},
url           = {http://arxiv.org/abs/1606.00298},
abstract      = {We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
address       = {New York City, United States},
arxivid       = {1606.00298},
bibsource     = {dblp computer science bibliography, https://dblp.org},
biburl        = {https://dblp.org/rec/bib/conf/ismir/ChoiFS16},
journaltitle  = {arXiv preprint arXiv:1606.00298},
keywords      = {music classification},
mendeley-tags = {music classification},
month         = {August},
timestamp     = {Thu, 08 Sep 2016 13:32:51 +0200},
year          = {2016},
}
@Article{2016OordConditional,
author       = {Oord, Aaron van den and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
date         = {2016},
journaltitle = {CoRR},
title        = {Conditional Image Generation with PixelCNN Decoders},
eprint       = {1606.05328},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1606.05328},
volume       = {abs/1606.05328},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/OordKVEGK16},
timestamp    = {Mon, 13 Aug 2018 16:46:40 +0200},
}
@Article{1958RosenblattPerceptron,
author        = {Rosenblatt, F.},
date          = {1958},
journaltitle  = {Psychological Review},
title         = {The perceptron: A probabilistic model for information storage and organization in the brain.},
doi           = {10.1037/h0042519},
issn          = {0033-295X},
number        = {6},
pages         = {386--408},
url           = {http://content.apa.org/journals/rev/65/6/386},
volume        = {65},
abstract      = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
isbn          = {0033-295X},
keywords      = {seminal},
mendeley-tags = {seminal},
pmid          = {13602029},
}
@InCollection{2008JuliusO.SmithDigital,
author    = {Julius O. Smith, I. I. I.},
booktitle = {Handbook of Signal Processing in Acoustics},
date      = {2008},
title     = {Digital Waveguide Architectures for Virtual Musical Instruments},
doi       = {10.1007/978-0-387-30441-0_25},
location  = {Traducao. [s.l.] p. 399417},
pages     = {399--417},
publisher = {Springer},
file      = {:done/2008JuliusO.SmithDigital Digital Waveguide Architectures for Virtual Musical Instruments.pdf:application/pdf},
groups    = {tesse:5},
year      = {2008},
}
@Article{1990SerraSpectral,
author       = {Serra, Xavier and Smith, Julius},
date         = {1990},
journaltitle = {Computer Music Journal},
title        = {Spectral Modeling Synthesis: A Sound Analysis/Synthesis System Based on a Deterministic Plus Stochastic Decomposition},
doi          = {10.2307/3680788},
issn         = {0148-9267},
number       = {4},
pages        = {12},
url          = {https://www.jstor.org/stable/3680788?origin=crossref},
urldate      = {2019-03-26},
volume       = {14},
file         = {:done/1990SerraSpectral Spectral Modeling Synthesis/\\_ a Sound Analysis/\\_Synthesis System Based on a Deterministic Plus Stochastic Decomposition.pdf:application/pdf},
groups       = {tesse:5},
journal      = {Computer Music Journal},
langid       = {english},
publisher    = {JSTOR},
shorttitle   = {Spectral Modeling Synthesis},
year         = {1990},
}
@Book{1996RojasNeural,
author     = {Rojas, Raúl},
date       = {1996-07-12},
title      = {Neural Networks: A Systematic Introduction},
doi        = {10.1007/978-3-642-61068-4},
edition    = {1 edition},
isbn       = {978-3-540-60505-8},
location   = {Berlin ; New York},
pagetotal  = {502},
publisher  = {Springer},
url        = {https://www.ebook.de/de/product/1427891/raul///////\\_rojas///////\\_neural///////\\_networks.html},
abstract   = {Neural networks are a computing paradigm that is finding increasing attention among computer scientists. In this book, theoretical laws and models previously scattered in the literature are brought together into a general theory of artificial neural nets. Always with a view to biology and starting with the simplest nets, it is shown how the properties of models change when more general computing elements and net topologies are introduced. Each chapter contains examples, numerous illustrations, and a bibliography. The book is aimed at readers who seek an overview of the field or who wish to deepen their knowledge. It is suitable as a basis for university courses in neurocomputing.},
ean        = {9783540605058},
file       = {:done/1996RojasNeural Neural Networks/\\_ a Systematic Introduction.pdf:application/pdf},
groups     = {tesse:5},
shorttitle = {Neural Networks},
year       = {1996},
}
@InProceedings{2013DengNew,
author       = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
booktitle    = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
date         = {2013-05},
title        = {New types of deep neural network learning for speech recognition and related applications: an overview},
doi          = {10.1109/ICASSP.2013.6639344},
eventtitle   = {ICASSP 2013 - 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn         = {978-1-4799-0356-6},
location     = {Vancouver, BC, Canada},
organization = {IEEE},
pages        = {8599--8603},
publisher    = {IEEE},
url          = {http://ieeexplore.ieee.org/document/6639344/},
urldate      = {2019-04-05},
abstract     = {In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP2013, entitled "New Types of Deep Neural Network Learning for Speech Recognition and Related Applications," as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed.},
langid       = {english},
shorttitle   = {New types of deep neural network learning for speech recognition and related applications},
}
@Article{2017JingNeural,
author       = {Jing, Yongcheng and Yang, Yezhou and Feng, Zunlei and Ye, Jingwen and Yu, Yizhou and Song, Mingli},
date         = {2017-05-11},
journaltitle = {arXiv:1705.04058 [cs, eess, stat]},
title        = {Neural Style Transfer: A Review},
eprint       = {1705.04058},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1705.04058},
urldate      = {2019-04-05},
abstract     = {The recent work of Gatys et al. demonstrated the power of Convolutional Neural Networks (CNN) in creating artistic fantastic imagery by separating and recombing the image content and style. This process of using CNN to migrate the semantic content of one image to different styles is referred to as Neural Style Transfer. Since then, Neural Style Transfer has become a trending topic both in academic literature and industrial applications. It is receiving increasing attention from computer vision researchers and several methods are proposed to either improve or extend the original neural algorithm proposed by Gatys et al. However, there is no comprehensive survey presenting and summarizing recent Neural Style Transfer literature. This review aims to provide an overview of the current progress towards Neural Style Transfer, as well as discussing its various applications and open problems for future research.},
arxivid      = {arXiv:1705.04058v1},
file         = {:done/2017JingNeural Neural Style Transfer\\_ a Review.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
shorttitle   = {Neural Style Transfer},
}
@InProceedings{2014KarpathyLarge,
author       = {Karpathy, A. and Toderici, G. and Shetty, S. and Leung, T. and Sukthankar, R. and Fei-Fei, L.},
booktitle    = {Proceedings of the 2014 IEEE Conference on Computer Vision and Pattern Recognition},
date         = {2014-06},
title        = {Large-Scale Video Classification with Convolutional Neural Networks},
doi          = {10.1109/CVPR.2014.223},
pages        = {1725--1732},
volume       = {2014},
file         = {:done/2014KarpathyLarge Large Scale Video Classification with Convolutional Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
keywords     = {Training, convolutional neural networks, Feature extraction, action, classification, CNN, Computational modeling, Computer architecture, convolutional, dataset, feature-based baselines, image recognition problems, large-scale, local spatiotemporal information, network, neural, recognition, Spatial resolution, spatiotemporal networks, sports, Streaming media, UCF-101 action recognition dataset, UCF-101 baseline model, video, video classification, YouTube videos},
}
@Article{2014MoonWhat,
author       = {Moon, Il Joon and Hong, Sung Hwa},
date         = {2014},
journaltitle = {Korean Journal of Audiology},
title        = {What Is Temporal Fine Structure and Why Is It Important ?},
doi          = {10.7874/kja.2014.18.1.1},
issn         = {2092-9862, 2093-3797},
number       = {1},
pages        = {1--7},
url          = {http://ejao.org/journal/view.php?doi=10.7874/kja.2014.18.1.1},
urldate      = {2019-03-28},
volume       = {18},
keywords     = {hearing,speech perception,temporal envelope,temporal fine structure},
langid       = {english},
publisher    = {The Korean Audiological Society},
year         = {2014},
}
@Report{2015BaharevExact,
author      = {Baharev, Ali and Schichl, Hermann and Neumaier, Arnold},
date        = {2015},
institution = {University of Vienna},
title       = {An exact method for the minimum feedback arc set problem},
type        = {resreport},
pages       = {34},
abstract    = {Given a directed graph G, a feedback arc set of G is a subset of its edges containing at least one edge of every cycle in G. Finding a feedback arc set of minimum cardinality is the minimum feedback arc set problem. The present paper focuses on large and sparse graphs. The minimum set cover formulation of the minimum feedback arc set problem is practical as long as all the simple cycles in G can be enumerated. Unfortunately, even sparse graphs can have Ω(2n) simple cycles, and such graphs appear in practice. An exact method is proposed that enumerates simple cycles in a lazy fashion, and extends an incomplete cycle matrix iteratively in the hope that only a tractable number of cycles has to be enumerated until a minimum feedback arc set is found. Numerical results are given on a test set containing large and sparse test graphs relevant for industrial applications.},
file        = {:done/2015BaharevExact An Exact Method for the Minimum Feedback Arc Set Problem.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
year        = {2015},
}
@Misc{2017RamachandranSearching,
author      = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
date        = {2017},
title       = {Searching for Activation Functions},
eprint      = {1710.05941},
eprintclass = {cs.NE},
eprinttype  = {arXiv},
url         = {https://arxiv.org/abs/1710.05941},
file        = {:done/2017RamachandranSearching Searching for Activation Functions.pdf:application/pdf},
groups      = {tesse:5},
pages       = {1--13},
}
@Book{2015RaschkaPython,
author     = {Raschka, Sebastian},
date       = {2016},
title      = {Python machine learning: unlock deeper insights into machine learning with this vital guide to cutting-edge predictive analytics},
isbn       = {978-1-78355-513-0},
location   = {Birmingham Mumbai},
note       = {OCLC: 927507196},
pagetotal  = {425},
publisher  = {Packt Publishing open source},
series     = {Community experience distilled},
url        = {https://www.ebook.de/de/product/25100042/sebastian///////\\_raschka///////\\_python///////\\_machine///////\\_learning.html},
ean        = {9781783555130},
langid     = {english},
shorttitle = {Python machine learning},
year       = {2015},
}
@Book{2016SalsaPartial,
author     = {Salsa, Sandro},
date       = {2016},
title      = {Partial differential equations in action: from modelling to theory},
edition    = {Third edition},
isbn       = {978-3-319-31237-8 978-3-319-31238-5},
location   = {Cham},
note       = {OCLC: 986521541},
number     = {volume 99},
pagetotal  = {686},
publisher  = {Springer},
series     = {UNITEXT - La Matematica per il 3+2},
url        = {https://www.ebook.de/de/product/27952550/sandro///////\\_salsa///////\\_partial///////\\_differential///////\\_equations///////\\_in///////\\_action.html},
ean        = {9783319312385},
file       = {:done/2016SalsaPartial Partial Differential Equations in Action/\\_ from Modelling to Theory.pdf:application/pdf},
groups     = {tesse:5},
langid     = {english},
shorttitle = {Partial differential equations in action},
year       = {2016},
}
@Book{2010LyonsUnderstanding,
author    = {Lyons, R G},
date      = {2011},
title     = {Understanding Digital Signal Processing, 3/E},
edition   = {3 edition},
isbn      = {0137027419},
location  = {Upper Saddle River, NJ},
pagetotal = {992},
publisher = {Traducao. [s.l.] Pearson Education India},
url       = {https://www.ebook.de/de/product/11024296/richard///////\\_g///////\\_lyons///////\\_understanding///////\\_digital///////\\_signal///////\\_processing.html},
ean       = {9780137027415},
year      = {2010},
}
@Thesis{2018PfalzGenerating,
author      = {Pfalz, Andrew},
date        = {2018-08},
institution = {Louisiana State University},
title       = {Generating Audio Using Recurrent Neural Networks},
type        = {phdthesis},
url         = {https://digitalcommons.lsu.edu/cgi/viewcontent.cgi?article=5621///////\\&context=gradschool///////\\_dissertations},
file        = {:done/2018PfalzGenerating Generating Audio Using Recurrent Neural Networks.pdf:application/pdf},
groups      = {tesse:5},
month       = {August},
school      = {Louisiana State University},
year        = {2018},
}
@InProceedings{1993DuynePhysical,
author       = {Duyne, Scott A. Van and Julius O. Smith, I. I. I.},
booktitle    = {Proceedings of the International Computer Music Conference},
date         = {1993},
title        = {Physical modeling with the 2-D digital waveguide mesh},
organization = {INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
pages        = {40--47},
publisher    = {INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
url          = {https://ccrma.stanford.edu/ jos/pdf/mesh.pdf},
edition      = {Proceeding},
file         = {:done/1993DuynePhysical Physical Modeling with the 2 D Digital Waveguide Mesh.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{1995DuyneTetrahedral,
author       = {Duyne, Scott A. Van and Julius O. Smith, I. I. I.},
booktitle    = {Proceedings of 1995 Workshop on Applications of Signal Processing to Audio and Accoustics},
date         = {1995},
title        = {The tetrahedral digital waveguide mesh},
doi          = {10.1109/ASPAA.1995.482998},
pages        = {234--237},
volume       = {1995},
file         = {:done/1995DuyneTetrahedral The Tetrahedral Digital Waveguide Mesh.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Applications of Signal Processing to Audio and Acoustics},
}
@PhdThesis{2001LairdPhysical,
author       = {Laird, Joel Augustus},
date         = {2001},
institution  = {University of Bristol},
title        = {The physical modelling of drums using digital waveguides},
type         = {phdthesis},
url          = {https://research-information.bristol.ac.uk/en/theses/the-physical-modelling-of-drums-using-digital-waveguides(ebd75b4b-bcdd-4cc7-b153-a6e0007682aa).html},
file         = {:done/2001LairdPhysical The Physical Modelling of Drums Using Digital Waveguides.pdf:application/pdf},
groups       = {tesse:5, Digital Waveguides},
journaltitle = {University of Bristol},
keywords     = {digital waveguides, physical modeling},
volume       = {2001},
}
@Misc{2014KopparapuOptimal,
author       = {Kopparapu, Sunil and Satish, M.},
date         = {2014-06-12},
title        = {Optimal Gaussian Filter for Effective Noise Filtering},
eprint       = {1406.3172},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1406.3172},
urldate      = {2019-03-26},
abstract     = {In this paper we show that the knowledge of noise statistics contaminating a signal can be effectively used to choose an optimal Gaussian ﬁlter to eliminate noise. Very speciﬁcally, we show that the additive white Gaussian noise (AWGN) contaminating a signal can be ﬁltered best by using a Gaussian ﬁlter of speciﬁc characteristics. The design of the Gaussian ﬁlter bears relationship with the noise statistics and also some basic information about the signal. We ﬁrst derive a relationship between the properties of the Gaussian ﬁlter, noise statistics and the signal and later show through experiments that this relationship can be used effectively to identify the optimal Gaussian ﬁlter that can effectively ﬁlter noise.},
arxivid      = {arXiv:1406.3172v1},
file         = {:done/2014KopparapuOptimal Optimal Gaussian Filter for Effective Noise Filtering.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {arXiv:1406.3172 [cs]},
langid       = {english},
number       = {2},
pages        = {1--7},
}
@Report{2016DozatIncorporating,
author      = {Dozat, Timothy},
date        = {2016},
institution = {Stanford University},
title       = {Incorporating Nesterov Momentum into Adam},
type        = {resreport},
url         = {http://cs229.stanford.edu/proj2015/054///////\\_report.pdf},
file        = {:done/2016DozatIncorporating Incorporating Nesterov Momentum into Adam.pdf:application/pdf},
groups      = {tesse:5},
year        = {2016},
}
@InProceedings{2016GatysImage,
author       = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
booktitle    = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
date         = {2016},
title        = {Image Style Transfer Using Convolutional Neural Networks},
doi          = {10.1109/CVPR.2016.265},
pages        = {2414--2423},
volume       = {2016},
file         = {:done/2016GatysImage Image Style Transfer Using Convolutional Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
keywords     = {convolutional neural networks, Feature extraction, arbitrary photograph, artistic style, high level image synthesis, image content, image processing, Image reconstruction, Image representation, image representations, image style transfer, neural algorithm, Neural networks, Neuroscience, object recognition, rendering, semantic content, semantic information, Semantics, Visualization},
}
@InProceedings{2016MollahosseiniGoing,
author       = {Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},
booktitle    = {Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
date         = {2016},
title        = {Going deeper in facial expression recognition using deep neural networks},
doi          = {10.1109/WACV.2016.7477450},
eprint       = {1511.04110},
eprinttype   = {arxiv},
organization = {IEEE},
pages        = {1--10},
url          = {http://arxiv.org/abs/1511.04110},
urldate      = {2019-03-28},
abstract     = {Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classiﬁer’s hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not signiﬁcant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Speciﬁcally, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classiﬁes them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.},
file         = {:done/2016MollahosseiniGoing Going Deeper in Facial Expression Recognition Using Deep Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {2016 IEEE Winter Conference on Applications of Computer Vision (WACV)},
langid       = {english},
}
@Report{2016StaudtDevelopment,
author      = {Staudt, Pascal},
date        = {2016},
institution = {Technische Universität Berlin},
title       = {Development of a Digital Musical Instrument with Embedded Sound Synthesis},
type        = {resreport},
url         = {https://www2.ak.tu-berlin.de//textasciitilde akgroup/ak///////\\_pub/abschlussarbeiten/2016/Staudt///////\\_Expose.pdf},
file        = {:done/2016StaudtDevelopment Development of a Digital Musical Instrument with Embedded Sound Synthesis.pdf:application/pdf},
groups      = {tesse:5},
publisher   = {Technische Universität Berlin},
year        = {2016},
}
@Report{2018HorowitzStrategic,
author      = {Horowitz, Michael C. and Allen, Gregory C. and Kania, Elsa B. and Scharre, Paul},
date        = {2018},
institution = {Center for a New American Security},
title       = {Strategic Competition in an Era of Artificial Intelligence},
type        = {techreport},
url         = {https://www.cnas.org/publications/reports/strategic-competition-in-an-era-of-artificial-intelligence},
file        = {:done/2018HorowitzStrategic Strategic Competition in an Era of Artificial Intelligence.pdf:application/pdf},
groups      = {tesse:5},
year        = {2018},
}
@Article{2016DumoulinGuide,
author       = {Dumoulin, Vincent and Visin, Francesco},
date         = {2016},
journaltitle = {CoRR},
title        = {A guide to convolution arithmetic for deep learning.},
url          = {http://dblp.uni-trier.de/db/journals/corr/corr1603.html///////\\#DumoulinV16},
volume       = {abs/1603.07285},
biburl       = {https://www.bibsonomy.org/bibtex/2f00abfcfc0627c65ea10b68ecef24cac/dblp},
ee           = {http://arxiv.org/abs/1603.07285},
file         = {:done/2016DumoulinGuide A Guide to Convolution Arithmetic for Deep Learning..pdf:application/pdf},
groups       = {tesse:5},
interhash    = {47f037084e2155cec24f09bf4897dcba},
intrahash    = {f00abfcfc0627c65ea10b68ecef24cac},
keywords     = {dblp},
location     = {mar},
publisher    = {ArXiv e-prints},
timestamp    = {2018-08-14T13:29:40.000+0200},
}
@InProceedings{2014SarroffMusical,
author       = {Sarroff, Andy M. and Casey, Michael},
booktitle    = {Joint 40th International Computer Music Conference (ICMC) and 11th Sound //\\\& Music Computing conference (SMC)},
date         = {2014},
title        = {Musical Audio Synthesis Using Autoencoding Neural Nets},
isbn         = {9789604661374},
number       = {September},
pages        = {14--20},
volume       = {1},
abstract     = {With an optimal network topology and tuning of hyperpa-rameters, artificial neural networks (ANNs) may be trained to learn a mapping from low level audio features to one or more higher-level representations. Such artificial neu-ral networks are commonly used in classification and re-gression settings to perform arbitrary tasks. In this work we suggest repurposing autoencoding neural networks as musical audio synthesizers. We offer an interactive musi-cal audio synthesis system that uses feedforward artificial neural networks for musical audio synthesis, rather than discriminative or regression tasks. In our system an ANN is trained on frames of low-level features. A high level representation of the musical audio is learned though an autoencoding neural net. Our real-time synthesis system allows one to interact directly with the parameters of the model and generate musical audio in real time. This work therefore proposes the exploitation of neural networks for creative musical applications.},
file         = {:done/2014SarroffMusical Musical Audio Synthesis Using Autoencoding Neural Nets.pdf:application/pdf},
groups       = {tesse:5},
institution  = {Dartmouth College},
journaltitle = {Proceedings of the International Computer Music Conference},
type         = {resreport},
}
@InProceedings{2012BockPolyphonic,
author        = {Bock, Sebastian and Schedl, Markus and Böck, Sebastian and Schedl, Markus},
booktitle     = {Proceedings of the 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date          = {2012},
title         = {Polyphonic Piano Note Transcription With Recurrent Neural Networks},
doi           = {10.1109/ICASSP.2012.6287832},
isbn          = {9781467300469},
pages         = {121--124},
publisher     = {AnaisIEEE},
file          = {:done/2012BockPolyphonic Polyphonic Piano Note Transcription with Recurrent Neural Networks.pdf:application/pdf},
groups        = {tesse:5},
institution   = {IEEE},
journaltitle  = {Network},
keywords      = {music transcription},
mendeley-tags = {music transcription},
year          = {2012},
}
@InProceedings{2014Boulanger-lewandowskiPhone,
author        = {Boulanger-lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong},
booktitle     = {Proceedings of the 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date          = {2014},
title         = {Phone Sequence Modeling with Recurrent Neural Networks},
doi           = {10.1109/ICASSP.2014.6854638},
isbn          = {9781479928934},
number        = {2},
pages         = {5454--5458},
file          = {:done/2014Boulanger-lewandowskiPhone Phone Sequence Modeling with Recurrent Neural Networks.pdf:application/pdf},
groups        = {tesse:5},
institution   = {IEEE},
journaltitle  = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
keywords      = {speech recognition},
mendeley-tags = {speech recognition},
}
@Report{2016HwangImage,
author       = {Hwang, Jeff and Zhou, You},
date         = {2016},
institution  = {Stanford University},
title        = {Image Colorization with Deep Convolutional Neural Networks},
type         = {resreport},
url          = {http://cs231n.stanford.edu/reports/2016/pdfs/219///////\\_Report.pdf},
abstract     = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
journaltitle = {Cs231N.Stanford.Edu},
keywords     = {image synthesis},
}
@Article{2015TheisGenerative,
author        = {Theis, Lucas and Bethge, Matthias},
date          = {2015},
journaltitle  = {Advances in Neural Information Processing Systems},
title         = {Generative Image Modeling Using Spatial LSTMs},
eprint        = {1506.03478},
eprinttype    = {arXiv},
issn          = {1049-5258},
note          = {event-place: Montreal, Canada},
pages         = {1927--1935},
series        = {NIPS'15},
url           = {http://dl.acm.org/citation.cfm?id=2969442.2969455},
abstract      = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
acmid         = {2969455},
address       = {Cambridge, MA, USA},
arxivid       = {1506.03478},
booktitle     = {Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 2},
keywords      = {image synthesis},
location      = {Cambridge, MA, USA},
mendeley-tags = {image synthesis},
numpages      = {9},
publisher     = {MIT Press},
}
@InCollection{2016LarssonLearning,
author    = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
booktitle = {Computer Vision (ECCV 2016)},
date      = {2016},
title     = {Learning Representations for Automatic Colorization},
doi       = {10.1007/978-3-319-46493-0///////\\_35},
edition   = {European C},
location  = {Anais},
pages     = {577--593},
publisher = {Springer International Publishing},
}
@Article{2016VeitResidual,
author        = {Veit, Andreas and Wilber, Michael J. and Belongie, Serge},
date          = {2016},
journaltitle  = {Advances in Neural Information Processing Systems},
title         = {Residual Networks Behave Like Ensembles of Relatively Shallow Networks},
eprint        = {1605.06431},
eprinttype    = {arXiv},
issn          = {1049-5258},
note          = {event-place: Barcelona, Spain},
pages         = {1--9},
series        = {NIPS'16},
url           = {http://arxiv.org/abs/1605.06431},
abstract      = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
arxivid       = {1605.06431},
booktitle     = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
file          = {:done/2016VeitResidual Residual Networks Behave like Ensembles of Relatively Shallow Networks.pdf:application/pdf},
groups        = {tesse:5},
isbn          = {978-1-5108-3881-9},
keywords      = {theory},
location      = {USA},
mendeley-tags = {theory},
publisher     = {Curran Associates Inc.},
}
@Article{2016ZhuGenerative,
author        = {Zhu, Jun-Yan and Krähenbühl, Philipp and Shechtman, Eli and Efros, Alexei A.},
date          = {2016},
journaltitle  = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
title         = {Generative Visual Manipulation on the Natural Image Manifold},
doi           = {10.1007/978-3-319-46454-1///////\\_36},
eprint        = {1609.03552},
eprinttype    = {arXiv},
issn          = {1611-3349},
pages         = {597--613},
volume        = {9909 LNCS},
abstract      = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
arxivid       = {1609.03552},
booktitle     = {Proceedings of European Conference on Computer Vision (ECCV)},
edition       = {European C},
institution   = {Springer},
isbn          = {9783319464534},
keywords      = {image synthesis},
location      = {Anais},
mendeley-tags = {image synthesis},
pmid          = {4520227},
publisher     = {Springer},
}
@Article{2017EricksonToolkits,
author       = {Erickson, Bradley J. and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy and Philbrick, Kenneth},
date         = {2017-03},
journaltitle = {Journal of Digital Imaging},
title        = {Toolkits and Libraries for Deep Learning},
doi          = {10.1007/s10278-017-9965-6},
number       = {4},
pages        = {400--405},
volume       = {30},
file         = {:done/2017EricksonToolkits Toolkits and Libraries for Deep Learning.pdf:application/pdf},
groups       = {tesse:5},
}
@Book{2016BovermannMusical,
author    = {Bovermann, T. and Others},
date      = {2016},
title     = {Musical Instruments in the 21st Century},
isbn      = {978-981-10-2950-9},
location  = {Traducao. [s.l.]},
publisher = {Springer},
url       = {https://www.ebook.de/de/product/26880613/musical///////\\_instrument///////\\_in///////\\_the///////\\_21st///////\\_century.html},
ean       = {9789811029509},
file      = {:done/2016BovermannMusical Musical Instruments in the 21st Century.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
year      = {2016},
}
@InProceedings{2012HooverGenerating,
author        = {Hoover, Amy K. and Szerlip, Paul A. and Norton, Marie E. and Brindle, Trevor A. and Merritt, Zachary and Stanley, Kenneth O.},
booktitle     = {ICCC},
date          = {2012},
title         = {Generating a Complete Multipart Musical Composition from a Single Monophonic Melody with Functional Scaffolding.},
isbn          = {9781905254668},
pages         = {111--118},
publisher     = {Citeseer},
file          = {:done/2012HooverGenerating Generating a Complete Multipart Musical Composition from a Single Monophonic Melody with Functional Scaffolding.pdf:application/pdf},
groups        = {tesse:5},
journaltitle  = {International Conference on Computational Creativity},
keywords      = {music generation},
mendeley-tags = {music generation},
}
@InProceedings{2016OordPixel,
author        = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
booktitle     = {Proceedings of The 33rd International Conference on Machine Learning},
date          = {2016},
title         = {Pixel Recurrent Neural Networks},
editor        = {Balcan, Maria Florina and Weinberger, Kilian Q.},
eprint        = {1601.06759},
eprinttype    = {arXiv},
isbn          = {9781510829008},
location      = {New York, New York, USA},
pages         = {1747--1756},
publisher     = {PMLR},
series        = {Proceedings of Machine Learning Research},
url           = {http://proceedings.mlr.press/v48/oord16.html},
volume        = {48},
abstract      = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
arxivid       = {1601.06759},
journaltitle  = {arXiv preprint arXiv:1601.06759},
keywords      = {image synthesis},
mendeley-tags = {image synthesis},
}
@InProceedings{2016ChoiConvolutional,
author        = {Choi, Keunwoo and Fazekas, George György and Sandler, Mark and Cho, Kyunghyun},
booktitle     = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
date          = {2016},
title         = {Convolutional Recurrent Neural Networks for Music Classification},
doi           = {10.1.1.302.7795},
eprint        = {1609.04243},
eprinttype    = {arXiv},
isbn          = {9789881701282},
organization  = {IEEE},
pages         = {1--5},
url           = {http://arxiv.org/abs/1609.04243},
abstract      = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
arxivid       = {1609.04243},
file          = {:done/2016ChoiConvolutional Convolutional Recurrent Neural Networks for Music Classification.pdf:application/pdf},
groups        = {tesse:5},
institution   = {IEEE},
issn          = {1520-9210},
journaltitle  = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
keywords      = {music classification},
mendeley-tags = {music classification},
}
@InProceedings{2002UnciniSound,
author      = {Uncini, Aurelio},
booktitle   = {Italian Workshop on Neural Nets},
date        = {2002},
title       = {Sound synthesis by flexible activation function recurrent neural networks},
pages       = {168--177},
publisher   = {Springer},
file        = {:done/2002UnciniSound Sound Synthesis by Flexible Activation Function Recurrent Neural Networks.pdf:application/pdf},
groups      = {tesse:5},
institution = {University of Rome "La Sapienza"},
type        = {resreport},
}
@Article{2016IizukaLet,
author        = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
date          = {2016},
journaltitle  = {ACM Transactions on Graphics},
title         = {Let there be color! :Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification},
doi           = {10.1145/2897824.2925974},
issn          = {0730-0301},
number        = {4},
pages         = {1--11},
url           = {http://dl.acm.org/citation.cfm?doid=2897824.2925974},
volume        = {35},
abstract      = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
file          = {:done/2016IizukaLet Let There Be Color! /\\_Joint End to End Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification.pdf:application/pdf;:todo/done/2016IizukaLet Let There Be Color! /\\_Joint End to End Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification.pdf:application/pdf},
groups        = {tesse:5},
isbn          = {9781450342797},
keywords      = {colorization,computing methodologies,convolutional neural network concepts,image processing,image synthesis,neural net-},
mendeley-tags = {image synthesis},
}
@Article{2019StollCarbon,
author    = {Stoll, Christian and Klaa{/ss}en, Lena and Gallersdörfer, Ulrich},
title     = {The Carbon Footprint of Bitcoin},
doi       = {10.1016/j.joule.2019.05.012},
groups    = {Neural Networks and Sustainability},
journal   = {Joule},
month     = {jun},
publisher = {Elsevier {BV}},
year      = {2019},
}
@Article{2000MaoProbabilistic,
author        = {Mao, K. Z. and Tan, K. C. and Ser, W.},
date          = {2000},
journaltitle  = {IEEE Transactions on Neural Networks},
title         = {Probabilistic neural network structure determination for pattern classification},
doi           = {10.1109/72.857781},
issn          = {1045-9227},
number        = {4},
pages         = {1009--1016},
url           = {http://ieeexplore.ieee.org/document/857781/},
urldate       = {2019-04-05},
volume        = {11},
abstract      = {Network structure determination is an important issue in pattern classification based on a probabilistic neural network. In this study, a supervised network structure determination algorithm is proposed. The proposed algorithm consists of two parts and runs in an iterative way. The first part identifies an appropriate smoothing parameter using a genetic algorithm, while the second part determines suitable pattern layer neurons using a forward regression orthogonal algorithm. The proposed algorithm is capable of offering a fairly small network structure with satisfactory classification accuracy.},
file          = {:done/2000MaoProbabilistic Probabilistic Neural Network Structure Determination for Pattern Classification.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {evolutive,seminal},
langid        = {english},
mendeley-tags = {evolutive,seminal},
publisher     = {IEEE},
}
@Article{2016ZweigAdvances,
author       = {Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
date         = {2017},
journaltitle = {CoRR},
title        = {Advances in all-neural speech recognition},
eprint       = {1609.05935},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1609.05935},
volume       = {abs/1609.05935},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/ZweigYDS16},
edition      = {Acoustics,},
journal      = {CoRR},
publisher    = {AnaisIEEE},
timestamp    = {Mon, 13 Aug 2018 16:47:53 +0200},
year         = {2016},
}
@InProceedings{2017YangTensor,
author        = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
booktitle     = {Proceedings of the International Conference on Machine Learning},
date          = {2017},
title         = {Tensor-Train Recurrent Neural Networks for Video Classification},
eprint        = {1707.01786},
eprinttype    = {arXiv},
url           = {https://arxiv.org/pdf/1707.01786.pdf},
abstract      = {The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
arxivid       = {1707.01786},
file          = {:done/2017YangTensor Tensor Train Recurrent Neural Networks for Video Classification.pdf:application/pdf},
groups        = {tesse:5},
journaltitle  = {arXiv preprint arXiv:1707.01786},
keywords      = {video classification},
mendeley-tags = {video classification},
}
@Article{2017SangkloyScribbler,
author       = {Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
date         = {2017-07},
journaltitle = {Computer Vision and Pattern Recognition (CVPR)},
title        = {Scribbler: Controlling Deep Image Synthesis with Sketch and Color},
doi          = {10.1109/CVPR.2017.723},
eprint       = {1612.00835},
eprinttype   = {arXiv},
pages        = {6836--6845},
url          = {http://ieeexplore.ieee.org/document/8100206/},
urldate      = {2019-04-05},
abstract     = {Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.},
arxivid      = {1612.00835},
booktitle    = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
eventtitle   = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
file         = {:done/2017SangkloyScribbler Scribbler\\_ Controlling Deep Image Synthesis with Sketch and Color.pdf:application/pdf},
groups       = {tesse:5},
isbn         = {978-1-5386-0457-1},
keywords     = {image synthesis,state of the art},
langid       = {english},
location     = {Honolulu, HI},
publisher    = {IEEE},
shorttitle   = {Scribbler},
}
@InProceedings{1992BoashashEstimating,
author    = {Boashash, Boualem},
booktitle = {PROCEEDINGS OF THE IEEE},
date      = {1992},
title     = {Estimating and Interpreting the Instantaneous Frequency of a Signal-Part 1: Fundamentals},
pages     = {19},
volume    = {80},
file      = {:done/1992BoashashEstimating Estimating and Interpreting the Instantaneous Frequency of a Signal Part 1/\\_ Fundamentals.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
}
@Article{2018VanDenOordWavenet,
author       = {Van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew W. and Kavukcuoglu, Koray},
date         = {2018},
journaltitle = {CoRR},
title        = {WaveNet: A Generative Model for Raw Audio},
eprint       = {1609.03499},
eprinttype   = {arXiv},
url          = {https://deepmind.com/blog/wavenet-generative-model-raw-audio/},
urldate      = {2018-06-30},
volume       = {abs/1609.03499},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/OordDZSVGKSK16},
file         = {:done/2016OordWaveneta WaveNet/\_ a Generative Model for Raw Audio.pdf:application/pdf;:todo/done/2016OordWaveneta WaveNet/\_ a Generative Model for Raw Audio.pdf:application/pdf},
groups       = {tesse:5},
howpublished = {online},
location     = {DeepMind Disponvel em},
publisher    = {<},
timestamp    = {Mon, 13 Aug 2018 16:49:15 +0200},
}
@Article{MizutaniDerivation,
author       = {Mizutani, E. and Dreyfus, S. E. and Nishio, K.},
date         = {2000-07},
journaltitle = {Neural Networks},
title        = {On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application},
doi          = {10.1109/IJCNN.2000.857892},
pages        = {167--172 vol.2},
volume       = {2},
booktitle    = {Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks (IJCNN 2000). Neural Computing: New Challenges and Perspectives for the New Millennium},
keywords     = {Neurons, Backpropagation, BP derivative computation process, Cost function, Discrete time systems, discrete-time optimal control theory, dynamic programming, Dynamic programming, Gradient methods, Industrial training, Kelley-Bryson optimal-control gradient formula, Laboratories, MLP backpropagation, multilayer perceptron learning, Multilayer perceptrons, Nonhomogeneous media, Optimal control, Optimized production technology, Poles and towers, Training data},
}
@InProceedings{2016GregorTowards,
author        = {Gregor, Karol and Besse, Frederic and Jimenez Rezende, Danilo and Danihelka, Ivo and Wierstra, Daan},
booktitle     = {Advances in Neural Information Processing Systems 29},
date          = {2016},
title         = {Towards Conceptual Compression},
editor        = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
eprint        = {1604.08772},
eprinttype    = {arXiv},
number        = {Nips},
pages         = {3549--3557},
publisher     = {Curran Associates, Inc.},
url           = {http://papers.nips.cc/paper/6542-towards-conceptual-compression.pdf},
urldate       = {2019-04-18},
abstract      = {We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
arxivid       = {1604.08772},
file          = {:done/2016GregorTowards Towards Conceptual Compression.pdf:application/pdf},
groups        = {tesse:5},
issn          = {1049-5258},
journaltitle  = {Advances In Neural Information Processing Systems},
keywords      = {image compression - conceptual,state of the art},
mendeley-tags = {image compression - conceptual,state of the art},
}
@InProceedings{2003LeungTuning,
author        = {Leung, Frank Hung-Fat F and Lam, Hak-Keung K and Ling, Sai-Ho H and Tam, Peter Kwong-Shun S},
booktitle     = {IECON'01. 27th Annual Conference of the IEEE Industrial Electronics Society (Cat. No.37243)},
date          = {2003},
title         = {Tuning of the structure and parameters of a neural network using an improved genetic algorithm.},
doi           = {10.1109/TNN.2002.804317},
eventtitle    = {IECON'01. 27th Annual Conference of the IEEE Industrial Electronics Society},
isbn          = {1045-9227 (Print)$/backslash$r1045-9227 (Linking)},
location      = {Denver, CO, USA},
number        = {1},
pages         = {79--88},
publisher     = {IEEE},
url           = {http://ieeexplore.ieee.org/document/976448/},
urldate       = {2019-04-05},
volume        = {14},
abstract      = {This paper presents the tuning of the structure and parameters of a neural network using an improved genetic algorithm (GA). It is also shown that the improved GA performs better than the standard GA based on some benchmark test functions. A neural network with switches introduced to its links is proposed. By doing this, the proposed neural network can learn both the input-output relationships of an application and the network structure using the improved GA. The number of hidden nodes is chosen manually by increasing it from a small number until the learning performance in terms of fitness value is good enough. Application examples on sunspot forecasting and associative memory are given to show the merits of the improved GA and the proposed neural network.},
issn          = {1045-9227},
journaltitle  = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords      = {evolutive},
langid        = {english},
mendeley-tags = {evolutive},
pmid          = {18237992},
}
@Article{2008AagerfalkOutsourcing,
author       = {Ågerfalk, Pär J. and Fitzgerald, Brian and Agerfalk, Par J and Fitzgerald, Brian},
date         = {2008},
journaltitle = {MIS Quarterly},
title        = {Outsourcing to an Unknown Workforce: Exploring Opensourcing as a Global Sourcing Strategy},
doi          = {Article},
issn         = {0276-7783},
number       = {2},
pages        = {385--409},
volume       = {32},
abstract     = {This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy-opensourcing, as we term it here-whereby commerical companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product. We followed this with a large-scale survey involving additional exemplars of the phenomenon. The study identifies a number of symmetrical and complementary customer and community obligations that are associated with opensourcing success. We also identify a number of tension points on which customer and community perceptions tend to vary. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness: The customer and community need to establish a trusted partnership of shared responsibility in building an overall opensourcing ecosystem. The study reveals an ongoing shift from OSS as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises. It also reveals that opensourcing provides ample opportunity for companies to headhunt top developers, hence moving from outsourcing to a largely unknown OSS workforce toward recruitment of developers from a global open source community whose talents have become known as a result of the opensourcing experience.},
booktitle    = {MIS Quarterly},
isbn         = {02767783},
keywords     = {crowdsourcing,global software development,multi-method research,offshoreing,open source,opensourcing,outsourcing},
pmid         = {31831011},
shorttitle   = {OUTSOURCING TO AN UNKNOWN WORKFORCE},
}
@Article{2017GabrielliIntroducing,
author   = {Gabrielli, Leonardo and Tomassetti, Stefano and Squartini, Stefano and Zinato, Carlo},
date     = {2017},
title    = {Introducing Deep Machine Learning for Parameter Estimation in Physical Modelling},
pages    = {6},
abstract = {One of the most challenging tasks in physically-informed sound synthesis is the estimation of model parameters to produce a desired timbre. Automatic parameter estimation procedures have been developed in the past for some speciﬁc parameters or application scenarios but, up to now, no approach has been proved applicable to a wide variety of use cases. A general solution to parameters estimation problem is provided along this paper which is based on a supervised convolutional machine learning paradigm. The described approach can be classiﬁed as "end-to-end" and requires, thus, no speciﬁc knowledge of the model itself. Furthermore, parameters are learned from data generated by the model, requiring no effort in the preparation and labeling of the training dataset. To provide a qualitative and quantitative analysis of the performance, this method is applied to a patented digital waveguide pipe organ model, yielding very promising results.},
file     = {:done/2017GabrielliIntroducing Introducing Deep Machine Learning for Parameter Estimation in Physical Modelling.pdf:application/pdf},
groups   = {tesse:2},
langid   = {english},
}
@Article{2014LillicrapRandom,
author       = {Lillicrap, Timothy P. and Cownden, Daniel and Tweed, Douglas B. and Akerman, Colin J.},
date         = {2014-11-02},
journaltitle = {arXiv:1411.0247 [cs, q-bio]},
title        = {Random feedback weights support learning in deep neural networks},
eprint       = {1411.0247},
eprinttype   = {arxiv},
pages        = {1--27},
url          = {http://arxiv.org/abs/1411.0247},
urldate      = {2019-03-26},
abstract     = {The brain processes information through many layers of neurons. This deep architecture is representationally powerful1,2,3,4, but it complicates learning by making it hard to identify the responsible neurons when a mistake is made1,5. In machine learning, the backpropagation algorithm1 assigns blame to a neuron by computing exactly how it contributed to an error. To do this, it multiplies error signals by matrices consisting of all the synaptic weights on the neuron’s axon and farther downstream. This operation requires a precisely choreographed transport of synaptic weight information, which is thought to be impossible in the brain 1,6,7,8,9,10,11,12,13,14. Here we present a surprisingly simple algorithm for deep learning, which assigns blame by multiplying error signals by random synaptic weights. We show that a network can learn to extract useful information from signals sent through these random feedback connections. In essence, the network learns to learn. We demonstrate that this new mechanism performs as quickly and accurately as backpropagation on a variety of problems and describe the principles which underlie its function. Our demonstration provides a plausible basis for how a neuron can be adapted using error signals generated at distal locations in the brain, and thus dispels long-held assumptions about the algorithmic constraints on learning in neural circuits.},
arxivid      = {arXiv:1411.0247v1},
file         = {:done/2014LillicrapRandom Random Feedback Weights Support Learning in Deep Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@InProceedings{2017BlaauwNeural,
author       = {Blaauw, Merlijn and Bonada, Jordi},
booktitle    = {INTERSPEECH},
date         = {2017-04-12},
title        = {A Neural Parametric Singing Synthesizer},
eprint       = {1704.03809},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1704.03809},
urldate      = {2019-03-28},
abstract     = {We present a new model for singing synthesis based on a modiﬁed version of the WaveNet architecture. Instead of modeling raw waveform, we model features produced by a parametric vocoder that separates the inﬂuence of pitch and timbre. This allows conveniently modifying pitch to match any target melody, facilitates training on more modest dataset sizes, and signiﬁcantly reduces training and generation times. Our model makes frame-wise predictions using mixture density outputs rather than categorical outputs in order to reduce the required parameter count. As we found overﬁtting to be an issue with the relatively small datasets used in our experiments, we propose a method to regularize the model and make the autoregressive generation process more robust to prediction errors. Using a simple multi-stream architecture, harmonic, aperiodic and voiced/unvoiced components can all be predicted in a coherent manner. We compare our method to existing parametric statistical and state-of-the-art concatenative methods using quantitative metrics and a listening test. While naive implementations of the autoregressive generation algorithm tend to be inefﬁcient, using a smart algorithm we can greatly speed up the process and obtain a system that’s competitive in both speed and quality.},
file         = {:done/2017BlaauwNeural A Neural Parametric Singing Synthesizer.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {arXiv:1704.03809 [cs]},
langid       = {english},
}
@Article{2017EstevaDermatologist,
author       = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
date         = {2017-01},
journaltitle = {Nature},
title        = {Dermatologist-level classification of skin cancer with deep neural networks},
doi          = {10.1038/nature21056},
number       = {7639},
pages        = {115--118},
volume       = {542},
file         = {:done/2017EstevaDermatologist Dermatologist Level Classification of Skin Cancer with Deep Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{2017ZhangVery,
author        = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
booktitle     = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
date          = {2017},
title         = {Very Deep Convolutional Networks for End-to-End Speech Recognition},
doi           = {10.1109/ICASSP.2017.7953077},
eprint        = {1610.03022},
eprinttype    = {arXiv},
isbn          = {9781509041176},
organization  = {IEEE},
pages         = {4845--4849},
url           = {http://arxiv.org/abs/1610.03022},
abstract      = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5///////\% word error rate without any dictionary or language using a 15 layer deep network.},
arxivid       = {1610.03022},
file          = {:done/2017ZhangVery Very Deep Convolutional Networks for End to End Speech Recognition.pdf:application/pdf},
groups        = {tesse:5},
institution   = {IEEE},
journaltitle  = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
keywords      = {Hidden Markov models, Training, Acoustics, speech recognition, word error rate, recurrent neural nets, Computational modeling, convolution, Biological neural networks, Automatic Speech Recognition, batch normalization, Brain modeling, convolutional LSTM, end-to-end ASR models, end-to-end speech recognition, End-to-End Speech Recognition, feature space, Logic gates, network-in-network principles, residual connections, sequence-to-sequence models, shallow acoustic encoder networks, spectral structure, very deep convolutional networks, Very Deep Convolutional Neural Networks, very deep recurrent structures},
mendeley-tags = {speech recognition,state of the art},
}
@Article{2000EdmundsProblem,
author       = {Edmunds, Angela and Morris, Anne},
date         = {2000},
journaltitle = {International Journal of Information Management},
title        = {The problem of information overload in business organisations: a review of the literature},
doi          = {10.1016/S0268-4012(99)00051-1},
issn         = {0268-4012},
number       = {1},
pages        = {17--28},
url          = {http://www.sciencedirect.com/science/article/pii/S0268401299000511},
volume       = {20},
abstract     = {This paper reviews the literature on the problem of information overload, with particular reference to business organisations. The literature reveals that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognised and experienced. Both perceptions and the actual effects of information overload have been exacerbated by the rapid advances made in information and communication technology, although it is not clear cut as to whether the Internet has worsened or improved the situation. A theme stressed in the literature is the paradoxical situation that, although there is an abundance of information available, it is often difficult to obtain useful, relevant information when it is needed. Some solutions put forward to reduce information overload are: a reduction in the duplication of information found in the professional literature; the adoption of personal information management strategies, together with the integration of software solutions such as push technology and intelligent agents; and the provision of value-added information (filtered by software or information specialists). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
booktitle    = {International Journal of Information Management},
isbn         = {0268-4012},
keywords     = {infoglut,information fatigue syndrome,information overload},
pmid         = {10272},
}
@InProceedings{2016BellOutsidea,
author     = {Bell, Sean and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross},
booktitle  = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
date       = {2016-06},
title      = {Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks},
doi        = {10.1109/CVPR.2016.314},
eventtitle = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
isbn       = {978-1-4673-8851-1},
location   = {Las Vegas, NV, USA},
pages      = {2874--2883},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/7780683/},
urldate    = {2019-03-28},
abstract   = {It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9///////\\% to 77.9///////\\% mAP. On the new and more challenging MS COCO dataset, we improve state-of-the-art from 19.7///////\\% to 33.1///////\\% mAP. In the 2015 MS COCO Detection Challenge, our ION model won "Best Student Entry" and ﬁnished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
langid     = {english},
shorttitle = {Inside-Outside Net},
}
@Report{1960WidrowAdaptive,
author       = {Widrow, Bernard and Hoff, Marcian E.},
date         = {1960},
institution  = {STANFORD UNIV CA STANFORD ELECTRONICS LABS},
title        = {Adaptive Switching Circuits},
type         = {techreport},
location     = {New York},
url          = {http://www.dtic.mil/dtic/tr/fulltext/u2/241531.pdf},
booktitle    = {Technical Report No. 1553-1},
comment      = {Reprinted in anderson:neurocomputing},
file         = {:done/1960WidrowAdaptive Adaptive Switching Circuits.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {[s.l.] STANFORD UNIV CA STANFORD ELECTRONICS LABS},
volume       = {1960},
}
@Article{2018TomarAmplitude,
author       = {Tomar, Shikha and Sumathi, Parasuraman},
date         = {2018-01},
journaltitle = {IEEE Transactions on Instrumentation and Measurement},
title        = {Amplitude and Frequency Estimation of Exponentially Decaying Sinusoids},
doi          = {10.1109/TIM.2017.2755998},
issn         = {0018-9456, 1557-9662},
number       = {1},
pages        = {229--237},
url          = {http://ieeexplore.ieee.org/document/8063390/},
urldate      = {2019-03-26},
volume       = {67},
abstract     = {An online method for amplitude and frequency estimation of exponentially decaying sinusoids is proposed with a moving-window discrete Fourier transform (MWDFT) ﬁlter and frequency-locked loop. The tuned ﬁlter characteristics of MWDFT is modiﬁed into more ﬂat characteristic around the center frequency with negative feedback, which increases the bandwidth of the ﬁlter. An adaptive sampling pulse adjustment mechanism is incorporated in the proposed structure for online estimation of frequency. Hence, the frequency error was exploited to achieve synchronization between in-phase component of MWDFT and input signal of estimation. The amplitude is estimated in online from the in-phase and quadrature-phase components of MWDFT. The performance of the proposed method is compared with the existing techniques and experimentally validated on single-link ﬂexible manipulator system for the online estimation of frequency and amplitude of tip deﬂection signal. The experimental investigation prove that the proposed online technique performs well over the existing techniques.},
file         = {:done/2018TomarAmplitude Amplitude and Frequency Estimation of Exponentially Decaying Sinusoids.pdf:application/pdf},
groups       = {tesse:5, Acoustics},
langid       = {english},
}
@Electronic{2017Neural,
author       = {Veen, F Van.},
date         = {2017},
title        = {The Neural Network Zoo},
url          = {http://www.asimovinstitute.org/neural-network-zoo/},
urldate      = {2018-06-30},
howpublished = {online},
publisher    = {<},
}
@Misc{MoscaritoloPebble,
author = {Moscaritolo, Autores and Angelamoscaritolopcmagcom, Angela},
title  = {Pebble Smartwatch Sells Out , Collects $ 10 Million on Kickstarter},
}
@Report{parker_learning-logic:_1985,
author      = {Parker, D.B.},
date        = {1985},
institution = {Massachusetts Institute of Technology, Center for Computational Research in Economics and Management Science},
title       = {Learning-logic: Casting the Cortex of the Human Brain in Silicon},
url         = {https://books.google.com.br/books?id=2kS9GwAACAAJ},
}
@Article{2015SmithCyclical,
author       = {Smith, Leslie N.},
date         = {2015-06-03},
journaltitle = {arXiv:1506.01186 [cs]},
title        = {Cyclical Learning Rates for Training Neural Networks},
eprint       = {1506.01186},
eprinttype   = {arxiv},
number       = {April},
url          = {http://arxiv.org/abs/1506.01186},
urldate      = {2019-03-26},
abstract     = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally ﬁnd the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of ﬁxed values achieves improved classiﬁcation accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate "reasonable bounds" – linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
arxivid      = {arXiv:1506.01186v6},
file         = {:done/2015SmithCyclical Cyclical Learning Rates for Training Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{klaus_comparison_nodate,
author   = {Klaus, Leonard},
date     = {2015},
title    = {Comparison of hilbert transform and sine fit approaches for the determination of damping parameters},
number   = {4},
pages    = {4},
volume   = {1},
abstract = {For the analysis of rotational damping measurements in the time domain, two different identiﬁcation procedures are compared. The ﬁrst procedure investigated incorporates a Hilbert transform of the data, which enables an analysis by a linear regression calculation. The second approach is a direct nonlinear regression calculation of a damped sine function. The two approaches are compared using both simulated data and measurement data. The results of the comparison are presented.},
langid   = {english},
}
@InProceedings{2018RobertsLearninga,
author    = {Roberts, Adam and Engel, Jesse and Oore, Sageev and Eck, Douglas},
booktitle = {Proceedings of the 2018 ACM Workshop on Intelligent Music Interfaces for Listening and Creation, MILC@IUI 2018},
date      = {2018},
title     = {Learning Latent Representations of Music to Generate Interactive Musical Palettes},
url       = {http://ceur-ws.org/Vol-2068/milc7.pdf},
}
@InProceedings{2013SutskeverImportance,
author       = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
booktitle    = {Proceedings of the 30th International Conference on Machine Learning},
date         = {2013-06-17},
title        = {On the importance of initialization and momentum in deep learning},
editor       = {Dasgupta, Sanjoy and McAllester, David},
location     = {Atlanta, Georgia, USA},
number       = {3},
pages        = {1139--1147},
publisher    = {PMLR},
series       = {Proceedings of Machine Learning Research},
url          = {http://proceedings.mlr.press/v28/sutskever13.html},
volume       = {2013},
abstract     = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned. Our success training these models suggests that previous attempts to train deep and recurrent neural networks from random initializations have likely failed due to poor initialization schemes. Furthermore, carefully tuned momentum methods suffice for dealing with the curvature issues in deep and recurrent network training objectives without the need for sophisticated second-order methods.},
file         = {:done/2013SutskeverImportance On the Importance of Initialization and Momentum in Deep Learning.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {International conference on machine learning},
}
@Article{2016BahrampourComparative,
author       = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
date         = {2016},
journaltitle = {CoRR},
title        = {Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning},
eprint       = {1511.06435},
eprinttype   = {arXiv},
url          = {http://arxiv.org/abs/1511.06435},
volume       = {abs/1511.06435},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/BahrampourRSS15},
file         = {:done/2016BahrampourComparative Comparative Study of Caffe, Neon, Theano, and Torch for Deep Learning.pdf:application/pdf},
groups       = {tesse:5},
location     = {neon, theano, and torch for deep learning},
publisher    = {Comparative study of caffe},
timestamp    = {Mon, 13 Aug 2018 16:47:26 +0200},
}
@InProceedings{2007SerraState,
author       = {Serra, Xavier},
booktitle    = {Proceedings of the IEEE 9th Workshop on Multimedia Signal Processing},
date         = {2007-10},
title        = {State of the Art and Future Directions in Musical Sound Synthesis},
doi          = {10.1109/MMSP.2007.4412805},
pages        = {2--5},
volume       = {2007},
file         = {:done/2007SerraState State of the Art and Future Directions in Musical Sound Synthesis.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Multimedia Signal Processing},
keywords     = {Instruments, music, acoustic signal processing, corpus-based concatenative methods, Humans, Multiple signal classification, Music, musical sound synthesis, perceptual characteristics, physical models, Physics computing, Psychoacoustic models, Psychology, Signal processing algorithms, signal synthesis, Signal synthesis, Sliding mode control, sound and music computing, sound source, sound synthesis},
}
@Report{2008BerdahlPlucked,
author      = {Berdahl, Edgar J. and Julius O. Smith, I. I. I.},
date        = {2008},
institution = {Stanford University},
title       = {Plucked String Digital Waveguide Model},
type        = {resreport},
pages       = {14},
file        = {:done/2008BerdahlPlucked Plucked String Digital Waveguide Model.pdf:application/pdf},
groups      = {tesse:5},
langid      = {english},
}
@InProceedings{2016KhorramiHow,
author        = {Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
booktitle     = {Proceedings of the 2016 IEEE International Conference on Image Processing (ICIP)},
date          = {2016-09},
title         = {How Deep Neural Networks Can Improve Emotion Recognition on Video Data},
doi           = {10.1109/ICIP.2016.7532431},
eprint        = {1602.07377},
eprinttype    = {arXiv},
pages         = {619--623},
publisher     = {AnaisIEEE},
url           = {http://arxiv.org/abs/1602.07377},
abstract      = {We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
arxivid       = {1602.07377},
edition       = {(ICIP), 20},
file          = {:done/2016KhorramiHow How Deep Neural Networks Can Improve Emotion Recognition on Video Data.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {Recurrent neural networks, recurrent neural networks, Convolution, convolutional neural networks, Feature extraction, deep neural networks, Audio/Visual+Emotion Challenge, AV+EC2015, Convolutional Neural Networks, deep learning, Deep Learning, emotion recognition, Emotion recognition, Emotion Recognition, Face, Predictive models, Recurrent Neural Networks, temporal neural network models, Video Processing},
mendeley-tags = {sentiment analysis,video classification},
}
@Article{2016KalchbrennerNeural,
author       = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen and Oord, Aaron van den and Graves, Alex and Kavukcuoglu, Koray},
date         = {2016-10-31},
journaltitle = {arXiv:1610.10099 [cs]},
title        = {Neural Machine Translation in Linear Time},
eprint       = {1610.10099},
eprinttype   = {arxiv},
url          = {http://arxiv.org/abs/1610.10099},
urldate      = {2019-03-26},
abstract     = {We present a novel neural network for processing sequences. The ByteNet is a one-dimensional convolutional neural network that is composed of two parts, one to encode the source sequence and the other to decode the target sequence. The two network parts are connected by stacking the decoder on top of the encoder and preserving the temporal resolution of the sequences. To address the differing lengths of the source and the target, we introduce an efﬁcient mechanism by which the decoder is dynamically unfolded over the representation of the encoder. The ByteNet uses dilation in the convolutional layers to increase its receptive ﬁeld. The resulting network has two core properties: it runs in time that is linear in the length of the sequences and it sidesteps the need for excessive memorization. The ByteNet decoder attains state-of-the-art performance on character-level language modelling and outperforms the previous best results obtained with recurrent networks. The ByteNet also achieves state-of-the-art performance on character-to-character machine translation on the English-to-German WMT translation task, surpassing comparable neural translation models that are based on recurrent networks with attentional pooling and run in quadratic time. We ﬁnd that the latent alignment structure contained in the representations reﬂects the expected alignment between the tokens.},
arxivid      = {arXiv:1610.10099v2},
file         = {:done/2016KalchbrennerNeural Neural Machine Translation in Linear Time.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1992SmithPhysical,
author       = {Smith, Julius O.},
date         = {1992-12},
journaltitle = {Computer Music Journal},
title        = {Physical Modeling Using Digital Waveguides},
number       = {4},
pages        = {74--91},
url          = {https://ccrma.stanford.edu/ jos/pmudw/},
volume       = {16},
file         = {:done/1992SmithPhysical Physical Modeling Using Digital Waveguides.pdf:application/pdf},
groups       = {tesse:5, Digital Waveguides},
keywords     = {digital waveguides, physical modeling},
}
@InProceedings{2015HeDeep,
author       = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle    = {Proceedings of the IEEE conference on computer vision and pattern recognition},
date         = {2015-12-10},
title        = {Deep Residual Learning for Image Recognition},
eprint       = {1512.03385},
eprinttype   = {arxiv},
pages        = {770--778},
url          = {http://arxiv.org/abs/1512.03385},
urldate      = {2019-04-05},
abstract     = {Deeper neural networks are more difﬁcult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [41] but still having lower complexity. An ensemble of these residual nets achieves 3.57 /\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classiﬁcation task. We also present analysis on CIFAR-10 with 100 and 1000 layers.},
file         = {:done/2015HeDeep Deep Residual Learning for Image Recognition.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {arXiv:1512.03385 [cs]},
langid       = {english},
}
@Article{2017BelloNeural,
author       = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V.},
date         = {2017-09-21},
journaltitle = {arXiv:1709.07417 [cs, stat]},
title        = {Neural Optimizer Search with Reinforcement Learning},
eprint       = {1709.07417},
eprinttype   = {arxiv},
number       = {2002},
url          = {http://arxiv.org/abs/1709.07417},
urldate      = {2019-03-26},
abstract     = {We present an approach to automate the process of discovering optimization methods, with a focus on deep learning architectures. We train a Recurrent Neural Network controller to generate a string in a domain speciﬁc language that describes a mathematical update equation based on a list of primitive functions, such as the gradient, running average of the gradient, etc. The controller is trained with Reinforcement Learning to maximize the performance of a model after a few epochs. On CIFAR-10, our method discovers several update rules that are better than many commonly used optimizers, such as Adam, RMSProp, or SGD with and without Momentum on a ConvNet model. We introduce two new optimizers, named PowerSign and AddSign, which we show transfer well and improve training on a variety of different tasks and architectures, including ImageNet classiﬁcation and Google’s neural machine translation system.},
arxivid      = {arXiv:1709.07417v2},
file         = {:done/2017BelloNeural Neural Optimizer Search with Reinforcement Learning.pdf:application/pdf},
groups       = {tesse:5},
langid       = {english},
}
@Article{1991LeshnoMultilayer,
author        = {Leshno, Moshe and Schocken, Shimon},
date          = {1991},
journaltitle  = {Neural Networks},
title         = {Multilayer Feedforward Networks with Non-Polynomial Activation Functions Can Approximate Any Function},
number        = {21},
pages         = {17},
url           = {https://archive.nyu.edu/bitstream/2451/14384/1/IS-91-26.pdf},
abstract      = {Several researchers characterized the activation functions under which multilayer feedforward networks can act as universal approximators. We show that all the characterizations that were reported thus far in the literature ark special cases of the following general result: a standard multilayer feedforward network can approximate any continuous function to any degree of accuracy if and only if the network's activation functions are not polynomial. We also emphasize the important role of the threshold, asserting that without it the last theorem doesn't hold.},
file          = {:done/1991LeshnoMultilayer Multilayer Feedforward Networks with Non Polynomial Activation Functions Can Approximate Any Function.pdf:application/pdf},
groups        = {tesse:5},
keywords      = {theory},
langid        = {english},
mendeley-tags = {theory},
year          = {1991},
}
@Article{2013PingpingZhuKernel,
author        = {Pingping Zhu, José and Príncipe, e C. and Zhu, Pingping and Pr'incipe, José C.},
date          = {2013-05},
journaltitle  = {Computer Engineering},
title         = {Kernel recurrent system trained by real-time recurrent learning algorithm},
doi           = {10.1109/ICASSP.2013.6638323},
pages         = {3572--3576},
url           = {http://ieeexplore.ieee.org/document/6638323/},
urldate       = {2019-04-05},
abstract      = {This paper presents a kernelized version of recurrent systems (KRS) and develops a kernel real-time recurrent learning (KRTRL) algorithm to train KRS. To avoid instabilities during training, the teacher forcing technique is adopted to modify the KRTRL learning. The proposed algorithm is compared with the KLMS in Lorenz time series prediction. The prediction performances of the proposed algorithm outperform the KLMS signiﬁcantly.},
booktitle     = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
eventtitle    = {ICASSP 2013 - 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
file          = {:done/2013PingpingZhuKernel Kernel Recurrent System Trained by Real Time Recurrent Learning Algorithm.pdf:application/pdf},
groups        = {tesse:5},
isbn          = {978-1-4799-0356-6},
keywords      = {forecasting},
langid        = {english},
location      = {Vancouver, BC, Canada},
mendeley-tags = {forecasting},
publisher     = {IEEE},
}
@Article{2003RitchieOptimizationa,
author        = {Ritchie, Marylyn D and White, Bill C and Parker, Joel S and Hahn, Lance W and Moore, Jason H},
date          = {2003},
journaltitle  = {BMC Bioinformatics},
title         = {Optimization of neural network architecture using genetic programming improves detection and modeling of gene-gene interactions in studies of human diseases},
doi           = {10.1186/1471-2105-4-28},
issn          = {1471-2105},
number        = {1},
pages         = {14},
url           = {http://www.ncbi.nlm.nih.gov/pubmed/12846935},
volume        = {4},
abstract      = {Background: Appropriate definition of neural network architecture prior to data analysis is crucial for successful data mining. This can be challenging when the underlying model of the data is unknown. The goal of this study was to determine whether optimizing neural network architecture using genetic programming as a machine learning strategy would improve the ability of neural networks to model and detect nonlinear interactions among genes in studies of common human diseases. Results: Using simulated data, we show that a genetic programming optimized neural network approach is able to model gene-gene interactions as well as a traditional back propagation neural network. Furthermore, the genetic programming optimized neural network is better than the traditional back propagation neural network approach in terms of predictive ability and power to detect gene-gene interactions when non-functional polymorphisms are present. Conclusion: This study suggests that a machine learning strategy for optimizing neural network architecture may be preferable to traditional trial-and-error approaches for the identification and characterization of gene-gene interactions in common, complex human diseases.},
isbn          = {1471-2105 (Electronic)$/backslash$r1471-2105 (Linking)},
keywords      = {*Epistasis,*Models,*Neural Networks (Computer),*Software,Algorithms,Artificial Intelligence,Gene Expression Regulation/*genetics,Genetic,Humans,Molecular Epidemiology/*methods/*trends,Polymorphism,Predictive Value of Tests,Research Design,Single Nucleotide/genetics,Software Validation,evolutive},
langid        = {english},
mendeley-tags = {evolutive},
pmid          = {12846935},
publisher     = {BioMed Central},
}
@PhdThesis{2006MullenPhysical,
author       = {Mullen, Jack},
date         = {2006-04},
institution  = {The University of York},
title        = {Physical modelling of the vocal tract with the 2D digital waveguide mesh.},
type         = {phdthesis},
url          = {http://www-users.york.ac.uk/ dtm3/Download/JackThesis.pdf},
file         = {:done/2006MullenPhysical Physical Modelling of the Vocal Tract with the 2D Digital Waveguide Mesh..pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {[s.l.] University of York},
volume       = {2006},
}
@InProceedings{2013MaasRectifier,
author        = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
booktitle     = {Proceedings of the International Conference on Machine Learning},
date          = {2013},
title         = {Rectifier nonlinearities improve neural network acoustic models},
number        = {1},
pages         = {6},
url           = {https://web.stanford.edu/{/textasciitilde }awni/papers/relu{////////\\_}hybrid{////////\\_}icml2013{////////\\_}final.pdf},
volume        = {2013},
abstract      = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2 /\% absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
file          = {:done/2013MaasRectifier Rectifier Nonlinearities Improve Neural Network Acoustic Models.pdf:application/pdf},
groups        = {tesse:5},
journaltitle  = {ICML. Anais},
keywords      = {speech recognition},
mendeley-tags = {speech recognition},
}
@InProceedings{2001SerafinBanded,
author       = {Serafin, Stefania and Huang, Patty and Iii, Julius O. Smith},
booktitle    = {Workshop on Future Directions of Computer Music (Mosart-01)},
date         = {2001},
title        = {The banded digital waveguide mesh},
organization = {Citeseer},
pages        = {2--5},
publisher    = {Citeseer},
url          = {http://imi.aau.dk/ sts/publications/mosart.pdf},
volume       = {1},
file         = {:done/2001SerafinBanded The Banded Digital Waveguide Mesh.pdf:application/pdf},
groups       = {tesse:5},
journaltitle = {Workshop on Future Directions of Computer Music (Mosart-0},
}
@Article{2008ForgeardPracticing,
author       = {Forgeard, Marie and Winner, Ellen and Norton, Andrea and Schlaug, Gottfried},
date         = {2008-10},
journaltitle = {PLoS ONE},
title        = {Practicing a Musical Instrument in Childhood is Associated with Enhanced Verbal Ability and Nonverbal Reasoning},
doi          = {10.1371/journal.pone.0003566},
editor       = {Fitch, Tecumseh},
number       = {10},
pages        = {e3566},
volume       = {3},
file         = {:done/2008ForgeardPracticing Practicing a Musical Instrument in Childhood Is Associated with Enhanced Verbal Ability and Nonverbal Reasoning.pdf:application/pdf},
groups       = {tesse:5},
}
@InProceedings{2014OhanlonPolyphonic,
author     = {O'Hanlon, Ken and Plumbley, Mark D.},
booktitle  = {2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
date       = {2014-05},
title      = {Polyphonic piano transcription using non-negative Matrix Factorisation with group sparsity},
doi        = {10.1109/ICASSP.2014.6854173},
eventtitle = {ICASSP 2014 - 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn       = {978-1-4799-2893-4},
location   = {Florence, Italy},
number     = {May},
pages      = {3112--3116},
publisher  = {IEEE},
url        = {http://ieeexplore.ieee.org/document/6854173/},
urldate    = {2019-04-05},
volume     = {1},
langid     = {english},
}
@InProceedings{2017SelfridgeReal,
author       = {Selfridge, Rod and Moffat, David J. and Reiss, Joshua D. and Avital, Eldad J.},
date         = {2017},
title        = {Real-time physical model of an aeolian harp},
pages        = {1--8},
abstract     = {A real-time physical sound synthesis model of an Aeolian harp is presented. The model uses semiempirical ﬂuid dynamics equations to inform its operation, providing suitable parameters for users to interact. A basic wind model is included as well as an interface allowing user adjustable parameters. Sounds generated by the model were subject to objective measurements against real-world recordings, which showed that many of the physical properties of the harp were replicated in our model, but a possible link between harmonics and vibration amplitude was not. A perceptual test was performed, where participants were asked to rate sounds in terms of how plausible they were in comparison with spectral modelling synthesis and recorded Aeolian Harp samples. Evaluation showed that our model performed as well as an alternative non-physical synthesis method, but was not as authentic as actual recorded samples.},
file         = {:done/2017SelfridgeReal Real Time Physical Model of an Aeolian Harp.pdf:application/pdf},
groups       = {tesse:5, Acoustics},
journaltitle = {ICSV24},
keywords     = {aeolian harp,physical model,real-time,sound synthesis},
langid       = {english},
}
@Article{2003BensaSimulation,
author       = {Bensa, Julien and Bilbao, Stefan and Kronland-Martinet, Richard and Smith, Julius O.},
date         = {2003-08},
journaltitle = {The Journal of the Acoustical Society of America},
title        = {The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
doi          = {10.1121/1.1587146},
number       = {2},
pages        = {1095--1107},
volume       = {114},
keywords     = {digital waveguides, physical modeling, finite difference,},
}
@PhdThesis{2012SpeedVoice,
author      = {Speed, Matthew David Adam},
date        = {2012},
institution = {University of York},
title       = {Voice Synthesis Using the Three-Dimensional Digital Waveguide Mesh},
type        = {phdthesis},
url         = {http://etheses.whiterose.ac.uk/2800/},
urldate     = {2019-03-28},
abstract    = {The acoustic response of the vocal tract is fundamental to our interpretation of voice production. As an acoustic filter, it shapes the spectral envelope of vocal fold vibration towards resonant modes, or formants, whose behaviours form the most basic building blocks of phonetics. Physical models of the voice exploit this effect by modelling the nature of wave propagation in abstracted cylindrical constructs. Whilst effective, the accuracy of such approaches is limited due to their limited geometrical analogue. Developments in numerical acoustics modelling meanwhile have seen the formalisation of higher dimensionality configurations of the same technologies, allowing a much closer geometrical representation of an acoustic field. The major focus of this thesis is the application of such a technique to the vocal tract, and comparison of its performance with lower dimensionality approaches. To afford the development of such models, a body of data is collected from Magnetic Resonance Imaging for a range of subjects, and procedures are developed for the decomposition of this imaging into suitable, efficient data structures for simulation. The simulation technique is exhaustively validated using a combination of bespoke measurement/inversion techniques and analytical determination of lower frequency behaviours. Finally, voice synthesis based on each numerical model is compared with acoustic recordings of the subjects involved and with equivalent simulations from lower dimensionality methods. It is found that application of a higher dimensionality method typically yields a more accurate frequency-domain representation of the voice, although in some cases lower dimensionality equivalents are seen to perform better at low frequencies.},
adsnote     = {Provided by the SAO/NASA Astrophysics Data System},
adsurl      = {http://adsabs.harvard.edu/abs/2012PhDT.......545S},
file        = {:done/2012SpeedVoice Voice Synthesis Using the Three Dimensional Digital Waveguide Mesh.html:URL},
groups      = {tesse:5},
school      = {The University of York (United Kingdom},
}
@Article{2017KlambauerSelf,
author       = {Klambauer, Günter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
date         = {2017},
journaltitle = {Advances in Neural Information Processing Systems 30},
title        = {Self-Normalizing Neural Networks},
editor       = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
eprint       = {arXiv:1706.02515v5},
eprinttype   = {arXiv},
pages        = {971--980},
url          = {http://papers.nips.cc/paper/6698-self-normalizing-neural-networks.pdf},
arxivid      = {arXiv:1706.02515v5},
booktitle    = {Advances in Neural Information Processing Systems 30},
file         = {:done/2017KlambauerSelf Self Normalizing Neural Networks.pdf:application/pdf},
groups       = {tesse:5},
publisher    = {Curran Associates, Inc.},
}
@InProceedings{2015ZenUnidirectionala,
author        = {Zen, Heiga and Sak, Hacsim},
booktitle     = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
date          = {2015},
title         = {Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis},
doi           = {10.1109/ICASSP.2015.7178816},
eventtitle    = {ICASSP 2015 - 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
isbn          = {978-1-4673-6997-8},
location      = {South Brisbane, Queensland, Australia},
organization  = {IEEE},
pages         = {4470--4474},
publisher     = {IEEE},
url           = {http://ieeexplore.ieee.org/document/7178816/},
urldate       = {2019-04-05},
abstract      = {Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the concerns for applying them to text-to-speech applications is its effect on latency. To address this concern, this paper proposes a low-latency, streaming speech synthesis architecture using unidirectional LSTMRNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of output acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch processing.},
institution   = {IEEE},
issn          = {9781467369978},
journaltitle  = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
keywords      = {speech synthesis},
langid        = {english},
mendeley-tags = {speech synthesis},
}
@Electronic{2015Open,
date         = {2015},
title        = {The Open Source Drumkit},
url          = {https://github.com/crabacus/the-open-source-drumkit},
urldate      = {2018-06-30},
howpublished = {online},
}
@Book{noauthor_spline_2015,
author    = {Averbuch, Amir Z and Zheludev, Valery A and Neittaanmäki, P and Kitchener and Foundation, Waterloo Community},
date      = {2015},
title     = {Spline and spline wavelet methods with applications to signal and image processing},
isbn      = {978-3-319-22302-5},
location  = {New York, NY},
note      = {OCLC: 906187660},
publisher = {Springer Berlin Heidelberg},
langid    = {english},
}
@Book{,
}
@Book{2009BilbaoNumericala,
author    = {Stefan Bilbao},
date      = {2009-10-23},
title     = {Numerical Sound Synthesis},
doi       = {10.1002/9780470749012},
isbn      = {978-0-470-74901-2 978-0-470-51046-9},
location  = {Chichester, UK},
pagetotal = {456},
publisher = {John Wiley {\&} Sons, Ltd},
url       = {https://www.ebook.de/de/product/9338750/stefan///////\\_bilbao///////\\_numerical///////\\_sound///////\\_synthesis.html},
urldate   = {2019-03-31},
ean       = {9780470510469},
file      = {:done/2009BilbaoNumerical Numerical Sound Synthesis.pdf:application/pdf},
groups    = {tesse:5},
langid    = {english},
month     = {oct},
year      = {2009},
}
@InProceedings{2017EngelNeural,
author       = {Jesse Engel and Cinjon Resnick and Adam Roberts and Sander Dieleman and Mohammad Norouzi and Douglas Eck and Karen Simonyan},
booktitle    = {Proceedings of the 34th International Conference on Machine Learning},
date         = {2017},
title        = {Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders},
editor       = {Doina Precup and Yee Whye Teh},
eprint       = {arXiv:1704.01279v1},
eprinttype   = {arXiv},
pages        = {1068--1077},
publisher    = {PMLR},
series       = {Proceedings of Machine Learning Research},
url          = {http://proceedings.mlr.press/v70/engel17a.html},
volume       = {70},
abstract     = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
address      = {International Convention Centre, Sydney, Australia},
arxivid      = {arXiv:1704.01279v1},
bibsource    = {dblp computer science bibliography, https://dblp.org},
biburl       = {https://dblp.org/rec/bib/journals/corr/EngelRRDESN17},
file         = {:done/2017EngelNeural Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders.pdf:application/pdf;engel17a.pdf:http\://proceedings.mlr.press/v70/engel17a/engel17a.pdf:PDF},
groups       = {tesse:5},
journaltitle = {CoRR},
month        = {06--11 Aug},
timestamp    = {Mon, 13 Aug 2018 16:47:29 +0200},
year         = {2017},
}
@book{Tarjano2019,
abstract = {{\textcopyright} 2019, Springer Nature Switzerland AG. Two main approaches are currently prevalent in the digital emulation of musical instruments: manipulation of pre-recorded samples and techniques of real-time synthesis, generally based on physical models with varying degrees of accuracy. Concerning the first, while the processing power of present-day computers enables their use in real-time, many restrictions arising from this sample-based design persist; the huge on disk space requirements and the stiffness of musical articulations being the most prominent. On the other side of the spectrum, pure synthesis approaches, while offering greater flexibility, fail to capture and reproduce certain nuances central to the verisimilitude of the generated sound, offering a dry, synthetic output, at a high computational cost. We propose a method where ensembles of lightweight neural networks working in parallel are learned, from crafted frequency-domain features of an instrument sound spectra, an arbitrary instrument's voice and articulations realistically and efficiently. We find that our method, while retaining perceptual sound quality on par with sampled approaches, exhibits 1/10 of latency times of industry standard real-time synthesis algorithms, and 1/100 of the disk space requirements of industry standard sample-based digital musical instruments. This method can, therefore, serve as a basis for more efficient implementations in dedicated devices, such as keyboards and electronic drumkits and in general purpose platforms, like desktops and tablets or open-source hardware like Arduino and Raspberry Pi. From a conceptual point of view, this work highlights the advantages of a closer integration of machine learning with other subjects, especially in the endeavor of new product development. Exploiting the synergy between neural networks, digital signal processing techniques and physical modelling, we illustrate the proposed method via the implementation of two virtual instruments: a conventional grand piano and a hibrid stringed instrument.},
author = {Tarjano, C. and Pereira, V.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-030-30490-4_30},
isbn = {9783030304898},
issn = {16113349},
keywords = {Acoustic modeling,Digital musical instruments,Neural networks,Real-time audio synthesis},
title = {{Neuro-Spectral Audio Synthesis: Exploiting Characteristics of the Discrete Fourier Transform in the Real-Time Simulation of Musical Instruments Using Parallel Neural Networks}},
volume = {11730 LNCS},
year = {2019}
}
@article{ISI:000183263200010,
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Williams, L and Cockburn, A},
doi = {10.1109/MC.2003.1204373},
issn = {0018-9162},
journal = {COMPUTER},
month = {jun},
number = {6},
pages = {39--43},
publisher = {IEEE COMPUTER SOC},
title = {{Agile software development: It's about feedback and change}},
type = {Editorial Material},
volume = {36},
year = {2003}
}
@article{ISI:000268944000023,
abstract = {Context. AGILE is an Italian Space Agency mission dedicated to observing
the gamma-ray Universe. The AGILE's very innovative instrumentation for
the first time combines a gamma-ray imager (sensitive in the energy
range 30 MeV-50 GeV), a hard X-ray imager (sensitive in the range 18-60
keV), a calorimeter (sensitive in the range 350 keV-100 MeV), and an
anticoincidence system. AGILE was successfully launched on 2007 April 23
from the Indian base of Sriharikota and was inserted in an equatorial
orbit with very low particle background.
Aims. AGILE provides crucial data for the study of active galactic
nuclei, gamma-ray bursts, pulsars, unidentified gamma-ray sources,
galactic compact objects, supernova remnants, TeV sources, and
fundamental physics by microsecond timing.
Methods. An optimal sky angular positioning (reaching 0.1 degrees in
gamma- rays and 1-2 arcmin in hard X-rays) and very large fields of view
(2.5 sr and 1 sr, respectively) are obtained by the use of Silicon
detectors integrated in a very compact instrument.
Results. AGILE surveyed the gamma- ray sky and detected many Galactic
and extragalactic sources during the first months of observations.
Particular emphasis is given to multifrequency observation programs of
extragalactic and galactic objects.
Conclusions. AGILE is a successful high-energy gamma-ray mission that
reached its nominal scientific performance. The AGILE Cycle-1 pointing
program started on 2007 December 1, and is open to the international
community through a Guest Observer Program.},
address = {17, AVE DU HOGGAR, PA COURTABOEUF, BP 112, F-91944 LES ULIS CEDEX A, FRANCE},
author = {Tavani, M and Barbiellini, G and Argan, A and Boffelli, F and Bulgarelli, A and Caraveo, P and Cattaneo, P W and Chen, A W and Cocco, V and Costa, E and D'Ammando, F and {Del Monte}, E and {De Paris}, G and {Di Cocco}, G and {Di Persio}, G and Donnarumma, I and Evangelista, Y and Feroci, M and Ferrari, A and Fiorini, M and Fornari, F and Fuschino, F and Froysland, T and Frutti, M and Galli, M and Gianotti, F and Giuliani, A and Labanti, C and Lapshov, I and Lazzarotto, F and Liello, F and Lipari, P and Longo, F and Mattaini, E and Marisaldi, M and Mastropietro, M and Mauri, A and Mauri, F and Mereghetti, S and Morelli, E and Morselli, A and Pacciani, L and Pellizzoni, A and Perotti, F and Piano, G and Picozza, P and Pontoni, C and Porrovecchio, G and Prest, M and Pucella, G and Rapisarda, M and Rappoldi, A and Rossi, E and Rubini, A and Soffitta, P and Traci, A and Trifoglio, M and Trois, A and Vallazza, E and Vercellone, S and Vittorini, V and Zambra, A and Zanello, D and Pittori, C and Preger, B and Santolamazza, P and Verrecchia, F and Giommi, P and Colafrancesco, S and Antonelli, A and Cutini, S and Gasparrini, D and Stellato, S and Fanari, G and Primavera, R and Tamburelli, F and Viola, F and Guarrera, G and Salotti, L and D'Amico, F and Marchetti, E and Crisconio, M and Sabatini, P and Annoni, G and Alia, S and Longoni, A and Sanquerin, R and Battilana, M and Concari, P and Dessimone, E and Grossi, R and Parise, A and Monzani, F and Artina, E and Pavesi, R and Marseguerra, G and Nicolini, L and Scandelli, L and Soli, L and Vettorello, V and Zardetto, E and Bonati, A and Maltecca, L and D'Alba, E and Patane, M and Babini, G and Onorati, F and Acquaroli, L and Angelucci, M and Morelli, B and Agostara, C and Cerone, M and Michetti, A and Tempesta, P and D'Eramo, S and Rocca, F and Giannini, F and Borghi, G and Garavelli, B and Conte, M and Balasini, M and Ferrario, I and Vanotti, M and Collavo, E and Giacomazzo, M},
doi = {10.1051/0004-6361/200810527},
issn = {1432-0746},
journal = {ASTRONOMY {\&} ASTROPHYSICS},
keywords = {instrumentation: detectors; techniques: high angul},
month = {aug},
number = {3},
pages = {995--1013},
publisher = {EDP SCIENCES S A},
title = {{The AGILE Mission}},
type = {Article},
volume = {502},
year = {2009}
}
@article{ISI:000238141400003,
abstract = {This article explores how agile practices can reduce three kinds of
``distance{\{}''{\}} - temporal, geographical, and sociocultural - in global
software development (GSD). On the basis of two in-depth case studies,
specific Scrum and eXtreme Programming (XP) practices are found to be
useful for reducing communication, coordination, and control problems
that have been associated with GSD.},
address = {C/O CRC PRESS L L C, 2000 CORPORATE BLVD NW, BOCA RATON, FL 33431 USA},
author = {Holmstrom, Helena and Fitzgerald, Brian and Agerfalk, Par J and Conchuir, Eoin O},
doi = {10.1201/1078.10580530/46108.23.3.20060601/93703.2},
issn = {1058-0530},
journal = {INFORMATION SYSTEMS MANAGEMENT},
number = {3},
pages = {7--18},
publisher = {AUERBACH PUBLICATIONS},
title = {{Agile practices reduce distance in global software development}},
type = {Article},
volume = {23},
year = {2006}
}
@article{ISI:000298773100005,
abstract = {The paper reviews the literature on supply partner decision-making
published between 2001 and 2011, a period that has seen a significant
increase in work published in this field. The progress made in
developing new models and methods that can be applied to this task is
assessed in the context of the previous literature. Particular attention
is given to those methods that are especially relevant for use in agile
supply chains. The paper uses a classification framework that enables
models intended for similar purposes to be compared and tracked over
time. It is also used to identify a number of gaps in the literature.
The findings highlight an on-going need to develop methods that are able
to meet the combination of qualitative and quantitative objectives that
are typically found in partner selection problems in practice. (C) 2011
Elsevier Ltd. All rights reserved.},
address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
author = {Wu, Chong and Barnes, David},
doi = {10.1016/j.pursup.2011.09.002},
issn = {1478-4092},
journal = {JOURNAL OF PURCHASING AND SUPPLY MANAGEMENT},
keywords = {Literature review; Partner selection; Agile supply},
month = {dec},
number = {4},
pages = {256--274},
publisher = {ELSEVIER SCI LTD},
title = {{A literature review of decision-making models and approaches for partner selection in agile supply chains}},
type = {Review},
volume = {17},
year = {2011}
}
@article{ISI:000233567300021,
address = {1515 BROADWAY, NEW YORK, NY 10036 USA},
author = {Augustine, S and Payne, B and Sencindiver, F and Woodcock, S},
doi = {10.1145/1101779.1101781},
issn = {0001-0782},
journal = {COMMUNICATIONS OF THE ACM},
month = {dec},
number = {12},
pages = {85--89},
publisher = {ASSOC COMPUTING MACHINERY},
title = {{Agile project management: Steering from the edges}},
type = {Article},
volume = {48},
year = {2005}
}
@article{ISI:000277107200002,
abstract = {Context Software development depends significantly on team performance,
as does any process that involves human interaction
Objective Most current development methods argue that teams should
self-manage Our objective is thus to provide a better understanding of
the nature of self-managing agile teams, and the teamwork challenges
that arise when introducing such teams
Method We conducted extensive fieldwork for 0 months in a software
development company that introduced Scrum. We focused on the human
sensemaking, on how mechanisms of teamwork were understood by the people
involved
Results We describe a project through Dickinson and McIntyre's teamwork
model, focusing on the interrelations between essential teamwork
components Problems with team orientation, team leadership and
coordination in addition to highly specialized skills and corresponding
division of work were important barriers for achieving team
effectiveness
Conclusion Transitioning from individual work to self-managing teams
requires a reorientation not only by developers but also by management
This transition takes time and resources, but should not be neglected In
addition to Dickinson and McIntyre's teamwork components, we found trust
and shared mental models to be of fundamental importance (C) 2009
Elsevier B V All rights reserved},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Moe, Nils Brede and Dingsoyr, Torgeir and Dyba, Tore},
doi = {10.1016/j.infsof.2009.11.004},
issn = {0950-5849},
journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
keywords = {Agile software development; Scrum; Software engine},
month = {may},
number = {5, SI},
pages = {480--491},
publisher = {ELSEVIER SCIENCE BV},
title = {{A teamwork model for understanding an agile team: A case study of a Scrum project}},
type = {Article},
volume = {52},
year = {2010}
}
@article{ISI:000084793700002,
abstract = {Feature recognition, from low level geometric entities of product design
representations within a CAD model to facilitate process planning and
manufacturing activities, has been of significant importance in computer
integrated manufacturing (CIM). However, the emerging paradigm of Agile
Manufacturing has imposed additional requirements of `'neutral
format{\{}''{\}} so that form-feature information can be readily shared among
multiple partners of a virtual enterprise. Recently, the STandard for
the Exchange of Product model data (STEP) has emerged as the means for
neutral form exchange of product related data. The ``STEP efforts{\{}''{\}}
have broken down the domain of manufacturing related activities in the
form of application protocols (APs) target for specific functions which
include drafting, configuration control and feature-based process
planning to mention a few. Efforts are still on to increase the
acceptance and use of this international standard (IS). This paper
focuses on our efforts to support the STEP standard with the development
of a standards-oriented form-feature extraction system. The developed
feature extraction system takes as a input a STEP file defining the
geometry and topology of a part and generates as output a STEP file with
form-feature information in AP224 format for form feature-based process
planning. The system can also be interfaced with a recent IGES to AP202
translator {\{}[{\}}M.P. Bhandarkar, B. Downie, M. Hardwick, R. Nagi,
Migration from ICES to STEP: one-to-one translation of IGES drawing to
STEP drafting data, accepted by Computers in Industry, July, 1999; M.P.
Bhandarkar, Satisfying information needs in Agile Manufacturing through
translation and feature extraction into STEP product data models, MS
Thesis, State University of New York at Buffalo, 1997.] to allow
conversion of legacy data. The feature recognition algorithm is
boundary-representation (B-Rep) based and follows a sequential approach
through an existing classification of features. Properties of each
feature class are exploited to enable their extraction. The algorithm is
currently developed for prismatic solids produced by milling operations
and that contain elementary shapes such as plane and cylindrical
surfaces (possibly using non-uniform rational B-splines (NURBS)).
Special attention has been paid to implementation issues. We demonstrate
the efficacy of the system using representative parts. (C) 2000
Published by Elsevier Science B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Bhandarkar, M P and Nagi, R},
doi = {10.1016/S0166-3615(99)00040-8},
issn = {0166-3615},
journal = {COMPUTERS IN INDUSTRY},
keywords = {STEP; form feature; feature extraction},
month = {jan},
number = {1},
pages = {3--24},
publisher = {ELSEVIER SCIENCE BV},
title = {{STEP-based feature extraction from STEP geometry for agile manufacturing}},
type = {Article},
volume = {41},
year = {2000}
}
@article{ISI:000225756200019,
abstract = {Developers need evidence that a new technology works in a certain
context before they promote and deploy it on a larger scale. This need
looms greater in large organizations because of their complexity and the
need to integrate new technologies and processes with existing ones.
To further evaluate agile methods and their underlying software
development practices, several Software Experience Center member
companies initiated a series of activities to discover if agile
practices match their organizations' needs. Based on the experiences of
these organizations, researchers concluded that agile practices match
the needs of large organizations, but integrating new practices with
existing processes and quality systems that govern the conduct of
software development requires further tailoring. The challenge here lies
not in applying agile practices to a project, but in efficiently
integrating the agile project into its environment.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Lindvall, M and Muthig, D and Dagnino, A and Wallin, C and Stupperick, M and Kiefer, D and May, J and Kahkonen, T},
doi = {10.1109/MC.2004.231},
issn = {0018-9162},
journal = {COMPUTER},
month = {dec},
number = {12},
pages = {26+},
publisher = {IEEE COMPUTER SOC},
title = {{Agile software development in large organizations}},
type = {Article},
volume = {37},
year = {2004}
}
@article{ISI:000309058000012,
address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
author = {Palunko, Ivana and Cruz, Patricio and Fierro, Rafael},
doi = {10.1109/MRA.2012.2205617},
issn = {1070-9932},
journal = {IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
month = {sep},
number = {3},
pages = {69--79},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{Agile Load Transportation Safe and Efficient Load Manipulation with Aerial Robots}},
type = {Article},
volume = {19},
year = {2012}
}
@article{ISI:000272058100007,
abstract = {This paper considers a supply chain design problem for a new market
opportunity with uncertain demand in an agile manufacturing setting. We
consider the integrated optimization of logistics and production costs
associated with the supply chain members. These problems routinely occur
in a wide variety of industries including semiconductor manufacturing,
multi-tier automotive supply chains, and consumer appliances to name a
few. There are two types of decision variables: binary variables for
selection of companies to form the supply chain and continuous variables
associated with production planning. A scenario approach is used to
handle the uncertainty of demand. The formulation is a robust
optimization model with three components in the objective function:
expected total costs, cost variability due to demand uncertainty, and
expected penalty for demand unmet at the end of the planning horizon.
The increase of computational time with the numbers of echelons and
members per echelon necessitates a heuristic. A heuristic based on a
k-shortest path algorithm is developed by using a surrogate distance to
denote the effectiveness of each member in the supply chain. The
heuristic can find an optimal solution very quickly in some small- and
medium-size cases. For large problems, a ``good{\{}''{\}} solution with a
small gap relative to our lower bound is obtained in a short
computational time. (C) 2009 Elsevier Ltd. All rights reserved.},
address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
author = {Pan, Feng and Nagi, Rakesh},
doi = {10.1016/j.cor.2009.06.017},
issn = {0305-0548},
journal = {COMPUTERS {\&} OPERATIONS RESEARCH},
keywords = {Supply chain formation; Integrated costs; Uncertai},
month = {apr},
number = {4},
pages = {668--683},
publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
title = {{Robust supply chain design under uncertain demand in agile manufacturing}},
type = {Article},
volume = {37},
year = {2010}
}
@article{ISI:000165846400007,
abstract = {Many enterprises have pursued the lean thinking paradigm to improve the
efficiency of their business processes. More recently, the agile
manufacturing paradigm has been highlighted as an alternative to, and
possibly an improvement on, leanness. In pursuing such arguments in
isolation, the power of each paradigm may be lost, which is basically
that agile manufacturing is adopted where demand is volatile, and lean
manufacturing adopted where there is a stable demand. However, in some
situations it is advisable to utilize a different paradigm on either
side of the material flow de-coupling point to enable a total supply
chain strategy. This approach we have termed the Leagile Paradigm. This
paper therefore considers the effect of the marketplace environment on
strategy selection to ensure optimal supply chain performance.
Real-world case studies in the mechanical precision products, carpet
making, and electronic products market sectors demonstrate the new
approach to matching supply chain design to the actual needs of the
marketplace.},
address = {11 NEW FETTER LANE, LONDON EC4P 4EE, ENGLAND},
annote = {15th International Conference of Production Research(ICPR-15), UNIV
LIMERICK, LIMERICK, IRELAND, AUG, 1999},
author = {Mason-Jones, R and Naylor, B and Towill, D R},
doi = {10.1080/00207540050204920},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month = {nov},
number = {17},
pages = {4061--4070},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{Lean, agile or leagile? Matching your supply chain to the marketplace}},
type = {Article; Proceedings Paper},
volume = {38},
year = {2000}
}
@article{ISI:000170758100024,
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Highsmith, J and Cockburn, A},
doi = {10.1109/2.947100},
issn = {0018-9162},
journal = {COMPUTER},
month = {sep},
number = {9},
pages = {120--122},
publisher = {IEEE COMPUTER SOC},
title = {{Agile software development: The business of innovation}},
type = {Article},
volume = {34},
year = {2001}
}
@article{ISI:000254515800002,
abstract = {Aims. Since the CGRO operation in 1991-2000, one of the primary
unresolved questions about the blazar gamma-ray emission has been its
possible correlation with the low-energy (in particular optical)
emission. To help answer this problem, the Whole Earth Blazar Telescope
(WEBT) consortium has organized the GLAST-AGILE Support Program (GASP)
to provide the optical-to-radio monitoring data to be compared with the
gamma-ray detections by the AGILE and GLAST satellites. This new WEBT
project started in early September 2007, just before a strong gamma-ray
detection of 0716+714 by AGILE.
Methods. We present the GASP-WEBT optical and radio light curves of this
blazar obtained in July-November 2007, about various AGILE pointings at
the source. We construct NIR-to-UV spectral energy distributions (SEDs),
by assembling GASP-WEBT data together with UV data from the Swift ToO
observations of late October.
Results. We observe a contemporaneous optical-radio outburst, which is a
rare and interesting phenomenon in blazars. The shape of the SEDs during
the outburst appears peculiarly wavy because of an optical excess and a
UV drop- and-rise. The optical light curve is well sampled during the
AGILE pointings, showing prominent and sharp flares. A future
cross-correlation analysis of the optical and AGILE data will shed light
on the expected relationship between these flares and the gamma-ray
events.},
address = {17, AVE DU HOGGAR, PA COURTABOEUF, BP 112, F-91944 LES ULIS CEDEX A, FRANCE},
author = {Villata, M and Raiteri, C M and Larionov, V M and Kurtanidze, O M and Nilsson, K and Aller, M F and Tornikoski, M and Volvach, A and Aller, H D and Arkharov, A A and Bach, U and Beltrame, P and Bhatta, G and Buemi, C S and Boettcher, M and Calcidese, P and Carosati, D and Castro-Tirado, A J and {Da Rio}, D and {Di Paola}, A and Dolci, M and Forne, E and Frasca, A and Hagen-Thorn, V A and Heidt, J and Hiriart, D and Jelinek, M and Kimeridze, G N and Konstantinova, T S and Kopatskaya, E N and Lanteri, L and Leto, P and Ligustri, R and Lindfors, E and Lahteenmaki, A and Marilli, E and Nieppola, E and Nikolashvili, M G and Pasanen, M and Ragozzine, B and Ros, J A and Sigua, L A and Smart, R L and Sorcia, M and Takalo, L O and Tavani, M and Trigilio, C and Turchetti, R and Uckert, K and Umana, G and Vercellone, S and Webb, J R},
doi = {10.1051/0004-6361:200809552},
issn = {0004-6361},
journal = {ASTRONOMY {\&} ASTROPHYSICS},
keywords = {galaxies : active; galaxies : BL Lacertae objects},
month = {apr},
number = {2},
pages = {L79--L82},
publisher = {EDP SCIENCES S A},
title = {{Multifrequency monitoring of the blazar 0716+714 during the GASP-WEBT-AGILE campaign of 2007}},
type = {Article},
volume = {481},
year = {2008}
}
@article{ISI:000303626300001,
abstract = {Ever since the agile manifesto was created in 2001, the research
community has devoted a great deal of attention to agile software
development. This article examines publications and citations to
illustrate how the research on agile has progressed in the 10 years
following the articulation of the manifesto. Specifically, we delineate
the conceptual structure underlying agile scholarship by performing an
analysis of authors who have made notable contributions to the field.
Further, we summarize prior research and introduce contributions in this
special issue on agile software development. We conclude by discussing
directions for future research and urging agile researchers to embrace a
theory-based approach in their scholarship. (C) 2012 Elsevier Inc. All
rights reserved.},
address = {360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
author = {Dingsoyr, Torgeir and Nerur, Sridhar and Balijepally, VenuGopal and Moe, Nils Brede},
doi = {10.1016/j.jss.2012.02.033},
issn = {0164-1212},
journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
keywords = {Agile software development; Theory; Software engin,XP; Scrum; Lean software development; Crystal met},
month = {jun},
number = {6},
pages = {1213--1221},
publisher = {ELSEVIER SCIENCE INC},
title = {{A decade of agile methodologies: Towards explaining agile software development}},
type = {Article},
volume = {85},
year = {2012}
}
@article{ISI:000223955900002,
abstract = {This paper outlines approaches for assessing and classifying
manufacturing and service operations in terms of their suitability for
use of cross-trained (flexible) workers. We refer to our overall
framework as agile workforce evaluation. The primary contributions of
this paper are: (i) a strategic assessment framework that structures the
key mechanisms by which cross-training can support organizational
strategy; (ii) a tactical framework that identifies key factors to guide
the selection of an architecture and worker coordination policy for
implementing workforce agility; (iii) a classification of workforce
agility architectures; (iv) a survey of a broad range of archetypical
classes of worker coordination policies; (v) a survey of the literature
with an operational perspective on workforce agility; and (vi)
identification of opportunities for research and development of
architectures for specific production environments.},
address = {4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND},
author = {Hopp, W J and {Van Oyen}, M P},
doi = {10.1080/07408170490487759},
issn = {0740-817X},
journal = {IIE TRANSACTIONS},
month = {oct},
number = {10},
pages = {919--940},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{Agile workforce evaluation: a framework for cross-training and coordination}},
type = {Review},
volume = {36},
year = {2004}
}
@article{ISI:000167124500008,
abstract = {Agility is increasingly mentioned as one of the coming challenges to the
international business world, given volatile markets and increasingly
dynamic performance requirements. Existing literature, however, mainly
presents agility as a general management or a strongly manufacturing
biased concept, but does not explicitly relate the concept to the supply
chain as a whole. Research also shows a bias towards the USA. This paper
presents an attempt to establish an audit of agility in the supply
chain. The audit is used in an empirical investigation of agile
capabilities in Europe. Using existing streams of supply chain research
as building blocks, a preliminary framework is introduced for creating
an agile supply chain. Based on a survey of agile efforts in the UK and
the Benelux the agile capabilities of companies are assessed and
approaches to outscore the benchmark are suggested.},
address = {60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
author = {van Hoek, R I and Harrison, A and Christopher, M},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {agility; supply chain; audit},
number = {1-2},
pages = {126--147},
publisher = {MCB UNIV PRESS LTD},
title = {{Measuring agile capabilities in the supply chain}},
type = {Article},
volume = {21},
year = {2001}
}
@article{ISI:000256077900014,
abstract = {Metamaterials exhibit numerous novel effects(1-5) and operate over a
large portion of the electromagnetic spectrum(6-10). Metamaterial
devices based on these effects include gradient-index lenses(11,12),
modulators for terahertz radiation(13-15) and compact waveguides(16).
The resonant nature of metamaterials results in frequency dispersion and
narrow bandwidth operation where the centre frequency is fixed by the
geometry and dimensions of the elements comprising the metamaterial
composite. The creation of frequency-agile metamaterials would extend
the spectral range over which devices function and, further, enable the
manufacture of new devices such as dynamically tunable notch filters.
Here, we demonstrate such frequency-agile metamaterials operating in the
far-infrared by incorporating semiconductors in critical regions of
metallic split-ring resonators. For this first-generation device,
external optical control results in tuning of the metamaterial resonance
frequency by similar to 20{\%}. Our approach is integrable with current
semiconductor technologies and can be implemented in other regions of
the electromagnetic spectrum.},
address = {MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
author = {Chen, Hou-Tong and O'Hara, John F and Azad, Abul K and Taylor, Antoinette J and Averitt, Richard D and Shrekenhamer, David B and Padilla, Willie J},
doi = {10.1038/nphoton.2008.52},
issn = {1749-4885},
journal = {NATURE PHOTONICS},
month = {may},
number = {5},
pages = {295--298},
publisher = {NATURE PUBLISHING GROUP},
title = {{Experimental demonstration of frequency-agile terahertz metamaterials}},
type = {Article},
volume = {2},
year = {2008}
}
@article{ISI:000171519800001,
abstract = {About a decade ago, the agile manufacturing paradigm was formulated in
response to the constantly changing `new economy' and as a basis for
returning to global competitiveness. While agility means different
things to different enterprises under different contexts, the following
elements capture its essential concept: agility is characterized by
cooperativeness and synergism (possibly resulting in virtual
corporations), by a strategic vision that enables thriving in face of
continuous and unpredictable change, by the responsive creation and
delivery of customer-valued, high quality and mass customized
goods/services, by nimble organization structures of a knowledgeable and
empowered workforce, and facilitated by an information infrastructure
that links constituent partners in a unified electronic network. During
this period, a significant amount of attention from both the academic
and industrial communities has produced a large body of results in
research and development related to this topic. Each contribution has
tackled a different aspect of this large field. In this paper, we review
a wide range of recent literature on agile manufacturing. About 73
papers from premier scientific journals and conferences have been
reviewed, and a classification scheme to organize these is proposed. We
critique these bodies of work and suggest directions for additional
research and identify topics where fruitful opportunities exist.},
address = {11 NEW FETTER LANE, LONDON EC4P 4EE, ENGLAND},
author = {Sanchez, L M and Nagi, R},
doi = {10.1080/00207540110068790},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month = {nov},
number = {16},
pages = {3561--3600},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{A review of agile manufacturing systems}},
type = {Review},
volume = {39},
year = {2001}
}
@article{ISI:000185086300001,
abstract = {This paper is based on longitudinal case studies of research into
strategy formulation within six plants from large firms - three in the
car industry and three from the computer industry - that have embarked
on mass customisation. The core theme of this paper is that, in spite of
the increasing attention given to manufacturing strategy from the
seminal work of Skinner through to the plethora of articles in recent
times, little is mentioned about its application to paradigms of agility
or mass customisation. As a consequence firms attempt to become agile
and to pursue mass customisation without appreciating the contribution
of plant-specific manufacturing strategies that might enable them to
achieve these aspirations. We examine the enablers and strategic
blockages in pursuing mass customisation, via a mapping process, and
reveal reasons why some firms remain unable to devise and implement
manufacturing strategies.},
address = {HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
author = {Brown, S and Bessant, J},
doi = {10.1108/01443570310481522},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {strategic planning; mass customization; agile prod},
number = {7-8},
pages = {707--730},
publisher = {EMERALD GROUP PUBLISHING LTD},
title = {{The manufacturing strategy-capabilities links in mass customisation and agile manufacturing - an exploratory study}},
type = {Article},
volume = {23},
year = {2003}
}
@article{ISI:000183263200013,
abstract = {Both agile and plan-driven approaches have situation-dependent
shortcomings that, if not addressed, can lead to project failure. The
challenge is to balance the two approaches to take advantage of their
strengths in a given situation while compensating for their weaknesses.
The authors present a risk-based approach for structuring projects to
incorporate both agile and plan-driven approaches in proportion to a
project's needs.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Boehm, B and Turner, R},
doi = {10.1109/MC.2003.1204376},
issn = {0018-9162},
journal = {COMPUTER},
month = {jun},
number = {6},
pages = {57+},
publisher = {IEEE COMPUTER SOC},
title = {{Using risk to balance agile and plan-driven methods}},
type = {Article},
volume = {36},
year = {2003}
}
@article{ISI:000175277300004,
abstract = {Agile Manufacturing (AM) is a relatively new operations concept that is
intended to improve the competitiveness of firms. Manufacturing/service
processes based on AM are characterized by customer-supplier integrated
processes for product design, manufacturing, marketing, and support
services. Agile manufacturing requires enriching of the customer;
cooperating with competitors; organizing to manage change, uncertainty
and complexity; and leveraging people and information. In recent years,
a number of research papers have been published in the area of AM. The
term `agile' was coined in 1991. However, there are still some serious
concerns that prevent companies from taking an entirely different
direction from AM. Considering the potential importance of agile
manufacturing in 21st century manufacturing competitiveness, an attempt
has been made in this paper to re-examine the scope, definitions and
strategies of AM. In addition, a framework has been presented as a basis
for understanding the major strategies and relevant technologies of AM.},
address = {4 PARK SQUARE, MILTON PARK,, ABINGDON OX14 4RN, OXON, ENGLAND},
author = {Gunasekaran, A and Yusuf, Y Y},
doi = {10.1080/00207540110118370},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month = {apr},
number = {6},
pages = {1357--1385},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{Agile manufacturing: a taxonomy of strategic and technological imperatives}},
type = {Article},
volume = {40},
year = {2002}
}
@article{ISI:000085104500005,
abstract = {Turbulent and volatile markets are becoming the norm as life cycles
shorten and global economic and competitive forces create additional
uncertainty. The risk attached to lengthy and slow-moving logistics
``pipelines{\{}''{\}} has become unsustainable, forcing organizations to look
again at how their supply chains are structured and managed. This paper
suggests that the key to survival in these changed conditions is through
``agility,{\{}''{\}} in particular by the creation of responsive supply
chains. A distinction is drawn between the philosophies of
``leanness{\{}''{\}} and ``agility, `` and the appropriate application of
these ideas is (C) 2000 Elsevier Science Inc. All rights reserved.},
address = {655 AVENUE OF THE AMERICAS, NEW YORK, NY 10010 USA},
author = {Christopher, M},
doi = {10.1016/S0019-8501(99)00110-8},
issn = {0019-8501},
journal = {INDUSTRIAL MARKETING MANAGEMENT},
month = {jan},
number = {1},
pages = {37--44},
publisher = {ELSEVIER SCIENCE INC},
title = {{The agile supply chain - Competing in volatile markets}},
type = {Article},
volume = {29},
year = {2000}
}
@article{ISI:000247547600012,
abstract = {This paper proposes a small-scale agile wall-climbing robot, which is
able to climb on smooth vertical surfaces using flat adhesive elastomer
materials for attachment. Using two actuated legs with rotary motion and
two passive revolute joints at each foot, this robot can climb and steer
in any orientation. Due to its compact design, a high degree of
miniaturization is possible. It has onboard power, computing, and
wireless communication, which allow for semiautonomous operation.
Various aspects of a functioning prototype design and performance are
discussed in detail, including leg and foot design and gait dynamics. A
model for the adhesion requirements and performance is developed and
verified through experiments. Using an adhesive elastomer (Vytaflex 10),
the current prototype can climb 90 slopes at a speed of up to 6 cm/s and
steer to any angle reliably on a,smooth acrylic surface as well as
transition from floor walking to wall climbing. This robot is intended
for inspection and surveillance applications, and ultimately, for space
missions.},
address = {445 HOES LANE, PISCATAWAY, NJ 08855-4141 USA},
author = {Murphy, Michael P and Sitti, Metin},
doi = {10.1109/TMECH.2007.897277},
issn = {1083-4435},
journal = {IEEE-ASME TRANSACTIONS ON MECHATRONICS},
keywords = {dry adhesives; mechatronics; miniature robotics; m},
month = {jun},
number = {3},
pages = {330--338},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{Waalbot: An agile small-scale wall-climbing robot utilizing dry elastomer adhesives}},
type = {Article},
volume = {12},
year = {2007}
}
@article{ISI:000251148000017,
abstract = {SuperAGILE is a coded mask experiment based on silicon microstrip
detectors. It operates in the 15-45 keV nominal energy range, providing
crossed one-dimensional images of the X-ray sky with an on-axis angular
resolution of 6 arcmin, over a field of view in excess of 1 sr. It was
designed as the hard X-ray monitor of the AGILE space mission, a small
satellite of the Italian Space Agency devoted to image the gamma-ray sky
in the 30 MeV-50 GeV energy band. The AGILE mission was launched in a
low-earth orbit on 23rd April 2007. In this paper we describe the
SuperAGILE experiment, its construction and test processes, and its
performance before flight, based on the on-ground test and calibrations.
(c) 2007 Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Feroci, M and Costa, E and Soffitta, P and {Del Monte}, E and {Di Persio}, G and Donnarumma, I and Evangelista, Y and Frutti, M and Lapshov, I and Lazzarotto, F and Mastropietro, M and Morelli, E and Pacciani, L and Porrovecchio, G and Rapisarda, M and Rubini, A and Tavani, M and Argan, A},
doi = {10.1016/j.nima.2007.07.147},
issn = {0168-9002},
journal = {NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
keywords = {high energy astrophysics; X-ray detectors; microst},
month = {nov},
number = {3},
pages = {728--754},
publisher = {ELSEVIER SCIENCE BV},
title = {{SuperAGILE: The hard X-ray imager for the AGILE space mission}},
type = {Article},
volume = {581},
year = {2007}
}
@article{ISI:000269983900004,
abstract = {In this paper, we draw on control theory to understand the conditions
under which the use of agile practices is most effective in improving
software project quality. Although agile development methodologies offer
the potential of improving software development outcomes, limited
research has examined how project managers can structure the software
development environment to maximize the benefits of agile methodology
use during a project. As a result, project managers have little guidance
on how to manage teams who are using agile methodologies. Arguing that
the most effective control modes are those that provide teams with
autonomy in determining the methods for achieving project objectives, we
propose hypotheses related to the interaction between control modes,
agile methodology use, and requirements change. We test the model in a
field study of 862 software developers in 110 teams. The model explains
substantial variance in four objective measures of project quality-bug
severity, component complexity, coordinative complexity, and dynamic
complexity. Results largely support our hypotheses, highlighting the
interplay between project control, agile methodology use, and
requirements change. The findings contribute to extant literature by
integrating control theory into the growing literature on agile
methodology use and by identifying specific contingencies affecting the
efficacy of different control modes. We discuss the theoretical and
practical implications of our results.},
address = {5521 RESEARCH PARK DR, SUITE 200, CATONSVILLE, MD 21228 USA},
author = {Maruping, Likoebe M and Venkatesh, Viswanath and Agarwal, Ritu},
doi = {10.1287/isre.1090.0238},
issn = {1047-7047},
journal = {INFORMATION SYSTEMS RESEARCH},
keywords = {agile methodologies; agility; control theory; requ},
month = {sep},
number = {3},
pages = {377--399},
publisher = {INFORMS},
title = {{A Control Theory Perspective on Agile Methodology Use and Changing User Requirements}},
type = {Article},
volume = {20},
year = {2009}
}
@article{ISI:A1996VZ99700005,
abstract = {The `'Agile Eye'' is a high-performance mechanism capable of orienting a
camera within a workspace larger than that of a human eye and with
velocities and accelerations larger than those of the human eye. The
mechanical design, control issues, and experimental results are
presented.},
address = {345 E 47TH ST, NEW YORK, NY 10017-2394},
author = {Gosselin, C M and StPierre, E and Gagne, M},
doi = {10.1109/100.556480},
issn = {1070-9932},
journal = {IEEE ROBOTICS {\&} AUTOMATION MAGAZINE},
keywords = {parallel mechanisms; dynamic control; camera-orien},
month = {dec},
number = {4},
pages = {29--37},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{On the development of the Agile Eye}},
type = {Article},
volume = {3},
year = {1996}
}
@article{ISI:000269428600001,
address = {BRUNEL RD BLDG, HOUNDMILLS, BASINGSTOKE RG21 6XS, HANTS, ENGLAND},
author = {Abrahamsson, Pekka and Conboy, Kieran and Wang, Xiaofeng},
doi = {10.1057/ejis.2009.27},
issn = {0960-085X},
journal = {EUROPEAN JOURNAL OF INFORMATION SYSTEMS},
month = {aug},
number = {4},
pages = {281--284},
publisher = {PALGRAVE MACMILLAN LTD},
title = {{`Lots done, more to do': the current state of agile systems development research}},
type = {Editorial Material},
volume = {18},
year = {2009}
}
@article{ISI:000180587100029,
abstract = {A swept-wavelength source is created by connecting four elements in
series: a femtosecond fiber laser at 1.56 mum, a non-linear fiber, a
dispersive fiber and a tunable spectral bandpass filter. The 1.56-mum
pulses are converted to supercontinuum (1.1-2.2 mum) pulses by the
non-linear fiber, and these broadband pulses are stretched and arranged
into wavelength scans by the dispersive fiber. The tunable bandpass
filter is used to select a portion of the super-continuum as a
scan-wavelength output. A variety of scan characteristics are possible
using this approach. As an example, an output with an effective
linewidth of approximately 1 cm(-1) is scanned from 1350-1550 nm every
20 us. Compared to previous scanning benchmarks of approximately 1
nm/mus, such broad, rapid scans offer new capabilities: a gas sensing
application is demonstrated by monitoring absorption bands of H2O, CO2,
C2H2 and C2H6O at a pressure of 10 bar.},
address = {175 FIFTH AVE, NEW YORK, NY 10010 USA},
author = {Sanders, S T},
doi = {10.1007/s00340-002-1044-z},
issn = {0946-2171},
journal = {APPLIED PHYSICS B-LASERS AND OPTICS},
month = {nov},
number = {6-7},
pages = {799--802},
publisher = {SPRINGER-VERLAG},
title = {{Wavelength-agile fiber laser using group-velocity dispersion of pulsed super-continua and application to broadband absorption spectroscopy}},
type = {Article},
volume = {75},
year = {2002}
}
@article{ISI:000235093400003,
abstract = {In the last decades significant changes in the manufacturing environment
have been noticed: moving from a local economy towards a global economy,
with markets asking for products with higher quality at lower costs,
highly customised and with short life cycle. In these circumstances, the
challenge is to develop manufacturing control systems with intelligence
capabilities, fast adaptation to the environment changes and more
robustness against the occurrence of disturbances. This paper presents
an agile and adaptive manufacturing control architecture that addresses
the need for the fast reaction to disturbances at the shop floor level,
increasing the agility and flexibility of the enterprise, when it works
in volatile environments. The proposed architecture introduces an
adaptive control that balances dynamically between a more centralised
structure and a more decentralised one, allowing combining the global
production optimisation with agile reaction to unexpected disturbances.
(c) 2005 Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Leitao, P and Restivo, F},
doi = {10.1016/j.compind.2005.05.005},
issn = {0166-3615},
journal = {COMPUTERS IN INDUSTRY},
keywords = {intelligent manufacturing control; holonic manufac},
month = {feb},
number = {2},
pages = {121--130},
publisher = {ELSEVIER SCIENCE BV},
title = {{ADACOR: A holonic architecture for agile and adaptive manufacturing control}},
type = {Article},
volume = {57},
year = {2006}
}
@article{ISI:000323654900005,
abstract = {We describe a prototype 75 g micro quadrotor with onboard attitude
estimation and control that operates autonomously with an external
localization system. The motivation for designing quadrotors at this
scale comes from two observations. First, the agility of the robot
increases with a reduction in size, a fact that is supported by
experimental results in this paper. Second, smaller robots are able to
operate in tight formations in constrained, indoor environments. We
describe the hardware and software used to operate the vehicle as well
our dynamic model. We also discuss the aerodynamics of vertical flight
and the contribution of ground effect to the vehicle performance.
Finally, we discuss architecture and algorithms to coordinate a team of
these quadrotors, and provide experimental results for a team of 20
micro quadrotors.},
address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
author = {Kushleyev, Alex and Mellinger, Daniel and Powers, Caitlin and Kumar, Vijay},
doi = {10.1007/s10514-013-9349-9},
issn = {0929-5593},
journal = {AUTONOMOUS ROBOTS},
keywords = {Micro aerial vehicles; Quadrotors; Trajectory gene},
month = {nov},
number = {4, SI},
pages = {287--300},
publisher = {SPRINGER},
title = {{Towards a swarm of agile micro quadrotors}},
type = {Article},
volume = {35},
year = {2013}
}
@article{ISI:000184714000004,
abstract = {The paper presents the background to why some manufacturing
organisations require a combination of agile and lean characteristics in
their manufacturing organisations. The paper also describes the
development of the virtual group (VG) concept, which is the application
of virtual cells to functional layouts. VGs enable the appropriate
application of lean and agile concepts to different stages of production
within a factory. The identification of VGs is achieved through the use
of a methodology called enhanced production flow analysis (EPFA), which
is described together with how it differs from Burbidge's PFA. Finally
the results of two case studies are presented which tested the ability
of EFPA to identify VGs, and assess its usability. (C) 2003 Elsevier
B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
annote = {16th International Conference on Production Research (ICPR), CZECH TECH
UNIV, PRAGUE, CZECH REPUBLIC, AUG, 2001},
author = {Prince, J and Kay, J M},
doi = {10.1016/S0925-5273(03)0118-X},
institution = {Czech Assoc Sci {\&} Tech Soc},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {functional layout; lean; agile; virtual groups},
month = {sep},
number = {3},
pages = {305--318},
publisher = {ELSEVIER SCIENCE BV},
title = {{Combining lean and agile characteristics: Creation of virtual groups by enhanced production flow analysis}},
type = {Article; Proceedings Paper},
volume = {85},
year = {2003}
}
@article{ISI:000255490900012,
abstract = {AGILE is an Italian Space Agency mission dedicated to the exploration of
the gamma-ray Universe. The AGILE, very innovative instrument, combines
for the first time a gamma-ray imager (sensitive in the range 30 MeV-50
GeV) and a hard X-ray imager (sensitive in the range 18-60 keV). An
optimal angular resolution and very large fields of view are obtained by
the use of state-of-the-art Silicon detectors integrated in a very
compact instrument. AGILE was successfully launched on April 23, 2007
from the Indian base of Sriharikota and was inserted in an optimal
low-particle background equatorial orbit. AGILE will provide crucial
data for the study of Active Galactic Nuclei, Gamma-Ray Bursts,
unidentified gamma-ray sources, galactic compact objects, supernova
remnants, TeV sources, and fundamental physics by microsecond timing.
The AGILE Cycle-1 pointing program started on 2007 December 1, and is
open to the international community through a Guest Observer Program.
(c) 2008 Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
annote = {1st Roma International Conference on Astroparticle Physics, Rome, ITALY,
JUN 20-22, 2007},
author = {Tavani, M and Barbiellini, G and Argan, A and Bulgarelli, A and Caraveo, P and Chen, A and Cocco, V and Costa, E and {De Paris}, G and {Del Monte}, E and {Di Cocco}, G and Donnarumma, I and Feroci, M and Florini, M and Froysland, T and Fuschino, F and Galli, M and Gianotti, F and Giuliani, A and Evangelista, Y and Labanti, C and Lapshov, I and Lazzarotto, F and Lipari, P and Longo, F and Marisaldi, M and Mastropietro, M and Mauri, F and Mereghetti, S and Morelli, E and Morselli, A and Pacciani, L and Pellizzoni, A and Perotti, F and Picozza, P and Pontoni, C and Porrovecchio, G and Prest, M and Pucella, G and Rapisarda, M and Rossi, E and Rubini, A and Soffitta, P and Trifoglio, M and Trois, A and Vallazza, E and Vercellone, S and Zarnbra, A and Zanello, D and Giommi, P and Antonelli, A and Pittori, C},
doi = {10.1016/j.nima.2008.01.023},
issn = {0168-9002},
journal = {NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
keywords = {astronomical and space-research instrumentation; a},
month = {apr},
number = {1-2},
pages = {52--62},
publisher = {ELSEVIER SCIENCE BV},
title = {{The AGILE space mission}},
type = {Article; Proceedings Paper},
volume = {588},
year = {2008}
}
@article{ISI:000080462600004,
abstract = {Agile manufacturing, a recently popularised concept, has been advocated
as the 21st century manufacturing paradigm. It is seen as the winning
strategy to be adopted by manufacturers bracing. themselves for dramatic
performance enhancements to become national and international leaders in
an increasingly competitive market of fast changing customer
requirements. This paper identifies the drivers of agility and discusses
the portfolio of competitive advantages that have emerged over time as a
result of the changing requirements of manufacturing. The need to
achieve the competitive advantages of manufacturing in synergy and
without trade-offs is fundamental to the agile paradigm. To further the
understanding of agility, this paper reviews the meaning of agility from
different perspectives and suggests a comprehensive definition which can
be adopted as a working definition by practitioners. Four underlining
concepts of agility has emerged from the working definition and the
paper presents a representation of these concepts and their
interactions. Finally, the paper highlights some of the key enablers of
agility and identifies potential future research directions. (C) 1999
Elsevier Science B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Yusuf, Y Y and Sarhadi, M and Gunasekaran, A},
doi = {10.1016/S0925-5273(98)00219-9},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {agility; drivers; concepts; attributes; enablers},
month = {may},
number = {1-2},
pages = {33--43},
publisher = {ELSEVIER SCIENCE BV},
title = {{Agile manufacturing: The drivers, concepts and attributes}},
type = {Article},
volume = {62},
year = {1999}
}
@article{ISI:000241037900009,
address = {1515 BROADWAY, NEW YORK, NY 10036 USA},
author = {Ramesh, Balasubramaniam and Cao, Lan and Mohan, Kannan and Xu, Peng},
doi = {10.1145/1164394.1164418},
issn = {0001-0782},
journal = {COMMUNICATIONS OF THE ACM},
month = {oct},
number = {10},
pages = {41--46},
publisher = {ASSOC COMPUTING MACHINERY},
title = {{Can distributed software development be agile?}},
type = {Article},
volume = {49},
year = {2006}
}
@inproceedings{ISI:000183140900022,
abstract = {Agile software development methods have caught the attention of software
engineers and researchers worldwide. Scientific research is yet scarce.
This paper reports results from a study, which aims to organize, analyze
and make sense out of the dispersed field of agile software development
methods. The comparative analysis is performed using the method's
life-cycle coverage, project management support, type of practical
guidance, fitness-for-use and empirical evidence as the analytical
lenses. The results show that agile software development methods,
without rationalization, cover certain/different phases of the software
development life-cycle and most of them do not offer adequate support
for project management. Yet, many methods still attempt to strive for
universal solutions (as opposed to situation appropriate) and the
empirical evidence is still very limited. Based on the results, new
directions are suggested In principal, it is suggested to place emphasis
on methodological quality - not method quantity.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1264 USA},
annote = {25th International Conference on Software Engineering (ICSE 2003),
PORTLAND, OR, MAY 03-10, 2003},
author = {Abrahamsson, P and Warsta, J and Siponen, M T and Ronkainen, J},
booktitle = {25TH INTERNATIONAL CONFERENCE ON SOFTWARE ENGINEERING, PROCEEDINGS},
doi = {10.1109/ICSE.2003.1201204},
isbn = {0-7695-1877-X},
issn = {0270-5257},
organization = {IEEE; IEEE Comp Soc, Tech Council Software Engn; ACM; ACM SIGSOFT; IBM; NORTHROP GRUMMAN Space Technol; BMW; NOKIA; SUN Microsyst; DaimlerChrysler; Microsoft Res},
pages = {244--254},
publisher = {IEEE COMPUTER SOC},
series = {International Conference on Software Engineering},
title = {{New directions on agile methods: A comparative analysis}},
type = {Proceedings Paper},
year = {2003}
}
@article{ISI:000251595500020,
abstract = {Weather radars with conventional antenna cannot provide desired volume
scan updates at intervals of one minute or less, which is essential for
significant improvement in warning lead time of impending storm hazards.
The agile-beam multimission phased array radar (MPAR) discussed herein
is one potential candidate that can provide faster scanning. It also
offers a unique potential for multipurpose use to not only sample
weather, but support air traffic needs and track noncooperative
airplanes, thus making it an affordable option. After introducing the
basic idea behind electronic beam steering, the needs for frequent
observations of convective weather are explained. Then, advantages of
the phased array radar (PAR) for weather monitoring and improving data
quality are examined. To explore and develop weather-related
applications of the PAR, a National Weather Radar Testbed (NWRT) has
been established in Norman, Oklahoma. The NWRT's main purpose is to
address the advanced capabilities anticipated within the next decade so
that these could be projected to a possible network of future weather
radars. Examples of data illustrating advantages of this advanced radar
are shown, and forthcoming plans are discussed.},
address = {45 BEACON ST, BOSTON, MA 02108-3693 USA},
author = {Zrnic, D S and Kimpel, J F and Forsyth, D E and Shapiro, A and Crain, G and Ferek, R and Heimmer, J and Benner, W and McNellis, T J and Vogt, R J},
doi = {10.1175/BAMS-88-11-1753},
issn = {0003-0007},
journal = {BULLETIN OF THE AMERICAN METEOROLOGICAL SOCIETY},
month = {nov},
number = {11},
pages = {1753+},
publisher = {AMER METEOROLOGICAL SOC},
title = {{Agile-beam phased array radar for weather observations}},
type = {Article},
volume = {88},
year = {2007}
}
@article{ISI:000173405800012,
abstract = {Study results of developing an attitude control system for agile
spacecraft that require rapid retargeting and fast transient settling
are presented. In particular, a nonlinear feedback control logic is
developed for large-angle, rapid multitarget acquisition and pointing
maneuvers subject to various physical constraints, including actuator
saturation, slew rate limit, and control bandwidth limit. The rapid
multitarget acquisition and pointing capability of the proposed attitude
control system is demonstrated for an agile spacecraft equipped with
redundant single-gimbal control moment gyros. A realistic case of
pointing the line of sight of an imaging satellite in low Earth orbit
toward multiple targets on the ground is also briefly discussed.},
address = {1801 ALEXANDER BELL DRIVE, STE 500, RESTON, VA 22091 USA},
author = {Wie, B and Bailey, D and Heiberg, C},
doi = {10.2514/2.4854},
issn = {0731-5090},
journal = {JOURNAL OF GUIDANCE CONTROL AND DYNAMICS},
number = {1},
pages = {96--104},
publisher = {AMER INST AERONAUT ASTRONAUT},
title = {{Rapid multitarget acquisition and pointing control of agile spacecraft}},
type = {Article},
volume = {25},
year = {2002}
}
@article{ISI:000080462600013,
abstract = {The business environment is one which is ever more demanding on
companies, due to its sheer dynamism, which means that they are
constantly having to improve their manufacturing performance.
Organisations are continuously having to cope with changing markets that
are unpredictable and more diversified, increasing global competition
and ever changing customer demands. Companies now have to be able to not
only predict variations and changes within the market and socio-economic
and political environments but must also be able to adapt and change in
accordance with these environments. As a result, this demands that an
organisation develops and sustains an inherent ability to continuously
change. Such a demand can be met by adopting the management philosophy
of agile manufacturing. Tn embracing such an approach, there are a lot
of key concepts and enabling technologies that are required to be able
to implement agile manufacturing and many companies do not know how far
down the path they are towards becoming agile manufacturing
organisations. Hence, in providing a deeper understanding, this paper
proposes a conceptual model, based on joint research, which has been
developed to identify where UK's best practice companies are in their
quest to become agile manufacturing organisations. In support of this, a
questionnaire has been developed and completed by best practitioners of
manufacturing, to assess the model, and establish whether they are
making progress to becoming agile manufacturing organisations. (C) 1999
Elsevier Science B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Sharp, J M and Irani, Z and Desai, S},
doi = {10.1016/S0925-5273(98)00228-X},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {manufacturing; agility; conceptual model},
month = {may},
number = {1-2},
pages = {155--169},
publisher = {ELSEVIER SCIENCE BV},
title = {{Working towards agile manufacturing in the UK industry}},
type = {Article},
volume = {62},
year = {1999}
}
@article{ISI:000173128600021,
abstract = {Although many of their advocates consider the agile and plan-driven
software development methods polar opposites, synthesizing the two can
provide developers with a comprehensive spectrum of tools and options.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Boehm, B},
doi = {10.1109/2.976920},
issn = {0018-9162},
journal = {COMPUTER},
month = {jan},
number = {1},
pages = {64+},
publisher = {IEEE COMPUTER SOC},
title = {{Get ready for agile methods, with care}},
type = {Article},
volume = {35},
year = {2002}
}
@article{ISI:000222980400008,
abstract = {Changing customer and technological requirements force manufacturers to
develop agile supply chain capabilities in order to be competitive.
Therefore, several companies are stressing flexibility and agility in
order to respond, real time, to the unique needs of customers and
markets. However, the resource competencies required are often difficult
to mobilise and retain by single companies. It is therefore imperative
for companies to co-operate and leverage complementary competencies. To
this end, legally separate and spatially distributed companies are
becoming integrated through Internet-based technologies. The paper
reviews emerging patterns in supply chain integration. It also explores
the relationship between the emerging patterns and attainment of
competitive objectives. The results reported in the paper are based on
the data collected from a survey using the standard questionnaire. The
survey involved 600 companies in the UK, as part of a larger study of
agile manufacturing. The study was driven by a conceptual model, which
relates supply chain practices to competitive objectives. The study
involves the use of factor analysis to reduce research variables to a
few principal components. Subsequently, multiple regression was
conducted to study the relationship amongst the selected variables. The
results validate the proposed conceptual model and lend credence to
current thinking that supply chain integration is a vital tool for
competitive advantage. (C) 2003 Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Yusuf, Y Y and Gunasekaran, A and Adeleye, E O and Sivayoganathan, K},
doi = {10.1016/j.ejor.2003.08.022},
issn = {0377-2217},
journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH},
keywords = {agile manufacturing; agile supply chains; enterpri},
month = {dec},
number = {2},
pages = {379--392},
publisher = {ELSEVIER SCIENCE BV},
title = {{Agile supply chain capabilities: Determinants of competitive objectives}},
type = {Article},
volume = {159},
year = {2004}
}
@article{ISI:000245954500003,
abstract = {Empirical validation of software metrics suites to predict fault
proneness in object-oriented (OO) components is essential to ensure
their practical use in industrial settings. In this paper, we
empirically validate three OO metrics suites for their ability to
predict software quality in terms of fault-proneness: the Chidamber and
Kemerer (CK) metrics, Abreu's Metrics for Object-Oriented Design (
MOOD), and Bansiya and Davis' Quality Metrics for Object-Oriented Design
(QMOOD). Some CK class metrics have previously been shown to be good
predictors of initial OO software quality. However, the other two suites
have not been heavily validated except by their original proposers.
Here, we explore the ability of these three metrics suites to predict
fault-prone classes using defect data for six versions of Rhino, an
open-source implementation of JavaScript written in Java. We conclude
that the CK and QMOOD suites contain similar components and produce
statistical models that are effective in detecting error-prone classes.
We also conclude that the class components in the MOOD metrics suite are
not good class fault-proneness predictors. Analyzing multivariate binary
logistic regression models across six Rhino versions indicates these
models may be useful in assessing quality in OO classes produced using
modern highly iterative or agile software development processes.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Olague, Hector M and Etzkorn, Letha H and Gholston, Sampson and Quattlebaum, Stephen},
doi = {10.1109/TSE.2007.1015},
issn = {0098-5589},
journal = {IEEE TRANSACTIONS ON SOFTWARE ENGINEERING},
keywords = {object-oriented software metrics; object-oriented},
month = {jun},
number = {6},
pages = {402--419},
publisher = {IEEE COMPUTER SOC},
title = {{Empirical validation of three software metrics suites to predict fault-proneness of object-oriented classes developed using highly iterative or agile software development processes}},
type = {Article},
volume = {33},
year = {2007}
}
@article{ISI:000173405800014,
abstract = {Planning the path of an autonomous, agile vehicle in a dynamic
environment is a very complex problem, especially when the vehicle is
required to use its full maneuvering capabilities. Recent efforts aimed
at using randomized algorithms for planning the path of kinematic and
dynamic vehicles have demonstrated considerable potential for
implementation on future autonomous platforms. This paper builds upon
these efforts by proposing a randomized path planning architecture for
dynamical systems in the presence of fixed and moving obstacles. This
architecture addresses the dynamic constraints on the vehicle's motion,
and it provides at the same time a consistent decoupling between
low-level control and motion planning. The path planning algorithm
retains the convergence properties of its kinematic counterparts. System
safety is also addressed in the face of finite computation times by
analyzing the behavior of the algorithm when the available onboard
computation resources are limited, and the planning must be performed in
real time. The proposed algorithm can be applied to vehicles whose
dynamics are described either by ordinary differential equations or by
higher-level, hybrid representations. Simulation examples involving a
ground robot and a small autonomous helicopter are presented and
discussed.},
address = {1801 ALEXANDER BELL DRIVE, STE 500, RESTON, VA 22091-4344 USA},
author = {Frazzoli, E and Dahleh, M A and Feron, E},
doi = {10.2514/2.4856},
issn = {0731-5090},
journal = {JOURNAL OF GUIDANCE CONTROL AND DYNAMICS},
number = {1},
pages = {116--129},
publisher = {AMER INST AERONAUT ASTRONAUT},
title = {{Real-time motion planning for agile autonomous vehicles}},
type = {Article},
volume = {25},
year = {2002}
}
@article{ISI:000243915300010,
abstract = {This paper reports an agile VCO frequency calibration technique and its
application on a 10-GHz CMOS integer-N phase-locked loop. The proposed
calibration method accomplishes efficient search for an optimum VCO
discrete tuning curve among a group of frequency sub-bands. The agility
is attributed to a proposed frequency comparison technique which is
based on measuring the period difference between two signals. Other
mixed-signal circuits are also developed to facilitate this approach.
The PLL incorporating the proposed calibration technique is implemented
in a 0.18-mu m CMOS process. The measured PLL phase noise at 10 GHz is
-102 dBc/Hz at 1-MHz offset frequency and the reference spurs are lower
than -48 dBc. The PLL consumes 44 mW in the low-current mode. The
calibration time is less than 4 mu s.},
address = {445 HOES LANE, PISCATAWAY, NJ 08855 USA},
author = {Lin, Tsung-Hsien and Lai, Yu-Jen},
doi = {10.1109/JSSC.2006.889360},
issn = {0018-9200},
journal = {IEEE JOURNAL OF SOLID-STATE CIRCUITS},
keywords = {calibration; CMOS integrated circuits; frequency s},
month = {feb},
number = {2},
pages = {340--349},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{An agile VCO frequency-calibration technique for a 10-GHz CMOS PLL}},
type = {Article},
volume = {42},
year = {2007}
}
@article{ISI:000182753200046,
abstract = {AGILE (Light Imager for Gamma-ray Astrophysics) is the first small
scientific mission of ASI, the Italian Space Agency. It is a light (100
kg for the scientific instrument) satellite for the detection of
gamma-ray sources in the energy range 30 MeV-50 GeV within a large field
of view (1 of the sky). It is planned to be operational in the years
2003-2006, a period in which no other gamma-ray mission in the same
energy range is foreseen.
AGILE is made of a silicon tungsten tracker, a CsI(Tl) minicalorimeter
(1.5X(0)), an anticoincidence system of segmented plastic scintillators
and a X-ray imaging detector sensitive in the 10-40 keV range. The
tracker consists of 14 planes, each of them made of two layers of 16
single-sided, AC coupled, 410 mum thick, 9.5 x 9.5 cm(2) silicon
detectors with a readout pitch of 242 mum and a floating strip. The
readout ASIC is the TAA1, an analog-digital, low noise, self-triggering
ASIC used in a very low power configuration ({\textless}400 {\&}mu;W/channel) with
full analog readout. The trigger of the satellite is given by the
tracker. The total number of readout channels is around 43 000.
We present a detailed description of the tracker, its trigger and
readout logic, its assembly procedures and the prototype performance in
several testbeam periods at the CERN PS. (C) 2002 Elsevier Science B.V.
All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
annote = {10th International Workshop on Vertex Detectors, BRUNNEN, SWITZERLAND,
SEP 23-28, 2001},
author = {Prest, M and Barbiellini, G and Bordignon, G and Fedel, G and Liello, F and Longo, F and Pontoni, C and Vallazza, E},
doi = {10.1016/S0168-9002(02)02047-8},
issn = {0168-9002},
journal = {NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
keywords = {satellite; silicon detector; self-triggering; floa},
month = {mar},
number = {1},
pages = {280--287},
publisher = {ELSEVIER SCIENCE BV},
title = {{The AGILE silicon tracker: an innovative gamma-ray instrument for space}},
type = {Article; Proceedings Paper},
volume = {501},
year = {2003}
}
@article{ISI:000283606800005,
abstract = {Agile supply chains need to be highly flexible in order to reconfigure
quickly in response to changes in their environment. An effective
supplier selection process is essential for this. This paper develops a
model that helps overcome the information-processing difficulties
inherent in screening a large number of potential suppliers in the early
stages of the selection process. Based on radial basis function
artificial neural network (RBF-ANN), the model enables potential
suppliers to be assessed against multiple criteria using both
quantitative and qualitative measures. Its efficacy is illustrated using
empirical data from the Chinese electrical appliance and equipment
manufacturing industries. (C) 2009 Elsevier Ltd. All rights reserved.},
address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, OXON, ENGLAND},
author = {Luo, Xinxing and Wu, Chong and Rosenberg, Duska and Barnes, David},
doi = {10.1016/j.pursup.2009.05.004},
issn = {1478-4092},
journal = {JOURNAL OF PURCHASING AND SUPPLY MANAGEMENT},
keywords = {Supplier selection; Agile supply chain; Artificial},
month = {dec},
number = {4},
pages = {249--262},
publisher = {ELSEVIER SCI LTD},
title = {{Supplier selection in agile supply chains: An information-processing model and an illustration}},
type = {Article},
volume = {15},
year = {2009}
}
@article{ISI:000367200400009,
abstract = {Spectroscopic gas sensing and its applications to, for example, trace
detection or chemical kinetics, require ever more demanding measurement
times, acquisition rates, sensitivities, precisions and broad tuning
ranges. Here, we propose a new approach to near-infrared molecular
spectroscopy, utilizing advanced concepts of optical telecommunications
and supercontinuum photonics. We generate, without mode-locked lasers,
two frequency combs of slightly different repetition frequencies and
moderate, but rapidly tunable, spectral span. The output of a
frequency-agile continuous-wave laser is split and sent into two
electro-optic intensity modulators. Flat-top low-noise frequency combs
are produced by wave-breaking in a nonlinear optical fibre of normal
dispersion. With a dual-comb spectrometer, we record Doppler-limited
spectra spanning 60 GHz within 13 mu s and an 80 kHz refresh rate, at a
tuning speed of 10 nm s(-1). The sensitivity for weak absorption is
enhanced by a long gas-filled hollow-core fibre. New opportunities for
real-time diagnostics may be opened up, even outside the laboratory.},
address = {MACMILLAN BUILDING, 4 CRINAN ST, LONDON N1 9XW, ENGLAND},
author = {Millot, Guy and Pitois, Stephane and Yan, Ming and Hovhannisyan, Tatevik and Bendahmane, Abdelkrim and Haensch, Theodor W and Picque, Nathalie},
doi = {10.1038/NPHOTON.2015.250},
issn = {1749-4885},
journal = {NATURE PHOTONICS},
month = {jan},
number = {1},
pages = {27--U37},
publisher = {NATURE PUBLISHING GROUP},
title = {{Frequency-agile dual-comb spectroscopy}},
type = {Article},
volume = {10},
year = {2016}
}
@article{ISI:000077216500002,
abstract = {As product complexity and the rate of market change have dramatically
increased over the last years, firms find it increasingly difficult to
forecast product requirements in their development processes. This
article redefines the problem from one of improving forecasting to one
of increasing product development agility and thus reducing the need for
accurate long-term forecasts. It introduces the notion of development
flexibility, shows how it can be measured, and presents results from a
large empirical study on integrated systems development, which found
that projects using flexible technologies outperformed projects using
inflexible technologies by a factor of 2.2 (in person-months). Finally,
the article proposes three major strategies for introducing flexibility
into organizations. These strategies can help firms increase their
agility and position themselves to succeed in accelerating and more
turbulent markets.},
address = {GRAD SCH BUSINESS ADMIN, BERKELEY, CA 94720 USA},
author = {Thomke, S and Reinertsen, D},
doi = {10.2307/41165973},
issn = {0008-1256},
journal = {CALIFORNIA MANAGEMENT REVIEW},
number = {1},
pages = {8+},
publisher = {UNIV CALIF},
title = {{Agile product development: Managing development flexibility in uncertain environments}},
type = {Article},
volume = {41},
year = {1998}
}
@article{ISI:000256966600004,
abstract = {Agile software development practices such as eXtreme Programming (XP)
and SCRUM have increasingly been adopted to respond to the challenges of
volatile business environments, where the markets and technologies
evolve rapidly and present the unexpected. In spite of the encouraging
results so far, little is known about how agile practices affect
communication. This article presents the results from a study which
examined the impact of XP and SCRUM practices on communication within
software development teams and within the focal organization. The
research was carried out as a case study in F-Secure where two agile
software development projects were compared from the communication
perspective. The goal of the study is to increase the understanding of
communication in the context of agile software development: internally
among the developers and project leaders and in the interface between
the development team and stakeholders (i.e. customers, testers, other
development teams). The study shows that agile practices improve both
informal and formal communication. However, it further indicates that,
in larger development situations involving multiple external
stakeholders, a mismatch of adequate communication mechanisms can
sometimes even hinder the communication. The study highlights the fact
that hurdles and improvements in the communication process can both
affect the feature requirements and task subtask dependencies as
described in coordination theory. While the use of SCRUM and some XP
practices facilitate team and organizational communication of the
dependencies between product features and working tasks, the use of
agile practices requires that the team and organization use also
additional plan-driven practices to ensure the efficiency of external
communication between all the actors of software development.},
address = {VAN GODEWIJCKSTRAAT 30, 3311 GZ DORDRECHT, NETHERLANDS},
author = {Pikkarainen, M and Haikara, J and Salo, O and Abrahamsson, P and Still, J},
doi = {10.1007/s10664-008-9065-9},
issn = {1382-3256},
journal = {EMPIRICAL SOFTWARE ENGINEERING},
keywords = {agile software development practices; communicatio},
month = {jun},
number = {3},
pages = {303--337},
publisher = {SPRINGER},
title = {{The impact of agile practices on communication in software development}},
type = {Article},
volume = {13},
year = {2008}
}
@article{ISI:000245650200061,
abstract = {Agile Protein Interaction DataAnalyzer (APID) is an interactive
bioinformatics web tool developed to integrate and analyze in a unified
and comparative platform main currently known information about
protein-protein interactions demonstrated by specific small-scale or
large-scale experimental methods. At present, the application includes
information coming from five main source databases enclosing an unified
sever to explore {\textgreater} 35 000 different proteins and 111 000 different
proven interactions. The web includes search tools to query and browse
upon the data, allowing selection of the interaction pairs based in
calculated parameters that weight and qualify the reliability of each
given protein interaction. Such parameters are for the `proteins':
connectivity, cluster coefficient, Gene Ontology ( GO) functional
environment, GO environment enrichment; and for the `interactions':
number of methods, GO overlapping, iPfam domain-domain interaction. APID
also includes a graphic interactive tool to visualize selected
sub-networks and to navigate on them or along the whole interaction
network. The application is available open access at
http://bioinfow.dep.usal.es/apid/.},
address = {GREAT CLARENDON ST, OXFORD OX2 6DP, ENGLAND},
author = {Prieto, Carlos and Rivas, Javier De Las},
doi = {10.1093/nar/gkl128},
issn = {0305-1048},
journal = {NUCLEIC ACIDS RESEARCH},
month = {jul},
number = {SI},
pages = {W298--W302},
publisher = {OXFORD UNIV PRESS},
title = {{APID: Agile Protein Interaction DataAnalyzer}},
type = {Article},
volume = {34},
year = {2006}
}
@article{ISI:000229359800020,
address = {1515 BROADWAY, NEW YORK, NY 10036 USA},
author = {Nerur, S and Mahapatra, R and Mangalaraj, G},
doi = {10.1145/1060710.1060712},
issn = {0001-0782},
journal = {COMMUNICATIONS OF THE ACM},
month = {may},
number = {5},
pages = {72--78},
publisher = {ASSOC COMPUTING MACHINERY},
title = {{Challenges of emigrating to agile methodologies}},
type = {Article},
volume = {48},
year = {2005}
}
@article{ISI:000071490100010,
abstract = {In industrial management, the 1980s marked the end of the twentieth
century, an epoch dominated by US manufacturers, the alleged masters of
mass production. This system has now been outstripped in several dynamic
sectors by flexible/agile production. Increases in the pace of
technological progress, training and aspirations have made the modern
context so dynamic that firms which manage to harness the creativity and
initiative of a good part of their workforce have an advantage over
those that can only count on the input of their experts and managers. In
sectors undergoing relatively broad and rapid change, twenty-first
century firms must adopt a more flexible and innovative type of
organization to achieve manufacturing excellence.},
address = {HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
author = {Duguay, C R and Landry, S and Pasin, F},
doi = {10.1108/01443579710182936},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {agile production; flexible manufacturing systems;},
number = {11-12},
pages = {1183+},
publisher = {EMERALD GROUP PUBLISHING LIMITED},
title = {{From mass production to flexible/agile production}},
type = {Article},
volume = {17},
year = {1997}
}
@article{ISI:000235785800021,
abstract = {A new type of microwave radiometer detector has been developed that is
capable of identifying high and low levels of radio-frequency
interference (RFI) and of reducing or eliminating its effect on the
measured brightness temperatures. High-level, localized RFI can be
easily identified by its unnatural appearance in brightness temperature
imagery. Low-level or persistent RFI can be much more difficult to
identify and filter out. The agile digital detector (ADD) can
discriminate between RFI and natural thermal emission signals by
directly measuring higher order moments of the signal than the variance
that is traditionally measured. After detection, the ADD then uses
spectral filtering methods to selectively remove the RFI. ADD
performance is experimentally verified in controlled laboratory tests
and in the field near a commercial air traffic control radar. High-level
RFI is easily identified and removed. Very low level RFI contamination,
with power levels as low as the radiometric measurement uncertainty of
the radiometer, is also shown to be reliably detected and removed.},
address = {445 HOES LANE, PISCATAWAY, NJ 08855 USA},
author = {Ruf, C S and Gross, S M and Misra, S},
doi = {10.1109/TGRS.2005.861411},
issn = {0196-2892},
journal = {IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING},
keywords = {microwave radiometry; radio spectrum management},
month = {mar},
number = {3},
pages = {694--706},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{RFI detection and mitigation for microwave radiometry with an agile digital detector}},
type = {Article},
volume = {44},
year = {2006}
}
@article{ISI:000080462600009,
abstract = {As the lean thinking and agile manufacturing paradigms have been
developed there has been a tendency to view them in a progression and in
isolation. This article shows that this is too simplistic a view. The
use of either paradigm has to be combined with a total supply chain
strategy particularly considering market knowledge and positioning of
the decoupling point as agile manufacturing is best suited to satisfying
a fluctuating demand and lean manufacturing requires a level schedule.
This view is supported by consideration of a PC supply chain case study.
(C) 1999 Elsevier Science B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Naylor, J B and Naim, M M and Berry, D},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {agile manufacturing; lean thinking; supply chain m},
month = {may},
number = {1-2},
pages = {107--118},
publisher = {ELSEVIER SCIENCE BV},
title = {{Leagility: Integrating the lean and agile manufacturing paradigms in the total supply chain}},
type = {Article},
volume = {62},
year = {1999}
}
@article{ISI:000231943500006,
abstract = {While there are many claims for the successful use of extreme
programming (XP) and agile modeling (AM), and the proponents can often
be vocal in the extreme regarding their supposed benefits, research
evidence supporting proponents' claims is somewhat lacking. Currently,
the only research appearing to investigate the phenomena consists of two
prominent streams. A small number of case studies and experience reports
that generally promote the success of XP in various development
environments, and a well-established stream of research into pair
programming has generated results that in part support the idea of XP
Research into AM appears to be even more sparse than that for XP Case
studies, comparative analyses, and experience reports comprise the
majority of the research in the area, while very few empirical research
efforts have been conducted. This article reviews the state of research
in XP and AM, and recommends areas that could benefit from further
study. Since nearly all empirical XP research relates to pair
programming, a closer look into the unstudied XP core practices would be
beneficial, although interaction between related core practice areas
could confound such efforts. It might also be possible to group related
core XP concepts and study the groups individually. Finally, there are
those who claim that XP and AM, or even agility in general, are really
nothing more than a repackaging of old concepts. This claim needs to be
investigated.},
address = {701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA},
author = {Erickson, J and Lyytinen, K and Siau, K},
doi = {10.4018/jdm.2005100105},
issn = {1063-8016},
journal = {JOURNAL OF DATABASE MANAGEMENT},
keywords = {agile software development; agility; agile modelin},
number = {4},
pages = {88--100},
publisher = {IGI PUBL},
title = {{Agile modeling, agile software development, and extreme programming: The state of research}},
type = {Review},
volume = {16},
year = {2005}
}
@article{ISI:000255824100022,
abstract = {Background: Capsule endoscopy (CE) of the small bowel has become a
standard diagnostic tool, but there have been concerns regarding the
risk of capsule retention in certain high-risk groups. The Agile patency
system, an ingestible and dissolvable capsule with an external scanner,
was developed to allow physicians to perform CE with greater confidence
that the capsule will be safely excreted in patients at risk for capsule
retention.
Objective: Our purpose was to assess the ability of the device to help
physicians identify which patients with known strictures may safely
undergo CE.
Design: Patients with known strictures ingested the new patency capsule
and under-went periodic scanning until it was excreted. The intestinal
tract was considered to be sufficiently patent if the capsule was
excreted intact or if the capsule was not detected by the scanner at 30
hours after ingestion. if patency was established, then standard CE was
performed.
Setting: International multicenter study.
Patients: A total of 106 patients with known strictures.
Intervention: Agile patency system.
Main Outcome Measurements: Performance and safety of Agile patency
system.
Results: A total of 106 patients ingested the patency capsule.
Fifty-nine (56{\%}) excreted it intact and subsequently underwent CE.
There were no cases of capsule retention. Significant findings on CE
were found in 24 (41{\%}). There were 3 severe adverse events.
Conclusions: These results suggest that the Agile patency system is a
useful tool for physicians to use before CE in patients with strictures
to avoid retention. This group of patients may have a high yield of
clinically significant findings at CE. This capsule may determine
whether patients who have a contraindication to CE may safely undergo CE
and obtain useful diagnostic information.},
address = {360 PARK AVENUE SOUTH, NEW YORK, NY 10010-1710 USA},
annote = {3rd International Symposium on Natural Orifice Translumenal Endoscopic
Surgery, San Francisco, CA, JUL 10, 2008},
author = {Herrerias, Juan M and Leighton, Jonathan A and Costamagna, Guido and Infantolino, Anthony and Eliakim, Rami and Fischer, Doron and Rubin, David T and Manten, Howard D and Scapa, Eitan and Morgan, Douglas R and Bergwerk, Ari J and Koslowsky, Binyamin and Adler, Samuel N},
doi = {10.1016/j.gie.2007.10.063},
issn = {0016-5107},
journal = {GASTROINTESTINAL ENDOSCOPY},
month = {may},
number = {6},
pages = {902--909},
publisher = {MOSBY-ELSEVIER},
title = {{Agile patency system eliminates risk of capsule retention in patients with known intestinal strictures who undergo capsule endoscopy}},
type = {Article; Proceedings Paper},
volume = {67},
year = {2008}
}
@article{ISI:000073433700004,
abstract = {Tougher competitive situations have led to increasing attention being
paid to customer satisfaction, of which timely and customized services
are the key concepts. As the product life cycle becomes shortened, high
product quality becomes necessary for survival. Markets become highly
diversified and global, and continuous and unexpected change become the
key factors for success. The need for a method of rapidly and
cost-effectively developing products, production facilities and
supporting software, including design, process planning and shop floor
control system has led to the concept of agile manufacturing.
Agile manufacturing can be defined as the capability to survive and
prosper in a competitive environment of continuous and unpredictable
change by reacting quickly and effectively to changing markets, driven
by customer-designed products and services. This article details the key
concepts and enablers of agile manufacturing. The key enablers of agile
manufacturing include: (i) virtual enterprise formation tools/metrics;
(ii) physically distributed manufacturing architecture and teams; (iii)
rapid partnership formation tools/metrics; (iv) concurrent engineering;
(v) integrated product/production/business information system; (vi)
rapid prototyping tools; and (vii) electronic commerce. A conceptual
framework for the development of an agile manufacturing system and
future research directions are presented in this paper. This framework
takes into account the customization and system integration with the
help of business process redesign, legal issues, concurrent engineering,
computer-integrated manufacturing, cost management, total quality
management and information technology.},
address = {ONE GUNPOWDER SQUARE, LONDON EC4A 3DE, ENGLAND},
author = {Gunasekaran, A},
doi = {10.1080/002075498193291},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month = {may},
number = {5},
pages = {1223--1247},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{Agile manufacturing: enablers and an implementation framework}},
type = {Article},
volume = {36},
year = {1998}
}
@article{ISI:000238914800019,
abstract = {A novel basis for beam steering with electrowetting microprisms (EMPs)
is reported. EMPs utilize electrowetting modulation of liquid contact
angle in order to mimic the refractive behavior for various classical
prism geometries. Continuous beam steering through an angle of 14
degrees (+/- 7 degrees) has been demonstrated with a liquid index of
n=1.359. Experimental results are well-matched to theoretical behavior
up to the point of electrowetting contact-angle saturation. Projections
show that use of higher index liquids (n similar to 1.6) will result in
steering through similar to 30 degrees(+/- 15 degrees). Fundamental
factors defining achievable deflection range, and issues for Ladar use,
are reviewed. This approach is capable of good switching speed (similar
to ms), polarization independent operation, modulation of beam
field-of-view (lensing), and high steering efficiency that is
independent of deflection angle. (c) 2006 Optical Society of America.},
address = {2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036 USA},
author = {Smith, Neil R and Abeysinghe, Don C and Haus, Joseph W and Heikenfeld, Jason},
doi = {10.1364/OE.14.006557},
issn = {1094-4087},
journal = {OPTICS EXPRESS},
month = {jul},
number = {14},
pages = {6557--6563},
publisher = {OPTICAL SOC AMER},
title = {{Agile wide-angle beam steering with electrowetting microprisms}},
type = {Article},
volume = {14},
year = {2006}
}
@article{ISI:000228020200008,
abstract = {Western populations of the Italian agile frog (Rana latastei) experience
widespread genetic depletion. Based on population genetic theory,
molecular models of immunity and previous empirical studies, population
genetic depletion predicts increased susceptibility of populations to
emergent pathogens. We experimentally compared susceptibility of R.
latastei populations upon exposure to an emerging strain of Ranavirus,
frog virus 3 (FV3), using six populations spanning the geographical
range and range of population genetic diversity found in nature. Our
findings confirm this prediction, suggesting that the loss of genetic
diversity accompanying range expansion and population isolation is
coincident with increased mortality risk from an emergent pathogen. Loss
of heterozygosity and escape from selection imposed by immunologically
cross-reactive pathogens may potentially generate range-wide variation
in disease resistance.},
address = {9600 GARSINGTON RD, OXFORD OX4 2DG, OXON, ENGLAND},
author = {Pearman, P B and Garner, T W J},
doi = {10.1111/j.1461-0248.2005.00735.x},
issn = {1461-023X},
journal = {ECOLOGY LETTERS},
keywords = {amphibian declines; disease emergence; frog virus},
month = {apr},
number = {4},
pages = {401--408},
publisher = {BLACKWELL PUBLISHING LTD},
title = {{Susceptibility of Italian agile frog populations to an emerging strain of Ranavirus parallels population genetic diversity}},
type = {Article},
volume = {8},
year = {2005}
}
@article{ISI:000308110200009,
abstract = {In modern business environments, an effective supply chain management
(SCM) is crucial to business continuity. Competition between supply
chains (SC) has replaced the traditional competition between companies.
Lean, Agile, Resilient and Green (LARG) paradigms are advocated as the
foundation of a competitive SCM. To make a supply chain more
competitive, capable of responding to the demands of customers with
agility and capable of responding effectively to unexpected disturbance,
in conjugation with environmental responsibilities and the necessity to
eliminate processes that add no value, companies must implement a set of
LARG SCM practices and key performance indicators (KPI) to measure their
influence on the SC performance. However, the selection of the best LARG
SCM practices and KPIs is a complex problem, involving dependencies and
feedbacks. This paper proposes an integrated LARG analytic network
process (ANP) model to support decision-making in choosing the most
appropriate practices and KPIs to be implemented by companies in an SC.
To validate the model in an exploratory approach, a case study in an
automaker supply chain is presented.},
address = {4 PARK SQUARE, MILTON PARK, ABINGDON OX14 4RN, OXON, ENGLAND},
author = {Cabral, Izunildo and Grilo, Antonio and Cruz-Machado, Virgilio},
doi = {10.1080/00207543.2012.657970},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
keywords = {lean; agile; resilient; green; SCM; ANP},
number = {17, SI},
pages = {4830--4845},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{A decision-making model for Lean, Agile, Resilient and Green supply chain management}},
type = {Article},
volume = {50},
year = {2012}
}
@article{ISI:000275321000002,
abstract = {We report the detection by the Astrorivelatore Gamma a Immagini Leggero
(AGILE) satellite of terrestrial gamma ray flashes (TGFs) obtained with
the minicalorimeter (MCAL) detector operating in the energy range
0.3-100 MeV. We select events typically lasting a few milliseconds with
spectral and directional selections consistent with the TGF
characteristics previously reported by other space missions. During the
period 1 June 2008 to 31 March 2009 we detect 34 high-confidence events
showing millisecond durations and a geographical distribution peaked
over continental Africa and Southeast Asia. For the first time,
AGILE-MCAL detects photons associated with TGF events up to 40 MeV. We
determine the cumulative spectral properties of the spectrum in the
range 0.5-40 MeV, which can be effectively described by a Bremsstrahlung
spectrum. We find that both the TGF cumulative spectral properties and
their geographical distribution are in good agreement with the Reuven
Ramaty High Energy Solar Spectroscopic Imager (RHESSI) results.},
address = {2000 FLORIDA AVE NW, WASHINGTON, DC 20009 USA},
author = {Marisaldi, M and Fuschino, F and Labanti, C and Galli, M and Longo, F and {Del Monte}, E and Barbiellini, G and Tavani, M and Giuliani, A and Moretti, E and Vercellone, S and Costa, E and Cutini, S and Donnarumma, I and Evangelista, Y and Feroci, M and Lapshov, I and Lazzarotto, F and Lipari, P and Mereghetti, S and Pacciani, L and Rapisarda, M and Soffitta, P and Trifoglio, M and Argan, A and Boffelli, F and Bulgarelli, A and Caraveo, P and Cattaneo, P W and Chen, A and Cocco, V and D'Ammando, F and {De Paris}, G and {Di Cocco}, G and {Di Persio}, G and Ferrari, A and Fiorini, M and Froysland, T and Gianotti, F and Morselli, A and Pellizzoni, A and Perotti, F and Picozza, P and Piano, G and Pilia, M and Prest, M and Pucella, G and Rappoldi, A and Rubini, A and Sabatini, S and Striani, E and Trois, A and Vallazza, E and Vittorini, V and Zambra, A and Zanello, D and Antonelli, L A and Colafrancesco, S and Gasparrini, D and Giommi, P and Pittori, C and Preger, B and Santolamazza, P and Verrecchia, F and Salotti, L},
doi = {10.1029/2009JA014502},
issn = {0148-0227},
journal = {JOURNAL OF GEOPHYSICAL RESEARCH-SPACE PHYSICS},
month = {mar},
publisher = {AMER GEOPHYSICAL UNION},
title = {{Detection of terrestrial gamma ray flashes up to 40 MeV by the AGILE satellite}},
type = {Article},
volume = {115},
year = {2010}
}
@article{ISI:000271681800011,
abstract = {Agile software development (ASD) is an emerging approach in software
engineering, initially advocated by a group of 17 software professionals
who practice a set of ``lightweight{\{}''{\}} methods, and share a common set
of values of software development. In this paper, we advance the
state-of-the-art of the research in this area by conducting a
survey-based ex-post-facto study for identifying factors from the
perspective of the ASD practitioners that will influence the success of
projects that adopt ASD practices. In this paper, we describe a
hypothetical success factors framework we developed to address our
research question, the hypotheses we conjectured, the research
methodology, the data analysis techniques we used to validate the
hypotheses, and the results we obtained from data analysis. The study
was conducted using an unprecedentedly large-scale survey-based
methodology, consisting of respondents who practice ASD and who had
experience practicing plan-driven software development in the past. The
study indicates that nine of the 14 hypothesized factors have
statistically significant relationship with ``Success{\{}''{\}}. The important
success factors that were found are: customer satisfaction, customer
collaboration, customer commitment, decision time, corporate culture,
control, personal characteristics, societal culture, and training and
learning. (C) 2009 Elsevier Inc. All rights reserved.},
address = {360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
author = {Misra, Subhas Chandra and Kumar, Vinod and Kumar, Uma},
doi = {10.1016/j.jss.2009.05.052},
issn = {0164-1212},
journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
keywords = {Success factors; Agile software},
month = {nov},
number = {11},
pages = {1869--1890},
publisher = {ELSEVIER SCIENCE INC},
title = {{Identifying some important success factors in adopting agile software development practices}},
type = {Article},
volume = {82},
year = {2009}
}
@article{ISI:A1997YE58900002,
abstract = {Agile manufacturing is a new expression that is used to represent the
ability of a producer of goods and services to thrive in the face of
continuous change. These changes can occur in markets, in technologies,
in business relationships and in all facets of the business enterprise.
This paper discusses the genesis of several of the Agile Manufacturing
Research Institutes (AMRIs) and their on-going activities and results to
dale. A vision for agile manufacturing research is articulated and
initial accomplishments identified. Additional research needs are also
discussed.},
address = {2-6 BOUNDARY ROW, LONDON, ENGLAND SE1 8HN},
author = {DeVor, R and Graves, R and Mills, J J},
issn = {0740-817X},
journal = {IIE TRANSACTIONS},
month = {oct},
number = {10},
pages = {813--823},
publisher = {CHAPMAN HALL LTD},
title = {{Agile manufacturing research: accomplishments and opportunities}},
type = {Article},
volume = {29},
year = {1997}
}
@article{ISI:000231388800019,
abstract = {Agile software development processes have shown positive impacts on
cost, schedule, and customer satisfaction. However, most implementations
of agile processes have been in smaller-scale, software-only
environments. In March 2004, a group of researchers and practitioners
addressed the implementation of agile processes in large
systems-engineering projects that rely on traditional development
processes and artifacts. They identified three management challenge
areas. Here, the authors discuss numerous ways in which to address them.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Boehm, B and Turner, R},
doi = {10.1109/MS.2005.129},
issn = {0740-7459},
journal = {IEEE SOFTWARE},
number = {5},
pages = {30+},
publisher = {IEEE COMPUTER SOC},
title = {{Management challanges to implementing Agile Processes in traditional development organizations}},
type = {Article},
volume = {22},
year = {2005}
}
@article{ISI:000078667300001,
abstract = {The objective of this paper is to introduce a decision methodology and
structure for manufacturing land organizational) agility improvement.
The methodology allows for the evaluation of alternatives (e.g.
projects) to help organizations become more agile, with a specific
objective of improving the manufacturing business processes. An agile
enterprise is one whose processes are designed to respond effectively to
unanticipated change. One of the difficulties in designing and analysing
business processes, in general, is that they are operational designs
that need to incorporate strategic attributes. In order to evaluate
alternatives that impact the business processes, a networked
hierarchical analysis model based on the various characteristics of
agility, is proposed. This evaluation model will be based on the
analytic network process methodology for solving complex and systemic
decisions. An actual example of a small manufacturing enterprise
provides some managerial insights into the methodology.},
address = {ONE GUNPOWDER SQUARE, LONDON EC4A 3DE, ENGLAND},
author = {Meade, L M and Sarkis, J},
doi = {10.1080/002075499191751},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month = {jan},
number = {2},
pages = {241--261},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{Analyzing organizational project alternatives for agile manufacturing processes: an analytical network approach}},
type = {Article},
volume = {37},
year = {1999}
}
@article{ISI:000180224400015,
abstract = {In response to changing success factors and environmental pressures,
companies are aspiring to break out of mass production and become lean
or agile. Whereas performance enhancements of lean practices have been
demonstrated, it is now the case that markets have become increasingly
characterized by turbulence, a situation in which reliance on lean
practices is insufficient, and that survival requires adoption of agile
practices. As a result, a comparative study of lean and agile
manufacturing with a related survey of current practices in the UK was
carried out, the results of which is presented in this paper. The paper
explored the threats to lean and the drivers of agile manufacturing.
Using data from a questionnaire survey, four hypotheses were tested,
which was indicative of the benefits of agile manufacturing. In contrast
to their lean counterparts, agile companies paid attention to a wider
range of competitive capabilities. They therefore had a lower range of
mean scores on competitive capabilities. Independent sample tests of
significant difference in business performance measures revealed that
the agile companies consistently outperformed their lean competitors on
all business performance measures studied. In addition, a wider range of
competitive capabilities and performance measures of the agile companies
correlated significantly and positively whilst such correlation was
observed for only a narrow range of capabilities and performance
measures for lean companies. The results suggest that competing
simultaneously on multiple competitive capabilities enhance performance
better than a rather narrow focus on cost and quality.},
address = {4 PARK SQUARE, MILTON PARK,, ABINGDON OX14 4RN, OXON, ENGLAND},
author = {Yusuf, Y Y and Adeleye, E O},
doi = {10.1080/00207540210157141},
issn = {0020-7543},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH},
month = {nov},
number = {17},
pages = {4545--4562},
publisher = {TAYLOR {\&} FRANCIS LTD},
title = {{A comparative study of lean and agile manufacturing with a related survey of current practices in the UK}},
type = {Article},
volume = {40},
year = {2002}
}
@article{ISI:000080462600008,
abstract = {Agile manufacturing (AM) is a new concept in manufacturing intended to
improve the competitiveness of firms. Manufacturing processes based on
AM are characterized by customer-supplier integrated process for product
design, manufacturing, marketing, and support services. This needs
decision-making at functional knowledge levels, stable unit costs,
flexible manufacturing, easy access to integrated data, and modular
production facilities. Agile manufacturing requires enriching of the
customer, co-operating with competitors, organizing to manage change,
uncertainty and complexity, and leveraging people and information. In
the recent years, a number of research papers have been published in the
area of AM. However, a framework for the development of AM has not
received due attention from both researchers and practitioners.
Realizing the importance of agile manufacturing in the 21st century
manufacturing competitiveness, an attempt has been made in this paper to
review the literature available on AM with the objective to: (i)
identify key strategies and techniques of AM,(ii) suggest some future
research directions and (iii) develop a framework for the development of
agile manufacturing systems (AMSs) along four key dimensions which
include strategies, technologies, systems and people. (C) 1999 Elsevier
Science B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Gunasekaran, A},
doi = {10.1016/S0925-5273(98)00222-9},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {agile manufacturing; review; future research; deve},
month = {may},
number = {1-2},
pages = {87--105},
publisher = {ELSEVIER SCIENCE BV},
title = {{Agile manufacturing: A framework for research and development}},
type = {Article},
volume = {62},
year = {1999}
}
@article{ISI:000265879200001,
abstract = {Dynamic capacity provisioning is a useful technique for handling the
multi-time-scale variations seen in Internet workloads. In this article,
we propose a novel dynamic provisioning technique for multi-tier
Internet applications that employs (1) a flexible queuing model to
determine how much of the resources to allocate to each tier of the
application, and (2) a combination of predictive and reactive methods
that determine when to provision these resources, both at large and
small time scales. We propose a novel data center architecture based on
virtual machine monitors to reduce provisioning overheads. Our
experiments on a forty-machine Xen/Linux-based hosting platform
demonstrate the responsiveness of our technique in handling dynamic
workloads. In one scenario where a flash crowd caused the workload of a
three-tier application to double, our technique was able to double the
application capacity within five minutes, thus maintaining response-time
targets. Our technique also reduced the overhead of switching servers
across applications from several minutes to less than a second, while
meeting the performance targets of residual sessions.},
address = {2 PENN PLAZA, STE 701, NEW YORK, NY 10121-0701 USA},
annote = {2nd International Conference on Autonomic Computing (ICAC 2005),
Seattle, WA, JUN 13-16, 2005},
author = {Urgaonkar, Bhuvan and Shenoy, Prashant and Chandra, Abhishek and Goyal, Pawan and Wood, Timothy},
doi = {10.1145/1342171.1342172},
institution = {IEEE Comp Soc; Natl Sci Fdn},
issn = {1556-4665},
journal = {ACM TRANSACTIONS ON AUTONOMOUS AND ADAPTIVE SYSTEMS},
keywords = {Design; Experimentation; Performance; Internet app},
month = {mar},
number = {1},
publisher = {ASSOC COMPUTING MACHINERY},
title = {{Agile Dynamic Provisioning of Multi-Tier Internet Applications}},
type = {Article},
volume = {3},
year = {2008}
}
@article{ISI:000184713800006,
abstract = {Lean supply is closely associated with enabling flow and the elimination
of wasteful variation within the supply chain. However, lean operations
depend on level scheduling and the growing need to accommodate variety
and demand uncertainty has resulted in the emergence of the concept of
agility. This paper explores the role of inventory and capacity in
accommodating such variation and identifies how TRIZ separation
principles and TOC tools may be combined in the integrated development
of responsive and efficient supply chains. A detailed apparel industry
case study is used to illustrate the application of these concepts and
tools. (C) 2003 Elsevier Science B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
annote = {16th International Conference on Production Research (ICPR), CZECH TECH
UNIV, PRAGUE, CZECH REPUBLIC, AUG, 2001},
author = {Stratton, R and Warburton, R D H},
doi = {10.1016/S0925-5273(03)00109-9},
institution = {Czech Assoc Sci {\&} Tech Soc},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {agile; trade-offs; lean; quick response},
month = {aug},
number = {2},
pages = {183--198},
publisher = {ELSEVIER SCIENCE BV},
title = {{The strategic integration of agile and lean supply}},
type = {Article; Proceedings Paper},
volume = {85},
year = {2003}
}
@article{ISI:000080462600006,
abstract = {Agility is herein interpreted as using market knowledge and a virtual
corporation to exploit profitable opportunities in a volatile market
place. This requires the slashing of process lead times throughout the
chain. However, as we demonstrate in the paper such action is simply not
enough to enable agility. Similar steps must also be taken to reduce
information lead times, resulting in the concept of the ``information
enriched{\{}''{\}} supply chain. Simulation results obtained on realistic
models of fashion trade supply chains confirm the superior agility
resulting from information enrichment. The paper concludes with a
Route-Map indicating the steps to be taken in achieving supply chain
agility in real world scenarios. (C) 1999 Elsevier Science B.V. All
rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Mason-Jones, R and Towill, D R},
doi = {10.1016/S0925-5273(98)00221-7},
issn = {0925-5273},
journal = {INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS},
keywords = {agile; supply chain; information; competitive adva},
month = {may},
number = {1-2},
pages = {61--73},
publisher = {ELSEVIER SCIENCE BV},
title = {{Total cycle time compression and the agile supply chain}},
type = {Article},
volume = {62},
year = {1999}
}
@article{ISI:000171886500026,
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Cockburn, A and Highsmith, J},
doi = {10.1109/2.963450},
issn = {0018-9162},
journal = {COMPUTER},
month = {nov},
number = {11},
pages = {131--133},
publisher = {IEEE COMPUTER SOC},
title = {{Agile software development: The people factor}},
type = {Editorial Material},
volume = {34},
year = {2001}
}
@article{ISI:000234695300029,
abstract = {AGILE is a gamma-ray astrophysics space mission which will operate,
starting from 2006, in the 30 MeV-50 GeV energy range with imaging
capability also in the 15-45 keV energy band. In order to achieve the
required detection sensitivity, all AGILE detectors are surrounded by an
anticoincidence detector aimed at charged particle background rejection
with an inefficiency as low as 10(-4). In this work, the design and the
structure of this anticoincidence detector are presented, as well as its
performances in terms of charged particles detection inefficiency as
derived from extensive calibrations performed at CERN PS. (c) 2005
Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Perotti, F and Fiorini, M and Incorvaia, S and Mattaini, E and Sant'Ambrogio, E},
doi = {10.1016/j.nima.2005.10.016},
issn = {0168-9002},
journal = {NUCLEAR INSTRUMENTS {\&} METHODS IN PHYSICS RESEARCH SECTION A-ACCELERATORS SPECTROMETERS DETECTORS AND ASSOCIATED EQUIPMENT},
keywords = {scintillation detector; charged particles detector},
month = {jan},
number = {1},
pages = {228--236},
publisher = {ELSEVIER SCIENCE BV},
title = {{The AGILE anticoincidence detector}},
type = {Article},
volume = {556},
year = {2006}
}
@article{ISI:000178571700015,
abstract = {Wavelength-agile photonic integrated circuits are fabricated using a
one-step ion implantation quantum-well intermixing process. In this
paper, we discuss the issues in processing optimized widely tunable
multisection lasers using this technique and present the results
achieved using this process. This quantum-well intermixing process is
general in its application and can be used to monolithically integrate a
wide variety of optoelectronic components with widely tunable lasers.},
address = {345 E 47TH ST, NEW YORK, NY 10017-2394 USA},
author = {Skogen, E J and Barton, J S and Denbaars, S P and Coldren, L A},
doi = {10.1109/JSTQE.2002.800849},
issn = {1077-260X},
journal = {IEEE JOURNAL OF SELECTED TOPICS IN QUANTUM ELECTRONICS},
keywords = {ion implantation; laser tuning; semiconductor lase},
number = {4},
pages = {863--869},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{A quantum-well-intermixing process for wavelength-agile photonic integrated circuits}},
type = {Article},
volume = {8},
year = {2002}
}
@article{ISI:000300611700007,
abstract = {Private for-profit institutions have been the fastest-growing part of
the U.S. higher education sector. For-profit enrollment increased from
0.2 percent to 9.1 percent of total enrollment in degree-granting
schools from 1970 to 2009, and for-profit institutions account for the
majority of enrollments in non-degree-granting postsecondary schools. We
describe the schools, students, and programs in the for-profit higher
education sector, its phenomenal recent growth, and its relationship to
the federal and state governments. Using the 2004 to 2009 Beginning
Postsecondary Students (BPS) longitudinal survey, we assess outcomes of
a recent cohort of first-time undergraduates who attended for-profits
relative to comparable students who attended community colleges or other
public or private non-profit institutions. We find that relative to
these other institutions, for-profits educate a larger fraction of
minority, disadvantaged, and older students, and they have greater
success at retaining students in their first year and getting them to
complete short programs at the certificate and AA levels. But we also
find that for-profit students end up with higher unemployment and
``idleness{\{}''{\}} rates and lower earnings six years after entering
programs than do comparable students from other schools and that, not
surprisingly, they have far greater default rates on their loans.},
address = {2014 BROADWAY, STE 305, NASHVILLE, TN 37203 USA},
author = {Deming, David J and Goldin, Claudia and Katz, Lawrence F},
doi = {10.1257/jep.26.1.139},
issn = {0895-3309},
journal = {JOURNAL OF ECONOMIC PERSPECTIVES},
number = {1},
pages = {139--163},
publisher = {AMER ECONOMIC ASSOC},
title = {{The For-Profit Postsecondary School Sector: Nimble Critters or Agile Predators?}},
type = {Article},
volume = {26},
year = {2012}
}
@article{ISI:A1956WD31400011,
address = {1325 MASSACHUSETTS AVENUE, NW, WASHINGTON, DC 20005-4171},
author = {GUNTER, S E and KOHN, H I},
issn = {0021-9193},
journal = {JOURNAL OF BACTERIOLOGY},
number = {5},
pages = {571--581},
publisher = {AMER SOC MICROBIOLOGY},
title = {{THE EFFECT OF X-RAYS ON THE SURVIVAL OF BACTERIA AND YEAST .1. A COMPARATIVE STUDY OF THE DOSE-SURVIVAL CURVES OF AZOTOBACTER-AGILE, ESCHERICHIA-COLI, PSEUDOMONAS-FLUORESCENS, RHODOPSEUDOMONAS-SPHEROIDES, AND SACCHAROMYCES-CEREVISIAE IRRADIATED IN THE RES}},
type = {Article},
volume = {71},
year = {1956}
}
@article{ISI:000271206100001,
abstract = {Purpose - Lean and agile manufacturing are two initiatives that are used
by manufacturing plant managers to improve operations capabilities. The
purpose of this paper is to investigate internal and external factors
that drive the choice of lean and agile operations capabilities and
their respective impact on operational performance.
Design/methodology/approach - Lean and agile manufacturing are each
conceptualized as a second-order factor and measured through a bundle of
distinct practices. The competitive intensity of industry and the
competitive strategy are modeled as potential external and internal
drivers, respectively, and the impact on quality, delivery, cost, and
flexibility performance is analyzed using structural equations modeling.
The model is tested with data from the high performance manufacturing
project comprising a total of 211 plants from three industries and seven
countries.
Findings - The results indicate that lean and agile manufacturing differ
in terms of drivers and outcomes. The choice of a cost-leadership
strategy fully mediates the impact of the competitive intensity of
industry as a driver of lean manufacturing, while agile manufacturing is
directly affected by both internal and external drivers, i.e. a
differentiation strategy as well as the competitive intensity of
industry. Agile manufacturing is found to be negatively associated with
a cost-leadership strategy, emphasizing the difference between lean and
agile manufacturing. The major differences in performance outcomes are
related to cost and flexibility, such that lean manufacturing has a
significant impact on cost performance (whereas agile manufacturing has
not), and that agile manufacturing has a stronger relationship with
volume as well as product mix flexibility than does lean manufacturing.
Research limitations/implications - Cross-sectional data from three
industries and seven countries are used, and it would be interesting to
test this model for more industries and countries.
Practical implications - The results provide insights into the factors
that influence the choice of lean or agile manufacturing for improving
operations, and the results that can be obtained.
Originality/value - To the authors' knowledge, this is the first
large-scale empirical survey of leanness and agility simultaneously,
using data from manufacturing firms in Europe, Asia, and North America.
The model incorporates a wide perspective on factors related to lean and
agile manufacturing, to be able to identify similarities and
differences.},
address = {HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
author = {Hallgren, Mattias and Olhager, Jan},
doi = {10.1108/01443570910993456},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {Lean production; Agile production; Operations mana},
number = {10},
pages = {976--999},
publisher = {EMERALD GROUP PUBLISHING LIMITED},
title = {{Lean and agile manufacturing: external and internal drivers and performance outcomes}},
type = {Article},
volume = {29},
year = {2009}
}
@article{ISI:000342721500040,
abstract = {The graceful and agile movements of animals are difficult to analyze and
emulate because locomotion is the result of a complex interplay of many
components: the central and peripheral nervous systems, the
musculoskeletal system, and the environment. The goals of biorobotics
are to take inspiration from biological principles to design robots that
match the agility of animals, and to use robots as scientific tools to
investigate animal adaptive behavior. Used as physical models, biorobots
contribute to hypothesis testing in fields such as hydrodynamics,
biomechanics, neuroscience, and prosthetics. Their use may contribute to
the design of prosthetic devices that more closely take human locomotion
principles into account.},
address = {1200 NEW YORK AVE, NW, WASHINGTON, DC 20005 USA},
author = {Ijspeert, Auke J},
doi = {10.1126/science.1254486},
issn = {0036-8075},
journal = {SCIENCE},
month = {oct},
number = {6206},
pages = {196--203},
publisher = {AMER ASSOC ADVANCEMENT SCIENCE},
title = {{Biorobotics: Using robots to emulate and investigate agile locomotion}},
type = {Review},
volume = {346},
year = {2014}
}
@article{ISI:000245226000011,
abstract = {We explore the price dynamics in a competitive market consisting of
spectrum agile network service providers and users. Here, multiple
self,interested spectrum providers operating with different technologies
and costs compete for potential customers. Different buyers or consumers
may evaluate the same seller differently depending on their
applications, operating technologies and locations. Two different buyer
populations, the quality-sensitive and the price-sensitive are
investigated, and the resulting collective price dynamics are studied
using a combination of analysis and simulations. Various scenarios are
considered regarding the nature and accuracy of information available to
the sellers. A myopically optimal strategy is studied when full
information is available, while a stochastic learning based strategy is
considered when the information is limited. Cooperating groups may be
formed among the sellers which will in-turn influence the group profit
for those participants. Free riding phenomenon is observed under certain
circumstances.},
address = {445 HOES LANE, PISCATAWAY, NJ 08855 USA},
author = {Xing, Yiping and Chandramouli, R and Cordeiro, Carlos},
doi = {10.1109/JSAC.2007.07041},
issn = {0733-8716},
journal = {IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS},
keywords = {wireless communication; radio spectrum management;},
month = {apr},
number = {3},
pages = {613--621},
publisher = {IEEE-INST ELECTRICAL ELECTRONICS ENGINEERS INC},
title = {{Price dynamics in competitive agile spectrum access markets}},
type = {Article},
volume = {25},
year = {2007}
}
@article{ISI:000251876200013,
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Cao, Lan and Ramesh, Balasubramaniam},
doi = {10.1109/MS.2008.1},
issn = {0740-7459},
journal = {IEEE SOFTWARE},
number = {1},
pages = {60--67},
publisher = {IEEE COMPUTER SOC},
title = {{Agile requirements engineering practices: An emprical study}},
type = {Article},
volume = {25},
year = {2008}
}
@article{ISI:000237770500016,
abstract = {With the emergence of a business era that embraces `change' as one of
its major characteristics, manufacturing success and survival are
becoming more and more difficult to ensure. The emphasis is on
adaptability to changes in the business environment and on addressing
market and customer needs proactively. Changes in the business
environment due to varying needs of the customers lead to uncertainty in
the decision parameters. Flexibility is needed in the supply chain to
counter the uncertainty in the decision parameters. A supply chain
adapts the changes if it is flexible and agile in nature. A framework is
presented in this paper, which encapsulates the market sensitiveness,
process integration, information driver and flexibility measures of
supply chain performance. The paper explores the relationship among
lead-time, cost, quality, and service level and the leanness and agility
of a case supply chain in fast moving consumer goods business. The paper
concludes with the justification of the framework, which analyses the
effect of market winning criteria and market qualifying criteria on the
three types of supply chains: lean, agile and leagile. (c) 2005 Elsevier
B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Agarwal, A and Shankar, R and Tiwari, M K},
doi = {10.1016/j.ejor.2004.12.005},
issn = {0377-2217},
journal = {EUROPEAN JOURNAL OF OPERATIONAL RESEARCH},
keywords = {agility; flexibility; supply chain; analytic netwo},
month = {aug},
number = {1},
pages = {211--225},
publisher = {ELSEVIER SCIENCE BV},
title = {{Modeling the metrics of lean, agile and leagile supply chain: An ANP-based approach}},
type = {Article},
volume = {173},
year = {2006}
}
@article{ISI:000271514900043,
abstract = {We present the first catalog of high-confidence gamma-ray sources
detected by the AGILE satellite during observations performed from July
9, 2007 to June 30, 2008. Cataloged sources were detected by merging all
the available data over the entire time period. AGILE, launched in April
2007, is an ASI mission devoted to gamma-ray observations in the 30
MeV-50 GeV energy range, with simultaneous X-ray imaging capability in
the 18-60 keV band. This catalog is based on Gamma-Ray Imaging Detector
(GRID) data for energies greater than 100 MeV. For the first AGILE
catalog, we adopted a conservative analysis, with a high-quality event
filter optimized to select gamma-ray events within the central zone of
the instrument field of view (radius of 40 degrees). This is a
significance-limited (4 sigma) catalog, and it is not a complete
flux-limited sample due to the non-uniform first-year AGILE sky
coverage. The catalog includes 47 sources, 21 of which are associated
with confirmed or candidate pulsars, 13 with blazars (7 FSRQ, 4 BL Lacs,
2 unknown type), 2 with HMXRBs, 2 with SNRs, 1 with a colliding-wind
binary system, and 8 with unidentified sources.},
address = {17, AVE DU HOGGAR, PA COURTABOEUF, BP 112, F-91944 LES ULIS CEDEX A, FRANCE},
author = {Pittori, C and Verrecchia, F and Chen, A W and Bulgarelli, A and Pellizzoni, A and Giuliani, A and Vercellone, S and Longo, F and Tavani, M and Giommi, P and Barbiellini, G and Trifoglio, M and Gianotti, F and Argan, A and Antonelli, A and Boffelli, F and Caraveo, P and Cattaneo, P W and Cocco, V and Colafrancesco, S and Contessi, T and Costa, E and Cutini, S and D'Ammando, F and {Del Monte}, E and {De Paris}, G and {Di Cocco}, G and {Di Persio}, G and Donnarumma, I and Evangelista, Y and Fanari, G and Feroci, M and Ferrari, A and Fiorini, M and Fornari, F and Fuschino, F and Froysland, T and Frutti, M and Galli, M and Gasparrini, D and Labanti, C and Lapshov, I and Lazzarotto, F and Liello, F and Lipari, P and Mattaini, E and Marisaldi, M and Mastropietro, M and Mauri, A and Mauri, F and Mereghetti, S and Morelli, E and Moretti, E and Morselli, A and Pacciani, L and Perotti, F and Piano, G and Picozza, P and Pilia, M and Pontoni, C and Porrovecchio, G and Preger, B and Prest, M and Primavera, R and Pucella, G and Rapisarda, M and Rappoldi, A and Rossi, E and Rubini, A and Sabatini, S and Santolamazza, P and Scalise, E and Soffitta, P and Stellato, S and Striani, E and Tamburelli, F and Traci, A and Trois, A and Vallazza, E and Vittorini, V and Zambra, A and Zanello, D and Salotti, L},
doi = {10.1051/0004-6361/200911783},
issn = {0004-6361},
journal = {ASTRONOMY {\&} ASTROPHYSICS},
keywords = {gamma rays: observations; catalogs},
month = {nov},
number = {3},
pages = {1563--1574},
publisher = {EDP SCIENCES S A},
title = {{First AGILE catalog of high-confidence gamma-ray sources}},
type = {Article},
volume = {506},
year = {2009}
}
@article{ISI:000231943500005,
abstract = {Agile processes focus on the early facilitation and fast production of
working code, and are based on software-development process models that
support iterative, incremental development of software. Although agile
methods have existed for a number of years now, answers to questions
concerning the suitability of agile processes to particular
software-development environments are still often based on anecdotal
accounts of experiences. An appreciation of the (often unstated)
assumptions underlying agile processes can lead to a better
understanding of the applicability of agile processes to particular
situations. Agile processes are less likely to be applicable in
situations in which core assumptions do not hold. This article examines
the principles and advocated practices of agile processes to identify
underlying assumptions. It also identifies limitations that may arise
from these assumptions and outlines how the limitations can be addressed
by incorporating other software-development techniques and practices
into agile development environments.},
address = {701 E CHOCOLATE AVE, STE 200, HERSHEY, PA 17033-1240 USA},
author = {Turk, D and France, R and Rumpe, B},
doi = {10.4018/jdm.2005100104},
issn = {1063-8016},
journal = {JOURNAL OF DATABASE MANAGEMENT},
keywords = {agile development; assumptions; extreme programmin},
number = {4},
pages = {62--87},
publisher = {IGI PUBL},
title = {{Assumptions underlying agile software-development processes}},
type = {Review},
volume = {16},
year = {2005}
}
@article{ISI:000319705100015,
abstract = {To enhance the therapeutic efficacy and reduce the adverse effects of
traditional Chinese medicine, practitioners often prescribe combinations
of plant species and/or minerals, called formulae. Unfortunately, the
working mechanisms of most of these compounds are difficult to determine
and thus remain unknown. In an attempt to address the benefits of
formulae based on current biomedical approaches, we analyzed the
components of Yinchenhao Tang, a classical formula that has been shown
to be clinically effective for treating hepatic injury syndrome. The
three principal components of Yinchenhao Tang are Artemisia annua L.,
Gardenia jasminoids Ellis, and Rheum Palmatum L., whose major active
ingredients are 6,7-dimethylesculetin (D), geniposide (G), and rhein
(R), respectively. To determine the mechanisms underlying the efficacy
of this formula, we conducted a systematic analysis of the therapeutic
effects of the DGR compound using immunohistochemistry, biochemistry,
metabolomics, and proteomics. Here, we report that the DGR combination
exerts a more robust therapeutic effect than any one or two of the three
individual compounds by hitting multiple targets in a rat model of
hepatic injury. Thus, DGR synergistically causes intensified dynamic
changes in metabolic biomarkers, regulates molecular networks through
target proteins, has a synergistic/additive effect, and activates both
intrinsic and extrinsic pathways.},
address = {9650 ROCKVILLE PIKE, BETHESDA, MD 20814-3996 USA},
author = {Wang, Xijun and Zhang, Aihua and Wang, Ping and Sun, Hui and Wu, Gelin and Sun, Wenjun and Lv, Haitao and Jiao, Guozheng and Xu, Hongying and Yuan, Ye and Liu, Lian and Zou, Dixin and Wu, Zeming and Han, Ying and Yan, Guangli and Dong, Wei and Wu, Fangfang and Dong, Tianwei and Yu, Yang and Zhang, Shuxiang and Wu, Xiuhong and Tong, Xin and Meng, Xiangcai},
doi = {10.1074/mcp.M112.021683},
issn = {1535-9476},
journal = {MOLECULAR {\&} CELLULAR PROTEOMICS},
month = {may},
number = {5},
pages = {1226--1238},
publisher = {AMER SOC BIOCHEMISTRY MOLECULAR BIOLOGY INC},
title = {{Metabolomics Coupled with Proteomics Advancing Drug Discovery toward More Agile Development of Targeted Combination Therapies}},
type = {Article},
volume = {12},
year = {2013}
}
@article{ISI:000269983900003,
abstract = {Despite the popularity of agile methods in software development and
increasing adoption by organizations there is debate about what agility
is and how it is achieved. The debate suffers from a lack of
understanding of agile concepts and how agile software development is
practiced. This paper develops a framework for the organization of agile
software development that identifies enablers and inhibitors of agility
and the emergent capabilities of agile teams. The work is grounded in
complex adaptive systems (CAS) and draws on three principles of
coevolving systems: match coevolutionary change rate, maximize
self-organizing, and synchronize exploitation and exploration. These
principles are used to study the processes of two software development
teams, one a team using eXtreme Programming (XP) and the other a team
using a more traditional, waterfall-based development cycle. From the
cases a framework for the organization of agile software development is
developed. Time pacing, self-management with discipline and
routinization of exploration are among the agile enablers found in the
cases studies while event pacing, centralized management, and lack of
resources allocated to exploration are found to be inhibitors to
agility. Emergent capabilities of agile teams that are identified from
the research include coevolution of business value, sustainable working
with rhythm, sharing and team learning, and collective mindfulness.},
address = {7240 PARKWAY DR, STE 310, HANOVER, MD 21076-1344 USA},
author = {Vidgen, Richard and Wang, Xiaofeng},
doi = {10.1287/isre.1090.0237},
issn = {1047-7047},
journal = {INFORMATION SYSTEMS RESEARCH},
keywords = {agile software development; coevolving systems; co},
month = {sep},
number = {3},
pages = {355--376},
publisher = {INFORMS},
title = {{Coevolving Systems and the Organization of Agile Software Development}},
type = {Article},
volume = {20},
year = {2009}
}
@article{ISI:000171242900006,
abstract = {Traditional approach for the design of missile guidance and autopilot
systems has been to design these subsystems separately and then to
integrate them. Such an approach does not exploit any beneficial
relationships between these and other subsystems. A technique for
integrated design of missile guidance and autopilot systems using the
feedback linearization technique is discussed. Numerical results using a
six degree-of-freedom missile simulation are given. Integrated
guidance-autopilot systems are expected to result in significant
improvements in missile performance, leading to lower weight and
enhanced lethality. These design methods have extensive applications in
high performance aircraft autopilot and guidance system design. (C) 2001
Elsevier Science Ltd. All rights reserved.},
address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD OX5 1GB, ENGLAND},
author = {Menon, P K and Ohlmeyer, E J},
doi = {10.1016/S0967-0661(01)00082-X},
issn = {0967-0661},
journal = {CONTROL ENGINEERING PRACTICE},
keywords = {integrated; guidance; autopilot; feedback lineariz},
month = {oct},
number = {10},
pages = {1095--1106},
publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
title = {{Integrated design of agile missile guidance and autopilot systems}},
type = {Article},
volume = {9},
year = {2001}
}
@article{ISI:000275074600005,
abstract = {As business and technology environments change at an unprecedented rate,
software development agility to respond to changing user requirements
has become increasingly critical for software development performance.
Agile software development approaches, which emphasize
sense-and-respond, self-organization, cross-functional teams, and
continuous adaptation, have been adopted by an increasing number of
organizations to improve their software development agility. However,
the agile development literature is largely anecdotal and prescriptive,
lacking empirical evidence and theoretical foundation to support the
principles and practices of agile development. Little research has
empirically examined the software development agility construct in terms
of its dimensions, determinants, and effects on software development
performance. As a result, there is a lack of understanding about how
organizations can effectively implement an agile development approach.
Using an integrated research approach that combines quantitative and
qualitative data analyses, this research opens the black box of agile
development by empirically examining the relationships among two
dimensions of software development agility (software team response
extensiveness and software team response efficiency), two antecedents
that can be con trolled (team autonomy and team diversity), and three
aspects of software development performance (on-time completion,
on-budget completion, and software functionality). Our PLS results of
survey, responses of 399 software project managers suggest that the
relationships among these variables are more complex than what has been
perceived by the literature. The results suggest a tradeoff relationship
between response extensiveness and response efficiency. These two
agility dimensions impact software development performance differently:
response efficiency positively affects all of on-time completion,
on-budget completion, and software functionality, whereas response
extensiveness positively affects only software functionality. The
results also suggest that team autonomy has a positive effect on
response efficiency and a negative effect on response extensiveness, and
that team diversity has a positive effect on response extensiveness, We
conducted 10 post hoc case studies to qualitatively cross-validate our
PLS results and provide rich, additional insights regarding the complex,
dynamic interplays between autonomy, diversity, agility, and
performance. The qualitative analysis also provides explanations for
both supported and unsupported hypotheses. We discuss these qualitative
analysis results and conclude with the theoretical and practical
implications of our research findings for agile development approaches.},
address = {UNIV MINNESOTA-SCH MANAGEMENT 271 19TH AVE SOUTH, MINNEAPOLIS, MN 55455 USA},
author = {Lee, Gwanhoo and Xia, Weidong},
issn = {0276-7783},
journal = {MIS QUARTERLY},
keywords = {Software development agility; agile software devel},
month = {mar},
number = {1},
pages = {87--114},
publisher = {SOC INFORM MANAGE-MIS RES CENT},
title = {{TOWARD AGILE: AN INTEGRATED ANALYSIS OF QUANTITATIVE AND QUALITATIVE FIELD DATA ON SOFTWARE DEVELOPMENT AGILITY}},
type = {Article},
volume = {34},
year = {2010}
}
@article{ISI:000251845500002,
abstract = {Purpose - Despite the fact that agile manufacturing has been frequently
promoted as a means of improving business competitiveness, little
empirical evidence exists in the literature validating its positive link
with business performance. The purpose of this research paper is to
analyse agile manufacturing in Spain and study whether it is a critical
factor for success in different industries.
Design/methodology/approach - A conceptual model is drawn up, based on
the literature and a previous case study, to relate turbulence in the
environment with agile manufacturing practices and business performance.
The model is tested on a large sample of Spanish manufacturers using a
survey methodology to obtain information and a structural equation model
to analyse the data.
Findings - The results obtained show that, in turbulent environments,
the integrated use of agile manufacturing practices promotes
manufacturing competitive strength, leading to better operational,
market and financial performance.
Research limitations/implications - This study has two main limitations.
First, it is difficult to determine the most suitable unit of analysis
when studying agile manufacturing. Second, single respondent bias may be
considered a limitation.
Practical implications - Managers should consider the integrated
implementation of agile manufacturing practices in order to develop
manufacturing strength and to outperform competitors in turbulent
business environments.
Originality/value - This study adopts a systematic approach to the
analysis of agile manufacturing, considering various agility practices
or enablers in an integrated way and relating them not only to
environmental characteristics but also to business performance. This
approach is especially interesting because most of the literature on
agile manufacturing deals with agility strategies or techniques in an
isolated way. The study also tests the suitability of agile
manufacturing in real organisations - for the first time in the Spanish
context.},
address = {HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
author = {Vazquez-Bustelo, Daniel and Avella, Lucia and Fernandez, Esteban},
doi = {10.1108/01443570710835633},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {agile production; operations and production manage},
number = {12},
pages = {1303--1332},
publisher = {EMERALD GROUP PUBLISHING LIMITED},
title = {{Agility drivers, enablers and outcomes - Empirical test of an integrated agile manufacturing model}},
type = {Article},
volume = {27},
year = {2007}
}
@article{ISI:000244664800021,
address = {1515 BROADWAY, NEW YORK, NY 10036 USA},
author = {Nerur, Sridhar and Balijepally, VanuGopal},
doi = {10.1145/1226736.1226739},
issn = {0001-0782},
journal = {COMMUNICATIONS OF THE ACM},
month = {mar},
number = {3},
pages = {79--83},
publisher = {ASSOC COMPUTING MACHINERY},
title = {{Theoretical reflections on agile development methodologies - The traditional goal of optimization and control is making way for learning and innovation.}},
type = {Article},
volume = {50},
year = {2007}
}
@article{ISI:000251845300005,
abstract = {Purpose - This paper aims to provide a practical model usable by
organizations to help form agile virtual enterprises. The model helps to
integrate a variety of factors, tangible and intangible, strategic and
operational, for decision-making purposes.
Design/methodology/approach - A comprehensive development of factors is
determined from the literature and an analytical network process (ANP)
methodology is introduced for decision model development. An
illustrative example is presented.
Findings - The results provide a robust model that will aid decision
makers and agile virtual enterprise brokers form partnerships within
these organizational structures.
Research limitations/implications - The paper introduces a conceptual
model with an illustrative validating example. A practical application
and reapplication of the model are required to further validate the
model. ANP can require significant managerial input for its application,
potentially causing fatigue for decision makers.
Practical implications - Practical implications include a partner
selection tool and framework for decision makers. The model may be
easily tweaked by the elimination or addition of decision factors and
their relationships.
Originality/value - The paper is useful to practitioners and
organizations seeking to manage partnership formation of agile virtual
enterprises, an emerging organizational form. This work expands the
number of factors and interrelationships among these factors that no
other model has explicitly addressed for the agile virtual enterprise
formation situation.},
address = {HOWARD HOUSE, WAGON LANE, BINGLEY BD16 1WA, W YORKSHIRE, ENGLAND},
author = {Sarkis, Joseph and Talluri, Srinivas and Gunasekaran, A},
doi = {10.1108/01443570710830601},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {agile production; partnership; virtual organizatio},
number = {11},
pages = {1213--1234},
publisher = {EMERALD GROUP PUBLISHING LIMITED},
title = {{A strategic model for agile virtual enterprise partner selection}},
type = {Article},
volume = {27},
year = {2007}
}
@article{ISI:000237552000009,
abstract = {Tailoring of methods is commonplace in the vast majority of software
development projects and organisations. However, there is not much known
about the tailoring and engineering of agile methods, or about how these
methods can be used to complement each other. This study investigated
tailoring of the agile methods, eXtreme programming (XP) and Scrum, at
Intel Shannon, and involved experienced software engineers who
continuously monitored and reflected on these methods over a 3- year
period. The study shows that agile methods may individually be
incomplete in supporting the overall development process, but XP and
Scrum complement each other well, with XP providing support for
technical aspects and Scrum providing support for project planning and
tracking. The principles of XP and Scrum were carefully selected (only
six of the 12 XP key practices were implemented, for example) and
tailored to suit the needs of the development environment at Intel
Shannon. Thus, the study refutes the suggestion that agile methods are
not divisible or individually selectable but achieve their benefits
through the synergistic combination of individual agile practices;
rather, this study shows that an a la carte selection and tailoring of
practices can work very well. In the case of Scrum, some local tailoring
has led to a very committed usage by developers, in contrast to many
development methods whose usage is limited despite being decreed
mandatory by management. The agile practices that were applied did lead
to significant benefits, including reductions in code defect density by
a factor of 7. Projects of 6-month and 1-year duration have been
delivered ahead of schedule, which bodes well for future ability to
accurately plan development projects.},
address = {BRUNEL RD BLDG, HOUNDMILLS, BASINGSTOKE RG21 6XS, HANTS, ENGLAND},
author = {Fitzgerald, Brian and Hartnett, Gerard and Conboy, Kieran},
doi = {10.1027/palgrave.ejis.300605},
issn = {0960-085X},
journal = {EUROPEAN JOURNAL OF INFORMATION SYSTEMS},
keywords = {agile methods; software development; XP; Scrum; me},
month = {apr},
number = {2},
pages = {200--213},
publisher = {PALGRAVE MACMILLAN LTD},
title = {{Customising agile methods to software practices at Intel Shannon}},
type = {Article},
volume = {15},
year = {2006}
}
@article{ISI:000296592600022,
abstract = {Coded aperture snapshot spectral imaging (CASSI) provides a mechanism
for capturing a 3D spectral cube with a single shot 2D measurement. In
many applications selective spectral imaging is sought since relevant
information often lies within a subset of spectral bands. Capturing and
reconstructing all the spectral bands in the observed image cube, to
then throw away a large portion of this data, is inefficient. To this
end, this paper extends the concept of CASSI to a system admitting
multiple shot measurements, which leads not only to higher quality of
reconstruction but also to spectrally selective imaging when the
sequence of code aperture patterns is optimized. The aperture code
optimization problem is shown to be analogous to the optimization of a
constrained multichannel filter bank. The optimal code apertures allow
the decomposition of the CASSI measurement into several subsets, each
having information from only a few selected spectral bands. The rich
theory of compressive sensing is used to effectively reconstruct the
spectral bands of interest from the measurements. A number of
simulations are developed to illustrate the spectral imaging
characteristics attained by optimal aperture codes. (C) 2011 Optical
Society of America},
address = {2010 MASSACHUSETTS AVE NW, WASHINGTON, DC 20036 USA},
author = {Arguello, Henry and Arce, Gonzalo R},
doi = {10.1364/JOSAA.28.002400},
issn = {1084-7529},
journal = {JOURNAL OF THE OPTICAL SOCIETY OF AMERICA A-OPTICS IMAGE SCIENCE AND VISION},
month = {nov},
number = {11},
pages = {2400--2413},
publisher = {OPTICAL SOC AMER},
title = {{Code aperture optimization for spectrally agile compressive imaging}},
type = {Article},
volume = {28},
year = {2011}
}
@article{ISI:000257529900001,
abstract = {Agile software development represents a major departure from
traditional, plan-based approaches to software engineering. A systematic
review of empirical studies of agile software development up to and
including 2005 was conducted. The search strategy identified 1996
studies, of which 36 were identified as empirical studies. The studies
were grouped into four themes: introduction and adoption, human and
social factors, perceptions on agile methods, and comparative studies.
The review investigates what is currently known about the benefits and
limitations of, and the strength of evidence for, agile methods.
Implications for research and practice are presented. The main
implication for research is a need for more and better empirical studies
of agile software development within a common research agenda. For the
industrial readership, the review provides a map of findings, according
to topic, that can be compared for relevance to their own settings and
situations. (C) 2008 Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Dyba, Tore and Dingsoyr, Torgeir},
doi = {10.1016/j.infsof.2008.01.006},
issn = {0950-5849},
journal = {INFORMATION AND SOFTWARE TECHNOLOGY},
keywords = {empirical software engineering; evidence-based sof},
month = {aug},
number = {9-10},
pages = {833--859},
publisher = {ELSEVIER SCIENCE BV},
title = {{Empirical studies of agile software development: A systematic review}},
type = {Review},
volume = {50},
year = {2008}
}
@article{ISI:000256391400011,
abstract = {While software is so important for all facets of the modern world,
software development itself is not a perfect process. Agile software
engineering methods have recently emerged as a new and different way of
developing software as compared to the traditional methodologies.
However, their success has mostly been anecdotal, and research in this
subject is still scant in the academic circles. This research study was
a survey study on the critical success factors of Agile software
development projects using quantitative approach.
Based on existing literature, a preliminary list of potential critical
success factors of Agile projects were identified and compiled.
Subsequently, reliability analysis and factor analysis were conducted to
consolidate this preliminary list into a final set of 12 possible
critical success factors for each of the four project success categories
- Quality, Scope, Time, and Cost.
A survey was conducted among Agile professionals, gathering survey data
from 109 Agile projects from 25 countries across the world. Multiple
regression techniques were used, both at the full regression model and
at the optimized regression model via the stepwise screening procedure.
The results revealed that only 10 out of 48 hypotheses were supported,
identifying three critical success factors for Agile software
development projects: (a) Delivery Strategy, (b) Agile Software
Engineering Techniques, and (c) Team Capability.
Limitations of the study are discussed together with interpretations for
practitioners. To ensure success of their projects, managers are urged
to focus on choosing a high-caliber team, practicing Agile engineering
techniques and following Agile-style delivery strategy. (C) 2007
Elsevier Inc. All rights reserved.},
address = {360 PARK AVE SOUTH, NEW YORK, NY 10010-1710 USA},
author = {Chow, Tsun and Cao, Dac-Buu},
doi = {10.1016/j.jss.2007.08.020},
issn = {0164-1212},
journal = {JOURNAL OF SYSTEMS AND SOFTWARE},
keywords = {software development; agile methods; critical succ},
month = {jun},
number = {6},
pages = {961--971},
publisher = {ELSEVIER SCIENCE INC},
title = {{A survey study of critical success factors in agile software projects}},
type = {Article},
volume = {81},
year = {2008}
}
@article{ISI:000178991000007,
abstract = {This article concerns the problem of managing the new generation of
Agile Earth Observing Satellites (AEOS). This kind of satellites is
presently studied by the French Centre National d'Etudes Spatiales
(PLEIADES project). The mission of an Earth Observing Satellite is to
acquire images of specified areas on the Earth surface, in response to
observation requests from customers. Whereas non-agile satellites such
as SPOT5 have only one degree of freedom for acquiring images, the new
generation satellites have three, giving opportunities for a more
efficient use of the satellite imaging capabilities. Counterwise to this
advantage, the selection and scheduling of observations becomes
significantly more difficult, due to the larger search space for
potential solutions. Hence, selecting and scheduling observations of
agile satellites is a highly combinatorial problem. This article sets
out the overall problem and analyses its difficulties. Then it presents
different methods which have been investigated in order to solve a
simplified version of the complete problem: a greedy algorithm, a
dynamic programming algorithm, a constraint programming approach and a
local search method. (C) 2002 Editions scientifiques et medicales
Elsevier SAS. All rights reserved.},
address = {23 RUE LINOIS, 75724 PARIS, FRANCE},
author = {Lemaitre, M and Verfaillie, G and Jouhaud, F and Lachiver, J M and Bataille, N},
doi = {10.1016/S1270-9638(02)01173-2},
issn = {1270-9638},
journal = {AEROSPACE SCIENCE AND TECHNOLOGY},
keywords = {Earth Observing Satellite; agile satellite; missio},
month = {sep},
number = {5},
pages = {367--381},
publisher = {ELSEVIER FRANCE-EDITIONS SCIENTIFIQUES MEDICALES ELSEVIER},
title = {{Selecting and scheduling observations of agile satellites}},
type = {Article},
volume = {6},
year = {2002}
}
@article{ISI:000086196000001,
abstract = {Since early anatomical descriptions, the existence of dendritic spines
has stimulated intense curiosity and speculation about their regulation
and function. Research over the past three decades has described an
impressive mutability in dendritic-spine number and morphology under a
variety of physiological circumstances. Current evidence favors a
proposed model in which two pools of actin filaments, one stable and the
other dynamic, support both persistent spine structure and rapid spine
motility. Potential functions of spine motility and dynamic actin
include regulated protein scaffolding, retrograde signaling and synapse
stabilization.},
address = {84 THEOBALDS RD, LONDON WC1X 8RR, ENGLAND},
author = {Halpain, S},
doi = {10.1016/S0166-2236(00)01576-9},
issn = {0166-2236},
journal = {TRENDS IN NEUROSCIENCES},
month = {apr},
number = {4},
pages = {141--146},
publisher = {ELSEVIER SCIENCE LONDON},
title = {{Actin and the agile spine: how and why do dendritic spines dance?}},
type = {Editorial Material},
volume = {23},
year = {2000}
}
@article{ISI:A1996UU40400002,
abstract = {As product life cycle becomes shortened, high product quality becomes
necessary for survival, markets become highly diversified and global,
and continuous and unexpected change becomes the key factor for success.
The need for a method of rapidly and cost-effectively developing
products, production facilities and supporting software including
design, process planning, shop door control systems is becoming urgent.
The essence of this concept of manufacturing would be characterized by
introducing a new term agility or rapidity. When compared with computer
integrated manufacturing, agile manufacturing can be defined as the
capability of surviving and prospering in a competitive environment of
continuous and unpredictable change by reacting quickly and effectively
to changing markets, driven by customer-designed products and services.
Critical to successfully accomplishing agile manufacturing are a few
enabling technologies such as the standard for the exchange of products
(STEP), concurrent engineering, virtual manufacturing, component-based
heterarchical shop floor control system, information and communication
infrastructure, etc. This article details key concepts of those enabling
technologies and presents various activities related to agile
manufacturing under development in Korea, especially an agile
manufacturing test-bed at Pohang University of Science and Technology
and a prototype of the life cycle engineering study of a product model
made in a consumer electronic industry. Copyright (C) 1996 Elsevier
Science Ltd.},
address = {THE BOULEVARD, LANGFORD LANE, KIDLINGTON, OXFORD, ENGLAND OX5 1GB},
author = {Cho, H and Jung, M Y and Kim, M},
doi = {10.1016/0360-8352(96)00001-0},
issn = {0360-8352},
journal = {COMPUTERS {\&} INDUSTRIAL ENGINEERING},
month = {jul},
number = {3},
pages = {323--334},
publisher = {PERGAMON-ELSEVIER SCIENCE LTD},
title = {{Enabling technologies of agile manufacturing and its related activities in Korea}},
type = {Article},
volume = {30},
year = {1996}
}
@unpublished{,
title = {{No Title}},
url = {https://github.com/tesserato/tesserato.github.io{\%}3E}
}
@article{ISI:000169462700014,
abstract = {A evolutionary transformation of the business environment, with change
as a main characteristic, is taking place. Manufacturing companies, even
those operating in relatively stable conditions with good market
positions, are facing rapid and often unanticipated changes in their
business environment. Agile manufacturing is proposed in response to the
circumstances as a solution and is perceived as a vital characteristic
that manufacturing companies need to have in order to maintain their
competitive advantages in the new order of world business, Each company
will respond in a specific and different way to the changing
circumstances by deploying its own agile characteristics. Agility in
manufacturing may be achieved through the implementation and integration
of appropriate practices which provide the required abilities for a
company to respond properly to changes. Based on this concept, a
methodology is applied in two manufacturing companies and data collected
from the applications are used to validate the methodology. This paper
provides a brief summary of the methodology and details its
implementation and validation in the two case study companies. Practices
are proposed to support the achievement of agility in the two
organisations.},
address = {60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
author = {Sharifi, H and Zhang, Z},
doi = {10.1108/01443570110390462},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {agile production; methodology; manufacturing},
number = {5-6},
pages = {772--794},
publisher = {MCB U P LIMITED},
title = {{Agile manufacturing in practice - Application of a methodology}},
type = {Article},
volume = {21},
year = {2001}
}
@article{ISI:000288926200006,
abstract = {A structural model incorporating agile manufacturing as the focal
construct is theorized and tested. The model includes the primary
components of JIT (JIT-purchasing and JIT-production) as antecedents and
operational performance and firm performance as consequences to agile
manufacturing. Using data collected from production and operations
managers working for large U.S. manufacturers, the model is assessed
following a structural equation modeling methodology. The results
indicate that JIT-purchasing has a direct positive relationship with
agile manufacturing while the positive relationship between
JIT-production and agile manufacturing is mediated by JIT-purchasing.
The results also indicate that agile manufacturing has a direct positive
relationship with the operational performance of the firm, that the
operational performance of the firm has a direct positive relationship
with the marketing performance of the firm, and that the positive
relationship between the operational performance of the firm and the
financial performance of the firm is mediated by the marketing
performance of the firm. (C) 2010 Elsevier B.V. All rights reserved.},
address = {PO BOX 211, 1000 AE AMSTERDAM, NETHERLANDS},
author = {Inman, R Anthony and Sale, R Samuel and {Green Jr.}, Kenneth W and Whitten, Dwayne},
doi = {10.1016/j.jom.2010.06.001},
issn = {0272-6963},
journal = {JOURNAL OF OPERATIONS MANAGEMENT},
keywords = {Agile manufacturing; JIT systems; Organizational p},
month = {may},
number = {4},
pages = {343--355},
publisher = {ELSEVIER SCIENCE BV},
title = {{Agile manufacturing: Relation to JIT, operational performance and firm performance}},
type = {Article},
volume = {29},
year = {2011}
}
@article{ISI:000236672000004,
abstract = {Purpose - The purpose of this article is to investigate the nature of
the humanitarian aid supply chain and discuss the extent to which
certain business supply chain concepts, particularly supply chain
agility, are relevant to humanitarian aid.
Design/methodology/approach - The paper identifies elements of good
practice in conventional business supply chains and applies them to the
humanitarian aid supply chain, making use of published practice-based
literature and web sites associated with humanitarian aid. Particular
emphasis is placed on the concept of ``agility{\{}''{\}} in supply chain
management. A model of an agile supply chain for humanitarian aid is
developed.
Findings - Humanitarian supply chains have similarities with business
supply chains, but there are significant differences. Many humanitarian
supply chains have a short and unstable existence with an inadequate
link between emergency aid and longer-term developmental aid. Unlike
many business supply chains, typical emergency aid appeals assign
inventory to a particular destination at the supply chain source.
Practical implications - This research note is a starting-point for
empirical studies to test the agile humanitarian supply chain model.
Originality/value - This paper seeks to integrate humanitarian aid
practice with concepts in the academic supply chain literature. In
particular, proposes that humanitarian donors need convincing of the
value of supply chain processes.},
address = {60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
author = {Oloruntoba, R and Gray, R},
doi = {10.1108/13598540610652492},
issn = {1359-8546},
journal = {SUPPLY CHAIN MANAGEMENT-AN INTERNATIONAL JOURNAL},
keywords = {aid agencies; supply chain management},
number = {2},
pages = {115--120},
publisher = {EMERALD GROUP PUBLISHING LIMITED},
title = {{Humanitarian aid: an agile supply chain?}},
type = {Article},
volume = {11},
year = {2006}
}
@article{ISI:000256780400001,
abstract = {Wireless networks and devices have rapidly been gaining popularity over
their wired counterparts. This popularity, in turn, has been generating
an explosive and ever-increasing demand for, and hence creating a
shortage of, the radio spectrum. The reason for this foreseen spectrum
shortage is reported not to be the scarcity of the radio spectrum but
the inefficiency of current spectrum access methods, thus leaving
spectrum opportunities along both the time and frequency dimensions that
wireless devices can exploit. Fortunately, recent technological advances
have made it possible to build software-defined radios (SDRs), which,
unlike traditional radios, can switch from one frequency band to another
at little or no cost. We propose a MAC protocol, called Opportunistic
Spectrum MAC (OS-MAC), for wireless networks equipped with cognitive
radios like SDRs. OS-MAC 1) adaptively and dynamically seeks and
exploits opportunities in both licensed and unlicensed spectra and along
both the time and frequency dimensions, 2) accesses and shares spectrum
among different unlicensed and licensed users, and 3) coordinates with
other unlicensed users for better spectrum utilization. Using extensive
simulation, OS-MAC is shown to be far more effective than current access
protocols from both the network's and the user's perspectives. By
comparing its performance with an Ideal-MAC protocol, OS-MAC is also
shown to not only outperform current access protocols, but also achieve
performance very close to that obtainable under the Ideal-MAC protocol.},
address = {10662 LOS VAQUEROS CIRCLE, PO BOX 3014, LOS ALAMITOS, CA 90720-1314 USA},
author = {Hamdaoui, Bechir and Shin, Kang G},
doi = {10.1109/TMC.2007.70758},
issn = {1536-1233},
journal = {IEEE TRANSACTIONS ON MOBILE COMPUTING},
keywords = {spectrum agility; opportunistic MAC protocols; sof},
month = {aug},
number = {8},
pages = {915--930},
publisher = {IEEE COMPUTER SOC},
title = {{OS-MAC: An efficient MAC protocol for spectrum-agile wireless networks}},
type = {Article},
volume = {7},
year = {2008}
}
@article{ISI:000220125400007,
abstract = {The textiles and apparel industry has been neglected in terms of supply
chain management research. Recently, the industry has undergone a great
deal of change, particularly with global sourcing and high levels of
price competition. In addition, textiles and clothing has market
characteristics, such as short product lifecycle, high volatility, low
predictability, and a high level of impulse purchase, making such issues
as quick response of Paramount importance. This article discusses
characteristics of the textiles and apparel industry and identifies the
perspectives of lean, agile and leagility (a combination of these)
within existing supply chain literature, which have been proffered as
solutions to achieving quick response and reduced lead times. Through
case studies of textile and apparel companies, different approaches to
supply chain management are illustrated.},
address = {60/62 TOLLER LANE, BRADFORD BD8 9BY, W YORKSHIRE, ENGLAND},
author = {Bruce, M and Daly, L and Towers, N},
doi = {10.1108/01443570410514867},
issn = {0144-3577},
journal = {INTERNATIONAL JOURNAL OF OPERATIONS {\&} PRODUCTION MANAGEMENT},
keywords = {supply chain management; textile industry},
number = {1-2},
pages = {151--170},
publisher = {EMERALD GROUP PUBLISHING LIMITED},
title = {{Lean or agile - A solution for supply chain management in the textiles and clothing industry?}},
type = {Article},
volume = {24},
year = {2004}
}
@unpublished{f17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1704.08834},
author = {Frans, K},
eprint = {1704.08834},
title = {{Outline Colorization through Tandem Adversarial Networks.}},
year = {2017}
}
@article{ojs17,
author = {Omar, N and Johari, Z �. and Smith, M},
journal = {Journal of Financial Crime, v.},
title = {{Predicting fraudulent financial reporting using artificial neural network}},
volume = {24},
year = {2017}
}
@book{g18,
address = {Traducao. [s.l.]},
author = {Gazi, O},
publisher = {Springer},
title = {{Understanding digital signal processing}},
year = {2018}
}
@book{sa97,
author = {Smith, S W and Others.},
title = {{The scientist and engineer�s guide to digital signal processing}},
year = {1997}
}
@unpublished{cfs16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1606.00298},
author = {Choi, K and Fazekas, G and Sandler, M},
eprint = {1606.00298},
title = {{Automatic tagging using deep convolutional neural networks.}},
year = {2016}
}
@article{s07,
author = {Stanley, K O},
journal = {Genetic programming and evolvable machines, v.},
title = {{Compositional pattern producing networks: A novel abstraction of development}},
volume = {8},
year = {2007}
}
@book{bbv13,
author = {Boulanger-lewandowski, N and Bengio, Y and Vincent, P},
publisher = {Anais�IEEE},
title = {{High-dimensional sequence transduction}},
year = {2013}
}
@article{e17,
author = {Erickson, B J and Others},
journal = {Journal of digital imaging, v.},
title = {{Toolkits and libraries for deep learning}},
volume = {30},
year = {2017}
}
@article{r,
author = {Rosenblatt, F},
journal = {Traducao. [s.l.] Cornell Aeronautical Laboratory},
title = {{The perceptron a perceiving and recognizing automaton Project Para.}},
volume = {1957}
}
@article{sfh,
author = {Sabour, S and Frosst, N and Hinton, G E},
journal = {Advances in Neural Information Processing Systems},
title = {{Dynamic routing between capsules}},
volume = {2017}
}
@unpublished{g17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1710.11385},
author = {Grinstein, E and Others},
eprint = {1710.11385},
title = {{Audio style transfer.}},
year = {2017}
}
@article{s,
author = {Samples, Sound},
title = {{Philharmonia Orchestra}},
url = {http://www.philharmonia.co.uk/explore/sound{\_}samples{\%}3E},
volume = {2018}
}
@article{18,
journal = {Multilayer Perceptron - DeepLearning 0.},
title = {{Theano}},
url = {http://deeplearning.net/software/theano/{\%}3E},
volume = {1},
year = {2018}
}
@article{a,
author = {Audio, Ivy},
title = {{Ivy Audio}},
url = {http://www.ivyaudio.com/{\%}3E},
volume = {2018}
}
@book{18,
address = {Intel AI Dispon�vel em},
publisher = {{\textless}},
title = {{Neon}},
url = {https://ai.intel.com/neon/{\%}3E},
year = {2018}
}
@article{03,
title = {{The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides}},
volume = {114},
year = {2003}
}
@book{18,
address = {TensorFlow Dispon�vel em},
publisher = {{\textless}},
title = {{TensorFlow}},
url = {https://www.tensorflow.org/{\%}3E},
year = {2018}
}
@unpublished{,
title = {{No Title}},
url = {https://github.com/tesserato/tesserato.github.io{\%}3E}
}
@book{18,
address = {GoogleGoogle Dispon�vel em},
publisher = {{\textless}},
title = {{NSynthSuper}},
url = {https://nsynthsuper.withgoogle.com/{\%}3E},
year = {2018}
}
@book{18,
address = {Caffe | Model Zoo Dispon�vel em},
publisher = {{\textless}},
title = {{Caffe}},
url = {http://caffe.berkeleyvision.org/{\%}3E},
year = {2018}
}
@unpublished{,
title = {{Torch}},
url = {http://torch.ch/{\%}3E}
}
@article{58,
title = {{The perceptron: a probabilistic model for information storage and organization in the brain}},
volume = {65},
year = {1958}
}
@book{18,
address = {Magenta Dispon�vel em},
publisher = {{\textless}},
title = {{Magenta}},
url = {https://magenta.tensorflow.org/{\%}3E},
year = {2018}
}
@article{18,
journal = {Multilayer Perceptron - DeepLearning 0.},
title = {{Theano}},
url = {http://deeplearning.net/software/theano/{\%}3E},
volume = {1},
year = {2018}
}
@book{t,
address = {Keras Documentation, [s.d.]. Dispon�vel em},
author = {The, Keras:},
publisher = {{\textless}},
title = {{Python Deep Learning library}},
url = {https://keras.io/{\%}3E}
}
@article{tpc,
author = {Tuohy, D R and Potter, W D and Center, A I},
journal = {CMC},
title = {{An Evolved Neural Network/HC Hybrid for Tablature Creation in GA-based Guitar Arranging}},
volume = {2006}
}
@incollection{08,
address = {Traducao. [s.l.] p. 399�417},
booktitle = {Handbook of Signal Processing in Acoustics},
publisher = {Springer},
title = {{Digital waveguide architectures for virtual musical instruments}},
year = {2008}
}
@article{f08,
author = {Forgeard, M and Others},
journal = {PloS one, v.},
title = {{Practicing a musical instrument in childhood is associated with enhanced verbal ability and nonverbal reasoning}},
volume = {3},
year = {2008}
}
@book{18,
address = {Intel AI Dispon�vel em},
publisher = {{\textless}},
title = {{Neon}},
url = {https://ai.intel.com/neon/{\%}3E},
year = {2018}
}
@book{lms16,
address = {Anais�},
author = {Larsson, G and Maire, M and Shakhnarovich, G},
edition = {European C},
publisher = {Springer},
title = {{Learning representations for automatic colorization}},
year = {2016}
}
@article{03,
journal = {The simulation of piano string vibration: From physical models to finite difference schemes and digital waveguides},
title = {{\_}{\_}{\_}},
volume = {114},
year = {2003}
}
@unpublished{h17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1706.09558},
author = {Hutchings, P},
eprint = {1706.09558},
title = {{Talking Drums: Generating drum grooves with neural networks.}},
year = {2017}
}
@book{ha17,
author = {Howard, D M and Angus, J},
publisher = {Traducao. [s.l.] Focal press},
title = {{Acoustics and psychoacoustics}},
year = {2017}
}
@article{s,
author = {Sutskever, I and Others},
journal = {International conference on machine learning},
title = {{On the importance of initialization and momentum in deep learning}},
volume = {2013}
}
@article{k,
author = {Kulkarni, T D and Others},
journal = {Advances in Neural Information Processing Systems},
title = {{Deep convolutional inverse graphics network}},
volume = {2015}
}
@unpublished{gs16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1603.05516},
author = {Gracia, X and Sanz-perela, T},
eprint = {1603.05516},
title = {{The wave equation for stiff strings and piano tuning.}},
year = {2016}
}
@book{mp69,
address = {MA},
author = {Minski, M L and Papert, S A Perceptrons:},
publisher = {MIT Press, Cambridge},
title = {an introduction to computational geometry},
year = {1969}
}
@book{das93,
author = {Duyne, Van and A., S and Smith, J O},
edition = {Proceeding},
publisher = {Anais�INTERNATIONAL COMPUTER MUSIC ACCOCIATION},
title = {{Physical modeling with the 2-D digital waveguide mesh}},
year = {1993}
}
@book{18,
address = {TensorFlow Dispon�vel em},
publisher = {{\textless}},
title = {{TensorFlow}},
url = {https://www.tensorflow.org/{\%}3E},
year = {2018}
}
@book{bs12,
author = {B�ck, S and Schedl, M},
publisher = {Anais�IEEE},
title = {{Polyphonic piano note transcription with recurrent neural networks}},
year = {2012}
}
@article{ke04,
author = {Karjalainen, M and Erkut, C},
journal = {EURASIP Journal on Applied Signal Processing, v.},
title = {{Digital waveguides versus finite difference structures: Equivalence and mixed modeling}},
volume = {2004},
year = {2004}
}
@article{rhw,
author = {Rumelhart, D E and Hinton, G E and Williams, R J},
journal = {[s.l.] California Univ San Diego La Jolla Inst for Cognitive Science},
title = {{Learning internal representations by error propagation.}},
volume = {1985}
}
@unpublished{e17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1704.01279},
author = {Engel, J and Others},
eprint = {1704.01279},
title = {{Neural audio synthesis of musical notes with wavenet autoencoders.}},
year = {2017}
}
@book{k16,
author = {Khorrami, P and Others},
edition = {(ICIP), 20},
publisher = {Anais�IEEE},
title = {{How deep neural networks can improve emotion recognition on video dataImage Processing}},
year = {2016}
}
@article{shs01,
author = {Serafin, S and Huang, P and Smith, J},
journal = {Workshop on Future Directions of Computer Music (Mosart-0},
title = {{The banded digital waveguide mesh}},
volume = {1},
year = {2001}
}
@book{yyk15,
address = {Traducao. [s.l.]},
author = {Yadav, N and Yadav, A and Kumar, M},
publisher = {Springer},
title = {{An introduction to neural network methods for differential equations}},
year = {2015}
}
@unpublished{,
title = {{No Title}},
url = {https://github.com/tesserato/tesserato.github.io{\%}3E}
}
@inproceedings{g17,
author = {Gully, A J and Others},
booktitle = {Proc. Interspeech 2017},
publisher = {p. 234�238},
title = {{Articulatory Text-to-Speech Synthesis using the Digital Waveguide Mesh driven by a Deep Neural Network}},
year = {2017}
}
@unpublished{p85,
author = {Parker, D B},
title = {{Learning logic}},
year = {1985}
}
@article{th12,
author = {Tieleman, T and Hinton, G Lecture 6.5-rmsprop:},
journal = {COURSERA: Neural networks for machine learning, v.},
title = {{rmsprop: Divide the gradient by a running average of its recent magnitude}},
volume = {4},
year = {2012}
}
@book{s16,
address = {Traducao. [s.l.] v. 99},
author = {Salsa, S},
publisher = {Springer},
title = {{Partial differential equations in action: from modelling to theory}},
year = {2016}
}
@article{b05,
author = {Bensa, J and Others},
journal = {Acta Acustica united with Acustica, v.},
title = {{Computational modeling of stiff piano strings using digital waveguides and finite differences}},
volume = {91},
year = {2005}
}
@book{d16,
author = {Dozat, T},
title = {{Incorporating nesterov momentum into adam}},
year = {2016}
}
@article{58,
journal = {The perceptron: a probabilistic model for information storage and organization in the brain},
title = {{\_}{\_}{\_}},
volume = {65},
year = {1958}
}
@book{b09,
address = {Traducao. [s.l.] John {\&} Sons},
author = {Bilbao, S},
publisher = {Wiley},
title = {{Numerical sound synthesis: finite difference schemes and simulation in musical acoustics}},
year = {2009}
}
@article{95,
journal = {Applications of Signal Processing to Audio and Acoustics},
title = {{The tetrahedral digital waveguide mesh}},
volume = {1995},
year = {1995}
}
@article{fra,
author = {Fontana, F and Rocchesso, D and Apollonio, E},
journal = {Anais�},
title = {{Using the waveguide mesh in modelling 3D}},
volume = {2000}
}
@article{k,
author = {Karpathy, A and Others},
journal = {Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
title = {{Large-scale video classification with convolutional neural networks}},
volume = {2014}
}
@article{mp43,
author = {Mcculloch, W S and Pitts, W A},
journal = {The bulletin of mathematical biophysics, v.},
title = {logical calculus of the ideas immanent in nervous activity},
volume = {5},
year = {1943}
}
@article{xac,
author = {Xu, W and Auli, M and Clark, S Ccg},
journal = {CL (},
title = {{Supertagging with a Recurrent Neural Network}},
volume = {2}
}
@unpublished{,
title = {the open source drumkit},
url = {https://github.com/crabacus/the-open-source-drumkit{\%}3E}
}
@book{gmh13,
author = {Graves, A and Mohamed, A.-r. and Hinton, G},
edition = {Acoustics,},
publisher = {Anais�IEEE},
title = {{Speech recognition with deep recurrent neural networks}},
year = {2013}
}
@article{o,
author = {Oord, A Van Den and Others},
journal = {Advances in Neural Information Processing Systems},
title = {{Conditional image generation with pixelcnn decoders}},
volume = {2016}
}
@article{k,
author = {Klambauer, G and Others},
journal = {Anais�},
title = {{Self-normalizing neural networksAdvances in Neural Information Processing Systems}},
volume = {2017}
}
@book{18,
address = {GoogleGoogle Dispon�vel em},
publisher = {{\textless}},
title = {{NSynthSuper}},
url = {https://nsynthsuper.withgoogle.com/{\%}3E},
year = {2018}
}
@article{w,
author = {Wiki, DrumGizmo},
title = {{DrumGizmo Wiki}},
url = {https://www.drumgizmo.org/wiki/doku.php{\%}3E},
volume = {2018}
}
@book{e,
address = {Dispon�vel em},
author = {Eck, D},
publisher = {{\textless}},
title = {{Making music using new sounds generated with machine learning}},
url = {https://www.blog.google/technology/ai/making-music-using-new-sounds-generated-machine-learning/{\%}3E}
}
@book{b16,
address = {Traducao. [s.l.]},
author = {Bovermann, T and Others},
publisher = {Springer},
title = {{Musical Instruments in the 21st Century}},
year = {2016}
}
@book{h15,
author = {He, L and Others},
edition = {Proceeding},
publisher = {Anais�ACM},
title = {{Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks}},
year = {2015}
}
@unpublished{s15,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1507.06947},
author = {Sak, H and Others},
eprint = {1507.06947},
title = {{Fast and accurate recurrent neural network acoustic models for speech recognition.}},
year = {2015}
}
@unpublished{e17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1704.01279},
author = {Engel, J and Others},
eprint = {1704.01279},
title = {{Neural audio synthesis of musical notes with wavenet autoencoders.}},
year = {2017}
}
@book{z17,
author = {Zweig, G and Others},
edition = {Acoustics,},
publisher = {Anais�IEEE},
title = {{Advances in all-neural speech recognition}},
year = {2017}
}
@book{ck16,
address = {Traducao. [s.l.]},
author = {Chaigne, A and Kergomard, J},
publisher = {Springer},
title = {{Acoustics of musical instruments}},
year = {2016}
}
@book{s16,
author = {Staudt, P},
title = {{Development of a Digital Musical Instrument with Embedded Sound Synthesis}},
year = {2016}
}
@article{bff09,
author = {Brunette, E S and Flemmer, R C and Flemmer, C L A},
pages = {4},
title = {review of artificial intelligence},
volume = {2009},
year = {2009}
}
@book{16,
address = {neon, theano, and torch for deep learning},
publisher = {Comparative study of caffe},
title = {{Comparative study of caffe, neon, theano, and torch for deep learning}},
year = {2016}
}
@unpublished{o16,
annote = {. arXiv preprint},
archivePrefix = {arXiv},
arxivId = {1609.03499},
author = {Et al, Van Den Oord A},
eprint = {1609.03499},
title = {{Wavenet: A generative model for raw audio}},
year = {2016}
}
@article{s87,
author = {Sorensen, H V and Others},
journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing, v.},
title = {{Real-valued fast Fourier transform algorithms}},
volume = {35},
year = {1987}
}
@article{18,
journal = {Multilayer Perceptron - DeepLearning 0.},
title = {{Theano}},
url = {http://deeplearning.net/software/theano/{\%}3E},
volume = {1},
year = {2018}
}
@book{s16,
author = {Shi, S and Others},
edition = {Cloud Comp},
publisher = {Anais�IEEE},
title = {{Benchmarking state-of-the-art deep learning software tools}},
year = {2016}
}
@article{sc,
author = {Sarroff, A M and Casey, M A},
journal = {ICMC},
title = {{Musical audio synthesis using autoencoding neural nets}},
volume = {2014}
}
@book{zie16,
address = {Anais�},
author = {Zhang, R and Isola, P and Efros, A A},
edition = {European C},
publisher = {Springer},
title = {{Colorful image colorization}},
year = {2016}
}
@article{rn,
author = {Russell, S J and Norvig, P},
title = {{Artificial intelligence: a modern approach}},
volume = {2016}
}
@article{s,
author = {Samples, Musical Instrument},
title = {{University of Iowa Electronic Music Studios}},
url = {http://theremin.music.uiowa.edu/MIS.html{\%}3E},
volume = {2018}
}
@unpublished{i16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, P and Others},
eprint = {1611.07004},
title = {{Image-to-image translation with conditional adversarial networks.}},
year = {2016}
}
@article{l,
author = {Lab, Berkeley Artificial Intelligence Research},
title = {{The Berkeley Artificial Intelligence Research Blog}},
url = {http://bair.berkeley.edu/{\%}3E},
volume = {2018}
}
@article{rdd13,
author = {Rigaud, F and David, B and Daudet, L},
journal = {The Journal of the Acoustical Society of America, v.},
title = {{A parametric model and estimation techniques for the inharmonicity and tuning of the piano}},
volume = {133},
year = {2013}
}
@article{dhs11,
author = {Duchi, J and Hazan, E and Singer, Y},
journal = {Journal of Machine Learning Research, v.},
title = {{Adaptive subgradient methods for online learning and stochastic optimization}},
volume = {12},
year = {2011}
}
@article{v00,
author = {Vaughn, K},
journal = {Journal of aesthetic education, v.},
title = {{Music and mathematics: Modest support for the oft-claimed relationship}},
volume = {34},
year = {2000}
}
@article{a,
author = {Audio, Ivy},
title = {{Ivy Audio}},
url = {http://www.ivyaudio.com/{\%}3E},
volume = {2018}
}
@unpublished{,
title = {{Torch}},
url = {http://torch.ch/{\%}3E}
}
@unpublished{z12,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1212.5701},
author = {Zeiler, M D},
eprint = {1212.5701},
title = {{Adadelta:an adaptive learning rate method.}},
year = {2012}
}
@article{oa,
author = {Olson, T and Levitt.},
title = {{Applied Fourier Analysis}},
volume = {2017}
}
@book{p17,
author = {Parvat, A and Others},
edition = {Inventive },
publisher = {Anais�IEEE},
title = {{A survey of deep-learning frameworks}},
year = {2017}
}
@article{dsf14,
author = {Dalgleish, M and Spencer, S and Foster, C},
journal = {CIM},
title = {{Blurring The Lines: An Integrated Compositional Model For Digital Musical Instrument}},
volume = {14},
year = {2014}
}
@article{rt17,
author = {Risi, S and Togelius, J},
journal = {IEEE Transactions on Computational Intelligence and AI in Games, v.},
title = {{Neuroevolution in games:State of the art and open challenges}},
volume = {9},
year = {2017}
}
@unpublished{b15,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1511.06435},
author = {Bahrampour, S and Others},
eprint = {1511.06435},
title = {{Comparative study of deep learning software frameworks. arXiv}},
year = {2015}
}
@book{dv16,
address = {mar},
author = {Dumoulin, V and Visin, F A},
publisher = {ArXiv e-prints},
title = {guide to convolution arithmetic for deep learning},
year = {2016}
}
@article{e17,
author = {Esteva, A and Others},
journal = {Nature, v.},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
volume = {542},
year = {2017}
}
@article{sbd16,
author = {Sigtia, S and Benetos, E and Dixon, S},
journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), v.},
title = {{An end-to-end neural network for polyphonic piano music transcription}},
volume = {24},
year = {2016}
}
@article{s15,
author = {Schmidhuber, J},
journal = {Neural networks, v.},
title = {{Deep learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@book{w18,
address = {DeepMind Dispon�vel em},
publisher = {{\textless}},
title = {{WaveNet:, A.Generative Model for Raw Audio}},
url = {https://deepmind.com/blog/wavenet-generative-model-raw-audio/{\%}3E},
year = {2018}
}
@article{ssh,
author = {Southall, C and Stables, R and Hockman, J},
journal = {SMIR},
title = {{Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks}},
volume = {2016}
}
@article{g16,
author = {Goodfellow, I and Others},
journal = {Traducao. [s.l.] MIT press Cambridge v.},
title = {{Deep learning}},
volume = {1},
year = {2016}
}
@book{hz16,
author = {Hwang, J and Zhou, Y},
title = {{Image Colorization with Deep Convolutional Neural Networks}},
year = {2016}
}
@article{r06,
author = {Reiffenstein, T Codification},
journal = {The Canadian Geographer/Le G�ographe canadien, v.},
title = {patents and the geography of knowledge transfer in the electronic musical instrument industry},
volume = {50},
year = {2006}
}
@book{p17,
author = {Pang, Y and Others},
publisher = {IEEE Transactions on Neural Networks and Learning Systems},
title = {{Convolution in convolution for network in network}},
year = {2017}
}
@unpublished{m17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1711.11160},
author = {Mital, P K},
eprint = {1711.11160},
title = {{Time Domain Neural Audio Style Transfer.}},
year = {2017}
}
@book{s06,
address = {Stanford University. stanford. edu/ jos/swgt},
author = {Smith, J A},
publisher = {Center for Computer Research in Music and Acoustics (CCRMA)},
title = {basic introduction to digital waveguide synthesis (for the technically inclined)},
url = {http://ccrma},
year = {2006}
}
@article{r,
author = {Raschka, S},
title = {{Python machine learning}},
volume = {2015}
}
@article{l,
author = {Laird, J A},
journal = {[s.l.] University of Bristol},
title = {{The physical modelling of drums using digital waveguides.}},
volume = {2001}
}
@article{mdn00,
author = {Mizutani, E and Dreyfus, S E and Nishio, K},
journal = {Neural Networks},
title = {{On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application}},
volume = {2000},
year = {2000}
}
@article{pw,
author = {Pereyra, M C and Ward, L A},
journal = {Traducao. [s.l.] {\{}A{\}}merican Mathematical Soc},
title = {{Harmonic analysis:from Fourier to wavelets.}},
volume = {2012}
}
@article{s92,
author = {Smith, J O},
journal = {Computer music journal, v.},
title = {{Physical modeling using digital waveguides}},
volume = {16},
year = {1992}
}
@inproceedings{nyc15,
author = {Nguyen, A and Yosinski, J and Clune, J},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {427--436},
title = {{Deep neural networks are easily fooled: High confidence predictions for unrecognizable images}},
year = {2015}
}
@article{zf,
author = {Zwicker, E and Fastl, H},
journal = {Traducao. [s.l.] Springer Science {\&} Business Media},
title = {{Psychoacoustics: Facts and models.}},
volume = {2013}
}
@article{c,
author = {Coppin, B},
title = {{Artificial intelligence illuminated}},
volume = {2004}
}
@article{h91,
author = {Hornik, K},
journal = {Neural networks, v.},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{isi16,
author = {Iizuka, S and Simo-serra, E and Ishikawa, H},
journal = {ACM Transactions on Graphics (TOG), v.},
title = {{Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification}},
volume = {35},
year = {2016}
}
@book{18,
address = {Magenta Dispon�vel em},
publisher = {{\textless}},
title = {{Magenta}},
url = {https://magenta.tensorflow.org/{\%}3E},
year = {2018}
}
@book{v,
address = {Dispon�vel em},
author = {Veen, F Van.},
edition = {The Asimov},
publisher = {{\textless}},
title = {{The Neural Network Zoo}},
url = {http://www.asimovinstitute.org/neural-network-zoo/{\%}3E}
}
@article{ss,
author = {Stein, E M and Shakarchi, R},
journal = {Traducao. [s.l.] Princeton University Press},
title = {{Princeton lectures in analysis.}},
volume = {2003}
}
@article{s15,
author = {Sainath, T N and Others},
journal = {Neural Networks, v.},
title = {{Deep convolutional neural networks for large-scale speech tasks}},
volume = {64},
year = {2015}
}
@book{l11,
author = {Lyons, R G},
publisher = {Traducao. [s.l.] Pearson Education India},
title = {{Understanding Digital Signal Processing, 3/E}},
year = {2011}
}
@book{haks18,
author = {Horowitz, M C and Allen, G C and Kania, E B and Scharre, P},
title = {{Strategic Competition in an Era of Artificial Intelligence}},
year = {2018}
}
@book{z16,
address = {Anais�},
author = {Zhu, J.-y. and Others},
edition = {European C},
publisher = {Springer},
title = {{Generative visual manipulation on the natural image manifold}},
year = {2016}
}
@article{wh,
author = {Widrow, B and Hoff, M E},
journal = {[s.l.] STANFORD UNIV CA STANFORD ELECTRONICS LABS},
title = {{Adaptive switching circuits}},
volume = {1960}
}
@book{r18,
author = {Roberts, A and Others},
title = {{Learning Latent Representations of Music to Generate Interactive Musical Palettes}},
year = {2018}
}
@unpublished{,
title = {{No Title}},
url = {https://github.com/tesserato/tesserato.github.io{\%}3E}
}
@unpublished{kb14,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1412.6980},
author = {Kingma, D P and Ba, J},
eprint = {1412.6980},
title = {{Adam: A method for stochastic optimization.}},
year = {2014}
}
@article{cos17,
author = {Costa, Y M and Oliveira, L S and Silla, C N},
journal = {Applied Soft Computing, v.},
title = {{An evaluation of Convolutional Neural Networks for music classification using spectrograms}},
volume = {52},
year = {2017}
}
@article{hsw89,
author = {Hornik, K and Stinchcombe, M and White, H},
journal = {Neural networks, v.},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{e90,
author = {Elman, J L},
journal = {Cognitive science, v.},
title = {{Finding structure in time}},
volume = {14},
year = {1990}
}
@unpublished{dmp18,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1802.04208},
author = {Donahue, C and Mcauley, J and Puckette, M},
eprint = {1802.04208},
title = {{Synthesizing Audio with Generative Adversarial Networks.}},
year = {2018}
}
@unpublished{r16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, S},
eprint = {1609.04747},
title = {{An overview of gradient descent optimization algorithms.}},
year = {2016}
}
@book{rkk18,
author = {Reddi, S J and Kale, S and Kumar, S},
title = {{On the convergence of adam and beyond}},
year = {2018}
}
@article{aa16,
author = {Aljumah, A and Ahamad, T A},
journal = {International Journal of Computer Science and Network Security, v.},
title = {{novel approach for detecting DDoS using artificial neural networks}},
volume = {16},
year = {2016}
}
@article{s07,
author = {Serra, X},
journal = {Multimedia Signal Processing},
title = {{State of the art and future directions in musical sound synthesis}},
volume = {2007},
year = {2007}
}
@article{pg,
author = {Patterson, Josh and Gibson, Adam},
title = {{Deep Learning: A Practitioner's Approach. " O'Reilly Media, Inc."}},
volume = {2017}
}
@book{hsf18,
author = {Hinton, G E and Sabour, S and Frosst, N},
title = {{Matrix capsules with EM routing}},
year = {2018}
}
@unpublished{z17,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1701.02720},
author = {Zhang, Y and Others},
eprint = {1701.02720},
title = {{Towards end-to-end speech recognition with deep convolutional neural networks.}},
year = {2017}
}
@article{vwb,
author = {Veit, A and Wilber, M J and Belongie, S},
journal = {Advances in Neural Information Processing Systems},
title = {{Residual networks behave like ensembles of relatively shallow networks}},
volume = {2016}
}
@unpublished{s16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1612.00835},
author = {Sangkloy, P and Others},
eprint = {1612.00835},
title = {{Scribbler: Controlling deep image synthesis with sketch and color.}},
year = {2016}
}
@book{ykt17,
author = {Yang, Y and Krompass, D and Tresp, V},
title = {{Tensor-Train Recurrent Neural Networks for Video Classification}},
year = {2017}
}
@book{p18,
author = {Pfalz, A},
title = {{Generating Audio Using Recurrent Neural Networks}},
year = {2018}
}
@article{geb,
author = {Gatys, L A and Ecker, A S and Bethge, M},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
title = {{Image style transfer using convolutional neural networks}},
volume = {2016}
}
@article{m,
author = {Mullen, J},
journal = {[s.l.] University of York},
title = {{Physical modelling of the vocal tract with the 2D digital waveguide mesh.}},
volume = {2006}
}
@unpublished{okk16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {Oord, A Van Den and Kalchbrenner, N and Kavukcuoglu, K},
eprint = {1601.06759},
title = {{Pixel recurrent neural networks.}},
year = {2016}
}
@article{18,
journal = {Multilayer Perceptron - DeepLearning 0.},
title = {{Theano}},
url = {http://deeplearning.net/software/theano/{\%}3E},
volume = {1},
year = {2018}
}
@book{18,
address = {Caffe | Model Zoo Dispon�vel em},
publisher = {{\textless}},
title = {{Caffe}},
url = {http://caffe.berkeleyvision.org/{\%}3E},
year = {2018}
}
@article{mhn,
author = {Maas, A L and Hannun, A Y and Ng, A Y},
journal = {ICML. Anais�},
title = {{Rectifier nonlinearities improve neural network acoustic models}},
volume = {2013}
}
@inproceedings{wl90,
address = {n. 9, p. 1415�1442},
author = {Widrow, B and Lehr, M A},
booktitle = {Proceedings of the IEEE},
publisher = {v. 78},
title = {30 years of adaptive neural networks: perceptron, madaline, and backpropagation},
year = {1990}
}
@article{Poczos1993,
author = {Poczos, Lecturer Barnabas},
number = {1982},
pages = {1--10},
title = {{Perceptron History of Artificial Neural Networks The Neuron}},
year = {1993}
}
@article{Mital2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1711.11160v1},
author = {Mital, Parag K},
eprint = {arXiv:1711.11160v1},
number = {Nips},
title = {{Time Domain Neural Audio Style Transfer}},
year = {2017}
}
@article{,
title = {{UNCLASSIFIED LIMITATION CHANGES TO : FROM :}},
year = {1995}
}
@article{Rosenblatt,
author = {Rosenblatt, F and Nonr-, Contract},
number = {6},
pages = {386--408},
title = {{THE PERCEPTRON : A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION}},
volume = {65}
}
@book{Papert,
author = {Papert, Seymour A},
isbn = {0262631113},
title = {{Perceptrons}}
}
@article{Diego1985,
author = {Diego, S A N},
number = {V},
title = {862 18 120,},
year = {1985}
}
@article{Staudt,
author = {Staudt, Pascal},
title = {{Development of a Digital Musical Instrument with Embedded Sound Synthesis}}
}
@article{Serra,
author = {Serra, Xavier},
keywords = {and music composition,experimental psychology and neurosciences,including,most smc research is,psychoacoustics,psychology,quite applied,signal processing and electronics,sound and music computing,sound synthesis},
pages = {2--5},
title = {{State of the Art and Future Directions in Musical Sound Synthesis}}
}
@article{,
title = {{No Title}}
}
@article{Harrison2015,
author = {Harrison, Reginald L and Bilbao, Stefan},
title = {{Edinburgh Research Explorer An algorithm for a valved brass instrument synthesis environment using finite-difference time-domain methods with performance optimisation AN ALGORITHM FOR A VALVED BRASS INSTRUMENT SYNTHESIS ENVIRONMENT USING FINITE-DIFFERENCE}},
year = {2015}
}
@article{Sound1990,
author = {Sound, A},
number = {4},
title = {{Spectral Modeling Syrthesis : A Sound Analysis / Sytrthesis System Based on a Detenninistic plus Stochastic Decomposition}},
volume = {14},
year = {1990}
}
@article{Serra2000,
author = {Serra, Xavier and Bonada, Jordi},
number = {February 2013},
title = {{Sound Transformations Based on the SMS High Level Attributes Sound Transformations Based on the SMS High Level Attributes}},
year = {2000}
}
@article{,
title = {{Percussion Instrument Modelling In 3D : Sound Synthesis Through Time Domain Numerical Simulation University of Edinburgh}}
}
@article{Hahn2017,
author = {Hahn, Henrik},
title = {{Expressive sampling synthesis. Learning extended source-filter models from instrument sound databases for expressive sample manipulations}},
year = {2017}
}
@article{Serafin1996,
author = {Serafin, Stefania and Erkut, Cumhur and Kojs, Juraj and Nilsson, Niels C and Nordahl, Rolf and Gables, Coral},
doi = {10.1162/COMJ},
title = {{Virtual Reality Musical Instruments : State of the Art , Design Principles , and Future Directions}},
year = {1996}
}
@article{Bonada2016,
author = {Bonada, Jordi and Blaauw, Merlijn},
keywords = {[Electronic Manuscript]},
pages = {1230--1234},
title = {{Expressive Singing Synthesis based on Unit Selection for the Singing Synthesis Challenge 2016}},
year = {2016}
}
@article{Selfridge,
author = {Selfridge, Rod and Moffat, David J and Reiss, Joshua D},
keywords = {aeolian harp,physical model,real-time,sound synthesis},
pages = {1--8},
title = {{REAL-TIME PHYSICAL MODEL OF AN AEOLIAN HARP}}
}
@article{Blaauw2010,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.03809v3},
author = {Blaauw, Merlijn and Bonada, Jordi and Group, Music Technology and Fabra, Universitat Pompeu},
eprint = {arXiv:1704.03809v3},
pages = {1--9},
title = {{A n p s s}},
year = {2010}
}
@article{Zappi2017,
author = {Zappi, Victor and Allen, Andrew and Fels, Sidney},
pages = {145--150},
title = {{Shader-based Physical Modelling for the Design of Massive Digital Musical Instruments}},
year = {2017}
}
@article{Unterthiner2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1706.02515v5},
author = {Unterthiner, Thomas and Sep, L G and Hochreiter, Sepp},
eprint = {arXiv:1706.02515v5},
title = {{Self-Normalizing Neural Networks}},
year = {2017}
}
@article{Bensa2003,
author = {Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Iii, Julius O Smith},
doi = {10.1121/1.1587146},
title = {{Edinburgh Research Explorer The simulation of piano string vibration The simulation of piano string vibration : From physical models to finite difference schemes and digital waveguides}},
year = {2003}
}
@article{Lokki2018,
author = {Lokki, Tapio},
doi = {10.3390/app8040518},
pages = {1--5},
title = {{applied sciences Special Issue on " Sound and Music Computing "}},
year = {2018}
}
@article{Morse,
author = {Morse, Bryan},
title = {{Magnitude and Phase The Fourier Transform : Examples , Properties , Common Pairs CS 450 : Introduction to Digital Signal and Image Processing Example : Fourier Transform of a Cosine Example : Fourier Transform of a Cosine Odd and Even Functions Sinusoids }}
}
@article{,
number = {2},
pages = {43--55},
title = {{Digital Synthesis of and Plucked-String Timbres}},
volume = {7},
year = {2008}
}
@article{Karjalainen2004,
author = {Karjalainen, Matti},
keywords = {acoustic signal processing,and phrases,digital waveguides,fdtd model structures,hybrid models,scattering},
pages = {978--989},
title = {{Digital Waveguides versus Finite Difference Structures : Equivalence and Mixed Modeling}},
year = {2004}
}
@article{Chaigne1994,
author = {Chaigne, Antoine and Cedex, Paris},
number = {February},
pages = {1112--1118},
title = {{Numerical simulations of piano strings . I . A physical model for a struck string using finite difference methods}},
volume = {95},
year = {1994}
}
@article{Corporation1959,
author = {Corporation, Westinghouse Electric and Pittsburgh, East and Arbor, Ann},
number = {4},
pages = {978--986},
title = {{L : L : {\{} L : L : {\{} L : L :}}}},
volume = {31},
year = {1959}
}
@misc{Dewick2002,
abstract = {This paper seeks to contribute towards the construction and application of a method to assess the long-term impact of the development of their technological technologies on the environment. The paper identifies the effect of three pervasive technologies – biotechnology, information technology and nanotechnology – on the production of a range of sectors and their consequent environmental effects. The sectors are selected according to taxonomies of characteristics. The technological impact is assessed qualitatively in terms of changes in production scale and resource intensity and their resulting impact on industrial greenhouse gas emissions},
author = {Dewick, Paul and Green, Ken and Miozzo, Marcela},
number = {January},
pages = {1--31},
title = {{Technological Change, Industry Structure and the Environment}},
year = {2002}
}
@misc{,
title = {{1983 Extensions of the Karplus-Strong Plucked String Algorithm.pdf}}
}
@article{Jarvelainen2001,
author = {J{\"{a}}rvel{\"{a}}inen, Hanna and V{\"{a}}lim{\"{a}}ki, Vesa and Karjalainen, Matti},
doi = {10.1121/1.1374756},
number = {April},
pages = {79--84},
title = {{Audibility of the timbral effects of inharmonicity in stringed instrument tones}},
volume = {2},
year = {2001}
}
@article{Masri,
author = {Masri, P},
number = {0},
title = {{USING DIGITAL WAVEGUIDES}},
volume = {44}
}
@article{Laird2001,
author = {Laird, Joel Augustus},
number = {November},
title = {{THE PHYSICAL MODELLING OF DRUMS USING DIGITAL}},
year = {2001}
}
@article{Molteno2004,
author = {Molteno, Timothy C},
doi = {10.1119/1.1764557},
title = {{An experimental investigation into the dynamics of a string}},
year = {2004}
}
@article{Oø,
author = {{\`{O}}{\o}, {\^{O}} {\"{O}} and {\^{U}}{\'{o}}{\"{o}}, {\"{O}} {\O} Ð{\textordmasculine} {\"{O}} {\~{N}} and {\O}{\"{o}}, {\`{O}} {\O} {\`{O}} {\'{O}} {\`{O}} {\^{U}} {\'{O}} {\"{O}} and {\`{O}}{\o}, {\O} {\`{O}} {\O}{\"{o}}{\`{u}}{\~{n}} and {\~{N}}{\'{o}}, Ð {\O} {\O}{\'{y}}{\textordmasculine} {\`{I}} and {\'{O}}{\"{o}}, Ð {\"{O}} {\`{O}}{\o}{\"{o}}{\'{o}} {\`{U}} {\`{O}} {\O} and {\O}, {\"{O}} {\'{O}} {\~{N}} {\O} {\'{O}} {\`{U}} {\O} {\'{O}} {\`{O}} {\'{U}} Ð {\'{O}} and {\"{E}}{\o}{\"{o}}, {\`{O}} {\`{O}} and {\'{O}}{\`{o}}, {\'{A}}{\`{o}}{\o}{\"{o}}{\'{o}} {\`{U}} {\O} and {\'{A}}{\`{o}}, Ð {\`{O}} {\O} and {\'{O}}{\`{o}}, {\O} Ð {\'{O}}{\"{o}}{\~{n}}{\`{u}}ð {\O} and {\O}{\'{o}}{\"{o}}, {\O} {\'{O}}{\`{o}}ð{\'{y}} {\"{O}} and {\`{O}}{\`{u}}, {\`{I}} {\^{O}} {\^{O}} {\"{O}} {\'{U}} {\`{O}} {\O} {\'{O}} {\'{U}} {\O} {\'{O}} {\`{O}} {\`{O}} {\`{O}}{\o}{\"{o}}{\'{o}} {\`{U}}},
title = {{{\`{I}}{\'{o}}{\^{u}} {\"{o}} × {\aa} {\o} {\"{o}} ð {\aa}{\'{o}} ðð {\`{o}} {\`{o}} {\`{e}} {\'{y}}× ð {\aa}{\'{o}} ð× {\'{i}}× {\`{o}} {\"{i}} {\'{u}} {\`{u}} ×}}
}
@article{Iii2003,
author = {Iii, Julius O Smith},
pages = {1--5},
title = {{A Basic Introduction to Digital Waveguide Synthesis ( for the Technically Inclined ) Basics of Digital Waveguide Modeling}},
year = {2003}
}
@article{Garder2005,
author = {G{\"{a}}rder, Anders},
title = {{Physical modeling of percussion instruments}},
year = {2005}
}
@article{Arts2008,
author = {Arts, Sonic},
number = {September},
title = {{Physical modelling of the piano : An investigation into the e ff ect of string sti ff ness on the hammer-string interaction}},
year = {2008}
}
@article{Huang2008,
author = {Huang, Norden E and Wu, Zhaohua},
doi = {10.1029/2007RG000228.1.INTRODUCTION},
number = {2007},
pages = {1--23},
title = {{A REVIEW ON HILBERT-HUANG TRANSFORM : METHOD AND ITS APPLICATIONS}},
year = {2008}
}
@article{Duda2011,
author = {Duda, Krzysztof and Magalas, Leszek B and Majewski, Mariusz},
number = {11},
pages = {3608--3618},
title = {{DFT-based Estimation of Damped Oscillation Parameters in Low-Frequency Mechanical Spectroscopy}},
volume = {60},
year = {2011}
}
@article{Bensa2006,
author = {Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Smith, Julius and Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Smith, Julius and Computa-, Thierry Voinier and Bensa, Julien and Bilbao, Stefan and Kronland-martinet, Richard and Iii, Julius O Smith and Voinier, Thierry},
pages = {0--10},
title = {{Computational modeling of stiff piano strings using digital waveguides and finite difference To cite this version : HAL Id : hal-00088061 Computational Modeling of Stiff Piano Strings Using Digital Waveguides and Finite Differences}},
year = {2006}
}
@article{Jos2009,
author = {Jos, Juan},
number = {April},
title = {{The Acoustics of the Piano}},
year = {2009}
}
@article{David2012,
author = {David, Matthew and Speed, Adam},
title = {{Voice Synthesis Using the Three-Dimensional Digital Waveguide Mesh}},
year = {2012}
}
@article{Society2012,
author = {Society, Acoustical and Identifier, Digital Object and Document, Explorer and Society, Acoustical and Publisher, America and Statement, Rights and Society, Acoustical},
doi = {10.1121/1.3651240},
number = {1},
pages = {914--925},
title = {{Edinburgh Research Explorer Time domain simulation and sound synthesis for the snare drum}},
volume = {131},
year = {2012}
}
@article{Ptb2015,
author = {Ptb, Physikalisch-technische Bundesanstalt},
number = {4},
title = {{COMPARISON OF HILBERT TRANSFORM AND SINE FIT APPROACHES FOR THE DETERMINATION OF DAMPING PARAMETERS}},
volume = {1},
year = {2015}
}
@article{Kartofelev,
author = {Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-maria},
title = {{MODELING A VIBRATING STRING TERMINATED AGAINST A BRIDGE WITH ARBITRARY GEOMETRY}}
}
@article{Gr2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.05516v2},
author = {Gr, Xavier},
eprint = {arXiv:1603.05516v2},
keywords = {inharmonic spectrum,musical,stiffness,vibrating string,wave equation},
pages = {1--16},
title = {{The wave equation for stiff strings and piano tuning}},
volume = {2},
year = {2016}
}
@article{,
title = {{MODELLING THE DECAY OF PIANO SOUNDS Tian Cheng , Simon Dixon , Matthias Mauch Centre for Digital Music , Queen Mary University of London , London , United Kingdom}},
volume = {1}
}
@article{Serra2014,
author = {Serra, Xavier},
number = {November},
title = {{Spectral Modeling Synthesis : Past and Present Spectral Modeling Synthesis : Past and Present Spectral Analysis / Synthesis}},
year = {2014}
}
@article{Berdahl,
author = {Berdahl, Edgar J and Iii, Julius O Smith},
pages = {1--14},
title = {{Plucked String Digital Waveguide Model}}
}
@article{Deserio,
author = {Deserio, Robert},
number = {6},
title = {{Addendum : The Fourier transform of decaying oscillations}}
}
@article{Serafin,
author = {Serafin, Stefania and Huang, Patty and Iii, Julius O Smith},
pages = {2--5},
title = {{The banded digital waveguide mesh}}
}
@article{Ði,
author = {Ð{\"{i}}, {\O} {\'{U}} {\`{U}}},
title = {{{\AA}{\`{u}}× ð {\'{a}}{\`{o}}×{\o}{\"{o}}{\`{u}}{\~{n}} {\`{o}}{\o} {\aa}{\'{o}} ðð {\`{o}} {\'{i}}× {\`{o}} {\o} ð{\"{i}} {\'{u}} {\`{u}} ×}}
}
@article{Iii,
author = {Iii, Smith},
title = {{Physical Modeling Using Digital Waveguides}}
}
@article{Bank2010,
author = {Bank, Balazs and Fontana, Federico},
doi = {10.1109/TASL.2010.2040524},
number = {May 2014},
title = {{A Modal-Based Real-Time Piano Synthesizer}},
year = {2010}
}
@article{Electronic2017,
author = {Electronic, A N and Of, Journal and Catalana, Societat and Atiques, D E Matem},
doi = {10.2436/20.2002.02.11.1},
keywords = {00a65,2010,35g16,35l05,dissonance,inharmonic spectrum,msc,musical scale,stiffness,string,vibrating,wave equation},
pages = {1--16},
title = {{The wave equation for stiff strings and piano tuning}},
volume = {3},
year = {2017}
}
@article{Iiia,
author = {Iii, Julius O Smith},
title = {{Digital Waveguide Architectures for Virtual Musical Instruments}}
}
@article{David2013,
author = {David, Bertrand and Daudet, Laurent},
doi = {10.1121/1.4799806},
title = {{A parametric model and estimation techniques for the inharmonicity and tuning of the piano a )}},
year = {2013}
}
@article{Salih2016,
author = {Salih, A},
number = {December},
pages = {1--24},
title = {{Second-Order Wave Equation d ' Alembert ' s Solution in Infinite Domain}},
year = {2016}
}
@article{Feldman2000,
author = {Feldman, Joel},
pages = {1--2},
title = {{Derivation of the Wave Equation}},
year = {2000}
}
@article{h96,
author = {Hagan, M T and Others},
journal = {Traducao. [s.l.] Pws Pub},
title = {{Neural network design}},
volume = {20},
year = {1996}
}
@article{tb,
author = {Theis, L and Bethge, M},
journal = {Advances in Neural Information Processing Systems},
title = {{Generative image modeling using spatial LSTMs}},
volume = {2015}
}
@article{hsw89,
author = {Hornik, K and Stinchcombe, M and White, H},
journal = {Neural networks, v.},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@unpublished{gs16,
annote = {preprint},
archivePrefix = {arXiv},
arxivId = {1603.05516},
author = {Gracia, X and Sanz-perela, T},
eprint = {1603.05516},
title = {{The wave equation for stiff strings and piano tuning.}},
year = {2016}
}
@incollection{l12,
address = {Traducao. [s.l.] p. 9�48},
author = {Lecun, Y A and Others},
booktitle = {Neural networks: Tricks of the trade},
publisher = {Springer},
title = {{Efficient backprop}},
year = {2012}
}
@unpublished{18,
annote = {arXiv preprint},
archivePrefix = {arXiv},
arxivId = {1803.05428},
eprint = {1803.05428},
title = {{A Hierarchical Latent Vector Model for Learning Long-Term Structure in Music.}},
year = {2018}
}
@article{,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.08243v2},
eprint = {arXiv:1709.08243v2},
title = {{No Title}}
}
@article{Liao,
author = {Liao, Qianli and Leibo, Joel Z and Poggio, Tomaso},
title = {{How Important Is Weight Symmetry in Backpropagation ?}}
}
@article{Ruder2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.04747v2},
author = {Ruder, Sebastian},
eprint = {arXiv:1609.04747v2},
pages = {1--14},
title = {{An overview of gradient descent optimization}},
year = {2016}
}
@article{Tomar2017,
author = {Tomar, Shikha and Sumathi, Parasuraman and Member, Senior},
pages = {1--9},
title = {{Amplitude and Frequency Estimation of Exponentially Decaying Sinusoids}},
year = {2017}
}
@article{Smith2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1506.01186v6},
author = {Smith, Leslie N},
eprint = {arXiv:1506.01186v6},
number = {April},
title = {{Cyclical Learning Rates for Training Neural Networks}},
year = {2015}
}
@article{Gully2017,
author = {Gully, Amelia J and Yoshimura, Takenori and Murphy, Damian T and Hashimoto, Kei and Nankaku, Yoshihiko and Tokuda, Keiichi},
keywords = {[Electronic Manuscript]},
pages = {234--238},
title = {{Articulatory Text-to-Speech Synthesis using the Digital Waveguide Mesh driven by a Deep Neural Network}},
year = {2017}
}
@article{Nielsen2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1612.01010v2},
author = {Nielsen, Frank},
eprint = {arXiv:1612.01010v2},
title = {{DeepBach: a Steerable Model for Bach Chorales Generation}},
year = {2017}
}
@article{Jaderberg,
archivePrefix = {arXiv},
arxivId = {arXiv:1608.05343v2},
author = {Jaderberg, Max and Graves, Alex},
eprint = {arXiv:1608.05343v2},
title = {{Decoupled Neural Interfaces using Synthetic}},
volume = {1}
}
@article{Louizos2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1712.01312v1},
author = {Louizos, Christos and Welling, Max},
eprint = {arXiv:1712.01312v1},
pages = {1--13},
title = {{L EARNING S PARSE N EURAL N ETWORKS THROUGH L 0 R EGULARIZATION}},
year = {2017}
}
@article{Gabrielli2017,
author = {Gabrielli, Leonardo and Squartini, Stefano and Tomassetti, Stefano and Zinato, Carlo},
pages = {11--16},
title = {{INTRODUCING DEEP MACHINE LEARNING FOR PARAMETER ESTIMATION IN PHYSICAL MODELLING}},
year = {2017}
}
@article{Bello2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1709.07417v2},
author = {Bello, Irwan and Zoph, Barret and Vasudevan, Vijay and Le, Quoc V},
eprint = {arXiv:1709.07417v2},
number = {2002},
title = {{Neural Optimizer Search with Reinforcement Learning}},
year = {2017}
}
@article{Kalchbrenner2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1610.10099v2},
author = {Kalchbrenner, Nal and Espeholt, Lasse and Simonyan, Karen},
eprint = {arXiv:1610.10099v2},
title = {{Neural Machine Translation in Linear Time}},
year = {2016}
}
@article{Peng2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.02717v1},
author = {Peng, X U E B I N and Abbeel, Pieter},
eprint = {arXiv:1804.02717v1},
title = {{DeepMimic : Example-Guided Deep Reinforcement Learning of Physics-Based Character Skills}},
year = {2017}
}
@article{Eisenach2017,
author = {Eisenach, Carson and Wang, Zhaoran},
pages = {1--23},
title = {{N ONPARAMETRICALLY L EARNING A CTIVATION F UNCTIONS IN D EEP N EURAL N ETS}},
year = {2017}
}
@article{For2018,
author = {For, Earching},
pages = {1--13},
title = {{Earching for}},
year = {2018}
}
@article{Ephrat2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1804.03619v1},
author = {Ephrat, Ariel and Hebrew, The and Freeman, William T and Rubinstein, Michael and Jon, Only and Rory, Only},
eprint = {arXiv:1804.03619v1},
title = {{Looking to Listen at the Cocktail Party : A Speaker-Independent Audio-Visual Model for Speech Separation}},
year = {2017}
}
@article{Donoso2007,
author = {Donoso, Pedro and Tann, Alberto and Guimar, Francisco},
keywords = {acoustics,helmholtz,musical instruments,resonance,violin},
number = {December},
title = {{A f ´ ısica do violino}},
year = {2007}
}
@article{Donahue2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1802.04208v1},
author = {Donahue, Chris and Mcauley, Julian and Puckette, Miller},
eprint = {arXiv:1802.04208v1},
title = {{Synthesizing Audio with Generative Adversarial Networks}},
year = {2014}
}
@misc{,
title = {{1962 On Estimation of a Probability Density Function and Mode.pdf}}
}
@article{,
title = {{Institute of Mathematical Statistics is collaborating with JSTOR to digitize, preserve, and extend access to The Annals of Mathematical Statistics. {\textregistered} www.jstor.org}}
}
@article{Bernroider2013,
author = {Bernroider, Edward and Schm{\"{o}}llerl, Patrick},
title = {{ePub WU Institutional Repository A technological , organisational , and environmental analysis}},
year = {2013}
}
@article{Zopounidis2002,
author = {Zopounidis, Constantin and Doumpos, Michael},
keywords = {classification,decision rules,multiple criteria analysis,outranking relations,preference,sorting,utility functions},
pages = {229--246},
title = {{Multicriteria classification and sorting methods : A literature review}},
volume = {138},
year = {2002}
}
@article{Ishizaka2017,
author = {Ishizaka, Alessio and Siraj, Sajid},
doi = {10.1016/j.ejor.2017.05.041},
issn = {0377-2217},
journal = {European Journal of Operational Research},
keywords = {AHP,Decision analysis,Experimental evaluation,MACBETH,SMART},
publisher = {Elsevier B.V.},
title = {{PT}},
url = {http://dx.doi.org/10.1016/j.ejor.2017.05.041},
year = {2017}
}
@article{Zamani-sabzi2016,
author = {Zamani-sabzi, Hamed and Phillip, James and Gard, Charlotte C and Abudu, Shalamu},
doi = {10.1016/j.orp.2016.11.001},
issn = {2214-7160},
journal = {Operations Research Perspectives},
keywords = {Statistical analysis of ranking methods,Fuzzy envi,methods,statistical analysis of ranking},
pages = {92--117},
publisher = {Elsevier Ltd},
title = {{Statistical and analytical comparison of multi-criteria decision-making techniques under fuzzy environment}},
url = {http://dx.doi.org/10.1016/j.orp.2016.11.001},
volume = {3},
year = {2016}
}
@article{Carayannis2018,
author = {Carayannis, Elias G and Ferreira, Jo{\~{a}}o J M and Jalali, Marjan S and Ferreira, Fernando A F},
doi = {10.1016/j.techfore.2018.01.028},
issn = {0040-1625},
journal = {Technological Forecasting {\&} Social Change},
number = {xxxx},
pages = {0--1},
publisher = {Elsevier},
title = {{Technological Forecasting {\&} Social Change MCDA in knowledge-based economies : Methodological developments and real world applications}},
url = {http://dx.doi.org/10.1016/j.techfore.2018.01.028},
year = {2018}
}
@article{Marttunen2017,
author = {Marttunen, Mika and Lienert, Judit and Belton, Valerie},
doi = {10.1016/j.ejor.2017.04.041},
issn = {0377-2217},
journal = {European Journal of Operational Research},
keywords = {Multi-methodology,Multi-stakeholder decision-making,Multiple Criteria Decision Analysis,Problem Structuring},
publisher = {Elsevier B.V.},
title = {{PT US CR}},
url = {http://dx.doi.org/10.1016/j.ejor.2017.04.041},
year = {2017}
}
@article{Guitouni1998,
author = {Guitouni, Adel and Martel, Jean-marc},
keywords = {aggregation procedure,behavioural considerations,comparative analysis,decision making situation,multicriteria analysis,multicriterion,multicriterion decision aid method,preferences modelling},
pages = {501--521},
title = {{Tentative guidelines to help choosing an appropriate MCDA method}},
volume = {109},
year = {1998}
}
@article{Solomon1998,
author = {Solomon, Anthony and Wishart, Nicole and Dublish, Sandipa},
keywords = {multiple criteria analysis},
number = {97},
title = {{Multi-attribute decision making : A simulation comparison of select methods}},
volume = {2217},
year = {1998}
}
@article{Jacquet-lagr2001,
author = {Jacquet-lagr, Eric},
keywords = {1,criteria,general philosophy,goal programming,in decision-making involving multiple,introduction and background,multicriteria analysis,preference disaggregation,regression},
pages = {233--245},
title = {{Preference disaggregation : 20 years of MCDA experience}},
volume = {130},
year = {2001}
}
@article{Information2000,
author = {Information, Background and Of, Description and Mcdm, Some},
title = {{Chapter 2 MULTI-CRITERIA DECISION MAKING METHODS 2.1}},
year = {2000}
}
@article{Dehe2015,
author = {Dehe, Benjamin and Bamford, David},
doi = {10.1016/j.eswa.2015.04.059},
issn = {0957-4174},
journal = {Expert Systems With Applications},
keywords = {ahp,analytical hierarchy process,er,evidential reasoning,mcda,multiple criteria decision analysis},
number = {19},
pages = {6717--6727},
publisher = {Elsevier Ltd},
title = {{Expert Systems with Applications Development , test and comparison of two Multiple Criteria Decision Analysis ( MCDA ) models : A case of healthcare infrastructure location}},
url = {http://dx.doi.org/10.1016/j.eswa.2015.04.059},
volume = {42},
year = {2015}
}
@article{Behzadian2010,
author = {Behzadian, Majid and Kazemzadeh, R B and Albadvi, A and Aghdasi, M},
doi = {10.1016/j.ejor.2009.01.021},
issn = {0377-2217},
journal = {European Journal of Operational Research},
number = {1},
pages = {198--215},
publisher = {Elsevier B.V.},
title = {{PROMETHEE : A comprehensive literature review on methodologies and applications}},
url = {http://dx.doi.org/10.1016/j.ejor.2009.01.021},
volume = {200},
year = {2010}
}
@article{Leoneti2016,
author = {Leoneti, Alexandre Bevilacqua},
doi = {10.1016/j.orp.2016.04.001},
issn = {2214-7160},
journal = {Operations Research Perspectives},
keywords = {Utility function,Group Multicriteria Decision,MAUT},
pages = {21--26},
publisher = {Elsevier Ltd},
title = {{Utility Function for modeling Group Multicriteria Decision Making problems as games}},
url = {http://dx.doi.org/10.1016/j.orp.2016.04.001},
volume = {3},
year = {2016}
}
@article{Govindan2015,
author = {Govindan, Kannan and Jepsen, Martin Brandt},
doi = {10.1016/j.ejor.2015.07.019},
issn = {0377-2217},
journal = {European Journal of Operational Research},
keywords = {ELECTRE,Literature review,Multiple criteria decision aiding (MCDA),Outranking},
publisher = {Elsevier Ltd.},
title = {{PT US CR}},
url = {http://dx.doi.org/10.1016/j.ejor.2015.07.019},
year = {2015}
}
@article{Stefano2015,
author = {Stefano, Nara Medianeira and Stefano, N M and Filho, N Casarotto and Vergara, L G L and Rocha, R U G},
doi = {10.1109/TLA.2015.7404925},
number = {January 2016},
title = {{State of the Art Research and its Applications COPRAS ( Complex Proportional Assessment ): State of the Art Research and its Applications}},
year = {2015}
}
@article{Costa2016,
author = {Costa, Helder Gomes},
doi = {10.1108/JM2-08-2013-0037},
title = {{Graphical interpretation of outranking principles Avoiding misinterpretation results from}},
year = {2016}
}
@article{Oliveira,
author = {Oliveira, G A Q S M and Seleme, R and Zattar, I C},
keywords = {multicriteria,project management,roadmap,smart grids},
title = {{Smart Grid Performance Assessment Via Approach Method}}
}
@article{Mix2017,
author = {Mix, Power and Dester, Mauricio and Dester, M},
doi = {10.1109/TLA.2016.7459613},
keywords = {energy storage systems,intermittent sources,mix,multi-criteria decision analysis,power,renewable sources},
number = {March 2016},
title = {{Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Reliability of Electricity Supply Regarding the Integration of Intermittent Sources in Brazil ' s Power Mix}},
year = {2017}
}
@book{Analysis,
author = {Analysis, Multi-criteria Decision},
isbn = {9781119974079},
title = {{No Title}}
}
@article{Jesus,
author = {Jesus, T O De and Soares, M S},
keywords = {management,requirements engineering,requirements traceability},
title = {{A Multi-Criteria Analysis of Techniques and Tools for Tracing Software Requirements}}
}
@article{Widrow1990,
author = {Widrow, Bernard and Lehr, Michael A},
number = {9},
pages = {1415--1442},
title = {{30 Years of Adaptive Neural Networks : Perceptron , Madaline , and Backpropagation}},
volume = {78},
year = {1990}
}
@misc{,
title = {{1988 A theoretical framework for back-propagation.pdf}}
}
@article{Mizutani,
author = {Mizutani, Eiji},
title = {{On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application}}
}
@book{,
isbn = {080581258X},
title = {{No Title}}
}
@article{Makin2006,
author = {Makin, J G},
pages = {1--8},
title = {{Backpropagation}},
year = {2006}
}
@article{Introduction1996,
author = {Introduction, A Systematic},
title = {{Neural Networks}},
year = {1996}
}
@article{,
number = {Xxx},
title = {{t{\~{a}}o simples e compacta quanto poss{\'{i}}vel , adotando-se , para tanto , a nota{\c{c}}{\~{a}}o matricial . Embora matematicamente equivalente {\`{a}}s deriva{\c{c}}{\~{o}}es apresentadas em ( XXX ), optou-se por uma abordagem direta com a inten{\c{c}}{\~{a}}o de tornar mais intuitivo o entendimento do}}
}
@article{Sathyanarayana2014,
author = {Sathyanarayana, Shashi and Ph, D},
pages = {1--15},
title = {{A Gentle Introduction to Backpropagation What is so difficult about designing a neural}},
year = {2014}
}
@book{Patterson,
author = {Patterson, Josh and Gibson, Adam},
isbn = {9781491914250},
title = {{Deep Learning}}
}
@misc{,
title = {c1992backpropagationand.pdf}
}
@article{Sabour2017,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.09829v1},
author = {Sabour, Sara and Oct, C V and Hinton, Geoffrey E},
eprint = {arXiv:1710.09829v1},
number = {Nips},
title = {{Dynamic Routing Between Capsules}},
year = {2017}
}
@book{Zocca,
author = {Zocca, Valentino and Slater, Daniel},
isbn = {9781786464453},
title = {{Python Deep Learning}}
}
@misc{,
title = {{Matthieu Ricard, Trinh Xuan Thuan-The quantum and the lotus{\_} a journey to the frontiers where science and Buddhism meet-Three Rivers Press (2004).pdf}}
}
@article{,
title = {{No Title}}
}
@article{Bahrampour2016,
author = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
pages = {1--11},
title = {{C OMPARATIVE S TUDY OF C AFFE , N EON , T HEANO , AND T ORCH FOR D EEP L EARNING}},
year = {2016}
}
@article{Hinton2018,
author = {Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
pages = {1--15},
title = {{M ATRIX CAPSULES WITH EM ROUTING}},
year = {2018}
}
@article{Shi,
archivePrefix = {arXiv},
arxivId = {arXiv:1608.07249v7},
author = {Shi, Shaohuai and Wang, Qiang and Xu, Pengfei and Chu, Xiaowen},
eprint = {arXiv:1608.07249v7},
title = {{Benchmarking State-of-the-Art Deep Learning Software Tools}}
}
@article{Bahrampour,
archivePrefix = {arXiv},
arxivId = {arXiv:1511.06435v3},
author = {Bahrampour, Soheil and Ramakrishnan, Naveen and Schott, Lukas and Shah, Mohak},
eprint = {arXiv:1511.06435v3},
title = {{Comparative Study of Deep Learning Software Frameworks}}
}
@article{Sarroff,
author = {Sarroff, Andy M and Casey, Michael},
number = {1},
title = {{MUSICAL AUDIO SYNTHESIS USING AUTOENCODING NEURAL NETS}},
volume = {1}
}
@article{Erickson2017,
author = {Erickson, Bradley J and Korfiatis, Panagiotis and Akkus, Zeynettin and Kline, Timothy and Philbrick, Kenneth},
doi = {10.1007/s10278-017-9965-6},
keywords = {Artificial intelligence,Machine learning,Deep lear,artificial intelligence,as the proper weighting,convolutional neural network,deep,deep learning approaches learn,learning,machine learning,of those features to,tant features as well,the algorithm learns,the impor-},
pages = {400--405},
publisher = {Journal of Digital Imaging},
title = {{Toolkits and Libraries for Deep Learning}},
year = {2017}
}
@article{Grinstein,
archivePrefix = {arXiv},
arxivId = {arXiv:1710.11385v1},
author = {Grinstein, Eric and Duong, Ngoc Q K and Ozerov, Alexey and Patrick, P},
eprint = {arXiv:1710.11385v1},
title = {{No Title}}
}
@article{Simonyan2016,
archivePrefix = {arXiv},
arxivId = {arXiv:1609.03499v2},
author = {Simonyan, Karen and Dieleman, Sander and Senior, Andrew and Graves, Alex},
eprint = {arXiv:1609.03499v2},
pages = {1--15},
title = {{W n : a g m r a}},
year = {2016}
}
@article{Jing,
archivePrefix = {arXiv},
arxivId = {arXiv:1705.04058v1},
author = {Jing, Yongcheng},
eprint = {arXiv:1705.04058v1},
title = {{Neural Style Transfer: A Review}}
}
@article{Engel2009,
archivePrefix = {arXiv},
arxivId = {arXiv:1704.01279v1},
author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas},
eprint = {arXiv:1704.01279v1},
title = {{Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders}},
year = {2009}
}
@article{Abnisa2014,
abstract = {What distinguishes a good manuscript from a bad one?},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Abnisa, Faisal and {Wan Daud}, Wan Mohd Ashri and Athanassiou, M and Zabaniotou, A and Bridgwater, a V and Brown, Duncan and Rowe, Andrew and Wild, Peter and Chen, Dezhen and Yin, Lijie and Wang, Huan and He, Pinjing and Chen, Qun and Yang, Ruiming and Zhao, Bo and Li, Yan and Wang, Shujuan Shaobin and Wu, Hongwei and Zhuo, Yuqun and Chen, Changhe and Cross, Andrew and Sohi, Saran P. and Ding, Hong-Sheng and Jiang, Hong and Francescato and G{\'{o}}mez, Natalia and Rosas, Jos{\'{e}} Guillermo and Cara, Jorge and Mart{\'{i}}nez, Olegario and Alburquerque, Jos{\'{e}} Antonio and S{\'{a}}nchez, Marta Elena and Kaiser, E R and Kempegowda, Rajesh S. and Skreiberg, {\O}yvind and Tran, Khanh-quang Quang and Kuppens, Tom and {Van Dael}, Miet and Vanreppelen, Kenny and Thewys, Theo and Yperman, Jan and Carleer, Robert and Schreurs, Sonja and {Van Passel}, Steven and Lehmann, Johannes and S, Naprave Podjetja Paloma D D and Soediono, Budi and Total, Outcome Defective Non-defective and Uslu, Ayla and Faaij, Andr{\'{e}} P.C. and Bergman, P.C.A. and Yang, Hua and Kudo, Shinji and Kuo, Hsiu Po and Norinaga, Koyo and Mori, Aska and Ma{\v{s}}ek, Ondřej and Hayashi, Jun Ichiro and Singh, M and S{\'{e}}t{\'{a}}l{\'{o}}, G and Guan, X and Warren, M and Toran-Allerand, C D and Brownsort, Peter and Dickinson, Dane and Rogers, J.G. and Brammer, J.G. and Sohi, Saran P. and Crombie, Kyle and Ma{\v{s}}ek, Ondřej and S, Naprave Podjetja Paloma D D and Ma{\v{s}}ek, Ondřej and Konno, Miki and Hosokai, Sou and Sonoyama, Nozomu and Norinaga, Koyo and Hayashi, Jun Ichiro and Athanassiou, M and Zabaniotou, A and Input, Biomass and Gases, Uncondensed H O T and Pyrolysis, Slow and Prost, Katharina and Borchard, Nils and Siemens, Jan and Kautz, Timo and S{\'{e}}quaris, Jean-Marie and M{\"{o}}ller, Andreas and Amelung, Wulf and Ronsse, Frederik and van Hecke, Sven and Dickinson, Dane and Prins, Wolter and Abiven, S and Singh, N and Maestrini, B and Rogovska, Natalia and Laird, David A. and Cruse, Richard and Fleming, Pierce and Parkin, Tim and Meek, David and Spokas, Kurt a and Yoder, Jonathan and Galinato, Suzette and Granatstein, David and Garcia-P{\'{e}}rez, Manuel and Galgani, Pietro and van der Voet, Ester and Korevaar, Gijsbert and {San Miguel}, G. and Dom{\'{i}}nguez, M. P. and Hern{\'{a}}ndez, M. and Sanz-P{\'{e}}rez, F. and Results, Publishable and Bernal, M P and Alburquerque, Jos{\'{e}} Antonio and Moral, R and Bott, R and Lewandowski, Clare M. and Co-investigator, New and Lewandowski, Clare M. and {(UKBRC/The University of Edinburgh)}, Peter a Brownsort and {Biochar Farms} and Preto, Fernando and Klinglm, Michaela and Crombie, Kyle and Ma{\v{s}}ek, Ondřej and Shackley, Simon and Hammond, Jim and Gaunt, John and Ibarrola, Rodrigo and Daugaard, Daren E. and Brown, Robert C. and Krull, E and Singh, Bp and Downie, Ms a and D, Deliverable W P and Kemp, Ian C. and Fyhr, B. Christran and Laurent, Stephane and Roques, Michel a. and Groenewold, Carda E. and Tsotsas, Evangelos and Sereno, Alberto a. and Bonazzi, Cathenne B. and Bimbenet, Jean-Jacques and Kind, Mathhues and EBC and Certificate, European Biochar and Troy, Shane M. and Nolan, Tereza and Leahy, James J. and Lawlor, Peadar G. and Healy, Mark G. and Kwapinski, Witold and Organisation, The International and Iso, E N and Iso, E N and Iso, E N and Iso, E N and Kauffman, Nathan and Dumortier, Jerome and Hayes, Dermot J. and Brown, Robert C. and Laird, David A. and Yang, Hua and Kudo, Shinji and Kuo, Hsiu Po and Norinaga, Koyo and Mori, Aska and Ma{\v{s}}ek, Ondřej and Hayashi, Jun Ichiro and Rogers, J.G. and Brammer, J.G. and Coskun, C. and Oktay, Z. and Ilten, N. and Energy, Relative and $\Delta$e, Fwhm $\Gamma$ and Lehmann, Johannes and Hansen, Veronika and M{\"{u}}ller-St{\"{o}}ver, Dorette and Ahrenfeldt, Jesper and Holm, Jens Kai and Henriksen, Ulrik Birk and Hauggaard-Nielsen, Henrik and Rauch, Sidney J. and G{\'{o}}mez, Natalia and Rosas, Jos{\'{e}} Guillermo and Cara, Jorge and Mart{\'{i}}nez, Olegario and Alburquerque, Jos{\'{e}} Antonio and S{\'{a}}nchez, Marta Elena and Chen, Qun and Yang, Ruiming and Zhao, Bo and Li, Yan and Wang, Shujuan Shaobin and Wu, Hongwei and Zhuo, Yuqun and Chen, Changhe and Raveendran, K and Definition, Product and Standards, Specification and Tomlinson, Thayer and Initiative, International Biochar and Minerals, Bio Carbon and Required, Inorganic C and Class, Minimum and Declaration, Required and Declaration, Required and Method, Standard Test and Determination, Rapid and Content, Carbonate and Method, Standard Test and Analysis, Chemical and Charcoal, Wood and Iso, D I N E N and Faculty, Power Engineering and Faculty, Power Engineering and Faculty, Power Engineering and Faculty, Power Engineering and He, Fang and Yi, Weiming and Bai, Xueyuan and Contents, Table O F and Conversion, Energy and Table, O and Kuppens, Tom and Dael, Miet Van and Vanreppelen, Kenny and Carleer, Robert and Miller-robbie, Leslie and Ulrich, Bridget A and Ramey, Dotti F and Spencer, Kathryn S and Herzog, Skuyler P and Cath, Tzahi Y and Stokes, Jennifer R and Higgins, Christopher P and Prot{\'{a}}sio, Thiago De Paula and Trugilho, Paulo Fernando and Napoli, Alfredo and Wei, Quanyuan and Qu, Yongshui and Tan, Tianwei and Dhillon, R S and Wuehlisch, George Von and Huff, G A and Vasalos, I A and Sharma, Abhishek and Pareek, Vishnu and Wang, Shujuan Shaobin and Zhang, Zhezi and Shinde, Yogesh and Pareek, Vishnu and Zhang, Dongke and Cowdery, T B Reed C D and Building, Crew and Gustafsson, Mattias and Reckamp, Joseph M and Garrido, Rene A and Satrio, Justinus A and Crombie, Kyle and Chen, Dezhen and Yin, Lijie and Wang, Huan and He, Pinjing and Chen, Qun and Yang, Ruiming and Zhao, Bo and Li, Yan and Wang, Shujuan Shaobin and Wu, Hongwei and Zhuo, Yuqun and Chen, Changhe and Bridgwater, a V and Crombie, Kyle and Ma{\v{s}}ek, Ondřej and Rosas, Guillermo and Cara, Jorge and Mart{\'{i}}nez, Olegario and Sohi, Saran P. and Brownsort, Peter and Carter, Sarah and Cook, Jason and Cunningham, Colin and Gaunt, John and Ibarrola, Rodrigo and Ma{\v{s}}ek, Ondřej and Sims, Kirsten and Thornley, Patricia and D, Deliverable W P and Welfle, Andrew and Gilbert, Paul and Thornley, Patricia and Him, Tsz and Pleissner, Daniel and Yan, Kin and Venus, Joachim and Pommeret, Aude and Sze, Carol and Lin, Ki and Sommerfeldt, Nelson and Madani, Hatef and Kuppens, Tom and Dael, Miet Van and Vanreppelen, Kenny and Thewys, Theo and Yperman, Jan and Carleer, Robert and Schreurs, Sonja and Passel, Steven Van and Kempegowda, Rajesh S. and Skreiberg, {\O}yvind and Tran, Khanh-quang Quang and Stelt, M J C Van Der and Gerhauser, H and Kiel, J H A and Ptasinski, K J and Uslu, Ayla and Beach, Long and Park, Won Chan and Service, Forest and Simpson, William T and Ivb, Interreg},
doi = {10.1016/j.biortech.2014.03.134},
eprint = {arXiv:1011.1669v3},
isbn = {1757-1693},
issn = {09619534},
journal = {Biomass and Bioenergy},
number = {1},
pages = {1--10},
pmid = {15003161},
title = {{How to get published}},
volume = {5},
year = {2014}
}
@article{,
abstract = {literature review},
journal = {2016},
title = {{A literature review is a description of the literature relevant to a particular field or topic. It gives an overview of:}}
}
@misc{Hastie2009,
abstract = {During the past decade there has been an explosion in computation and information technology. With it has come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It should be a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting-the first comprehensive treatment of this topic in any book. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie wrote much of the statistical modeling software in S-PLUS and invented principal curves and surfaces. Tibshirani proposed the Lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, and projection pursuit. FROM THE REVIEWS: TECHNOMETRICS "This is a vast and complex book. Generally, it concentrates on explaining why and how the methods work, rather than how to use them. Examples and especially the visualizations are principle features...As a source for the methods of statistical learning...it will probably be a long time before there is a competitor to this book."},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
booktitle = {Bayesian Forecasting and Dynamic Models},
doi = {10.1007/b94608},
eprint = {arXiv:1011.1669v3},
isbn = {978-0-387-84857-0},
issn = {0172-7397},
pages = {1--694},
pmid = {12377617},
title = {{The Elements of Statistical Learning}},
url = {http://www.springerlink.com/index/10.1007/b94608},
volume = {1},
year = {2009}
}
@book{Thesis,
author = {Thesis, Better},
isbn = {9783319042855},
title = {{No Title}}
}
@article{Doctoral,
author = {Doctoral, Helping and Write, Students},
title = {{No Title}}
}
@book{Petre2010,
abstract = {"I feel grateful to have found this book only a year into my PhD. It has opened my eyes to the world of academia. There is more to a PhD than just research in the sense of working on a problem, getting some results and publishing your findings. This book has allowed me to open my eyes and see all the other things I should be doing to fully succeed at my endeavour of becoming a researcher myself." Dominic Hosler, University of Sheffield This bestselling book on the process of PhD research provides readers with engaging discussion and comprehensive guidance on aspects that other books don't usually mention. Covering all the key topics of the previous edition, including what a PhD is really about, how to do one well, how to decipher what your supervisor actually means by terms like 'good referencing' and 'clean research question', and how to design, report and defend your research,the authors continue to offer an accessible, down-to-earth, and insightful account of the whole PhD process. Their advice addresses how to avoid some of the pitfalls en route to a successful submission. Updated throughout, the new edition includes new material on: Critical thinking Research skills The route to research independence Different models of study The Unwritten Rules of PhD Research is essential reading for anyone considering a PhD or embarking on one. It will tell you the things many students wish someone had told them before they started.},
author = {Petre, M and Rugg, G},
booktitle = {Vasa},
doi = {10.1049/em:20040508},
isbn = {9780335237029},
issn = {14724677},
pages = {320},
pmid = {1275585},
title = {{The unwritten rules of PhD research}},
url = {http://books.google.com/books?id={\_}DDwCqx6wpcC{\&}printsec=frontcover{\&}dq=unwritten+rules+of+phd+research{\&}hl={\&}cd=1{\&}source=gbs{\_}api{\%}255Cnpapers2://publication/uuid/48967E01-55F9-4397-B941-310D9C5405FA{\%}255Cnhttp://medcontent.metapress.com/index/A65RM03P4874243N.p},
year = {2010}
}
@article{Tabei1996,
author = {Tabei, Makoto and Musicus, Bruce R.},
doi = {10.1109/78.506615},
issn = {1053587X},
journal = {IEEE Transactions on Signal Processing},
number = {6},
pages = {1504--1511},
title = {{A simple estimator for frequency and decay rate}},
volume = {44},
year = {1996}
}
@article{Robel,
author = {Robel, Axel},
title = {{Signals}}
}
@article{Brown1991,
abstract = {In two recent papers, a description is given of a means of obtaining an arbitrarily narrow peak in the calculation of the autocorrelation function [J. C. Brown and M. S. Puckette, "Calculation of a narrowed autocorrelation function," J. Acoust. Soc. Am. 85, 1595?1601 (1989)] or of a narrow valley in the calculation of an inverse autocorrelation [J. C. Brown and M. S. Puckette, "Musical information from a narrowed autocorrelation function," Proceedings of the 1987 International Conference on Computer Music, Urbana, Illinois, 84?88 (1987)]. These calculations are applied to the determination of the fundamental frequency of musical signals produced by keyboard, wind, and string instruments. These results are compared to frequency tracking results obtained on these sounds with conventional autocorrelation. In so doing it is determined first whether the method of autocorrelation is well-adapted to the problem of tracking the frequency of musical signals, and, second, under what conditions "narrowed" autocorrelation is advantageous. {\textcopyright}1991 Acoustical Society of America.},
author = {Brown, Judith C and Zhang, Bin},
doi = {10.1121/1.400923},
issn = {00014966},
journal = {J. Acoust. Soc. Am.},
number = {5},
pages = {2346--2354},
title = {{Musical frequency tracking using the methods of conventional and "narrowed" autocorrelation}},
volume = {89},
year = {1991}
}
@article{Provencher1976,
abstract = {A method based on the Fourier convolution theorem is developed for the analysis of data composed of random noise, plus an unknown constant "base line," plus a sum of (or an integral over a continuous spectrum of) exponential decay functions. The Fourier method's usual serious practical limitation of needing high accuracy data over a very wide range is eliminated by the introduction of convergence parameters and a Gaussian taper window. A computer program is described for the analysis of discrete spectra, where the data involves only a sum of exponentials. The program is completely automatic in that the only necessary inputs are the raw data (not necessarily in equal intervals of time); no potentially biased initial guesses concerning either the number or the values of the components are needed. The outputs include the number of components, the amplitudes and time constants together with their estimated errors, and a spectral plot of the solution. The limiting resolving power of the method is studied by analyzing a wide range of simulated two-, three-, and four-component data. The results seem to indicate that the method is applicable over a considerably wider range of conditions than nonlinear least squares or the method of moments. {\textcopyright} 1976, The Biophysical Society. All rights reserved.},
author = {Provencher, S. W.},
doi = {10.1016/S0006-3495(76)85660-3},
isbn = {0006-3495},
issn = {00063495},
journal = {Biophysical Journal},
number = {1},
pages = {27--41},
pmid = {1244888},
title = {{A Fourier method for the analysis of exponential decay curves}},
volume = {16},
year = {1976}
}
@misc{Martucci1994,
abstract = {This paper discusses the use of symmetric convolution and the$\backslash$ndiscrete sine and cosine transforms (DSTs and DCTs) for general digital$\backslash$nsignal processing. The operation of symmetric convolution is a$\backslash$nformalized approach to convolving symmetrically extended sequences. The$\backslash$nresult is the same as that obtained by taking an inverse discrete$\backslash$ntrigonometric transform (DTT) of the product of the forward DTTs of$\backslash$nthose two sequences. There are 16 members in the family of DTTs. Each$\backslash$nprovides a representation for a corresponding distinct type of$\backslash$nsymmetric-periodic sequence. The author defines symmetric convolution,$\backslash$nrelates the DSTs and DCTs to symmetric-periodic sequences, and then use$\backslash$nthese principles to develop simple but powerful$\backslash$nconvolution-multiplication properties for the entire family of DSTs and$\backslash$nDCTs. Symmetric convolution can be used for discrete linear filtering$\backslash$nwhen the filter is symmetric or antisymmetric. The filtering will be$\backslash$nefficient because fast algorithms exist for all versions of the DTTs.$\backslash$nConventional linear convolution is possible if one first zero-pad the$\backslash$ninput data. Symmetric convolution and its fast implementation using DTTs$\backslash$nare now an alternative to circular convolution and the DFT},
author = {Martucci, Stephen A.},
booktitle = {IEEE Transactions on Signal Processing},
doi = {10.1109/78.295213},
issn = {19410476},
number = {5},
pages = {1038--1051},
title = {{Symmetric Convolution and the Discrete Sine and Cosine Transforms}},
volume = {42},
year = {1994}
}
@article{Smith1999,
abstract = {Use of a bilinear conformai map to achieve a frequency warping nearly identical to that of the Bark frequency scale is described. Because the map takes the unit circle to itself, its form is that of the transfer function of a first-order allpass filter. Since it is a first-order map, it preserves the model order of rational systems, making it a valuable frequency warping technique for use in audio filter design. A closed-form weighted-equationor method is derived that computes the optimal mapping coefficient as a function of sampling rate, and the solution is shown to be generally indistinguishable from the optimal least-squares solution. The optimal Chebyshev mapping is also found to be essentially identical to the optimal least-squares solution. The expression 0.8517 [arctan(0.06583fs)]1/2-0.916 is shown to accurately approximate the optimal allpass coefficient as a function of sampling rate fs in kHz for sampling rates greater than 1 kHz. A filter design example is included that illustrates improvements due to carrying out the design over a Bark scale. Corresponding results are also given and compared for approximating the related "equivalent rectangular bandwidth (ERB) scale≤ of Moore and Glasberg using a first-order allpass transformation. Due to the higher frequency resolution called for by the ERB scale, particularly at low frequencies, the first-order conformal map is less able to follow the desired mapping, and the error is two to three times greater than the Bark-scale case, depending on the sampling rate. {\textcopyright} 1999 IEEE Publisher Item Identifier S 1063-6676(99)07979-1.},
author = {Smith, Julius O.},
doi = {10.1109/89.799695},
isbn = {- 1063-6676},
issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Bark,Bilinear transform,ERB,Filter design,Frequency warping},
number = {6},
pages = {697--708},
pmid = {799695},
title = {{Bark and ERB Bilinear Transforms}},
volume = {7},
year = {1999}
}
@article{Silvescu1999,
abstract = {A new kind of neuron model that has a Fourier-like in/out function$\backslash$nis introduced. The model is discussed in a general theoretical framework$\backslash$nand some completeness theorems are presented. Current experimental$\backslash$nresults show that the new model outperforms, by a large margin both in$\backslash$nrepresentational power and convergence speed, the classical mathematical$\backslash$nmodel of neuron based on weighted sum of inputs filtered by a nonlinear$\backslash$nfunction. The new model is also appealing from a neurophysiological$\backslash$npoint of view because it produces a more realistic representation by$\backslash$nconsidering the inputs as oscillations},
author = {Silvescu, A.},
doi = {10.1109/IJCNN.1999.831544},
isbn = {0-7803-5529-6},
issn = {1098-7576},
journal = {IJCNN'99. International Joint Conference on Neural Networks. Proceedings (Cat. No.99CH36339)},
pages = {488--491},
title = {{Fourier neural networks}},
url = {http://ieeexplore.ieee.org/document/831544/},
volume = {1},
year = {1999}
}
@article{Monti2000,
author = {Monti, Giuliano and Sandler, Mark},
number = {1},
pages = {7--10},
title = {{No Title}},
year = {2000}
}
@article{Rowland1999,
abstract = {The usual suggestion for the longitudinally propagating momentum carried by a transverse wave on a string is shown to lead to paradoxes. Numerical simulations provide clues for resolving these paradoxes. The usual formula for wave momentum should be changed by a factor of 2 and the involvement of the cogenerated longitudinal waves is shown to be of crucial importance.},
author = {Rowland, David R. and Pask, Colin},
doi = {10.1119/1.19272},
issn = {0002-9505},
journal = {American Journal of Physics},
number = {5},
pages = {378--388},
title = {{The missing wave momentum mystery}},
url = {http://aapt.scitation.org/doi/10.1119/1.19272},
volume = {67},
year = {1999}
}
@article{Cecotti2008,
abstract = {In BCI (brain - computer interface) systems, brain signals must be processed to identify distinct activities that convey different mental states. We propose a new technique for the classification of electroencephalographic (EEG) steady-state visual e...},
author = {Cecotti, Hubert and Graeser, Axel},
isbn = {1051-4651},
journal = {2008 19th International Conference on Pattern Recognition (ICPR)},
pages = {1--4},
title = {{Convolutional Neural Network with embedded Fourier Transform for EEG classification}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4761638{\%}5Cnpapers3://publication/doi/10.1109/ICPR.2008.4761638},
year = {2008}
}
@article{Uncini,
author = {Uncini, Aurelio},
title = {{Sound Synthesis by Flexible Activation Function Recurrent Neural Networks}}
}
@article{G-1999,
abstract = {This paper presents a model of the envelope of the additive parameters of isolated musical sounds, along with a new method for the estimation of the important envelope split- point times. The model consists of start, attack, sustain, release, and end segments with variable split-point amplitude and time. The estimation of the times is done using smoothed derivatives of the envelopes. The estimated split-point values can be used together with a curve-form model introduced in this paper in the analysis/synthesis of musical sounds. The envelope model can recreate noise-less musical sounds with good fidelity, and the method for the estimation of the envelope times performs significantly better than the classical percentage- based method.},
author = {G-, Cost and Effects, Digital Audio},
doi = {10.1109/MMSP.2001.962718},
journal = {Audio},
number = {1},
pages = {9--12},
title = {{Envelope model of isolated musical sounds}},
year = {1999}
}
@article{Graves2013b,
abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
archivePrefix = {arXiv},
arxivId = {1308.0850},
author = {Graves, Alex},
doi = {10.1145/2661829.2661935},
eprint = {1308.0850},
isbn = {2000201075},
issn = {18792782},
pages = {1--43},
pmid = {23459267},
title = {{Generating Sequences With Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1308.0850},
year = {2013}
}
@article{Embrechts2009,
abstract = {Numerical evaluation of compound distributions is an important task in insurance mathematics and quantitative risk management. In practice, both recursive methods as well as transform based techniques are widely used.We give a survey of these tools, point out the respective merits and provide some numerical examples.},
author = {Embrechts, Paul and Frei, Marco},
doi = {10.1007/s00186-008-0249-2},
issn = {14322994},
journal = {Mathematical Methods of Operations Research},
keywords = {Compound distributions,Fast Fourier transform,Panjer recursion,Risk management},
number = {3},
pages = {497--508},
title = {{Panjer recursion versus FFT for compound distributions}},
volume = {69},
year = {2009}
}
@article{Merri2014,
archivePrefix = {arXiv},
arxivId = {arXiv:1409.1259v2},
author = {Merri, Bart Van},
eprint = {arXiv:1409.1259v2},
title = {{On the Properties of Neural Machine Translation: Encoder–Decoder Approaches}},
year = {2014}
}
@article{Lillicrap,
archivePrefix = {arXiv},
arxivId = {arXiv:1411.0247v1},
author = {Lillicrap, Timothy P and Cownden, Daniel and Tweed, Douglas B and Akerman, Colin J},
eprint = {arXiv:1411.0247v1},
pages = {1--27},
title = {{Random feedback weights support learning in deep neural networks}}
}
@article{Kopparapu,
archivePrefix = {arXiv},
arxivId = {arXiv:1406.3172v1},
author = {Kopparapu, Sunil and Satish, M},
eprint = {arXiv:1406.3172v1},
number = {2},
pages = {1--7},
title = {{Optimal Gaussian Filter for Effective Noise Filtering}}
}
@article{Sarroff2014,
abstract = {With an optimal network topology and tuning of hyperpa-rameters, artificial neural networks (ANNs) may be trained to learn a mapping from low level audio features to one or more higher-level representations. Such artificial neu-ral networks are commonly used in classification and re-gression settings to perform arbitrary tasks. In this work we suggest repurposing autoencoding neural networks as musical audio synthesizers. We offer an interactive musi-cal audio synthesis system that uses feedforward artificial neural networks for musical audio synthesis, rather than discriminative or regression tasks. In our system an ANN is trained on frames of low-level features. A high level representation of the musical audio is learned though an autoencoding neural net. Our real-time synthesis system allows one to interact directly with the parameters of the model and generate musical audio in real time. This work therefore proposes the exploitation of neural networks for creative musical applications.},
author = {Sarroff, Andy M and Casey, Michael},
isbn = {9789604661374},
journal = {Proceedings of the International Computer Music Conference},
number = {September},
pages = {14--20},
title = {{Musical Audio Synthesis Using Autoencoding Neural Nets}},
year = {2014}
}
@article{Courtney2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1507.01832v1},
author = {Courtney, Elya and Courtney, Michael},
eprint = {arXiv:1507.01832v1},
title = {{arXiv : 1507 . 01832v1 [ physics . data-an ] 6 Jul 2015}},
year = {2015}
}
@article{Vincent,
archivePrefix = {arXiv},
arxivId = {arXiv:1412.7091v3},
author = {Vincent, Pascal and Bouthillier, Xavier},
eprint = {arXiv:1412.7091v3},
pages = {1--15},
title = {{Efficient Exact Gradient Update for training Deep Networks with Very Large Sparse Targets [ Technical report ]}}
}
@article{Baharev2015,
author = {Baharev, Ali and Schichl, Hermann and Neumaier, Arnold},
pages = {1--34},
title = {{An exact method for the minimum feedback arc set problem}},
year = {2015}
}
@article{Moon2014,
author = {Moon, Il Joon and Hong, Sung Hwa},
keywords = {hearing,speech perception,temporal envelope,temporal fine structure},
number = {1},
pages = {1--7},
title = {{What Is Temporal Fine Structure and Why Is It Important ?}},
volume = {18},
year = {2014}
}
@article{Nayebi,
author = {Nayebi, Aran and Vitelli, Matt},
pages = {1--6},
title = {{GRUV : Algorithmic Music Generation using Recurrent Neural Networks}}
}
@article{Ioffe,
archivePrefix = {arXiv},
arxivId = {arXiv:1502.03167v3},
author = {Ioffe, Sergey and Szegedy, Christian},
eprint = {arXiv:1502.03167v3},
title = {{Batch Normalization : Accelerating Deep Network Training by Reducing Internal Covariate Shift}}
}
@article{Highlander2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1601.06815v1},
author = {Highlander, Tyler},
eprint = {arXiv:1601.06815v1},
pages = {1--9},
title = {{arXiv : 1601 . 06815v1 [ cs . NE ] 25 Jan 2016 Very Efficient Training of Convolutional Neural Networks using Fast Fourier}},
year = {2015}
}
@article{Reyes2015,
archivePrefix = {arXiv},
arxivId = {arXiv:1510.03735v1},
author = {Reyes, Marco A and Arcos-olalla, Rafael},
eprint = {arXiv:1510.03735v1},
keywords = {02,05,1,10,11,30,dawson,error function,gp,ln,mv,pacs numbers,pb,s function,supersymmetry,the error function,the integral,which is defined by},
number = {2},
pages = {1--13},
title = {{Supersymmetric features of the Error and Dawson ' s functions arXiv : 1510 . 03735v1 [ math-ph ] 13 Oct 2015}},
year = {2015}
}
@article{He2016,
author = {He, Lei and Dellwo, Volker},
number = {September},
pages = {530--534},
title = {{A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform A Praat-Based Algorithm to Extract the Amplitude Envelope and Temporal Fine Structure Using the Hilbert Transform}},
year = {2016}
}
@article{Esser,
archivePrefix = {arXiv},
arxivId = {arXiv:1603.08270v2},
author = {Esser, Steven K and Merolla, Paul A and Arthur, John V and Cassidy, Andrew S and Appuswamy, Rathinakumar and Berg, David J and Mckinstry, Jeffrey L and Melano, Timothy and Barch, Davis R and Nolfo, Carmelo and Amir, Arnon and Taba, Brian and Flickner, Myron D and Modha, Dharmendra S},
eprint = {arXiv:1603.08270v2},
number = {Figure 1},
pages = {1--7},
title = {{Convolutional Networks for Fast , Energy-Efficient Neuromorphic Computing}}
}
@misc{Moscaritolo,
author = {Moscaritolo, Autores and Angelamoscaritolopcmagcom, Angela},
title = {{Pebble Smartwatch Sells Out , Collects {\$} 10 Million on Kickstarter}}
}
@article{Balle2016b,
abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
archivePrefix = {arXiv},
arxivId = {1611.01704},
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
doi = {10.1016/S0197-3975(03)00059-6},
eprint = {1611.01704},
isbn = {0197-3975},
issn = {01973975},
pmid = {16508805},
title = {{End-to-end Optimized Image Compression}},
url = {http://arxiv.org/abs/1611.01704},
year = {2016}
}
@misc{,
title = {{.{\_}Sawhney, Verona e Prandelli{\_}2005(JIM){\_}COLLABORATING TO CREATE- THE INTERNET AS A PLATFORM FOR CUSTOMER ENGAGEMENT IN PRODUCT INNOVATION.pdf}}
}
@misc{Hoover2012a,
author = {Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
booktitle = {International Conference on Computational Creativity},
isbn = {9781905254668},
pages = {111},
title = {{Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding}},
year = {2012}
}
@article{Fini2010,
abstract = {Research and public policy on academic entrepreneurship are largely based on the assumption that faculty members start businesses to commercialize inventions that have been disclosed to university administrators and have been patented. In this paper, we analyze a sample of 11,572 professors and find that much academic entrepreneurship occurs outside the university intellectual property system. Specifically, about 2/3 of businesses started by academics are not based on disclosed and patented inventions. Moreover, we show that individual characteristics, departmental and organizational affiliations, and time allocation of academics that have started business outside the IP system are different from those of academics that have started businesses to exploit disclosed and patented inventions. We discuss the implications for research on and the practice of academic entrepreneurship. {\textcopyright} 2010 Elsevier B.V. All rights reserved.},
author = {Fini, Riccardo and Lacetera, Nicola and Shane, Scott},
doi = {10.1016/j.respol.2010.05.014},
isbn = {0048-7333},
issn = {00487333},
journal = {Research Policy},
keywords = {Academic entrepreneurship,Business creation,Knowledge transfer},
number = {8},
pages = {1060--1069},
title = {{Inside or outside the IP system? Business creation in academia}},
volume = {39},
year = {2010}
}
@misc{,
title = {{Unknown - Unknown - Leaps and bounds.pdf.pdf}}
}
@misc{,
title = {{Poverty and profits in the information age.pdf}}
}
@article{Toderici2016b,
abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
archivePrefix = {arXiv},
arxivId = {1608.05148},
author = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
doi = {10.4135/9781412985277},
eprint = {1608.05148},
isbn = {9780761914402},
issn = {08936080},
pmid = {21655600},
title = {{Full Resolution Image Compression with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1608.05148},
year = {2016}
}
@misc{,
title = {{Unknown - Unknown - Democratizing Innovation.pdf.pdf}}
}
@misc{Alves1992,
abstract = {O artigo analisa o papel da revis{\~{a}}o da bibliografia em trabalhos de pesquisa e aponta as principais defici{\^{e}}ncias observadas em teses de mestrado e doutorado, no que se refere a esse aspecto. A primeira se{\c{c}}{\~{a}}o destaca a import{\^{a}}ncia da an{\'{a}}lise cr{\'{i}}tica do estado atual do conhecimento na {\'{a}}reade interesse do pesquisador para a problematiza{\c{c}}{\~{a}}o do temaa ser investigado. A segunda trata do referencial te{\'{o}}rico e discute as dificuldades encontradas na constru{\c{c}}{\~{a}}o te{\'{o}}rica no campo da educa{\c{c}}{\~{a}}o. Finalmente, a terceira se{\c{c}}{\~{a}}o apresenta os equ{\'{i}}vocos mais freq{\"{u}}entes observados em revis{\~{o}}es de bibliografia, utilizando o recurso da caricatura para tornar mais vis{\'{i}}veis certos tra{\c{c}}os.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Alves, Alda Judith},
booktitle = {Cadernos de Pesquisa},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
number = {81},
pages = {53--60},
pmid = {25246403},
title = {{A "revis{\~{a}}o da bibliografia"em teses e disserta{\c{c}}{\~{o}}s: meus tipos inesquec{\'{i}}veis}},
url = {http://www.fcc.org.br/pesquisa/publicacoes/cp/arquivos/916.pdf},
year = {1992}
}
@article{,
isbn = {9781479903566},
journal = {Computer Engineering},
pages = {3572--3576},
title = {{KERNEL RECURRENT SYSTEM TRAINED BY REAL-TIME RECURRENT LEARNING ALGORITHM Pingping Zhu , Jos ´ University of Florida Electrical and Computer Engineering}},
year = {2013}
}
@misc{,
title = {{Wang et al. - 2011 - Rapid parametric design methods for shoe-last customization.pdf}}
}
@article{Tu2017,
abstract = {This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.},
archivePrefix = {arXiv},
arxivId = {1603.05594},
author = {Tu, Enmei and Kasabov, Nikola and Yang, Jie},
doi = {10.1109/TNNLS.2016.2536742},
eprint = {1603.05594},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Early event prediction,NeuCube architecture,spatiotemporal data,spiking neural network (SNN)},
number = {6},
pages = {1305--1317},
pmid = {26992179},
title = {{Mapping Temporal Variables Into the NeuCube for Improved Pattern Recognition, Predictive Modeling, and Understanding of Stream Data}},
volume = {28},
year = {2017}
}
@article{Theis2017a,
abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
archivePrefix = {arXiv},
arxivId = {1703.00395},
author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Husz{\'{a}}r, Ferenc},
eprint = {1703.00395},
pages = {1--19},
title = {{Lossy Image Compression with Compressive Autoencoders}},
url = {http://arxiv.org/abs/1703.00395},
year = {2017}
}
@misc{Agerfalk2008,
abstract = {This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy-opensourcing, as we term it here-whereby commerical companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product. We followed this with a large-scale survey involving additional exemplars of the phenomenon. The study identifies a number of symmetrical and complementary customer and community obligations that are associated with opensourcing success. We also identify a number of tension points on which customer and community perceptions tend to vary. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness: The customer and community need to establish a trusted partnership of shared responsibility in building an overall opensourcing ecosystem. The study reveals an ongoing shift from OSS as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises. It also reveals that opensourcing provides ample opportunity for companies to headhunt top developers, hence moving from outsourcing to a largely unknown OSS workforce toward recruitment of developers from a global open source community whose talents have become known as a result of the opensourcing experience.},
author = {{\AA}gerfalk, P{\"{a}}r J. and Fitzgerald, Brian and Agerfalk, Par J and Fitzgerald, Brian},
booktitle = {MIS Quarterly},
doi = {Article},
isbn = {02767783},
issn = {02767783},
keywords = {crowdsourcing,global software development,multi-method research,offshoreing,open source,opensourcing,outsourcing},
number = {2},
pages = {385--409},
pmid = {31831011},
title = {{Outsourcing to an unknown workforce: exploring opensourcing as a global sourcing strategy}},
volume = {32},
year = {2008}
}
@article{Balle2015a,
abstract = {We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
archivePrefix = {arXiv},
arxivId = {1511.06281},
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
eprint = {1511.06281},
pages = {1--14},
title = {{Density Modeling of Images using a Generalized Normalization Transformation}},
url = {http://arxiv.org/abs/1511.06281},
year = {2015}
}
@article{Oord2016a,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
eprint = {1601.06759},
isbn = {9781510829008},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
volume = {48},
year = {2016}
}
@article{Egmont-Petersen2002a,
abstract = {We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments. {\textcopyright} 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Egmont-Petersen, M. and {De Ridder}, D. and Handels, H.},
doi = {10.1016/S0031-3203(01)00178-9},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Digital image processing,Feature extraction,Image compression,Image understanding,Invariant pattern recognition,Neural networks,Object recognition,Optimization,Preprocessing,Segmentation},
number = {10},
pages = {2279--2301},
title = {{Image processing with neural networks- A review}},
volume = {35},
year = {2002}
}
@article{Kulkarni2015c,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
issn = {10897550},
pages = {1--10},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Toderici2015b,
abstract = {A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32{\$}\backslashtimes{\$}32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10{\%} or more.},
archivePrefix = {arXiv},
arxivId = {1511.06085},
author = {Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
eprint = {1511.06085},
pages = {1--12},
title = {{Variable Rate Image Compression with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06085},
year = {2015}
}
@article{Khorrami2016b,
abstract = {We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
archivePrefix = {arXiv},
arxivId = {1602.07377},
author = {Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
doi = {10.1017/pasa.2016.3},
eprint = {1602.07377},
pages = {1--5},
title = {{How Deep Neural Networks Can Improve Emotion Recognition on Video Data}},
url = {http://arxiv.org/abs/1602.07377},
year = {2016}
}
@article{Sainath2015a,
abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12{\%}-14{\%} relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.},
archivePrefix = {arXiv},
arxivId = {1309.1501},
author = {Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and rahman Mohamed, Abdel and Dahl, George and Ramabhadran, Bhuvana},
doi = {10.1016/j.neunet.2014.08.005},
eprint = {1309.1501},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Neural networks,Speech recognition},
pages = {39--48},
pmid = {25439765},
publisher = {Elsevier Ltd},
title = {{Deep Convolutional Neural Networks for Large-scale Speech Tasks}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.08.005},
volume = {64},
year = {2015}
}
@article{Frans2017c,
abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
archivePrefix = {arXiv},
arxivId = {1704.08834},
author = {Frans, Kevin},
eprint = {1704.08834},
title = {{Outline Colorization through Tandem Adversarial Networks}},
url = {http://arxiv.org/abs/1704.08834},
year = {2017}
}
@article{Hoover2012b,
author = {Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
isbn = {9781905254668},
journal = {International Conference on Computational Creativity},
pages = {111},
title = {{Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding}},
year = {2012}
}
@article{Zen2015a,
abstract = {Long short-term memory recurrent neural networks (LSTM-RNNs) have been applied to various speech applications including acoustic modeling for statistical parametric speech synthesis. One of the con- cerns for applying them to text-to-speech applications is its effect on latency. To address this concern, this paper proposes a low-latency, streaming speech synthesis architecture using unidirectional LSTM- RNNs with a recurrent output layer. The use of unidirectional RNN architecture allows frame-synchronous streaming inference of out- put acoustic features given input linguistic features. The recurrent output layer further encourages smooth transition between acoustic features at consecutive frames. Experimental results in subjective listening tests show that the proposed architecture can synthesize natural sounding speech without requiring utterance-level batch pro- cessing.},
author = {Zen, Heiga and Sak, Hasim},
doi = {10.1109/ICASSP.2015.7178816},
isbn = {9781467369978},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Statistical parametric speech synthesis,long short-term memory,low-latency,recurrent neural networks},
pages = {4470--4474},
pmid = {18557655},
title = {{Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis}},
volume = {2015-Augus},
year = {2015}
}
@article{Graves2013a,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {1303.5778},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {1303.5778},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
number = {3},
pmid = {27295638},
title = {{Speech Recognition with Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1303.5778},
year = {2013}
}
@article{Raczynski2013a,
author = {Raczy{\'{n}}ski, Stanis{\l}aw A. and Vincent, Emmanuel and Sagayama, Shigeki},
doi = {10.1109/TASL.2013.2258012},
issn = {1558-7916},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
number = {9},
pages = {1830 -- 1840},
title = {{Dynamic Bayesian networks for symbolic polyhonic pitch modeling}},
volume = {21},
year = {2013}
}
@article{Zhu2016a,
abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
archivePrefix = {arXiv},
arxivId = {1609.03552},
author = {Zhu, Jun Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
doi = {10.1007/978-3-319-46454-1_36},
eprint = {1609.03552},
isbn = {9783319464534},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {597--613},
pmid = {4520227},
title = {{Generative visual manipulation on the natural image manifold}},
volume = {9909 LNCS},
year = {2016}
}
@article{Choi2016b,
abstract = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
archivePrefix = {arXiv},
arxivId = {1609.04243},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark and Cho, Kyunghyun},
doi = {10.1.1.302.7795},
eprint = {1609.04243},
isbn = {9789881701282},
issn = {15209210},
pages = {1--5},
title = {{Convolutional Recurrent Neural Networks for Music Classification}},
url = {http://arxiv.org/abs/1609.04243},
year = {2016}
}
@article{Bock2012a,
author = {Bock, Sebastian and Schedl, Markus},
isbn = {9781467300469},
journal = {Network},
pages = {121--124},
title = {{Polyphonic Piano Note Transcription With Recurrent Neural Networks}},
year = {2012}
}
@article{Boulanger-Lewandowski2013a,
abstract = {We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.},
archivePrefix = {arXiv},
arxivId = {1212.1936},
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
doi = {10.1109/ICASSP.2013.6638244},
eprint = {1212.1936},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Sequence transduction,polyphonic transcription,recurrent neural network,restricted Boltzmann machine},
number = {5},
pages = {3178--3182},
title = {{High-dimensional sequence transduction}},
year = {2013}
}
@article{Tuohy2006a,
abstract = {In this paper we describe a technique for creating guitar tablature using a neural network. Training data was parsed from an online repository of human-created tablatures. The contents of both the input layer and the set of training data have been optimized through genetic search in order to maximize the accuracy of the network. The output of the network is im- proved upon with a local heuristic hill-climber (HC). We implement this model in an existing system for generating guitar arrangements via genetic algorithm (GA). When compared to the original system for generating tablature, we note modest improvement in tablature quality and drastic improvements in execution time.},
author = {Tuohy, D R and Potter, W D},
isbn = {{\%}(},
journal = {Procs. of the International Computer Music Conference (ICMC06)},
keywords = {fingering{\_}prediction,guitar},
number = {January 2006},
pages = {576--579},
title = {{An Evolved Neural Network/HC Hybrid for Tablature Creation in GA- based Guitar Arranging}},
url = {http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2006.119},
year = {2006}
}
@article{,
number = {May},
pages = {3136--3140},
title = {{POLYPHONIC PIANO TRANSCRIPTION USING NON-NEGATIVE MATRIX FACTORISATION WITH GROUP SPARSITY Ken O ' Hanlon and Mark D . Plumbley Queen Mary University of London}},
volume = {1},
year = {2014}
}
@article{Hinton2012a,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
eprint = {1207.0580},
isbn = {1053-5888},
issn = {1053-5888},
journal = {Ieee Signal Processing Magazine},
number = {November},
pages = {82--97},
pmid = {13057166},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
year = {2012}
}
@article{Chen2017b,
abstract = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
doi = {10.1016/j.eswa.2016.10.065},
eprint = {1404.7828},
isbn = {0925-2312},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep neural network,Natural language processing,Sentiment analysis},
pages = {221--230},
pmid = {19932002},
title = {{Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN}},
volume = {72},
year = {2017}
}
@article{Stanley2002b,
abstract = {The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
archivePrefix = {arXiv},
arxivId = {1407.0576},
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
eprint = {1407.0576},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving Neural Networks through Augmenting Topologies}},
url = {http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
volume = {10},
year = {2002}
}
@article{He2015a,
abstract = {This paper presents our system design for the Audio-Visual Emotion Challenge (AV +EC 2015). Besides the baseline features, we extract from audio the functionals on low-level descriptors (LLDs) obtained via the YAAFE toolbox, and from video the Local Phase Quantization from Three Or- thogonal Planes (LPQ-TOP) features. From the physiologi- cal signals, we extract 52 electro-cardiogram (ECG) features and 22 electro-dermal activity (EDA) features from various analysis domains. The extracted features along with the AV +EC 2015 baseline features of audio, ECG or EDA are concatenated for a further feature selection step, in which the concordance correlation coefficient (CCC), instead of the usual Pearson correlation coefficient (CC), has been used as objective function. In addition, offsets between the features and the arousal/valence labels are considered in both feature selection and modeling of the affective dimensions. For the fusion of multimodal features, we propose a Deep Bidirec- tional Long Short-Term Memory Recurrent Neural Network (DBLSTM-RNN) based multimodal affect prediction frame- work, in which the initial predictions from the single modali- ties via the DBLSTM-RNNs are firstly smoothed with Gaus- sian smoothing, then input into a second layer of DBLSTM- RNN for the final prediction of affective state. Experimental results show that our proposed features and the DBLSTM- RNN based fusion framework obtain very promising results. On the development set, the obtained CCC is up to 0.824 for arousal and 0.688 for valence, and on the test set, the CCC is 0.747 for arousal and 0.609 for valence},
author = {He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
doi = {10.1145/2808196.2811641},
isbn = {9781450337434},
journal = {Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge - AVEC '15},
number = {May},
pages = {73--80},
title = {{Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks}},
url = {http://dl.acm.org/citation.cfm?doid=2808196.2811641},
year = {2015}
}
@article{DiPersio2016a,
abstract = {We present an Artificial Neural Network (ANN) approach to predict stock market indices, particularly with respect to the forecast of their trend movements up or down. Exploiting different Neural Networks archi-tectures, we provide numerical analysis of concrete financial time series. In particular, after a brief r{\'{e}}sum{\'{e}} of the existing literature on the subject, we consider the Multi-layer Perceptron (MLP), the Convolutional Neural Net-works (CNN), and the Long Short-Term Memory (LSTM) recurrent neural networks techniques. We focus on the importance of choosing the correct input features, along with their preprocessing, for the specific learning algo-rithm one wants to use. Eventually, we consider the S{\&}P500 historical time series, predicting trend on the basis of data from the past days, and proposing a novel approach based on combination of wavelets and CNN, which outperforms the basic neural networks ones. We show, that neural networks are able to predict financial time series movements even trained only on plain time series data and propose more ways to improve results.},
author = {{Di Persio}, Luca and Honchar, Oleksandr},
issn = {19984464},
journal = {International Journal of Circuits, Systems and Signal Processing},
keywords = {Artificial neural networks,Convolutional neural network,Deep Learning,Financial forecasting,Long shortterm memory,Multi-layer neural network,Recurrent neural network,Stock markets analysis,Time series analysis},
pages = {403--413},
title = {{Artificial neural networks architectures for stock price prediction: Comparisons and applications}},
volume = {10},
year = {2016}
}
@article{Zhao2016a,
abstract = {Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
archivePrefix = {arXiv},
arxivId = {1607.06997},
author = {Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
doi = {10.1007/978-3-319-46475-6_27},
eprint = {1607.06997},
pages = {1--18},
title = {{Peak-Piloted Deep Network for Facial Expression Recognition}},
url = {http://arxiv.org/abs/1607.06997},
year = {2016}
}
@article{Wu2015a,
abstract = {In this paper, a drum transcription algorithm using partially fixed non-negative matrix factorization is presented. The proposed method allows users to identify percussive events in complex mixtures with a minimal training set. The algorithm decomposes the music signal into two parts: percussive part with pre-defined drum templates and harmonic part with undefined entries. The harmonic part is able to adapt to the music content, allowing the algorithm to work in polyphonic mixtures. Drum event times can be simply picked from the percussive activation matrix with onset detection. The system is efficient and robust even with a minimal training set. The recognition rates for the ENST dataset vary from 56.7 to 78.9{\%} for three percussive instruments extracted from polyphonic music.},
author = {Wu, Chih Wei and Lerch, Alexander},
doi = {10.1109/EUSIPCO.2015.7362590},
isbn = {9780992862633},
journal = {2015 23rd European Signal Processing Conference, EUSIPCO 2015},
keywords = {Automatic Music Transcription,Drum Transcription,MIR,NMF},
pages = {1281--1285},
title = {{Drum transcription using partially fixed non-negative matrix factorization}},
year = {2015}
}
@article{Sotelo2017b,
author = {Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Kastner, Kyle},
number = {2015},
pages = {1--6},
title = {{C Har 2W Av : E Nd - To -E Nd S Peech S Ynthesis}},
year = {2017}
}
@article{Wu2016a,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@article{Ding2017,
abstract = {Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully- connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1609.06591},
author = {Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
doi = {10.1109/FG.2017.23},
eprint = {1609.06591},
isbn = {9781509040230},
issn = {2160-7508},
journal = {Proceedings - 12th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2017 - 1st International Workshop on Adaptive Shot Learning for Gesture Understanding and Production, ASL4GUP 2017, Biometrics in the Wild, Bwild 2017, Heteroge},
pages = {118--126},
title = {{FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition}},
year = {2017}
}
@article{Wang2016b,
abstract = {In recent years, financialmarket dynamics forecasting has been a focus of economic research. To predict the price indices of stock markets,we developed an architecturewhich combinedElman recurrent neural networkswith stochastic time effective function.By analyzing the proposedmodelwith the linear regression, complexity invariant distance (CID), andmultiscaleCID(MCID) analysis methods and taking themodel compared with differentmodels such as the backpropagation neural network (BPNN), the stochastic time effective neural network (STNN), and the Elman recurrent neural network (ERNN), the empirical results show that the proposed neural network displays the best performance among these neural networks in financial time series forecasting. Further, the empirical research is performed in testing the predictive effects of SSE, TWSE, KOSPI, and Nikkei225 with the established model, and the corresponding statistical comparisons of the above market indices are also exhibited. The experimental results show that this approach gives good performance in predicting the values fromthe stockmarket indices. 1.},
author = {Wang, Jie and Wang, Jun and Fang, Wen and Niu, Hongli},
doi = {10.1155/2016/4742515},
isbn = {1687-5265},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
title = {{Financial Time Series Prediction Using Elman Recurrent Random Neural Networks}},
volume = {2016},
year = {2016}
}
@article{Zweig2017,
abstract = {This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.},
archivePrefix = {arXiv},
arxivId = {1609.05935},
author = {Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
doi = {10.1109/ICASSP.2017.7953069},
eprint = {1609.05935},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {CTC,end-to-end training,recurrent neural network,speech recognition},
pages = {4805--4809},
title = {{Advances in all-neural speech recognition}},
year = {2017}
}
@article{Krishna2016a,
author = {Krishna, Tushar and Emer, Joel and Sze, Vivienne and Conference, International Solid-state Circuits and Francisco, San and Chen, Yu-hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
title = {{Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks The MIT Faculty has made this article openly available . Please share Citation " Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Publisher Version}},
year = {2016}
}
@article{Zhang2017a,
abstract = {Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.},
archivePrefix = {arXiv},
arxivId = {1701.02720},
author = {Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon and Zhang, Saizheng and Bengio, Cesar Laurent Yoshua and Courville, Aaron},
doi = {10.21437/Interspeech.2016-1446},
eprint = {1701.02720},
title = {{Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1701.02720},
year = {2017}
}
@article{Esteva2017a,
abstract = {Skin cancer, the most common human malignancy1, 2, 3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4, 5 show potential for general and highly variable tasks across many fine-grained object categories6, 7, 8, 9, 10, 11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care. 最も多いヒト悪性腫瘍である皮膚がんは、主に視覚的に診断され、初期臨床スクリーニングが行われた後、ダーモスコピー解析、生検、および病理組織検査が行われることがある。画像を用いた皮膚病変の自動分類は、皮膚病変の外観には細かなばらつきがあるため、難しい課題である。深層畳み込みニューラルネットワーク（CNN）には、多くの細かな対象カテゴリー全体にわたり、一般的で変動性の高い課題をこなせる可能性がある。本研究で我々は、入力としてピクセルと疾病ラベルのみを用い、画像からエンドツーエンドで直接学習させた、単一のCNNを用いた皮膚病変の分類を実証する。我々は、2032の異なる疾病からなる12万9450の臨床画像（以前のデータセットよりも2桁多い）のデータセットを用いてCNNを学習させた。我々は生検で確認した臨床画像について、このCNNの成績を21人の皮膚科認定医と比較検証した。ここでは「角化細胞がん」対「良性脂漏性角化症」、「悪性黒色腫」対「良性母斑」という、2つの重要な二項分類使用症例を用いた。前者は最も多いがんの識別であり、後者は最も致命的な皮膚がんの識別である。CNNはどちらの課題においても、試験に参加した全ての専門家と同等の成績を達成したことから、人工知能は皮膚科医に相当する能力で皮膚がんを分類できることが示された。深層ニューラルネットワークを装備した携帯デバイスにより、皮膚科医の診察が診療所の外でも受けられる可能性がある。2021年までにスマートフォン契約者は63億人に達すると見込まれ、従って、重要な診断があらゆる場所で安価に受けられるようになるかもしれない。},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
isbn = {0028-0836},
issn = {14764687},
journal = {Nature},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {http://dx.doi.org/10.1038/nature21056},
volume = {542},
year = {2017}
}
@article{Saxena2016b,
abstract = {Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02492},
author = {Saxena, Shreyas and Verbeek, Jakob},
eprint = {1606.02492},
issn = {10495258},
title = {{Convolutional Neural Fabrics}},
url = {http://arxiv.org/abs/1606.02492},
year = {2016}
}
@article{Arik2017a,
abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
archivePrefix = {arXiv},
arxivId = {1702.07825},
author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
eprint = {1702.07825},
issn = {1938-7228},
number = {Icml},
title = {{Deep Voice: Real-time Neural Text-to-Speech}},
url = {http://arxiv.org/abs/1702.07825},
year = {2017}
}
@article{Wang2017,
abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc},
doi = {10.21437/Interspeech.2017-1452},
eprint = {1703.10135},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.},
pages = {4006--4010},
title = {{Tacotron: Towards end-To-end speech synthesis}},
volume = {2017-Augus},
year = {2017}
}
@article{Omar2017a,
abstract = {This study explores the effectiveness of an Artificial Neural Network (ANN) in predicting fraudulent financial reporting in small market capitalization companies in Malaysia. Design/methodology/approach Based on the concepts of ANN, a mathematical model is developed to compare non-fraud and fraud companies selected from among small market capitalization companies in Malaysia; the fraud companies had already been charged by the Securities Commission for the falsification of financial statements. Ten financial ratios are used as fraud risk indicators to predict fraudulent financial reporting using ANN. Findings Indicate that the proposed ANN methodology outperforms other statistical techniques widely used for predicting fraudulent financial reporting. Originality/value The study is one of few to adopt the ANN approach to the prediction of financial reporting fraud.},
author = {Omar, Normah and Johari, Zulaikha 'Amirah and Smith, Malcolm},
doi = {10.1108/13590791011082797},
isbn = {1359079051062},
issn = {1359-0790},
journal = {Journal of Financial Crime Iss},
number = {2},
title = {{Predicting fraudulent financial reporting using artificial neural network}},
url = {http://dx.doi.org/10.1108/eb025814{\%}5Cnhttp://},
volume = {24},
year = {2017}
}
@article{Deng2015a,
abstract = {Extreme learning machine (ELM), as a new learning framework, draws increasing attractions in the areas of large-scale computing, high-speed signal processing, artificial intelligence, and so on. ELM aims to break the barriers between the conventional artificial learning techniques and biological learning mechanism and represents a suite of machine learning techniques in which hidden neurons need not to be tuned. ELM theories and algorithms argue that "random hidden neurons" capture the essence of some brain learning mechanisms as well as the intuitive sense that the efficiency of brain learning need not rely on computing power of neurons. Thus, compared with traditional neural networks and support vector machine, ELM offers significant advantages such as fast learning speed, ease of implementation, and minimal human intervention. Due to its remarkable generalization performance and implementation efficiency, ELM has been applied in various applications. In this paper, we first provide an overview of newly derived ELM theories and approaches. On the other hand, with the ongoing development of multilayer feature representation, some new trends on ELM-based hierarchical learning are discussed. Moreover, we also present several interesting ELM applications to showcase the practical advances on this subject.},
author = {Deng, ChenWei and Huang, GuangBin and Xu, Jia and Tang, JieXiong},
doi = {10.1007/s11432-014-5269-3},
issn = {1674-733X},
journal = {Science China Information Sciences},
number = {2},
pages = {1--16},
title = {{Extreme learning machines: new trends and applications}},
url = {http://link.springer.com/10.1007/s11432-014-5269-3},
volume = {58},
year = {2015}
}
@article{Phumrattanaprapin2016a,
author = {Phumrattanaprapin, Khanittha},
keywords = {chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
number = {2},
pages = {196--204},
title = {{Machine with Multilayer Perceptron}},
volume = {10},
year = {2016}
}
@article{Shi2016a,
abstract = {In this paper, we propose a novel method to im-prove object recognition accuracies of convolu-tional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The Min-Max objective explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin be-tween different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computa-tion cost. Experiments with shallow and deep mod-els on four benchmark datasets including CIFAR-10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max ob-jective achieve remarkable performance improve-ments compared to the corresponding baseline models.},
author = {Shi, Weiwei and Gong, Yihong and Wang, Jinjun},
doi = {10.1109/TNNLS.2017.2705682},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {2004--2010},
title = {{Improving CNN performance with min-max objective}},
volume = {2016-Janua},
year = {2016}
}
@article{Zhang2016c,
abstract = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
archivePrefix = {arXiv},
arxivId = {1610.03022},
author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
doi = {10.1109/ICASSP.2017.7953077},
eprint = {1610.03022},
isbn = {9781509041176},
issn = {15206149},
pages = {10--14},
title = {{Very Deep Convolutional Networks for End-to-End Speech Recognition}},
url = {http://arxiv.org/abs/1610.03022},
year = {2016}
}
@article{Floreano2008b,
abstract = {Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
author = {Floreano, Dario and D{\"{u}}rr, Peter and Mattiussi, Claudio},
doi = {10.1007/s12065-007-0002-4},
isbn = {1206500700},
issn = {18645909},
journal = {Evolutionary Intelligence},
keywords = {Evolution,Learning,Neural networks},
number = {1},
pages = {47--62},
title = {{Neuroevolution: From architectures to learning}},
volume = {1},
year = {2008}
}
@article{Tang2015a,
abstract = {— Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parame-ters are randomly generated and the output weights are analyti-cally computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme. Index Terms— Deep learning (DL), deep neural network (DNN), extreme learning machine (ELM), multilayer perceptron (MLP), random feature mapping.},
author = {Tang, Jiexiong and ChenweiDeng and Huang, Guang-Bin},
doi = {10.1109/TNNLS.2015.2424995},
isbn = {2162-2388 (Electronic) 2162-237X (Linking)},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
pages = {1--13},
pmid = {25966483},
title = {{Extreme Learning Machine for Multilayer Perceptron}},
year = {2015}
}
@article{Schaffner2017,
author = {Schaffner, Michael and Member, Student and Scheidegger, Florian and Cavigelli, Lukas and Member, Student and Kaeslin, Hubert and Member, Senior and Benini, Luca and Smolic, Aljosa},
title = {{Ro of Ro of}},
volume = {4},
year = {2017}
}
@article{Shizhou2016b,
abstract = {In this paper, we choose to learn useful cues from object recognition mechanisms of the human vi-sual cortex, and propose a DCNN performance im-provement method without the need for increasing the network complexity. Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As ex-perimental results show, when applying the pro-posed method to the " Quick " model and NIN models, image classification performances are re-markably improved on four widely used bench-mark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
author = {Shizhou, Zhang and Gong, Yihong and Jinjun, Wang},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning},
pages = {2343--2349},
title = {{Improving DCNN performance with sparse category-selective objective function}},
volume = {2016-Janua},
year = {2016}
}
@article{Liao2016b,
abstract = {Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
archivePrefix = {arXiv},
arxivId = {1508.00330},
author = {Liao, Zhibin and Carneiro, Gustavo},
doi = {10.1109/WACV.2016.7477624},
eprint = {1508.00330},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
title = {{On the importance of normalisation layers in deep learning with piecewise linear activation units}},
year = {2016}
}
@article{Ludermir2006a,
abstract = {This paper introduces a methodology for neural network global optimization. The aim is the simultaneous optimization of multilayer perceptron (MLP) network weights and architectures, in order to generate topologies with few connections and high classification performance for any data sets. The approach combines the advantages of simulated annealing, tabu search and the backpropagation training algorithm in order to generate an automatic process for producing networks with high classification performance and low complexity. Experimental results obtained with four classification problems and one prediction problem has shown to be better than those obtained by the most commonly used optimization techniques.},
author = {Ludermir, Teresa B. and Yamazaki, Akio and Zanchettin, Cleber},
doi = {10.1109/TNN.2006.881047},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Multilayer perceptron (MLP),Optimization of weights and architectures,Simulating annealing,Tabu search},
number = {6},
pages = {1452--1459},
pmid = {17131660},
title = {{An optimization methodology for neural network weights and architectures}},
volume = {17},
year = {2006}
}
@article{Tsai2006a,
abstract = {In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.},
author = {Tsai, Jinn Tsong and Chou, Jyh Horng and Liu, Tung Kuan},
doi = {10.1109/TNN.2005.860885},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Genetic algorithm (GA),Neural networks (NN),Taguchi method},
number = {1},
pages = {69--80},
pmid = {16526477},
title = {{Tuning the structure and parameters of a neural network by using hybrid Taguchi-genetic algorithm}},
volume = {17},
year = {2006}
}
@article{Leung2003a,
archivePrefix = {arXiv},
arxivId = {arXiv:1403.7012v1},
author = {Leung, F.H.F. and Lam, H.K. and Ling, S.H. and Tam, P.K.S.},
doi = {10.1109/TNN.2002.804317},
eprint = {arXiv:1403.7012v1},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {1045-9227},
journal = {IEEE Transactions on Neural Networks},
number = {1},
pages = {79--88},
pmid = {18237992},
title = {{Tuning of the structure and parameters of a neural network using an improved genetic algorithm}},
url = {http://ieeexplore.ieee.org/document/1176129/},
volume = {14},
year = {2003}
}
@article{McCulloch1943a,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Hopfield1982a,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
archivePrefix = {arXiv},
arxivId = {arXiv:1411.3159v1},
author = {Hopfield, J. J.},
doi = {10.1073/pnas.79.8.2554},
eprint = {arXiv:1411.3159v1},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {8},
pages = {2554--2558},
pmid = {6953413},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.79.8.2554},
volume = {79},
year = {1982}
}
@article{Benardos2007a,
abstract = {Despite the fact that feedforward artificial neural networks (ANNs) have been a hot topic of research for many years there still are certain issues regarding the development of an ANN model, resulting in a lack of absolute guarantee that the model will perform well for the problem at hand. The multitude of different approaches that have been adopted in order to deal with this problem have investigated all aspects of the ANN modelling procedure, from training data collection and pre/post-processing to elaborate training schemes and algorithms. Increased attention is especially directed to proposing a systematic way to establish an appropriate architecture in contrast to the current common practice that calls for a repetitive trial-and-error process, which is time-consuming and produces uncertain results. This paper proposes such a methodology for determining the best architecture and is based on the use of a genetic algorithm (GA) and the development of novel criteria that quantify an ANN's performance (both training and generalization) as well as its complexity. This approach is implemented in software and tested based on experimental data capturing workpiece elastic deflection in turning. The intention is to present simultaneously the approach's theoretical background and its practical application in real-life engineering problems. Results show that the approach performs better than a human expert, at the same time offering many advantages in comparison to similar approaches found in literature. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Benardos, P. G. and Vosniakos, G. C.},
doi = {10.1016/j.engappai.2006.06.005},
isbn = {0952-1976},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {ANN architecture,Engineering problems,Feedforward artificial neural networks,Generalization,Genetic algorithms},
number = {3},
pages = {365--382},
title = {{Optimizing feedforward artificial neural network architecture}},
volume = {20},
year = {2007}
}
@article{Deng2017,
abstract = {With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major benefit of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user's trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users' interests and their trusted friends' interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.},
author = {Deng, Shuiguang and Huang, Longtao and Xu, Guandong and Wu, Xindong and Wu, Zhaohui},
doi = {10.1109/TNNLS.2016.2514368},
isbn = {2162-2388 (Electronic)$\backslash$r2162-237X (Linking)},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Deep learning,Recommender systems (RSs),Social network,Trust},
number = {5},
pages = {1164--1177},
pmid = {26915135},
title = {{On Deep Learning for Trust-Aware Recommendations in Social Networks}},
volume = {28},
year = {2017}
}
@article{Hornik1991a,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp($\mu$) performance criteria, for arbitrary finite input environment measures $\mu$, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. {\textcopyright} 1991.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
eprint = {arXiv:1011.1669v3},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp($\mu$) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities},
number = {2},
pages = {251--257},
pmid = {25246403},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{Elman1990,
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report},
author = {Elman, J},
isbn = {0364-0213},
journal = {Cognitive Science},
number = {1 990},
pages = {179--211},
title = {{Finding structure in time* 1}},
url = {http://linkinghub.elsevier.com/retrieve/pii/036402139090002E{\%}0Apapers2://publication/uuid/BF1D3C8F-9A67-4350-ACE9-30BDA3C1FEC8},
volume = {14},
year = {1990}
}
@article{DavidA.Cacchione1997,
author = {{David A. Cacchione}},
doi = {10.1511/2011.89.106},
isbn = {2136240900},
issn = {0003-0996},
number = {2},
pages = {108--112},
title = {{American Scientist}},
volume = {85},
year = {1997}
}
@article{Boser1992a,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
doi = {10.1145/130385.130401},
eprint = {arXiv:1011.1669v3},
isbn = {089791497X},
issn = {0-89791-497-X},
journal = {Proceedings of the fifth annual workshop on Computational learning theory  - COLT '92},
pages = {144--152},
pmid = {25246403},
title = {{A training algorithm for optimal margin classifiers}},
url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
year = {1992}
}
@article{Graves2014a,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {1410.5401},
isbn = {0028-0836},
issn = {2041-1723},
pages = {1--26},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Paper2016,
author = {Paper, Conference},
doi = {10.1109/IJCNN.2016.7727308},
number = {July},
title = {{SAM : A Rethinking of prominent convolutional neural network architectures for visual object recognition SAM : A Rethinking of Prominent Convolutional Neural Network Architectures for}},
year = {2016}
}
@article{Hornik1989a,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. {\textcopyright} 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{,
archivePrefix = {arXiv},
arxivId = {1102.0183},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
pmid = {15823584},
title = {{Lecun-98}}
}
@article{Guang-BinHuang2014a,
author = {{Guang-Bin Huang} and {Qin-Yu Zhu} and {Chee-Kheong Siew}},
doi = {10.1109/IJCNN.2004.1380068},
isbn = {0-7803-8359-1},
journal = {2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
number = {February},
pages = {985--990},
title = {{Extreme learning machine: a new learning scheme of feedforward neural networks}},
url = {http://ieeexplore.ieee.org/document/1380068/},
volume = {2},
year = {2014}
}
@misc{Edmunds2000,
abstract = {This paper reviews the literature on the problem of information overload, with particular reference to business organisations. The literature reveals that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognised and experienced. Both perceptions and the actual effects of information overload have been exacerbated by the rapid advances made in information and communication technology, although it is not clear cut as to whether the Internet has worsened or improved the situation. A theme stressed in the literature is the paradoxical situation that, although there is an abundance of information available, it is often difficult to obtain useful, relevant information when it is needed. Some solutions put forward to reduce information overload are: a reduction in the duplication of information found in the professional literature; the adoption of personal information management strategies, together with the integration of software solutions such as push technology and intelligent agents; and the provision of value-added information (filtered by software or information specialists). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
author = {Edmunds, Angela and Morris, Anne},
booktitle = {International Journal of Information Management},
doi = {10.1016/S0268-4012(99)00051-1},
isbn = {0268-4012},
issn = {02684012},
keywords = {infoglut,information fatigue syndrome,information overload},
number = {1},
pages = {17--28},
pmid = {10272},
title = {{The problem of information overload in business organisations: a review of the literature}},
url = {http://www.sciencedirect.com/science/article/pii/S0268401299000511},
volume = {20},
year = {2000}
}
@article{Bengio2007a,
author = {Bengio, Yoshua and Lecun, Yann},
number = {1},
pages = {1--41},
title = {{Scaling Learning Algorithms towards AI To appear in " Large-Scale Kernel Machines ",}},
year = {2007}
}
@article{Kim2014a,
abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
archivePrefix = {arXiv},
arxivId = {1408.5882},
author = {Kim, Yoon},
doi = {10.3115/v1/D14-1181},
eprint = {1408.5882},
isbn = {9781937284961},
issn = {10709908},
pmid = {10463930},
title = {{Convolutional Neural Networks for Sentence Classification}},
url = {http://arxiv.org/abs/1408.5882},
year = {2014}
}
@article{Paper2012,
author = {Paper, Conference},
doi = {10.1109/ICICTA.2012.89},
keywords = {associative memory,hebbian,hopfield model,learning,perceptron learning},
number = {February 2014},
title = {{Storage Capacity of the Hopfield Network Associative Memory}},
year = {2012}
}
@article{Tang2014a,
abstract = {In this paper, we propose a joint segmenta- tion and classification framework for sen- timent analysis. Existing sentiment clas- sification algorithms typically split a sen- tence as a word sequence, which does not effectively handle the inconsistent senti- ment polarity between a phrase and the words it contains, such as "not bad" and "a great deal of ". We address this issue by developing a joint segmentation and classification framework (JSC), which si- multaneously conducts sentence segmen- tation and sentence-level sentiment classi- fication. Specifically, we use a log-linear model to score each segmentation candi- date, and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier. A marginal log-likelihood objective function is de- vised for the segmentation model, which is optimized for enhancing the sentiment classification performance. The joint mod- el is trained only based on the annotat- ed sentiment polarity of sentences, with- out any segmentation annotations. Experi- ments on a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that, our joint model performs com- parably with the state-of-the-art methods.},
author = {Tang, Duyu and Wei, Furu and Qin, Bing and Dong, Li and Liu, Ting and Zhou, Ming},
doi = {10.1109/TASLP.2015.2449071},
isbn = {2329-9290 VO  - 23},
issn = {2329-9290},
journal = {Proceedings of the 2014 Conferenve on Empirical Methods in Natural Language Processing (EMNLP)},
number = {2002},
pages = {477--487},
title = {{A Joint Segmentation and Classification Framework for Sentiment Analysis}},
volume = {23},
year = {2014}
}
@article{Boulanger-lewandowski,
author = {Boulanger-lewandowski, Nicolas},
number = {5},
title = {{PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS Universit ´ e de Montr ´ Montr ´ Jasha Droppo Mike Seltzer Dong Yu One Microsoft Way}}
}
@article{Bagozzi2006,
abstract = {We conceptualize participation in Linux user groups (LUGs) in terms of group-referent intentional actions and investigate cognitive (attitudes, perceived behavioral control, identification with the open source movement), affective (positive and negative anticipated emotions), and social (social identity) determinants of participation and its consequences on Linux-related behaviors of users. This survey-based study, conducted with 402 active LUG members representing 191 different LUGs from 23 countries and employing structural equation modeling methodology, supports the proposed model. Furthermore, we find that the Linux user's experience level moderates the extent of the LUG's social influence and its impact on the user's participation. We conclude with a consideration of the managerial and research implications of the study's findings.},
author = {Bagozzi, Richard P. and Dholakia, Utpal M.},
doi = {10.1287/mnsc.1060.0545},
isbn = {0025-1909},
issn = {0025-1909},
journal = {Management Science},
keywords = {2004,accepted by eric von,anticipated emotions,for 3 revisions,hippel and georg von,history,krogh,linux,model of goal-directed behavior,novice versus experienced users,open source software,received september 1,social identity,special issue editors,the authors 4 months,this paper was with,virtual communities,we-intentions},
number = {7},
pages = {1099--1115},
pmid = {21517591},
title = {{Open Source Software User Communities: A Study of Participation in Linux User Groups}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1060.0545},
volume = {52},
year = {2006}
}
@article{Balle2016c,
abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
archivePrefix = {arXiv},
arxivId = {1611.01704},
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
doi = {10.1016/S0197-3975(03)00059-6},
eprint = {1611.01704},
isbn = {0197-3975},
issn = {01973975},
pmid = {16508805},
title = {{End-to-end Optimized Image Compression}},
url = {http://arxiv.org/abs/1611.01704},
year = {2016}
}
@article{Arik2017b,
abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
archivePrefix = {arXiv},
arxivId = {1702.07825},
author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad},
eprint = {1702.07825},
issn = {1938-7228},
number = {Icml},
title = {{Deep Voice: Real-time Neural Text-to-Speech}},
url = {http://arxiv.org/abs/1702.07825},
year = {2017}
}
@misc{,
title = {{Estelles-Arolas, Gonzalez-Ladron-de-Guevara - 2012 - Towards an integrated crowdsourcing definition.pdf}}
}
@article{DIppolito2014,
abstract = {Scholars dedicated increasing attention towards appreciating how design has changed individualsperception of new products, firmsunderstanding and formulation of strategy, or other relevant actorsapproach to innovation and technology management. By emphasising the importance of design for the definition of consumersneeds, the restructuring of firmsorganisational structures and strategies, and the evolution of firmsvalue creation processes, this review paper identifies relevant research gaps and questions that would benefit from future scholarly attention. In particular, it is suggested that such effort should address the analysis of how design consumption can help better comprehend consumersneeds; what are the implications of design thinking on the skill sets of design professionals; the organisational structure of firms, including the reconfiguration of other business functions, and their strategy; and whether and how design thinking can shape firmsvalue creation processes and contribute to the formalisation of design tasks.},
author = {D'Ippolito, Beatrice},
doi = {10.1016/j.technovation.2014.01.007},
isbn = {0166-4972},
issn = {01664972},
journal = {Technovation},
keywords = {Consumers' needs,Design,Firm competitiveness,Literature review,Research gaps,Strategy making,Value creation},
number = {11},
pages = {716--730},
pmid = {1629546633},
title = {{The importance of design for firmscompetitiveness: A review of the literature}},
volume = {34},
year = {2014}
}
@article{Arik2017,
abstract = {We present Deep Voice, a production-quality text-to-speech system constructed entirely from deep neural networks. Deep Voice lays the groundwork for truly end-to-end neural speech synthesis. The system comprises five major building blocks: a segmentation model for locating phoneme boundaries, a grapheme-to-phoneme conversion model, a phoneme duration prediction model, a fundamental frequency prediction model, and an audio synthesis model. For the segmentation model, we propose a novel way of performing phoneme boundary detection with deep neural networks using connectionist temporal classification (CTC) loss. For the audio synthesis model, we implement a variant of WaveNet that requires fewer parameters and trains faster than the original. By using a neural network for each component, our system is simpler and more flexible than traditional text-to-speech systems, where each component requires laborious feature engineering and extensive domain expertise. Finally, we show that inference with our system can be performed faster than real time and describe optimized WaveNet inference kernels on both CPU and GPU that achieve up to 400x speedups over existing implementations.},
archivePrefix = {arXiv},
arxivId = {1702.07825},
author = {Arik, Sercan O. and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad and Others and Ng, Andrew and Raiman, Jonathan and Sengupta, Shubho and Shoeybi, Mohammad and Others},
eprint = {1702.07825},
issn = {1938-7228},
journal = {arXiv preprint arXiv:1702.07825},
keywords = {speech synthesis,state of the art},
mendeley-tags = {speech synthesis,state of the art},
number = {Icml},
title = {{Deep Voice: Real-time Neural Text-to-Speech}},
url = {http://arxiv.org/abs/1702.07825},
year = {2017}
}
@article{Balle2016a,
abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
archivePrefix = {arXiv},
arxivId = {1611.01704},
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
doi = {10.1016/S0197-3975(03)00059-6},
eprint = {1611.01704},
isbn = {0197-3975},
issn = {01973975},
journal = {arXiv preprint arXiv:1611.01704},
keywords = {image compression,image compression- lossy},
mendeley-tags = {image compression,image compression- lossy},
pmid = {16508805},
title = {{End-to-end Optimized Image Compression}},
url = {http://arxiv.org/abs/1611.01704},
year = {2016}
}
@article{Kim2014,
abstract = {We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification.},
archivePrefix = {arXiv},
arxivId = {1408.5882},
author = {Kim, Yoon},
doi = {10.3115/v1/D14-1181},
eprint = {1408.5882},
isbn = {9781937284961},
issn = {10709908},
journal = {arXiv preprint arXiv:1408.5882},
keywords = {sentence classification,sentiment analysis},
mendeley-tags = {sentence classification,sentiment analysis},
title = {{Convolutional Neural Networks for Sentence Classification}},
url = {http://arxiv.org/abs/1408.5882},
year = {2014}
}
@article{Hoover2012,
author = {Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
isbn = {9781905254668},
journal = {International Conference on Computational Creativity},
keywords = {music generation},
mendeley-tags = {music generation},
pages = {111},
title = {{Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding}},
year = {2012}
}
@article{Zweig2016,
abstract = {This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.},
archivePrefix = {arXiv},
arxivId = {1609.05935},
author = {Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
doi = {10.1109/ICASSP.2017.7953069},
eprint = {1609.05935},
institution = {IEEE},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {CTC,end-to-end training,recurrent neural network,speech recognition},
mendeley-tags = {speech recognition},
pages = {4805--4809},
title = {{Advances in all-neural speech recognition}},
url = {http://arxiv.org/abs/1609.05935},
year = {2017}
}
@article{Risi2017,
abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
archivePrefix = {arXiv},
arxivId = {1410.7326},
author = {Risi, Sebastian and Togelius, Julian},
doi = {10.1109/TCIAIG.2015.2494596},
eprint = {1410.7326},
isbn = {1943-068X VO  - PP},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Evolutionary algorithms,evolutive,games,neural networks,neuroevolution,review},
mendeley-tags = {evolutive,games,review},
number = {1},
pages = {25--41},
publisher = {IEEE},
title = {{Neuroevolution in Games: State of the Art and Open Challenges}},
volume = {9},
year = {2017}
}
@article{Gatys2016,
abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic in-formation and, thus, allow to separate image content from style. Here we use image representations derived from Con-volutional Neural Networks optimised for object recogni-tion, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can sep-arate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an ar-bitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neu-ral Networks and demonstrate their potential for high level image synthesis and manipulation.},
archivePrefix = {arXiv},
arxivId = {1505.07376},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1109/CVPR.2016.265},
eprint = {1505.07376},
isbn = {9781467388511},
issn = {10636919},
journal = {The IEEE conference on computer vision and pattern recognition},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {2414--2423},
pmid = {15430064963552939126},
title = {{Image style transfer using convolutional neural networks}},
year = {2016}
}
@article{wang2017tacotron,
abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, RJ J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
doi = {10.21437/Interspeech.2017-1452},
eprint = {1703.10135},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.,speech synthesis,state of the art},
mendeley-tags = {speech synthesis,state of the art},
pages = {1--10},
title = {{Tacotron: Towards End-to-End Speech Synthesis}},
url = {http://arxiv.org/abs/1703.10135},
volume = {2017-Augus},
year = {2017}
}
@article{Khorrami2016,
abstract = {We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
archivePrefix = {arXiv},
arxivId = {1602.07377},
author = {Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S. and {Le Paine}, Tom and Brady, Kevin and Dagli, Charlie and Huang, Thomas S. and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S. and {Le Paine}, Tom and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
doi = {10.1017/pasa.2016.3},
eprint = {1602.07377},
institution = {IEEE},
journal = {Image Processing (ICIP), 2016 IEEE International Conference on},
keywords = {sentiment analysis,video classification},
mendeley-tags = {sentiment analysis,video classification},
pages = {619--623},
title = {{How Deep Neural Networks Can Improve Emotion Recognition on Video Data}},
url = {http://arxiv.org/abs/1602.07377},
year = {2016}
}
@article{stanley2007compositional,
abstract = {Natural DNA can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks (CPPNs), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, CPPNs map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with CPPNs through interactive evolution of two dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
author = {Stanley, Kenneth O.},
doi = {10.1007/s10710-007-9028-8},
isbn = {1389-2576},
issn = {13892576},
journal = {Genetic programming and evolvable machines},
keywords = {Artificial embryogeny,Complexity,Developmental encoding,Evolutionary computation,Generative systems,Indirect encoding,Representation},
number = {2},
pages = {131--162},
publisher = {Springer},
title = {{Compositional pattern producing networks: A novel abstraction of development}},
volume = {8},
year = {2007}
}
@article{Wang2016a,
author = {Wang, Zhenyang and Deng, Zhidong and Wang, Shiyao},
doi = {10.1109/IJCNN.2016.7727308},
institution = {IEEE},
isbn = {9781509006205},
journal = {Neural Networks (IJCNN), 2016 International Joint Conference on},
keywords = {image classification},
mendeley-tags = {image classification},
number = {July},
pages = {1008--1014},
title = {{SAM : A Rethinking of Prominent Convolutional Neural Network Architectures for Visual Object Recognition}},
year = {2016}
}
@article{Lin2013,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
doi = {10.1109/ASRU.2015.7404828},
eprint = {1312.4400},
isbn = {9781479972913},
issn = {03029743},
journal = {arXiv preprint arXiv:1312.4400},
keywords = {image classification},
mendeley-tags = {image classification},
pages = {1--10},
pmid = {24356345},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Ojha2017,
abstract = {Over the past two decades, the feedforward neural network (FNN) optimization has been a key interest among the researchers and practitioners of multiple disciplines. The FNN optimization is often viewed from the various perspectives: the optimization of weights, network architecture, activation nodes, learning parameters, learning environment, etc. Researchers adopted such different viewpoints mainly to improve the FNN's generalization ability. The gradient-descent algorithm such as backpropagation has been widely applied to optimize the FNNs. Its success is evident from the FNN's application to numerous real-world problems. However, due to the limitations of the gradient-based optimization methods, the metaheuristic algorithms including the evolutionary algorithms, swarm intelligence, etc., are still being widely explored by the researchers aiming to obtain generalized FNN for a given problem. This article attempts to summarize a broad spectrum of FNN optimization methodologies including conventional and metaheuristic approaches. This article also tries to connect various research directions emerged out of the FNN optimization practices, such as evolving neural network (NN), cooperative coevolution NN, complex-valued NN, deep learning, extreme learning machine, quantum NN, etc. Additionally, it provides interesting research challenges for future research to cope-up with the present information processing era.},
archivePrefix = {arXiv},
arxivId = {arXiv:1705.05584v1},
author = {Ojha, Varun Kumar and Abraham, Ajith and Sn{\'{a}}{\v{s}}el, V{\'{a}}clav},
doi = {10.1016/j.engappai.2017.01.013},
eprint = {arXiv:1705.05584v1},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {Ensemble,Feedforward neural network,Metaheuristics,Multiobjective,Nature-inspired algorithms,evolutive,review},
mendeley-tags = {evolutive,review},
number = {2017},
pages = {97--116},
publisher = {Elsevier},
title = {{Metaheuristic design of feedforward neural networks: A review of two decades of research}},
volume = {60},
year = {2017}
}
@article{Choi2016a,
abstract = {We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
archivePrefix = {arXiv},
arxivId = {1606.00298},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
eprint = {1606.00298},
journal = {arXiv preprint arXiv:1606.00298},
keywords = {music classification},
mendeley-tags = {music classification},
title = {{Automatic tagging using deep convolutional neural networks}},
url = {http://arxiv.org/abs/1606.00298},
year = {2016}
}
@article{Sak2015,
abstract = {We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.},
archivePrefix = {arXiv},
arxivId = {1507.06947},
author = {Sak, Ha$\backslash$csim Haşim and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
eprint = {1507.06947},
journal = {arXiv preprint arXiv:1507.06947},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
pmid = {1000285842},
title = {{Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition}},
url = {http://arxiv.org/abs/1507.06947},
year = {2015}
}
@article{Ding2016,
abstract = {Relatively small data sets available for expression recognition research make the training of deep networks for expression recognition very challenging. Although fine-tuning can partially alleviate the issue, the performance is still below acceptable levels as the deep features probably contain redun- dant information from the pre-trained domain. In this paper, we present FaceNet2ExpNet, a novel idea to train an expression recognition network based on static images. We first propose a new distribution function to model the high-level neurons of the expression network. Based on this, a two-stage training algorithm is carefully designed. In the pre-training stage, we train the convolutional layers of the expression net, regularized by the face net; In the refining stage, we append fully- connected layers to the pre-trained convolutional layers and train the whole network jointly. Visualization shows that the model trained with our method captures improved high-level expression semantics. Evaluations on four public expression databases, CK+, Oulu-CASIA, TFD, and SFEW demonstrate that our method achieves better results than state-of-the-art.},
archivePrefix = {arXiv},
arxivId = {1609.06591},
author = {Ding, Hui and Zhou, Shaohua Kevin and Chellappa, Rama},
doi = {10.1109/FG.2017.23},
eprint = {1609.06591},
institution = {IEEE},
isbn = {9781509040230},
journal = {Automatic Face {\&} Gesture Recognition (FG 2017), 2017 12th IEEE International Conference on},
keywords = {facial expression,state of the art},
mendeley-tags = {facial expression,state of the art},
pages = {118--126},
title = {{FaceNet2ExpNet: Regularizing a Deep Face Recognition Net for Expression Recognition}},
url = {http://arxiv.org/abs/1609.06591},
year = {2016}
}
@article{Risi2017,
abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
archivePrefix = {arXiv},
arxivId = {1410.7326},
author = {Risi, Sebastian and Togelius, Julian},
doi = {10.1109/TCIAIG.2015.2494596},
eprint = {1410.7326},
isbn = {1943-068X VO  - PP},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Evolutionary algorithms,evolutive,games,neural networks,neuroevolution,review},
mendeley-tags = {evolutive,games,review},
number = {1},
pages = {25--41},
publisher = {IEEE},
title = {{Neuroevolution in Games: State of the Art and Open Challenges}},
volume = {9},
year = {2017}
}
@article{Stanley2002,
abstract = {The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
archivePrefix = {arXiv},
arxivId = {1407.0576},
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
eprint = {1407.0576},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {evolutive},
mendeley-tags = {evolutive},
number = {2},
pages = {99--127},
pmid = {12180173},
publisher = {MIT Press},
title = {{Evolving Neural Networks through Augmenting Topologies}},
url = {http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
volume = {10},
year = {2002}
}
@article{Hopfield1982,
abstract = {Computational properties of use of biological organisms or to the construction of computers can emerge as collective properties of systems having a large number of simple equivalent components (or neurons). The physical meaning of content-addressable memory is described by an appropriate phase space flow of the state of a system. A model of such a system is given, based on aspects of neurobiology but readily adapted to integrated circuits. The collective properties of this model produce a content-addressable memory which correctly yields an entire memory from any subpart of sufficient size. The algorithm for the time evolution of the state of the system is based on asynchronous parallel processing. Additional emergent collective properties include some capacity for generalization, familiarity recognition, categorization, error correction, and time sequence retention. The collective properties are only weakly sensitive to details of the modeling or the failure of individual devices.},
author = {Hopfield, John J.},
doi = {10.1073/pnas.79.8.2554},
isbn = {0027-8424},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
keywords = {seminal},
mendeley-tags = {seminal},
number = {8},
pages = {2554--2558},
pmid = {6953413},
publisher = {National Acad Sciences},
title = {{Neural networks and physical systems with emergent collective computational abilities.}},
url = {http://www.pnas.org/cgi/doi/10.1073/pnas.79.8.2554},
volume = {79},
year = {1982}
}
@article{wang2017tacotron,
abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, RJ J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
doi = {10.21437/Interspeech.2017-1452},
eprint = {1703.10135},
issn = {19909772},
journal = {Proceedings of the Annual Conference of the International Speech Communication Association, INTERSPEECH},
keywords = {Sequence-To-sequence,Text-To-speech synthesis,end-To-end model.,speech synthesis,state of the art},
mendeley-tags = {speech synthesis,state of the art},
pages = {4006--4010},
title = {{Tacotron: Towards end-To-end speech synthesis}},
url = {http://arxiv.org/abs/1703.10135},
volume = {2017-Augus},
year = {2017}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
doi = {10.1007/978-3-319-46493-0_35},
eprint = {1603.06668},
institution = {Springer},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {577--593},
title = {{Learning representations for automatic colorization}},
volume = {9908 LNCS},
year = {2016}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L{\'{e}}on L??on L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR),seminal},
mendeley-tags = {seminal},
number = {11},
pages = {2278--2323},
pmid = {15823584},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Xu2015,
author = {Xu, Wenduan and Auli, Michael and Clark, Stephen},
doi = {10.3115/v1/p15-2041},
isbn = {9781941643730},
journal = {Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
keywords = {sentence classification},
mendeley-tags = {sentence classification},
number = {2014},
pages = {250--255},
publisher = {Association for Computational Linguistics},
title = {{CCG Supertagging with a Recurrent Neural Network}},
url = {https://doi.org/10.3115/v1/p15-2041},
year = {2015}
}
@article{Boser1992,
abstract = {A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms. 1 INTRODUCTION Good generalization performance of pattern classifiers is achieved when the capacity of the classification function is matched to the size of the training set. Classifiers with a large numb...},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Boser, Bernhard E. and Guyon, Isabelle M. and Vapnik, Vladimir N.},
doi = {10.1145/130385.130401},
eprint = {arXiv:1011.1669v3},
institution = {ACM},
isbn = {089791497X},
issn = {0-89791-497-X},
journal = {Proceedings of the fifth annual workshop on Computational learning theory  - COLT '92},
keywords = {seminal},
mendeley-tags = {seminal},
pages = {144--152},
pmid = {25246403},
title = {{A training algorithm for optimal margin classifiers}},
url = {http://portal.acm.org/citation.cfm?doid=130385.130401},
year = {1992}
}
@article{Boulanger-lewandowski2014,
author = {Boulanger-Lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong},
institution = {IEEE},
isbn = {9781479928934},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
number = {2},
pages = {5454--5458},
title = {{PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS}},
year = {2014}
}
@article{Veit2016,
abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
archivePrefix = {arXiv},
arxivId = {1605.06431},
author = {Veit, Andreas and Wilber, Michael J and Belongie, Serge},
eprint = {1605.06431},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {theory},
mendeley-tags = {theory},
pages = {1--9},
title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}},
url = {http://arxiv.org/abs/1605.06431},
year = {2016}
}
@article{Leung2003,
abstract = {This paper presents the tuning of the structure and parameters of a neural network using an improved genetic algorithm (GA). It is also shown that the improved GA performs better than the standard GA based on some benchmark test functions. A neural network with switches introduced to its links is proposed. By doing this, the proposed neural network can learn both the input-output relationships of an application and the network structure using the improved GA. The number of hidden nodes is chosen manually by increasing it from a small number until the learning performance in terms of fitness value is good enough. Application examples on sunspot forecasting and associative memory are given to show the merits of the improved GA and the proposed neural network.},
author = {Leung, Frank Hung-Fat F and Lam, Hak-Keung K and Ling, Sai-Ho H and Tam, Peter Kwong-Shun S},
doi = {10.1109/TNN.2002.804317},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {1045-9227},
journal = {IEEE transactions on neural networks / a publication of the IEEE Neural Networks Council},
keywords = {evolutive},
mendeley-tags = {evolutive},
number = {1},
pages = {79--88},
pmid = {18237992},
publisher = {IEEE},
title = {{Tuning of the structure and parameters of a neural network using an improved genetic algorithm.}},
volume = {14},
year = {2003}
}
@article{Theis2015,
abstract = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
archivePrefix = {arXiv},
arxivId = {1506.03478},
author = {Theis, Lucas and Bethge, Matthias},
eprint = {1506.03478},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {1--9},
title = {{Generative Image Modeling Using Spatial LSTMs}},
url = {http://arxiv.org/abs/1506.03478},
year = {2015}
}
@article{Sigtia2016b,
abstract = {We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yields the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
archivePrefix = {arXiv},
arxivId = {1508.01774v2},
author = {Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
doi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774v2},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Automatic music transcription,Deep learning,Music language models,Recurrent neural networks,music transcription,state of the art},
mendeley-tags = {music transcription,state of the art},
number = {5},
pages = {927--939},
publisher = {IEEE Press},
title = {{An end-to-end neural network for polyphonic piano music transcription}},
url = {https://arxiv.org/abs/1508.01774},
volume = {24},
year = {2016}
}
@article{Sainath2015,
abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12{\%}-14{\%} relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.},
author = {Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and rahman Mohamed, Abdel-rahman and Dahl, George and Ramabhadran, Bhuvana},
doi = {10.1016/j.neunet.2014.08.005},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Neural networks,Speech recognition,speech recognition},
mendeley-tags = {speech recognition},
pages = {39--48},
pmid = {25439765},
publisher = {Elsevier Ltd},
title = {{Deep Convolutional Neural Networks for Large-scale Speech Tasks}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.08.005},
volume = {64},
year = {2015}
}
@article{Park2017,
author = {Park, Sungheon and Kwak, Nojun},
doi = {10.1007/978-3-319-54184-6_12},
institution = {Springer},
isbn = {9783319541839},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {optimization},
mendeley-tags = {optimization},
pages = {189--204},
title = {{Analysis on the dropout effect in convolutional neural networks}},
volume = {10112 LNCS},
year = {2017}
}
@article{Hutchings2017,
abstract = {Presented is a method of generating a full drum kit part for a provided kick-drum sequence. A sequence to sequence neural network model used in natural language translation was adopted to encode multiple musical styles and an online survey was developed to test different techniques for sampling the output of the softmax function. The strongest results were found using a sampling technique that drew from the three most probable outputs at each subdivision of the drum pattern but the consistency of output was found to be heavily dependent on style.},
archivePrefix = {arXiv},
arxivId = {1706.09558},
author = {Hutchings, P.},
eprint = {1706.09558},
journal = {arXiv preprint arXiv:1706.09558},
keywords = {generative music,music generation,percussion,rnn,state of the art,translation},
mendeley-tags = {music generation,state of the art},
number = {1},
pages = {43--47},
title = {{Talking Drums: Generating drum grooves with neural networks}},
url = {http://arxiv.org/abs/1706.09558},
volume = {1},
year = {2017}
}
@article{Zhang2017,
abstract = {Convolutional Neural Networks (CNNs) are effective models for reducing spectral variations and modeling spectral correlations in acoustic features for automatic speech recognition (ASR). Hybrid speech recognition systems incorporating CNNs with Hidden Markov Models/Gaussian Mixture Models (HMMs/GMMs) have achieved the state-of-the-art in various benchmarks. Meanwhile, Connectionist Temporal Classification (CTC) with Recurrent Neural Networks (RNNs), which is proposed for labeling unsegmented sequences, makes it feasible to train an end-to-end speech recognition system instead of hybrid settings. However, RNNs are computationally expensive and sometimes difficult to train. In this paper, inspired by the advantages of both CNNs and the CTC approach, we propose an end-to-end speech framework for sequence labeling, by combining hierarchical CNNs with CTC directly without recurrent connections. By evaluating the approach on the TIMIT phoneme recognition task, we show that the proposed model is not only computationally efficient, but also competitive with the existing baseline systems. Moreover, we argue that CNNs have the capability to model temporal correlations with appropriate context information.},
archivePrefix = {arXiv},
arxivId = {1701.02720},
author = {Zhang, Ying and Pezeshki, Mohammad and Brakel, Philemon Phil{\'{e}}mon Philemon and Zhang, Saizheng and Bengio, Cesar Laurent Yoshua and Courville, Aaron},
doi = {10.21437/Interspeech.2016-1446},
eprint = {1701.02720},
journal = {arXiv preprint arXiv:1701.02720},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
title = {{Towards End-to-End Speech Recognition with Deep Convolutional Neural Networks}},
url = {http://arxiv.org/abs/1701.02720},
year = {2017}
}
@article{Toderici2015,
abstract = {A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32{\$}\backslashtimes{\$}32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10{\%} or more.},
archivePrefix = {arXiv},
arxivId = {1511.06085},
author = {Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
eprint = {1511.06085},
journal = {arXiv preprint arXiv:1511.06085},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
pages = {1--12},
title = {{Variable Rate Image Compression with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06085},
year = {2015}
}
@article{Wang2015,
abstract = {Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, self- adaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
author = {Wang, Bo and Gao, Yubin},
doi = {10.12928/telkomnika.v13i1.1270},
issn = {2302-9293},
journal = {TELKOMNIKA (Telecommunication Computing Electronics and Control)},
keywords = {fuzzy theory,image compression,image compression- lossy,neural network},
mendeley-tags = {image compression- lossy},
number = {1},
pages = {137},
title = {{An Image Compression Scheme Based on Fuzzy Neural Network}},
url = {http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
volume = {13},
year = {2015}
}
@article{Benardos2007,
abstract = {Despite the fact that feedforward artificial neural networks (ANNs) have been a hot topic of research for many years there still are certain issues regarding the development of an ANN model, resulting in a lack of absolute guarantee that the model will perform well for the problem at hand. The multitude of different approaches that have been adopted in order to deal with this problem have investigated all aspects of the ANN modelling procedure, from training data collection and pre/post-processing to elaborate training schemes and algorithms. Increased attention is especially directed to proposing a systematic way to establish an appropriate architecture in contrast to the current common practice that calls for a repetitive trial-and-error process, which is time-consuming and produces uncertain results. This paper proposes such a methodology for determining the best architecture and is based on the use of a genetic algorithm (GA) and the development of novel criteria that quantify an ANN's performance (both training and generalization) as well as its complexity. This approach is implemented in software and tested based on experimental data capturing workpiece elastic deflection in turning. The intention is to present simultaneously the approach's theoretical background and its practical application in real-life engineering problems. Results show that the approach performs better than a human expert, at the same time offering many advantages in comparison to similar approaches found in literature. {\textcopyright} 2006 Elsevier Ltd. All rights reserved.},
author = {Benardos, P. G. and Vosniakos, G-C C.},
doi = {10.1016/j.engappai.2006.06.005},
isbn = {0952-1976},
issn = {09521976},
journal = {Engineering Applications of Artificial Intelligence},
keywords = {ANN architecture,Engineering problems,Feedforward artificial neural networks,Generalization,Genetic algorithms,evolutive},
mendeley-tags = {evolutive},
number = {3},
pages = {365--382},
publisher = {Elsevier},
title = {{Optimizing feedforward artificial neural network architecture}},
volume = {20},
year = {2007}
}
@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy1–3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4,5 show potential for general and highly variable tasks across many fine-grained object categories6–11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
keywords = {health,image classification,state of the art},
mendeley-tags = {health,image classification,state of the art},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {http://www.nature.com/doifinder/10.1038/nature21056},
volume = {542},
year = {2017}
}
@article{Barrow2016,
abstract = {This paper evaluates k-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to k-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
author = {Barrow, Devon K. and Crone, Sven F.},
doi = {10.1016/j.ijforecast.2015.12.011},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Bootstrapping,Cross-validation,Forecast combination,Monte Carlo,Time series,forecasting},
mendeley-tags = {forecasting},
number = {4},
pages = {1120--1137},
publisher = {Elsevier B.V.},
title = {{Cross-validation aggregation for combining autoregressive neural network forecasts}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2015.12.011},
volume = {32},
year = {2016}
}
@article{Yang2017,
abstract = {The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
archivePrefix = {arXiv},
arxivId = {1707.01786},
author = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
eprint = {1707.01786},
journal = {arXiv preprint arXiv:1707.01786},
keywords = {video classification},
mendeley-tags = {video classification},
title = {{Tensor-Train Recurrent Neural Networks for Video Classification}},
url = {https://arxiv.org/pdf/1707.01786.pdf},
year = {2017}
}
@article{Balle2015,
abstract = {We introduce a parametric nonlinear transformation that is well-suited for Gaussianizing data from natural images. The data are linearly transformed, and each component is then normalized by a pooled activity measure, computed by exponentiating a weighted sum of rectified and exponentiated components and a constant. We optimize the parameters of the full transformation (linear transform, exponents, weights, constant) over a database of natural images, directly minimizing the negentropy of the responses. The optimized transformation substantially Gaussianizes the data, achieving a significantly smaller mutual information between transformed components than alternative methods including ICA and radial Gaussianization. The transformation is differentiable and can be efficiently inverted, and thus induces a density model on images. We show that samples of this model are visually similar to samples of natural image patches. We demonstrate the use of the model as a prior probability density that can be used to remove additive noise. Finally, we show that the transformation can be cascaded, with each layer optimized using the same Gaussianization objective, thus offering an unsupervised method of optimizing a deep network architecture.},
archivePrefix = {arXiv},
arxivId = {1511.06281},
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
eprint = {1511.06281},
journal = {arXiv preprint arXiv:1511.06281},
keywords = {image classification},
mendeley-tags = {image classification},
pages = {1--14},
title = {{Density Modeling of Images using a Generalized Normalization Transformation}},
url = {http://arxiv.org/abs/1511.06281},
year = {2015}
}
@article{Frans2017,
abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
archivePrefix = {arXiv},
arxivId = {1704.08834},
author = {Frans, Kevin},
eprint = {1704.08834},
journal = {arXiv preprint arXiv:1704.08834},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
title = {{Outline Colorization through Tandem Adversarial Networks}},
url = {http://arxiv.org/abs/1704.08834},
year = {2017}
}
@article{Sato2015,
abstract = {Deep neural networks have been exhibiting splendid accuracies in many of visual pattern classification problems. Many of the state-of-the-art methods employ a technique known as data augmentation at the training stage. This paper addresses an issue of decision rule for classifiers trained with augmented data. Our method is named as APAC: the Augmented PAttern Classification, which is a way of classification using the optimal decision rule for augmented data learning. Discussion of methods of data augmentation is not our primary focus. We show clear evidences that APAC gives far better generalization performance than the traditional way of class prediction in several experiments. Our convolutional neural network model with APAC achieved a state-of-the-art accuracy on the MNIST dataset among non-ensemble classifiers. Even our multilayer perceptron model beats some of the convolutional models with recently invented stochastic regularization techniques on the CIFAR-10 dataset.},
archivePrefix = {arXiv},
arxivId = {1505.03229},
author = {Sato, Ikuro and Nishimura, Hiroki and Yokoi, Kensuke},
eprint = {1505.03229},
journal = {arXiv preprint arXiv:1505.03229},
keywords = {image classification,optimization},
mendeley-tags = {image classification,optimization},
title = {{APAC: Augmented PAttern Classification with Neural Networks}},
url = {http://arxiv.org/abs/1505.03229},
year = {2015}
}
@article{Zhang2016,
abstract = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
archivePrefix = {arXiv},
arxivId = {1610.03022},
author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
eprint = {1610.03022},
institution = {IEEE},
isbn = {9781509041176},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
keywords = {speech recognition,state of the art},
mendeley-tags = {speech recognition,state of the art},
pages = {10--14},
title = {{Very Deep Convolutional Networks for End-to-End Speech Recognition}},
url = {http://arxiv.org/abs/1610.03022},
year = {2016}
}
@article{Fritzke1994,
abstract = {We present a new self-organizing neural network model that has two variants. The first variant performs unsupervised learning and can be used for data visualization, clustering, and vector quantization. The main advantage over existing approaches (e.g., the Kohonen feature map) is the ability of the model to automatically find a suitable network structure and size. This is achieved through a controlled growth process that also includes occasional removal of units. The second variant of the model is a supervised learning method that results from the combination of the above-mentioned self-organizing network with the radial basis function (RBF) approach. In this model it is possible—in contrast to earlier approaches—to perform the positioning of the RBF units and the supervised training of the weights in parallel. Therefore, the current classification error can be used to determine where to insert new RBF units. This leads to small networks that generalize very well. Results on the two-spirals benchmark and a vowel classification problem are presented that are better than any results previously published.},
author = {Fritzke, Bernd},
doi = {10.1016/0893-6080(94)90091-4},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {--self-organization,1,as proposed by,classification,clustering,data visualization,evolutive,feature map,i n t r,incremental learning,o d u c,pattern,radial basis function,self-organizing neural network models,seminal,t i o n,two-spiral problem},
mendeley-tags = {evolutive,seminal},
number = {9},
pages = {1441--1460},
publisher = {Elsevier},
title = {{Growing cell structures—A self-organizing network for unsupervised and supervised learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
volume = {7},
year = {1994}
}
@article{Liao2016,
abstract = {Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
archivePrefix = {arXiv},
arxivId = {1508.00330},
author = {Liao, Zhibin and Carneiro, Gustavo},
doi = {10.1109/WACV.2016.7477624},
eprint = {1508.00330},
institution = {IEEE},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
keywords = {image classification,optimization,theory},
mendeley-tags = {image classification,optimization,theory},
pages = {1--8},
title = {{On the importance of normalisation layers in deep learning with piecewise linear activation units}},
year = {2016}
}
@article{Pang2017,
abstract = {Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and {\$} 1\backslashtimes 1 {\$} convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method.},
archivePrefix = {arXiv},
arxivId = {1603.06759},
author = {Pang, Yanwei and Sun, Manli and Jiang, Xiaoheng and Li, Xuelong},
doi = {10.1109/TNNLS.2017.2676130},
eprint = {1603.06759},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {image classification},
mendeley-tags = {image classification},
pages = {1--11},
publisher = {IEEE},
title = {{Convolution in Convolution for Network in Network}},
year = {2017}
}
@article{Hoover2012,
author = {Hoover, Amy K and Szerlip, Paul A and Norton, Marie E and Brindle, Trevor A and Merritt, Zachary and Stanley, Kenneth O},
isbn = {9781905254668},
journal = {International Conference on Computational Creativity},
keywords = {music generation},
mendeley-tags = {music generation},
pages = {111},
title = {{Generating a complete multipart musical composition from a single monophonic melody with functional scaffolding}},
year = {2012}
}
@article{Goldberg2015,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
archivePrefix = {arXiv},
arxivId = {1510.00726},
author = {Goldberg, Yoav},
doi = {10.1613/jair.4992},
eprint = {1510.00726},
issn = {1076-9757},
journal = {J. Artif. Intell. Res.(JAIR)},
keywords = {review,speech recognition,speech synthesis},
mendeley-tags = {review,speech recognition,speech synthesis},
pages = {1--76},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://arxiv.org/abs/1510.00726},
volume = {57},
year = {2015}
}
@article{Costa2017,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Classifiers trained with textural descriptors (e.g., Local Binary Patterns, Local Phase Quantization, and Gabor filters) extracted from the spectrograms have achieved state-of-the-art results on several music datasets. In this work, though, we argue that we can go further with the time-frequency analysis through the use of representation learning. To show that, we compare the results obtained with a Convolutional Neural Network (CNN) with the results obtained by using handcrafted features and SVM classifiers. In addition, we have performed experiments fusing the results obtained with learned features and handcrafted features to assess the complementarity between these representations for the music classification task. Experiments were conducted on three music databases with distinct characteristics, specifically a western music collection largely used in research benchmarks (ISMIR 2004 Database), a collection of Latin American music (LMD database), and a collection of field recordings of ethnic African music. Our experiments show that the CNN compares favorably to other classifiers in several scenarios, hence, it is a very interesting alternative for music genre recognition. Considering the African database, the CNN surpassed the handcrafted representations and also the state-of-the-art by a margin. In the case of the LMD database, the combination of CNN and Robust Local Binary Pattern achieved a recognition rate of 92{\%}, which to the best of our knowledge, is the best result (using an artist filter) on this dataset so far. On the ISMIR 2004 dataset, although the CNN did not improve the state of the art, it performed better than the classifiers based individually on other kind of features.},
author = {Costa, Yandre M.G. G and Oliveira, Luiz S. and Silla, Carlos N.},
doi = {10.1016/j.asoc.2016.12.024},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {music classification,music genre recognition,state of the art},
mendeley-tags = {music classification,state of the art},
pages = {28--38},
publisher = {Elsevier B.V.},
title = {{An evaluation of Convolutional Neural Networks for music classification using spectrograms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1568494616306421},
volume = {52},
year = {2017}
}
@article{Janai2017,
abstract = {Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
archivePrefix = {arXiv},
arxivId = {1704.05519},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
eprint = {1704.05519},
journal = {arXiv preprint arXiv:1704.05519},
keywords = {and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,image classification,manner 1,object detection,overview,review,survey will become a,useful tool,we hope that our},
mendeley-tags = {image classification,object detection,review},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
url = {http://arxiv.org/abs/1704.05519},
year = {2017}
}
@article{Choi2016,
abstract = {We introduce a convolutional recurrent neural network (CRNN) for music tagging. CRNNs take advantage of convolutional neural networks (CNNs) for local feature extraction and recurrent neural networks for temporal summarisation of the extracted features. We compare CRNN with three CNN structures that have been used for music tagging while controlling the number of parameters with respect to their performance and training time per sample. Overall, we found that CRNNs show a strong performance with respect to the number of parameter and training time, indicating the effectiveness of its hybrid structure in music feature extraction and feature summarisation.},
archivePrefix = {arXiv},
arxivId = {1609.04243},
author = {Choi, Keunwoo and Fazekas, George Gy{\"{o}}rgy and Sandler, Mark and Cho, Kyunghyun},
doi = {10.1.1.302.7795},
eprint = {1609.04243},
institution = {IEEE},
isbn = {9789881701282},
issn = {15209210},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
keywords = {music classification},
mendeley-tags = {music classification},
pages = {1--5},
title = {{Convolutional Recurrent Neural Networks for Music Classification}},
url = {http://arxiv.org/abs/1609.04243},
year = {2016}
}
@article{Ludermir2006,
abstract = {This paper introduces a methodology for neural network global optimization. The aim is the simultaneous optimization of multilayer perceptron (MLP) network weights and architectures, in order to generate topologies with few connections and high classification performance for any data sets. The approach combines the advantages of simulated annealing, tabu search and the backpropagation training algorithm in order to generate an automatic process for producing networks with high classification performance and low complexity. Experimental results obtained with four classification problems and one prediction problem has shown to be better than those obtained by the most commonly used optimization techniques.},
author = {Ludermir, Teresa B. and Yamazaki, Akio and Zanchettin, Cleber},
doi = {10.1109/TNN.2006.881047},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Multilayer perceptron (MLP),Optimization of weights and architectures,Simulating annealing,Tabu search,evolutive},
mendeley-tags = {evolutive},
number = {6},
pages = {1452--1459},
pmid = {17131660},
publisher = {IEEE},
title = {{An optimization methodology for neural network weights and architectures}},
volume = {17},
year = {2006}
}
@article{Collobert2004,
abstract = {We propose to study links between three important classification algorithms: Perceptrons, Multi-Layer Perceptrons (MLPs) and Support Vector Machines (SVMs). We first study ways to control the capacity of Perceptrons (mainly regularization parameters and early stopping), using the margin idea introduced with SVMs. After showing that under simple conditions a Perceptron is equivalent to an SVM, we show it can be computationally expensive in time to train an SVM (and thus a Perceptron) with stochastic gradient descent, mainly because of the margin maximization term in the cost function. We then show that if we remove this margin maximization term, the learning rate or the use of early stopping can still control the margin. These ideas are extended afterward to the case of MLPs. Moreover, under some assumptions it also appears that MLPs are a kind of mixture of SVMs, maximizing the margin in the hidden layer space. Finally, we present a very simple MLP based on the previous findings, which yields better performances in generalization and speed than the other models.},
author = {Collobert, Ronan and Bengio, Samy},
doi = {10.1145/1015330.1015415},
institution = {ACM},
isbn = {1581138285},
issn = {1581138385},
journal = {Twenty-first international conference on Machine learning - ICML '04},
keywords = {theory},
mendeley-tags = {theory},
pages = {23},
title = {{Links between perceptrons, MLPs and SVMs}},
url = {http://portal.acm.org/citation.cfm?doid=1015330.1015415},
year = {2004}
}
@article{Zagoruyko2016,
abstract = {Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at https://github.com/szagoruyko/wide-residual-networks},
archivePrefix = {arXiv},
arxivId = {1605.07146},
author = {Zagoruyko, Sergey and Komodakis, Nikos},
eprint = {1605.07146},
journal = {arXiv preprint arXiv:1605.07146},
keywords = {image classification,object detection,state of the art},
mendeley-tags = {image classification,object detection,state of the art},
title = {{Wide Residual Networks}},
url = {http://arxiv.org/abs/1605.07146},
year = {2016}
}
@article{Rehman2014,
abstract = {Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction. {\textcopyright} Maxwell Scientific Organization, 2014.},
author = {Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
isbn = {9233351788872},
issn = {20407459},
journal = {Research Journal of Applied Sciences, Engineering and Technology},
keywords = {Compression,Image,Lossless,Lossy,Review,image compression- lossy},
mendeley-tags = {image compression- lossy},
number = {4},
pages = {656--672},
publisher = {Maxwell Science Publishing},
title = {{Image compression: A survey}},
volume = {7},
year = {2014}
}
@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
eprint = {arXiv:1011.1669v3},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities,theory},
mendeley-tags = {theory},
number = {2},
pages = {251--257},
pmid = {25246403},
publisher = {Elsevier},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{Song2016,
author = {Song, Qing and Zhao, Xu and Fan, Haijin and Wang, Danwei},
doi = {10.1109/TNNLS.2016.2518223},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {forecasting},
mendeley-tags = {forecasting},
number = {5},
pages = {1068--1081},
publisher = {IEEE},
title = {{Robust Recurrent Kernel Online Learning}},
volume = {28},
year = {2016}
}
@article{Saxena2016,
abstract = {Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02492},
author = {Saxena, Shreyas and Verbeek, Jakob},
eprint = {1606.02492},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {evolutive},
mendeley-tags = {evolutive},
pages = {4053--4061},
title = {{Convolutional Neural Fabrics}},
url = {http://arxiv.org/abs/1606.02492},
year = {2016}
}
@article{Oord2016,
abstract = {Modeling the distribution of natural images is a landmark problem in unsupervised learning. This task requires an image model that is at once expressive, tractable and scalable. We present a deep neural network that sequentially predicts the pixels in an image along the two spatial dimensions. Our method models the discrete probability of the raw pixel values and encodes the complete set of dependencies in the image. Architectural novelties include fast two-dimensional recurrent layers and an effective use of residual connections in deep recurrent networks. We achieve log-likelihood scores on natural images that are considerably better than the previous state of the art. Our main results also provide benchmarks on the diverse ImageNet dataset. Samples generated from the model appear crisp, varied and globally coherent.},
archivePrefix = {arXiv},
arxivId = {1601.06759},
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
eprint = {1601.06759},
isbn = {9781510829008},
journal = {arXiv preprint arXiv:1601.06759},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
title = {{Pixel Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1601.06759},
volume = {48},
year = {2016}
}
@article{Santurkar2017,
abstract = {Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes.},
archivePrefix = {arXiv},
arxivId = {1703.01467},
author = {Santurkar, Shibani and Budden, David and Shavit, Nir},
eprint = {1703.01467},
journal = {arXiv preprint arXiv:1703.01467},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
title = {{Generative Compression}},
url = {http://arxiv.org/abs/1703.01467},
year = {2017}
}
@article{Palmes2005,
abstract = {Evolving gradient-learning artificial neural networks (ANNs) using an evolutionary algorithm (EA) is a popular approach to address the local optima and design problems of ANN. The typical approach is to combine the strength of backpropagation (BP) in weight learning and EA's capability of searching the architecture space. However, the BP's "gradient descent" approach requires a highly computer-intensive operation that relatively restricts the search coverage of EA by compelling it to use a small population size. To address this problem, we utilized mutation-based genetic neural network (MGNN) to replace BP by using the mutation strategy of local adaptation of evolutionary programming (EP) to effect weight learning. The MGNN's mutation enables the network to dynamically evolve its structure and adapt its weights at the same time. Moreover, MGNN's EP-based encoding scheme allows for a flexible and less restricted formulation of the fitness function and makes fitness computation fast and efficient. This makes it feasible to use larger population sizes and allows MGNN to have a relatively wide search coverage of the architecture space. MGNN implements a stopping criterion where overfitness occurrences are monitored through "sliding-windows" to avoid premature learning and overlearning. Statistical analysis of its performance to some well-known classification problems demonstrate its good generalization capability. It also reveals that locally adapting or scheduling the strategy parameters embedded in each individual network may provide a proper balance between the local and global searching capabilities of MGNN.},
author = {Palmes, Paulito P. and Hayasaka, Taichi and Usui, Shiro},
doi = {10.1109/TNN.2005.844858},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Artificial neural networks (ANNs),Evolutionary algorithm (EA),Evolutionary programming (EP),Evolutionary strategies (ESs),Genetic algorithm (GA),Hybrid algorithm (HA),evolutive},
mendeley-tags = {evolutive},
number = {3},
pages = {587--600},
pmid = {15940989},
publisher = {IEEE},
title = {{Mutation-based genetic neural network}},
volume = {16},
year = {2005}
}
@article{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {1303.5778},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {1303.5778},
institution = {IEEE},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {Acoustics, speech and signal processing (icassp), 2013 ieee international conference on},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
number = {3},
pages = {6645--6649},
pmid = {27295638},
title = {{Speech Recognition with Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1303.5778},
year = {2013}
}
@article{Theis2017,
abstract = {We propose a new approach to the problem of optimizing autoencoders for lossy image compression. New media formats, changing hardware technology, as well as diverse requirements and content types create a need for compression algorithms which are more flexible than existing codecs. Autoencoders have the potential to address this need, but are difficult to optimize directly due to the inherent non-differentiabilty of the compression loss. We here show that minimal changes to the loss are sufficient to train deep autoencoders competitive with JPEG 2000 and outperforming recently proposed approaches based on RNNs. Our network is furthermore computationally efficient thanks to a sub-pixel architecture, which makes it suitable for high-resolution images. This is in contrast to previous work on autoencoders for compression using coarser approximations, shallower architectures, computationally expensive methods, or focusing on small images.},
archivePrefix = {arXiv},
arxivId = {1703.00395},
author = {Theis, Lucas and Shi, Wenzhe and Cunningham, Andrew and Husz{\'{a}}r, Ferenc},
eprint = {1703.00395},
journal = {arXiv preprint arXiv:1703.00395},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
pages = {1--19},
title = {{Lossy Image Compression with Compressive Autoencoders}},
url = {http://arxiv.org/abs/1703.00395},
year = {2017}
}
@article{Deng2013,
abstract = {In this paper, we provide an overview of the invited and contributed papers presented at the special session at ICASSP-2013, entitled "New Types of Deep Neural Network Learning for Speech Recognition and Related Applications," as organized by the authors. We also describe the historical context in which acoustic models based on deep neural networks have been developed. The technical overview of the papers presented in our special session is organized into five ways of improving deep learning methods: (1) better optimization; (2) better types of neural activation function and better network architectures; (3) better ways to determine the myriad hyper-parameters of deep neural networks; (4) more appropriate ways to preprocess speech for deep neural networks; and (5) ways of leveraging multiple languages or dialects that are more easily achieved with deep neural networks than with Gaussian mixture models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1303.5778v1},
author = {Deng, Li and Hinton, Geoffrey E. and Kingsbury, Brian},
doi = {10.1109/ICASSP.2013.6639344},
eprint = {arXiv:1303.5778v1},
institution = {IEEE},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {2013 IEEE International Conference on Acoustics, Speech and Signal Processing},
keywords = {Acoustics,Hidden Markov models,ICASSP- 2013,Neural networks,Optimization,Speech,Speech recognition,Training,acoustic models,acoustic signal processing,convolutional neural network,deep neural network,deep neural network learning,dialects,learning (artificial intelligence),multilingual,multiple languages,multitask,music processing,myriad hyper-parameter determination,network architectures,neural activation function,neural net architecture,optimisation,parameter estimation,recurrent neural network,review,spectrogram features,speech preprocessing,speech recognition},
mendeley-tags = {review,speech recognition},
pages = {8599--8603},
pmid = {23127789},
title = {{New types of deep neural network learning for speech recognition and related applications: An overview}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6639344},
year = {2013}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation,seminal,theory},
mendeley-tags = {seminal,theory},
number = {5},
pages = {359--366},
pmid = {74},
publisher = {Elsevier},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Gregor2016,
abstract = {We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
archivePrefix = {arXiv},
arxivId = {1604.08772},
author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
eprint = {1604.08772},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
keywords = {image compression - conceptual,state of the art},
mendeley-tags = {image compression - conceptual,state of the art},
number = {Nips},
pages = {3549--3557},
title = {{Towards Conceptual Compression}},
url = {http://arxiv.org/abs/1604.08772},
year = {2016}
}
@article{Mollahosseini2015,
abstract = {Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.},
archivePrefix = {arXiv},
arxivId = {1511.04110},
author = {Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},
doi = {10.1109/WACV.2016.7477450},
eprint = {1511.04110},
institution = {IEEE},
journal = {Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
keywords = {facial expression},
mendeley-tags = {facial expression},
pages = {1--10},
title = {{Going Deeper in Facial Expression Recognition using Deep Neural Networks}},
url = {http://arxiv.org/abs/1511.04110{\%}0Ahttp://dx.doi.org/10.1109/WACV.2016.7477450},
year = {2015}
}
@article{Hou2015,
author = {Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong and Member, Senior and Tao, Dacheng and Member, Senior},
journal = {IEEE transactions on neural networks and learning systems},
keywords = {image classification,sentiment analysis},
mendeley-tags = {image classification,sentiment analysis},
number = {6},
pages = {1275--1286},
publisher = {IEEE},
title = {{Blind Image Quality Assessment via Deep Learning}},
volume = {26},
year = {2015}
}
@article{Wu2016,
abstract = {Recently, recurrent neural networks (RNNs) as powerful sequence models have re-emerged as a potential acoustic model for statistical parametric speech synthesis (SPSS). The long short-term memory (LSTM) architecture is particularly attractive because it addresses the vanishing gradient problem in standard RNNs, making them easier to train. Although recent studies have demonstrated that LSTMs can achieve significantly better performance on SPSS than deep feed-forward neural networks, little is known about why. Here we attempt to answer two questions: a) why do LSTMs work well as a sequence model for SPSS; b) which component (e.g., input gate, output gate, forget gate) is most important. We present a visual analysis alongside a series of experiments, resulting in a proposal for a simplified architecture. The simplified architecture has significantly fewer parameters than an LSTM, thus reducing generation complexity considerably without degrading quality.},
archivePrefix = {arXiv},
arxivId = {1601.02539},
author = {Wu, Zhizheng and King, Simon},
doi = {10.1109/ICASSP.2016.7472657},
eprint = {1601.02539},
institution = {IEEE},
isbn = {9781479999880},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Speech synthesis,acoustic modelling,gated recurrent network,long short-term memory,recurrent network network,speech synthesis},
mendeley-tags = {speech synthesis},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
volume = {2016-May},
year = {2016}
}
@article{L.Elman1990,
author = {L.Elman, Jeffrey and Elman, Jeffrey L},
doi = {10.1207/s15516709cog1402_1},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
keywords = {seminal},
mendeley-tags = {seminal},
number = {2},
pages = {179--211},
pmid = {19563812},
publisher = {Wiley Online Library},
title = {{Finding structure in time}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=17969258453606390073related:OdlkvFuhX{\_}kJ{\%}5Cnpapers2://publication/uuid/3DEB06EE-B169-47A0-BD84-BFEE3386098E},
volume = {14},
year = {1990}
}
@article{JeffHwang2016,
abstract = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
author = {Hwang, Jeff and Zhou, You and {Jeff Hwang}, You Zhou},
journal = {Cs231N.Stanford.Edu},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
title = {{Image Colorization with Deep Convolutional Neural Networks}},
year = {2016}
}
@article{Bell2015,
abstract = {It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9{\%} to 76.4{\%} mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7{\%} to 33.1{\%} mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
archivePrefix = {arXiv},
arxivId = {1512.04143},
author = {Bell, Sean and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross and {Lawrence Zitnick}, C and Bala, Kavita and Girshick, Ross and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross},
doi = {10.1109/CVPR.2016.314},
eprint = {1512.04143},
isbn = {978-1-4673-8851-1},
issn = {978-953-7619-08-4},
journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
keywords = {object detection},
mendeley-tags = {object detection},
pages = {2874--2883},
pmid = {21803542},
title = {{Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.04143},
year = {2015}
}
@article{Wang2016,
abstract = {In recent years, financialmarket dynamics forecasting has been a focus of economic research. To predict the price indices of stock markets,we developed an architecturewhich combinedElman recurrent neural networkswith stochastic time effective function.By analyzing the proposedmodelwith the linear regression, complexity invariant distance (CID), andmultiscaleCID(MCID) analysis methods and taking themodel compared with differentmodels such as the backpropagation neural network (BPNN), the stochastic time effective neural network (STNN), and the Elman recurrent neural network (ERNN), the empirical results show that the proposed neural network displays the best performance among these neural networks in financial time series forecasting. Further, the empirical research is performed in testing the predictive effects of SSE, TWSE, KOSPI, and Nikkei225 with the established model, and the corresponding statistical comparisons of the above market indices are also exhibited. The experimental results show that this approach gives good performance in predicting the values fromthe stockmarket indices. 1.},
author = {Wang, Jie Jun Jie and Wang, Jie Jun Jie and Fang, Wen and Niu, Hongli},
doi = {10.1155/2016/4742515},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
keywords = {forecasting},
mendeley-tags = {forecasting},
publisher = {Hindawi Publishing Corporation},
title = {{Financial Time Series Prediction Using Elman Recurrent Random Neural Networks}},
volume = {2016},
year = {2016}
}
@article{Tsai2006,
abstract = {In this paper, a hybrid Taguchi-genetic algorithm (HTGA) is applied to solve the problem of tuning both network structure and parameters of a feedforward neural network. The HTGA approach is a method of combining the traditional genetic algorithm (TGA), which has a powerful global exploration capability, with the Taguchi method, which can exploit the optimum offspring. The Taguchi method is inserted between crossover and mutation operations of a TGA. Then, the systematic reasoning ability of the Taguchi method is incorporated in the crossover operations to select the better genes to achieve crossover, and consequently enhance the genetic algorithms. Therefore, the HTGA approach can be more robust, statistically sound, and quickly convergent. First, the authors evaluate the performance of the presented HTGA approach by studying some global numerical optimization problems. Then, the presented HTGA approach is effectively applied to solve three examples on forecasting the sunspot numbers, tuning the associative memory, and solving the XOR problem. The numbers of hidden nodes and the links of the feedforward neural network are chosen by increasing them from small numbers until the learning performance is good enough. As a result, a partially connected feedforward neural network can be obtained after tuning. This implies that the cost of implementation of the neural network can be reduced. In these studied problems of tuning both network structure and parameters of a feedforward neural network, there are many parameters and numerous local optima so that these studied problems are challenging enough for evaluating the performances of any proposed GA-based approaches. The computational experiments show that the presented HTGA approach can obtain better results than the existing method reported recently in the literature.},
author = {Tsai, Jinn-Tsong Tsong and Chou, Jyh-Horng Horng and Liu, Tung-Kuan Kuan},
doi = {10.1109/TNN.2005.860885},
isbn = {1045-9227 (Print)$\backslash$r1045-9227 (Linking)},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Genetic algorithm (GA),Neural networks (NN),Taguchi method,evolutive},
mendeley-tags = {evolutive},
number = {1},
pages = {69--80},
pmid = {16526477},
publisher = {IEEE},
title = {{Tuning the structure and parameters of a neural network by using hybrid Taguchi-genetic algorithm}},
volume = {17},
year = {2006}
}
@article{Bock2012,
author = {Bock, Sebastian and Schedl, Markus and B{\"{o}}ck, Sebastian and Schedl, Markus},
institution = {IEEE},
isbn = {9781467300469},
journal = {Network},
keywords = {music transcription},
mendeley-tags = {music transcription},
pages = {121--124},
title = {{Polyphonic Piano Note Transcription With Recurrent Neural Networks}},
year = {2012}
}
@article{Guang-BinHuang2014,
author = {Huang, Guang-Bin and Zhu, Qin-Yu and Siew, Chee-Kheong and {Guang-Bin Huang} and {Qin-Yu Zhu} and {Chee-Kheong Siew}},
doi = {10.1109/IJCNN.2004.1380068},
institution = {IEEE},
isbn = {0-7803-8359-1},
journal = {2004 IEEE International Joint Conference on Neural Networks (IEEE Cat. No.04CH37541)},
keywords = {seminal},
mendeley-tags = {seminal},
number = {August 2004},
pages = {985--990},
title = {{Extreme learning machine: a new learning scheme of feedforward neural networks}},
url = {http://ieeexplore.ieee.org/document/1380068/},
volume = {2},
year = {2014}
}
@article{Zhu2016,
abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
archivePrefix = {arXiv},
arxivId = {1609.03552},
author = {Zhu, Jun-Yan Yan and Kr??henb??hl, Philipp and Shechtman, Eli and Efros, Alexei A. and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A.},
doi = {10.1007/978-3-319-46454-1_36},
eprint = {1609.03552},
institution = {Springer},
isbn = {9783319464534},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {597--613},
pmid = {4520227},
title = {{Generative visual manipulation on the natural image manifold}},
volume = {9909 LNCS},
year = {2016}
}
@article{Sotelo2017a,
author = {Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Kastner, Kyle},
number = {2015},
pages = {1--6},
title = {{C Har 2W Av : E Nd - To -E Nd S Peech S Ynthesis}},
year = {2017}
}
@article{Khorrami2016c,
abstract = {We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
archivePrefix = {arXiv},
arxivId = {1602.07377},
author = {Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
doi = {10.1017/pasa.2016.3},
eprint = {1602.07377},
pages = {1--5},
title = {{How Deep Neural Networks Can Improve Emotion Recognition on Video Data}},
url = {http://arxiv.org/abs/1602.07377},
year = {2016}
}
@article{Isola2016,
abstract = {We investigate conditional adversarial networks as a general-purpose solution to image-to-image translation problems. These networks not only learn the mapping from input image to output image, but also learn a loss function to train this mapping. This makes it possible to apply the same generic approach to problems that traditionally would require very different loss formulations. We demonstrate that this approach is effective at synthesizing photos from label maps, reconstructing objects from edge maps, and colorizing images, among other tasks. As a community, we no longer hand-engineer our mapping functions, and this work suggests we can achieve reasonable results without hand-engineering our loss functions either.},
archivePrefix = {arXiv},
arxivId = {1611.07004},
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A.},
doi = {arXiv:1611.07004},
eprint = {1611.07004},
journal = {arXiv preprint arXiv:1611.07004},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
title = {{Image-to-Image Translation with Conditional Adversarial Networks}},
url = {http://arxiv.org/abs/1611.07004},
year = {2016}
}
@article{DiPersio2016,
abstract = {? 2016, North Atlantic University Union. All rights reserved.We present an Artificial Neural Network (ANN) approach to predict stock market indices, particularly with respect to the forecast of their trend movements up or down. Exploiting different Neural Networks architectures, we provide numerical analysis of concrete financial time series. In particular, after a brief r?esum?e of the existing literature on the subject, we consider the Multi-layer Perceptron (MLP), the Convolutional Neural Networks (CNN), and the Long Short-Term Memory (LSTM) recurrent neural networks techniques. We focus on the importance of choosing the correct input features, along with their preprocessing, for the specific learning algorithm one wants to use. Eventually, we consider the S{\&}P500 historical time series, predicting trend on the basis of data from the past days, and proposing a novel approach based on combination of wavelets and CNN, which outperforms the basic neural networks ones. We show, that neural networks are able to predict financial time series movements even trained only on plain time series data and propose more ways to improve results.},
author = {{Di Persio}, Luca and Honchar, Oleksandr},
issn = {19984464},
journal = {International Journal of Circuits, Systems and Signal Processing},
keywords = {Artificial neural networks,Convolutional neural network,Deep Learning,Financial forecasting,Long shortterm memory,Multi-layer neural network,Recurrent neural network,Stock markets analysis,Time series analysis,forecasting},
mendeley-tags = {forecasting},
pages = {403--413},
title = {{Artificial neural networks architectures for stock price prediction: Comparisons and applications}},
volume = {10},
year = {2016}
}
@article{He2015,
abstract = {This paper presents our system design for the Audio-Visual Emotion Challenge (AV +EC 2015). Besides the baseline features, we extract from audio the functionals on low-level descriptors (LLDs) obtained via the YAAFE toolbox, and from video the Local Phase Quantization from Three Or- thogonal Planes (LPQ-TOP) features. From the physiologi- cal signals, we extract 52 electro-cardiogram (ECG) features and 22 electro-dermal activity (EDA) features from various analysis domains. The extracted features along with the AV +EC 2015 baseline features of audio, ECG or EDA are concatenated for a further feature selection step, in which the concordance correlation coefficient (CCC), instead of the usual Pearson correlation coefficient (CC), has been used as objective function. In addition, offsets between the features and the arousal/valence labels are considered in both feature selection and modeling of the affective dimensions. For the fusion of multimodal features, we propose a Deep Bidirec- tional Long Short-Term Memory Recurrent Neural Network (DBLSTM-RNN) based multimodal affect prediction frame- work, in which the initial predictions from the single modali- ties via the DBLSTM-RNNs are firstly smoothed with Gaus- sian smoothing, then input into a second layer of DBLSTM- RNN for the final prediction of affective state. Experimental results show that our proposed features and the DBLSTM- RNN based fusion framework obtain very promising results. On the development set, the obtained CCC is up to 0.824 for arousal and 0.688 for valence, and on the test set, the CCC is 0.747 for arousal and 0.609 for valence},
author = {He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
doi = {10.1145/2808196.2811641},
institution = {ACM},
isbn = {9781450337434},
journal = {Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge - AVEC '15},
keywords = {audio and video features,dblstm-rnn,multimodal fusion,offset,physiological fea-,state of the art,ture,video classification},
mendeley-tags = {state of the art,video classification},
number = {October 2015},
pages = {73--80},
title = {{Multimodal Affective Dimension Prediction Using Deep Bidirectional Long Short-Term Memory Recurrent Neural Networks}},
year = {2015}
}
@article{Yao1999,
abstract = {Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone},
archivePrefix = {arXiv},
arxivId = {1108.1530},
author = {Yao, Xin},
doi = {10.1109/5.784219},
eprint = {1108.1530},
isbn = {9780470287194},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {evolutionary computation,evolutive,intelligent systems,neu-,seminal},
mendeley-tags = {evolutive,seminal},
number = {9},
pages = {1423--1447},
pmid = {9821520},
publisher = {IEEE},
title = {{Evolving artificial neural networks}},
volume = {87},
year = {1999}
}
@article{Choi2016a,
abstract = {We present a content-based automatic music tagging algorithm using fully convolutional neural networks (FCNs). We evaluate different architectures consisting of 2D convolutional layers and subsampling layers only. In the experiments, we measure the AUC-ROC scores of the architectures with different complexities and input types using the MagnaTagATune dataset, where a 4-layer architecture shows state-of-the-art performance with mel-spectrogram input. Furthermore, we evaluated the performances of the architectures with varying the number of layers on a larger dataset (Million Song Dataset), and found that deeper models outperformed the 4-layer architecture. The experiments show that mel-spectrogram is an effective time-frequency representation for automatic tagging and that more complex models benefit from more training data.},
archivePrefix = {arXiv},
arxivId = {1606.00298},
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
eprint = {1606.00298},
journal = {arXiv preprint arXiv:1606.00298},
keywords = {music classification},
mendeley-tags = {music classification},
title = {{Automatic tagging using deep convolutional neural networks}},
url = {http://arxiv.org/abs/1606.00298},
year = {2016}
}
@article{Theis2015,
abstract = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
archivePrefix = {arXiv},
arxivId = {1506.03478},
author = {Theis, Lucas and Bethge, Matthias},
eprint = {1506.03478},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {1--9},
title = {{Generative Image Modeling Using Spatial LSTMs}},
url = {http://arxiv.org/abs/1506.03478},
year = {2015}
}
@article{Gatys2016,
abstract = {Rendering the semantic content of an image in different styles is a difficult image processing task. Arguably, a major limiting factor for previous approaches has been the lack of image representations that explicitly represent semantic in-formation and, thus, allow to separate image content from style. Here we use image representations derived from Con-volutional Neural Networks optimised for object recogni-tion, which make high level image information explicit. We introduce A Neural Algorithm of Artistic Style that can sep-arate and recombine the image content and style of natural images. The algorithm allows us to produce new images of high perceptual quality that combine the content of an ar-bitrary photograph with the appearance of numerous well-known artworks. Our results provide new insights into the deep image representations learned by Convolutional Neu-ral Networks and demonstrate their potential for high level image synthesis and manipulation.},
archivePrefix = {arXiv},
arxivId = {1505.07376},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1109/CVPR.2016.265},
eprint = {1505.07376},
isbn = {9781467388511},
issn = {10636919},
journal = {The IEEE conference on computer vision and pattern recognition},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {2414--2423},
pmid = {15430064963552939126},
title = {{Image style transfer using convolutional neural networks}},
year = {2016}
}
@article{Boulanger-Lewandowski2013,
abstract = {We investigate the problem of transforming an input sequence into a high-dimensional output sequence in order to transcribe polyphonic audio music into symbolic notation. We introduce a probabilistic model based on a recurrent neural network that is able to learn realistic output distributions given the input and we devise an efficient algorithm to search for the global mode of that distribution. The resulting method produces musically plausible transcriptions even under high levels of noise and drastically outperforms previous state-of-the-art approaches on five datasets of synthesized sounds and real recordings, approximately halving the test error rate.},
archivePrefix = {arXiv},
arxivId = {1212.1936},
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
doi = {10.1109/ICASSP.2013.6638244},
eprint = {1212.1936},
institution = {IEEE},
isbn = {9781479903566},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {Sequence transduction,music transcription,polyphonic transcription,recurrent neural network,restricted Boltzmann machine},
mendeley-tags = {music transcription},
number = {5},
pages = {3178--3182},
title = {{High-dimensional sequence transduction}},
year = {2013}
}
@article{Sotelo2017,
abstract = {We present Char2Wav, an end-to-end model for speech synthesis. Char2Wav has two components: a reader and a neural vocoder . The reader is an encoder- decoder model with attention. The encoder is a bidirectional recurrent neural net- work that accepts text or phonemes as inputs, while the decoder is a recurrent neu- ral network (RNN) with attention that produces vocoder acoustic features. Neural vocoder refers to a conditional extension of SampleRNN which generates raw waveform samples from intermediate representations. Unlike traditional models for speech synthesis, Char2Wav learns to produce audio directly from text.},
author = {Sotelo, Jose and Mehri, Soroush and Kumar, Kundan and Santos, Joao Felipe and Kastner, Kyle and Courville, Aaron and Bengio, Yoshua},
journal = {Iclr},
keywords = {speech synthesis},
mendeley-tags = {speech synthesis},
number = {October},
pages = {44--51},
title = {{Char2Wav: End-to-End Speech Synthesis}},
url = {https://openreview.net/pdf?id=B1VWyySKx},
year = {2017}
}
@article{Esteva2017,
abstract = {Skin cancer, the most common human malignancy1–3, is primarily diagnosed visually, beginning with an initial clinical screening and followed potentially by dermoscopic analysis, a biopsy and histopathological examination. Automated classification of skin lesions using images is a challenging task owing to the fine-grained variability in the appearance of skin lesions. Deep convolutional neural networks (CNNs)4,5 show potential for general and highly variable tasks across many fine-grained object categories6–11. Here we demonstrate classification of skin lesions using a single CNN, trained end-to-end from images directly, using only pixels and disease labels as inputs. We train a CNN using a dataset of 129,450 clinical images—two orders of magnitude larger than previous datasets12—consisting of 2,032 different diseases. We test its performance against 21 board-certified dermatologists on biopsy-proven clinical images with two critical binary classification use cases: keratinocyte carcinomas versus benign seborrheic keratoses; and malignant melanomas versus benign nevi. The first case represents the identification of the most common cancers, the second represents the identification of the deadliest skin cancer. The CNN achieves performance on par with all tested experts across both tasks, demonstrating an artificial intelligence capable of classifying skin cancer with a level of competence comparable to dermatologists. Outfitted with deep neural networks, mobile devices can potentially extend the reach of dermatologists outside of the clinic. It is projected that 6.3 billion smartphone subscriptions will exist by the year 2021 (ref. 13) and can therefore potentially provide low-cost universal access to vital diagnostic care.},
author = {Esteva, Andre and Kuprel, Brett and Novoa, Roberto A. and Ko, Justin and Swetter, Susan M. and Blau, Helen M. and Thrun, Sebastian},
doi = {10.1038/nature21056},
isbn = {0028-0836},
issn = {0028-0836},
journal = {Nature},
keywords = {health,image classification,state of the art},
mendeley-tags = {health,image classification,state of the art},
number = {7639},
pages = {115--118},
pmid = {28117445},
publisher = {Nature Publishing Group},
title = {{Dermatologist-level classification of skin cancer with deep neural networks}},
url = {http://www.nature.com/doifinder/10.1038/nature21056},
volume = {542},
year = {2017}
}
@article{Wang2015,
abstract = {Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, self- adaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
author = {Wang, Bo and Gao, Yubin},
doi = {10.12928/telkomnika.v13i1.1270},
issn = {2302-9293},
journal = {TELKOMNIKA (Telecommunication Computing Electronics and Control)},
keywords = {fuzzy theory,image compression,image compression- lossy,neural network},
mendeley-tags = {image compression- lossy},
number = {1},
pages = {137},
title = {{An Image Compression Scheme Based on Fuzzy Neural Network}},
url = {http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
volume = {13},
year = {2015}
}
@article{Shin2016,
abstract = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and the revival of deep CNN. CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, with 85{\%} sensitivity at 3 false positive per patient, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
archivePrefix = {arXiv},
arxivId = {1602.03409},
author = {Shin, Hoo-Chang Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
doi = {10.1109/TMI.2016.2528162},
eprint = {1602.03409},
isbn = {0278-0062 VO - 35},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Biomedical imaging,computer aided diagnosis,image analysis,image classification,machine learning,neural networks,optimization},
mendeley-tags = {image classification,optimization},
number = {5},
pages = {1285--1298},
pmid = {26886976},
publisher = {IEEE},
title = {{Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning}},
volume = {35},
year = {2016}
}
@article{Salama2015,
abstract = {Ant colony optimization (ACO) has been successfully applied to classification, where the aim is to build a model that captures the relationships between the input attributes and the target class in a given domain's dataset. The constructed classification model can then be used to predict the unknown class of a new pattern. While artificial neural networks are one of the most widely used models for pattern classification, their application is commonly restricted to fully connected three-layer topologies. In this paper, we present a new algorithm, ANN-Miner, which uses ACO to learn the structure of feed-forward neural networks. We report computational results on 40 benchmark datasets for several variations of the algorithm. Performance is compared to the standard three-layer structure trained with two different weight-learning algorithms (back propagation, and the ACOℝ algorithm), and also to a greedy algorithm for learning NN structures. A nonparametric Friedman test is used to determine statistical significance. In addition, we compare our proposed algorithm with NEAT, a prominent evolutionary algorithm for evolving neural networks, as well as three different well-known state-of-the-art classifiers, namely the C4.5 decision tree induction algorithm, the Ripper classification rule induction algorithm, and support vector machines.},
author = {Salama, Khalid M. and Abdelbar, Ashraf M.},
doi = {10.1007/s11721-015-0112-z},
issn = {19353820},
journal = {Swarm Intelligence},
keywords = {Ant colony optimization (ACO),Machine learning,Neural networks,Pattern classification,evolutive},
mendeley-tags = {evolutive},
number = {4},
pages = {229--265},
publisher = {Springer US},
title = {{Learning neural network structures with ant colony algorithms}},
volume = {9},
year = {2015}
}
@article{Chen2017,
abstract = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
author = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
doi = {10.1016/j.eswa.2016.10.065},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep neural network,Natural language processing,Sentiment analysis,sentence classification,state of the art},
mendeley-tags = {sentence classification,state of the art},
pages = {221--230},
publisher = {Elsevier Ltd},
title = {{Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.10.065},
volume = {72},
year = {2017}
}
@article{Costa2017,
abstract = {Music genre recognition based on visual representation has been successfully explored over the last years. Classifiers trained with textural descriptors (e.g., Local Binary Patterns, Local Phase Quantization, and Gabor filters) extracted from the spectrograms have achieved state-of-the-art results on several music datasets. In this work, though, we argue that we can go further with the time-frequency analysis through the use of representation learning. To show that, we compare the results obtained with a Convolutional Neural Network (CNN) with the results obtained by using handcrafted features and SVM classifiers. In addition, we have performed experiments fusing the results obtained with learned features and handcrafted features to assess the complementarity between these representations for the music classification task. Experiments were conducted on three music databases with distinct characteristics, specifically a western music collection largely used in research benchmarks (ISMIR 2004 Database), a collection of Latin American music (LMD database), and a collection of field recordings of ethnic African music. Our experiments show that the CNN compares favorably to other classifiers in several scenarios, hence, it is a very interesting alternative for music genre recognition. Considering the African database, the CNN surpassed the handcrafted representations and also the state-of-the-art by a margin. In the case of the LMD database, the combination of CNN and Robust Local Binary Pattern achieved a recognition rate of 92{\%}, which to the best of our knowledge, is the best result (using an artist filter) on this dataset so far. On the ISMIR 2004 dataset, although the CNN did not improve the state of the art, it performed better than the classifiers based individually on other kind of features.},
author = {Costa, Yandre M.G. G and Oliveira, Luiz S. and Silla, Carlos N.},
doi = {10.1016/j.asoc.2016.12.024},
issn = {15684946},
journal = {Applied Soft Computing},
keywords = {music classification,music genre recognition,state of the art},
mendeley-tags = {music classification,state of the art},
pages = {28--38},
publisher = {Elsevier B.V.},
title = {{An evaluation of Convolutional Neural Networks for music classification using spectrograms}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1568494616306421},
volume = {52},
year = {2017}
}
@article{Wu2012,
abstract = {The Hop field model is a well-known dynamic associative-memory model. In this paper, we investigate various aspects of the Hop field model for associative memory. We conduct a systematic simulation investigation of several storage algorithms for Hop field networks, and conclude that the perceptron learning based storage algorithms can achieve much better storage capacity than the Hebbian learning based algorithms.},
author = {Wu, Yue and Hu, Jianqing and Wu, Wei and Zhou, Yong and Du, K. L.},
doi = {10.1109/ICICTA.2012.89},
institution = {IEEE},
isbn = {9780769546377},
journal = {Proceedings - 2012 5th International Conference on Intelligent Computation Technology and Automation, ICICTA 2012},
keywords = {Associative memory,Hebbian learning,Hopfield model,Perceptron learning,theory},
mendeley-tags = {theory},
number = {January},
pages = {330--336},
title = {{Storage capacity of the hopfield network associative memory}},
year = {2012}
}
@article{Araque2017,
abstract = {Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-Score.},
author = {Araque, Oscar and Corcuera-Platas, Ignacio and S{\'{a}}nchez-Rada, J. Fernando and Iglesias, Carlos A.},
doi = {10.1016/j.eswa.2017.02.002},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep learning,Ensemble,Machine learning,Natural language processing,Sentiment analysis,sentiment analysis},
mendeley-tags = {sentiment analysis},
pages = {236--246},
publisher = {Elsevier Ltd},
title = {{Enhancing deep learning sentiment analysis with ensemble techniques in social applications}},
volume = {77},
year = {2017}
}
@article{werbos74,
author = {Werbos, Paul John},
journal = {Doctoral Dissertation, Applied Mathematics, Harvard University, MA},
title = {{Beyond regression: New tools for prediction and analysis in the behavioral sciences}},
year = {1974}
}
@article{Iizuka2016,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
doi = {10.1145/2897824.2925974},
isbn = {9781450342797},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {colorization,computing methodologies,convolutional neural network concepts,image processing,image synthesis,neural net-},
mendeley-tags = {image synthesis},
number = {4},
pages = {1--11},
publisher = {ACM},
title = {{Let there be color! :Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
url = {http://dl.acm.org/citation.cfm?doid=2897824.2925974},
volume = {35},
year = {2016}
}
@article{lin2013network,
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
journal = {arXiv preprint arXiv:1312.4400},
title = {{Network in network}},
year = {2013}
}
@article{Hornik1991,
abstract = {We show that standard multilayer feedforward networks with as few as a single hidden layer and arbitrary bounded and nonconstant activation function are universal approximators with respect to Lp(??) performance criteria, for arbitrary finite input environment measures ??, provided only that sufficiently many hidden units are available. If the activation function is continuous, bounded and nonconstant, then continuous mappings can be learned uniformly over compact input sets. We also give very general conditions ensuring that networks with sufficiently smooth activation functions are capable of arbitrarily accurate approximation to a function and its derivatives. ?? 1991.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt},
doi = {10.1016/0893-6080(91)90009-T},
eprint = {arXiv:1011.1669v3},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {Activation function,Input environment measure,Lp(??) approximation,Multilayer feedforward networks,Smooth approximation,Sobolev spaces,Uniform approximation,Universal approximation capabilities,theory},
mendeley-tags = {theory},
number = {2},
pages = {251--257},
pmid = {25246403},
publisher = {Elsevier},
title = {{Approximation capabilities of multilayer feedforward networks}},
volume = {4},
year = {1991}
}
@article{Rehman2014,
abstract = {Image Compression is a demanding field in this era of communication. There is a need to study and analyze the literature for image compression, as the demand for images, video sequences and computer animation has increased at very high rate so that the increment is drastically over the years. Multimedia data whether graphics, audio, video data which is uncompress requires considerable transmission bandwidth and storage capacity. So this leads to the need of compression of images and all multimedia applications to save storage and transmission time. In this study we discuss different compression algorithms used to reduce size of images without quality reduction. {\textcopyright} Maxwell Scientific Organization, 2014.},
author = {Rehman, Mehwish and Sharif, Muhammad and Raza, Mudassar},
isbn = {9233351788872},
issn = {20407459},
journal = {Research Journal of Applied Sciences, Engineering and Technology},
keywords = {Compression,Image,Lossless,Lossy,Review,image compression- lossy},
mendeley-tags = {image compression- lossy},
number = {4},
pages = {656--672},
publisher = {Maxwell Science Publishing},
title = {{Image compression: A survey}},
volume = {7},
year = {2014}
}
@article{Johnston2017,
abstract = {We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1703.10114},
author = {Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
eprint = {1703.10114},
journal = {arXiv preprint arXiv:1703.10114},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
title = {{Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks}},
url = {http://arxiv.org/abs/1703.10114},
year = {2017}
}
@article{Toderici2016,
abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
archivePrefix = {arXiv},
arxivId = {1608.05148},
author = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
doi = {10.4135/9781412985277},
eprint = {1608.05148},
isbn = {9780761914402},
issn = {08936080},
journal = {arXiv preprint arXiv:1608.05148},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
pmid = {21655600},
title = {{Full Resolution Image Compression with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1608.05148},
year = {2016}
}
@article{Yang2017,
abstract = {The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
archivePrefix = {arXiv},
arxivId = {1707.01786},
author = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
eprint = {1707.01786},
journal = {arXiv preprint arXiv:1707.01786},
keywords = {video classification},
mendeley-tags = {video classification},
title = {{Tensor-Train Recurrent Neural Networks for Video Classification}},
url = {https://arxiv.org/pdf/1707.01786.pdf},
year = {2017}
}
@article{Sangkloy2016,
abstract = {Recently, there have been several promising methods to generate realistic imagery from deep convolutional networks. These methods sidestep the traditional computer graphics rendering pipeline and instead generate imagery at the pixel level by learning from large collections of photos (e.g. faces or bedrooms). However, these methods are of limited utility because it is difficult for a user to control what the network produces. In this paper, we propose a deep adversarial image synthesis architecture that is conditioned on sketched boundaries and sparse color strokes to generate realistic cars, bedrooms, or faces. We demonstrate a sketch based image synthesis system which allows users to 'scribble' over the sketch to indicate preferred color for objects. Our network can then generate convincing images that satisfy both the color and the sketch constraints of user. The network is feed-forward which allows users to see the effect of their edits in real time. We compare to recent work on sketch to image synthesis and show that our approach can generate more realistic, more diverse, and more controllable outputs. The architecture is also effective at user-guided colorization of grayscale images.},
archivePrefix = {arXiv},
arxivId = {1612.00835},
author = {Sangkloy, Patsorn and Lu, Jingwan and Fang, Chen and Yu, Fisher and Hays, James},
eprint = {1612.00835},
journal = {arXiv preprint arXiv:1612.00835},
keywords = {image synthesis,state of the art},
mendeley-tags = {image synthesis,state of the art},
title = {{Scribbler: Controlling Deep Image Synthesis with Sketch and Color}},
url = {http://arxiv.org/abs/1612.00835},
year = {2016}
}
@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation,seminal,theory},
mendeley-tags = {seminal,theory},
number = {5},
pages = {359--366},
pmid = {74},
publisher = {Elsevier},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}
@article{Cortes2016,
abstract = {We present new algorithms for adaptively learning artificial neural networks. Our algorithms (AdaNet) adaptively learn both the structure of the network and its weights. They are based on a solid theoretical analysis, including data-dependent generalization guarantees that we prove and discuss in detail. We report the results of large-scale experiments with one of our algorithms on several binary classification tasks extracted from the CIFAR-10 dataset. The results demonstrate that our algorithm can automatically learn network structures with very competitive performance accuracies when compared with those achieved for neural networks found by standard approaches.},
archivePrefix = {arXiv},
arxivId = {1607.01097},
author = {Cortes, Corinna and Gonzalvo, Xavi and Kuznetsov, Vitaly and Mohri, Mehryar and Yang, Scott},
eprint = {1607.01097},
journal = {arXiv preprint arXiv:1607.01097},
keywords = {evolutive},
mendeley-tags = {evolutive},
title = {{AdaNet: Adaptive Structural Learning of Artificial Neural Networks}},
url = {http://arxiv.org/abs/1607.01097},
year = {2016}
}
@article{Li2015,
author = {Li, Junnan and Lam, Edmund Y.},
doi = {10.1109/IST.2015.7294547},
institution = {IEEE},
isbn = {9781479986330},
issn = {1558-2809},
journal = {Imaging Systems and Techniques (IST), 2015 IEEE International Conference on},
keywords = {emo,facial expression,gabor filters,kernel principal component analysis,multi-layer neural network},
mendeley-tags = {facial expression},
pages = {1--6},
title = {{Facial expression recognition using deep neural networks}},
year = {2015}
}
@article{Xu2017,
author = {Xu, Lamei and Lin, Jin and Wang, Lina and Yin, Chunyong and Wang, Jin},
journal = {Advanced Science and Technology Letters},
keywords = {aspect-based sentiment,convolution neural network,sentiment analysis,word2vec},
mendeley-tags = {sentiment analysis},
number = {Ast},
pages = {199--204},
title = {{Deep Convolutional Neural Network based Approach for Aspect-based Sentiment Analysis}},
volume = {143},
year = {2017}
}
@article{Shizhou2016,
abstract = {In this paper, we choose to learn useful cues from object recognition mechanisms of the human vi-sual cortex, and propose a DCNN performance im-provement method without the need for increasing the network complexity. Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As ex-perimental results show, when applying the pro-posed method to the " Quick " model and NIN models, image classification performances are re-markably improved on four widely used bench-mark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
author = {Zhang, Shizhou and Gong, Yihong and Wang, Jinjun and Shizhou, Zhang and Gong, Yihong and Jinjun, Wang},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning,image synthesis},
mendeley-tags = {image synthesis},
pages = {2343--2349},
title = {{Improving DCNN Performance with Sparse Category-Selective Objective Function.}},
volume = {2016-Janua},
year = {2016}
}
@article{Graves2013,
abstract = {Recurrent neural networks (RNNs) are a powerful model for sequential data. End-to-end training methods such as Connectionist Temporal Classification make it possible to train RNNs for sequence labelling problems where the input-output alignment is unknown. The combination of these methods with the Long Short-term Memory RNN architecture has proved particularly fruitful, delivering state-of-the-art results in cursive handwriting recognition. However RNN performance in speech recognition has so far been disappointing, with better results returned by deep feedforward networks. This paper investigates $\backslash$emph{\{}deep recurrent neural networks{\}}, which combine the multiple levels of representation that have proved so effective in deep networks with the flexible use of long range context that empowers RNNs. When trained end-to-end with suitable regularisation, we find that deep Long Short-term Memory RNNs achieve a test set error of 17.7{\%} on the TIMIT phoneme recognition benchmark, which to our knowledge is the best recorded score.},
archivePrefix = {arXiv},
arxivId = {1303.5778},
author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
doi = {10.1109/ICASSP.2013.6638947},
eprint = {1303.5778},
institution = {IEEE},
isbn = {978-1-4799-0356-6},
issn = {1520-6149},
journal = {Acoustics, speech and signal processing (icassp), 2013 ieee international conference on},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
number = {3},
pages = {6645--6649},
pmid = {27295638},
title = {{Speech Recognition with Deep Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1303.5778},
year = {2013}
}
@article{Hou2015,
author = {Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong and Member, Senior and Tao, Dacheng and Member, Senior},
journal = {IEEE transactions on neural networks and learning systems},
keywords = {image classification,sentiment analysis},
mendeley-tags = {image classification,sentiment analysis},
number = {6},
pages = {1275--1286},
publisher = {IEEE},
title = {{Blind Image Quality Assessment via Deep Learning}},
volume = {26},
year = {2015}
}
@article{Zhu2016,
abstract = {Realistic image manipulation is challenging because it requires modifying the image appearance in a user-controlled way, while preserving the realism of the result. Unless the user has considerable artistic skill, it is easy to "fall off" the manifold of natural images while editing. In this paper, we propose to learn the natural image manifold directly from data using a generative adversarial neural network. We then define a class of image editing operations, and constrain their output to lie on that learned manifold at all times. The model automatically adjusts the output keeping all edits as realistic as possible. All our manipulations are expressed in terms of constrained optimization and are applied in near-real time. We evaluate our algorithm on the task of realistic photo manipulation of shape and color. The presented method can further be used for changing one image to look like the other, as well as generating novel imagery from scratch based on user's scribbles.},
archivePrefix = {arXiv},
arxivId = {1609.03552},
author = {Zhu, Jun-Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A},
doi = {10.1007/978-3-319-46454-1_36},
eprint = {1609.03552},
institution = {Springer},
isbn = {9783319464534},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {597--613},
pmid = {4520227},
title = {{Generative visual manipulation on the natural image manifold}},
volume = {9909 LNCS},
year = {2016}
}
@article{Bas2016,
abstract = {In recent years, artificial neural networks have been commonly used for time series forecasting by researchers from various fields. There are some types of artificial neural networks and feed forward artificial neural networks model is one of them. Although feed forward artificial neural networks gives successful forecasting results they have a basic problem. This problem is architecture selection problem. In order to eliminate this problem, Yadav et al. (2007) proposed multiplicative neuron model artificial neural network. In this study, differential evolution algorithm is proposed for the training of multiplicative neuron model for forecasting. The proposed method is applied to two well-known different real world time series data.},
author = {Bas, Eren},
doi = {10.1515/jaiscr-2016-0001},
issn = {24496499},
journal = {Journal of Artificial Intelligence and Soft Computing Research},
keywords = {Artificial neural networks,Differential evolution algorithm,Forecasting,Multiplicative neuron model,evolutive,forecasting},
mendeley-tags = {evolutive,forecasting},
number = {1},
pages = {5--11},
title = {{The training of multiplicative neuron model based artificial neural networks with differential evolution algorithm for forecasting}},
volume = {6},
year = {2016}
}
@article{Boulanger-lewandowski2014,
author = {Boulanger-lewandowski, Nicolas and Droppo, Jasha and Seltzer, Mike and Yu, Dong},
institution = {IEEE},
isbn = {9781479928934},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE International Conference on},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
number = {2},
pages = {5454--5458},
title = {{PHONE SEQUENCE MODELING WITH RECURRENT NEURAL NETWORKS}},
year = {2014}
}
@article{L.Elman1990,
author = {L.Elman, Jeffrey and Elman, Jeffrey L},
doi = {10.1207/s15516709cog1402_1},
isbn = {1551-6709},
issn = {03640213},
journal = {Cognitive science},
keywords = {seminal},
mendeley-tags = {seminal},
number = {2},
pages = {179--211},
pmid = {19563812},
publisher = {Wiley Online Library},
title = {{Finding structure in time}},
url = {http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?db=pubmed{\&}cmd=Retrieve{\&}dopt=AbstractPlus{\&}list{\_}uids=17969258453606390073related:OdlkvFuhX{\_}kJ{\%}5Cnpapers2://publication/uuid/3DEB06EE-B169-47A0-BD84-BFEE3386098E},
volume = {14},
year = {1990}
}
@article{JeffHwang2016,
abstract = {We present a convolutional-neural-network-based sys-tem that faithfully colorizes black and white photographic images without direct human assistance. We explore var-ious network architectures, objectives, color spaces, and problem formulations. The final classification-based model we build generates colorized images that are significantly more aesthetically-pleasing than those created by the base-line regression-based model, demonstrating the viability of our methodology and revealing promising avenues for fu-ture work.},
author = {Hwang, Jeff and Zhou, You and {Jeff Hwang}, You Zhou},
journal = {Cs231N.Stanford.Edu},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
title = {{Image Colorization with Deep Convolutional Neural Networks}},
year = {2016}
}
@article{PingpingZhu2013,
author = {{Pingping Zhu}, Jos{\'{e}} and Pr{\'{i}}ncipe, e C. and Zhu, Pingping and Pr$\backslash$'$\backslash$incipe, Jos{\'{e}} C},
isbn = {9781479903566},
journal = {Computer Engineering},
keywords = {forecasting},
mendeley-tags = {forecasting},
pages = {3572--3576},
title = {{KERNEL RECURRENT SYSTEM TRAINED BY REAL-TIME RECURRENT LEARNING ALGORITHM}},
year = {2013}
}
@article{Zhao2016,
abstract = {Objective functions for training of deep networks for face-related recognition tasks, such as facial expression recognition (FER), usually consider each sample independently. In this work, we present a novel peak-piloted deep network (PPDN) that uses a sample with peak expression (easy sample) to supervise the intermediate feature responses for a sample of non-peak expression (hard sample) of the same type and from the same subject. The expression evolving process from non-peak expression to peak expression can thus be implicitly embedded in the network to achieve the invariance to expression intensities. A special purpose back-propagation procedure, peak gradient suppression (PGS), is proposed for network training. It drives the intermediate-layer feature responses of non-peak expression samples towards those of the corresponding peak expression samples, while avoiding the inverse. This avoids degrading the recognition capability for samples of peak expression due to interference from their non-peak expression counterparts. Extensive comparisons on two popular FER datasets, Oulu-CASIA and CK+, demonstrate the superiority of the PPDN over state-ofthe-art FER methods, as well as the advantages of both the network structure and the optimization strategy. Moreover, it is shown that PPDN is a general architecture, extensible to other tasks by proper definition of peak and non-peak samples. This is validated by experiments that show state-of-the-art performance on pose-invariant face recognition, using the Multi-PIE dataset.},
archivePrefix = {arXiv},
arxivId = {1607.06997},
author = {Zhao, Xiangyun and Liang, Xiaodan and Liu, Luoqi and Li, Teng and Han, Yugang and Vasconcelos, Nuno and Yan, Shuicheng},
doi = {10.1007/978-3-319-46475-6_27},
eprint = {1607.06997},
institution = {Springer},
isbn = {9783319464749},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Deep network,Facial expression recognition,Peak gradient suppression,Peak-piloted,facial expression,state of the art},
mendeley-tags = {facial expression,state of the art},
pages = {425--442},
pmid = {4520227},
title = {{Peak-piloted deep network for facial expression recognition}},
volume = {9906 LNCS},
year = {2016}
}
@article{Sak2015,
abstract = {We have recently shown that deep Long Short-Term Memory (LSTM) recurrent neural networks (RNNs) outperform feed forward deep neural networks (DNNs) as acoustic models for speech recognition. More recently, we have shown that the performance of sequence trained context dependent (CD) hidden Markov model (HMM) acoustic models using such LSTM RNNs can be equaled by sequence trained phone models initialized with connectionist temporal classification (CTC). In this paper, we present techniques that further improve performance of LSTM RNN acoustic models for large vocabulary speech recognition. We show that frame stacking and reduced frame rate lead to more accurate models and faster decoding. CD phone modeling leads to further improvements. We also present initial results for LSTM RNN models outputting words directly.},
archivePrefix = {arXiv},
arxivId = {1507.06947},
author = {Sak, Ha$\backslash$csim Haşim and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
eprint = {1507.06947},
journal = {arXiv preprint arXiv:1507.06947},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
pmid = {1000285842},
title = {{Fast and Accurate Recurrent Neural Network Acoustic Models for Speech Recognition}},
url = {http://arxiv.org/abs/1507.06947},
year = {2015}
}
@article{Larsson2016,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
doi = {10.1007/978-3-319-46493-0_35},
eprint = {1603.06668},
institution = {Springer},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {577--593},
title = {{Learning representations for automatic colorization}},
volume = {9908 LNCS},
year = {2016}
}
@article{LeCun1998,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L??on L{\'{e}}on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR),seminal},
mendeley-tags = {seminal},
number = {11},
pages = {2278--2323},
pmid = {15823584},
publisher = {IEEE},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Veit2016,
abstract = {In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.},
archivePrefix = {arXiv},
arxivId = {1605.06431},
author = {Veit, Andreas and Wilber, Michael J and Belongie, Serge},
eprint = {1605.06431},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems},
keywords = {theory},
mendeley-tags = {theory},
pages = {1--9},
title = {{Residual Networks Behave Like Ensembles of Relatively Shallow Networks}},
url = {http://arxiv.org/abs/1605.06431},
year = {2016}
}
@article{Sainath2015,
abstract = {Convolutional Neural Networks (CNNs) are an alternative type of neural network that can be used to reduce spectral variations and model spectral correlations which exist in signals. Since speech signals exhibit both of these properties, we hypothesize that CNNs are a more effective model for speech compared to Deep Neural Networks (DNNs). In this paper, we explore applying CNNs to large vocabulary continuous speech recognition (LVCSR) tasks. First, we determine the appropriate architecture to make CNNs effective compared to DNNs for LVCSR tasks. Specifically, we focus on how many convolutional layers are needed, what is an appropriate number of hidden units, what is the best pooling strategy. Second, investigate how to incorporate speaker-adapted features, which cannot directly be modeled by CNNs as they do not obey locality in frequency, into the CNN framework. Third, given the importance of sequence training for speech tasks, we introduce a strategy to use ReLU+dropout during Hessian-free sequence training of CNNs. Experiments on 3 LVCSR tasks indicate that a CNN with the proposed speaker-adapted and ReLU+dropout ideas allow for a 12{\%}-14{\%} relative improvement in WER over a strong DNN system, achieving state-of-the art results in these 3 tasks.},
author = {Sainath, Tara N. and Kingsbury, Brian and Saon, George and Soltau, Hagen and rahman Mohamed, Abdel-rahman and Dahl, George and Ramabhadran, Bhuvana},
doi = {10.1016/j.neunet.2014.08.005},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Neural networks,Speech recognition,speech recognition},
mendeley-tags = {speech recognition},
pages = {39--48},
pmid = {25439765},
publisher = {Elsevier Ltd},
title = {{Deep Convolutional Neural Networks for Large-scale Speech Tasks}},
url = {http://dx.doi.org/10.1016/j.neunet.2014.08.005},
volume = {64},
year = {2015}
}
@article{Wang2016,
abstract = {In recent years, financialmarket dynamics forecasting has been a focus of economic research. To predict the price indices of stock markets,we developed an architecturewhich combinedElman recurrent neural networkswith stochastic time effective function.By analyzing the proposedmodelwith the linear regression, complexity invariant distance (CID), andmultiscaleCID(MCID) analysis methods and taking themodel compared with differentmodels such as the backpropagation neural network (BPNN), the stochastic time effective neural network (STNN), and the Elman recurrent neural network (ERNN), the empirical results show that the proposed neural network displays the best performance among these neural networks in financial time series forecasting. Further, the empirical research is performed in testing the predictive effects of SSE, TWSE, KOSPI, and Nikkei225 with the established model, and the corresponding statistical comparisons of the above market indices are also exhibited. The experimental results show that this approach gives good performance in predicting the values fromthe stockmarket indices. 1.},
author = {Wang, Jun Jie and Wang, Jun Jie and Fang, Wen and Niu, Hongli},
doi = {10.1155/2016/4742515},
issn = {16875273},
journal = {Computational Intelligence and Neuroscience},
keywords = {forecasting},
mendeley-tags = {forecasting},
publisher = {Hindawi Publishing Corporation},
title = {{Financial Time Series Prediction Using Elman Recurrent Random Neural Networks}},
volume = {2016},
year = {2016}
}
@article{Zhang2016a,
abstract = {Given a grayscale photograph as input, this paper attacks the problem of hallucinating a plausible color version of the photograph. This problem is clearly underconstrained, so previous approaches have either relied on significant user interaction or resulted in desaturated colorizations. We propose a fully automatic approach that produces vibrant and realistic colorizations. We embrace the underlying uncertainty of the problem by posing it as a classification task and use class-rebalancing at training time to increase the diversity of colors in the result. The system is implemented as a feed-forward pass in a CNN at test time and is trained on over a million color images. We evaluate our algorithm using a "colorization Turing test," asking human participants to choose between a generated and ground truth color image. Our method successfully fools humans on 32{\%} of the trials, significantly higher than previous methods. Moreover, we show that colorization can be a powerful pretext task for self-supervised feature learning, acting as a cross-channel encoder. This approach results in state-of-the-art performance on several feature learning benchmarks.},
archivePrefix = {arXiv},
arxivId = {1603.08511},
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A.},
doi = {10.1007/978-3-319-46487-9_40},
eprint = {1603.08511},
institution = {Springer},
isbn = {9783319464862},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {CNNs,Colorization,Self-supervised learning,Vision for graphics,image synthesis},
mendeley-tags = {image synthesis},
pages = {649--666},
pmid = {10463930},
title = {{Colorful image colorization}},
volume = {9907 LNCS},
year = {2016}
}
@article{Shi2016,
abstract = {In this paper, we propose a novel method to im-prove object recognition accuracies of convolu-tional neural networks (CNNs) by embedding the proposed Min-Max objective into a high layer of the models during the training process. The Min-Max objective explicitly enforces the learned object feature maps to have the minimum compactness for each object manifold and the maximum margin be-tween different object manifolds. The Min-Max objective can be universally applied to different CNN models with negligible additional computa-tion cost. Experiments with shallow and deep mod-els on four benchmark datasets including CIFAR-10, CIFAR-100, SVHN and MNIST demonstrate that CNN models trained with the Min-Max ob-jective achieve remarkable performance improve-ments compared to the corresponding baseline models.},
author = {Shi, Weiwei and Gong, Yihong and Wang, Jinjun},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning,object detection},
mendeley-tags = {object detection},
pages = {2004--2010},
title = {{Improving CNN performance with min-max objective}},
volume = {2016-Janua},
year = {2016}
}
@article{Southall2016,
abstract = {Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in-struments in audio recordings. Neural networks have al-ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro-pose the use of neural networks for ADT in order to ex-ploit their ability to capture a complex configuration of fea-tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu-ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit-able for online operation. In both systems, a separate net-work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis-ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec-tively. The results demonstrate the effectiveness of the pre-sented methods for solo percussion and a capacity for iden-tifying snare drums, which are historically the most diffi-cult drum class to detect.},
author = {Southall, Carl and Stables, Ryan and Hockman, Jason},
isbn = {978-0-692-75506-8},
journal = {Proceedings of International Society for Music Information Retrieval Conference (ISMIR)},
keywords = {music transcription,tesis},
mendeley-tags = {music transcription,tesis},
pages = {591--597},
title = {{Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks}},
year = {2016}
}
@article{wang2017tacotron,
abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
doi = {10.21437/Interspeech.2017-1452},
eprint = {1703.10135},
keywords = {speech synthesis,state of the art},
mendeley-tags = {speech synthesis,state of the art},
pages = {1--10},
title = {{Tacotron: Towards End-to-End Speech Synthesis}},
url = {http://arxiv.org/abs/1703.10135},
year = {2017}
}
@article{Zen2015,
author = {Zen, Heiga and Sak, Hasim Ha$\backslash$csim},
institution = {IEEE},
isbn = {9781467369978},
issn = {9781467369978},
journal = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
keywords = {speech synthesis},
mendeley-tags = {speech synthesis},
pages = {4470--4474},
title = {{Unidirectional Long Short-Term Memory Recurrent Neural Network with Recurrent Output Layer for Low-Latency Speech Synthesis}},
year = {2015}
}
@article{Karpathy2014,
abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new...},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Karpathy, Andrej and Toderici, George and Shetty, Sanketh Sachin and Leung, Tommy Thomas and Sukthankar, Rahul and Fei-Fei, Li},
doi = {10.1109/CVPR.2014.223},
eprint = {1412.0767},
isbn = {978-1-4799-5118-5},
issn = {978-1-4799-5118-5},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {video classification},
mendeley-tags = {video classification},
pages = {1725--1732},
title = {{Large-Scale Video Classification with Convolutional Neural Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909619{\%}0Apapers3://publication/doi/10.1109/CVPR.2014.223},
year = {2014}
}
@article{Lee2015,
abstract = {We seek to improve deep neural networks by generalizing the pooling operations that play a central role in current architectures. We pursue a careful exploration of approaches to allow pooling to learn and to adapt to complex and variable patterns. The two primary directions lie in (1) learning a pooling function via (two strategies of) combining of max and average pooling, and (2) learning a pooling function in the form of a tree-structured fusion of pooling filters that are themselves learned. In our experiments every generalized pooling operation we explore improves performance when used in place of average or max pooling. We experimentally demonstrate that the proposed pooling operations provide a boost in invariance properties relative to conventional pooling and set the state of the art on several widely adopted benchmark datasets; they are also easy to implement, and can be applied within various deep neural network architectures. These benefits come with only a light increase in computational overhead during training and a very modest increase in the number of model parameters.},
archivePrefix = {arXiv},
arxivId = {1509.08985},
author = {Lee, Chen-Yu and Gallagher, Patrick W. and Tu, Zhuowen},
doi = {10.1109/TPAMI.2017.2703082},
eprint = {1509.08985},
issn = {0162-8828},
journal = {Artificial Intelligence and Statistics},
keywords = {optimization},
mendeley-tags = {optimization},
pages = {464--472},
pmid = {67101},
title = {{Generalizing Pooling Functions in Convolutional Neural Networks: Mixed, Gated, and Tree}},
url = {http://arxiv.org/abs/1509.08985},
volume = {51},
year = {2015}
}
@article{Kulkarni2015,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Tejas D. and Whitney, William F and Kohli, Pushmeet and Tenenbaum, Joshua B.},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
issn = {10897550},
journal = {Advances in Neural Information Processing Systems},
keywords = {image synthesis,tesis},
mendeley-tags = {image synthesis,tesis},
pages = {1--10},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Tu2016,
abstract = {This paper proposes a new method for an optimized mapping of temporal variables, describing a temporal stream data, into the recently proposed NeuCube spiking neural network architecture. This optimized mapping extends the use of the NeuCube, which was initially designed for spatiotemporal brain data, to work on arbitrary stream data and to achieve a better accuracy of temporal pattern recognition, a better and earlier event prediction and a better understanding of complex temporal stream data through visualization of the NeuCube connectivity. The effect of the new mapping is demonstrated on three bench mark problems. The first one is early prediction of patient sleep stage event from temporal physiological data. The second one is pattern recognition of dynamic temporal patterns of traffic in the Bay Area of California and the last one is the Challenge 2012 contest data set. In all cases the use of the proposed mapping leads to an improved accuracy of pattern recognition and event prediction and a better understanding of the data when compared to traditional machine learning techniques or spiking neural network reservoirs with arbitrary mapping of the variables.},
archivePrefix = {arXiv},
arxivId = {1603.05594},
author = {Tu, Enmei and Kasabov, Nikola and Yang, Jie},
doi = {10.1109/TNNLS.2016.2536742},
eprint = {1603.05594},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {forecasting,state of the art},
mendeley-tags = {forecasting,state of the art},
number = {6},
pages = {1--14},
pmid = {26992179},
publisher = {IEEE},
title = {{Mapping Temporal Variables Into the NeuCube for Improved Pattern Recognition, Predictive Modeling, and Understanding of Stream Data}},
volume = {28},
year = {2016}
}
@article{Goldberg2015,
abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
archivePrefix = {arXiv},
arxivId = {1510.00726},
author = {Goldberg, Yoav},
doi = {10.1613/jair.4992},
eprint = {1510.00726},
issn = {1076-9757},
journal = {J. Artif. Intell. Res.(JAIR)},
keywords = {review,speech recognition,speech synthesis},
mendeley-tags = {review,speech recognition,speech synthesis},
pages = {1--76},
title = {{A Primer on Neural Network Models for Natural Language Processing}},
url = {http://arxiv.org/abs/1510.00726},
volume = {57},
year = {2015}
}
@article{Maas2013,
abstract = {Deep neural network acoustic models produce substantial gains in large vocabulary continuous speech recognition systems. Emerging work with rectified linear (ReL) hidden units demonstrates additional gains in final system performance relative to more commonly used sigmoidal nonlinearities. In this work, we explore the use of deep rectifier networks as acoustic models for the 300 hour Switchboard conversational speech recognition task. Using simple training procedures without pretraining, networks with rectifier nonlinearities produce 2{\%} absolute reductions in word error rates over their sigmoidal counterparts. We analyze hidden layer representations to quantify differences in how ReL units encode inputs as compared to sigmoidal units. Finally, we evaluate a variant of the ReL unit with a gradient more amenable to optimization in an attempt to further improve deep rectifier networks.},
author = {Maas, Andrew L. and Hannun, Awni Y. and Ng, Andrew Y.},
journal = {Proceedings of the 30 th International Conference on Machine Learning},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
number = {1},
pages = {6},
title = {{Rectifier Nonlinearities Improve Neural Network Acoustic Models}},
url = {https://web.stanford.edu/{~}awni/papers/relu{\_}hybrid{\_}icml2013{\_}final.pdf},
volume = {28},
year = {2013}
}
@article{Yadav2007,
abstract = {Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Yadav, Ram N. and Kalra, Prem Kumar and John, Joseph},
doi = {10.1016/j.asoc.2006.01.003},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Capacity of single neuron,Financial time series prediction,Mackey-Glass time series,Multiplicative neuron model,Time series prediction,forecasting},
mendeley-tags = {forecasting},
number = {4},
pages = {1157--1163},
publisher = {Elsevier},
title = {{Time series prediction with single multiplicative neuron model}},
volume = {7},
year = {2007}
}
@inproceedings{he2016deep,
author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
pages = {770--778},
title = {{Deep residual learning for image recognition}},
year = {2016}
}
@article{Floreano2008,
abstract = {Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
author = {Floreano, Dario and D{\"{u}}rr, Peter and Mattiussi, Claudio},
doi = {10.1007/s12065-007-0002-4},
isbn = {1206500700},
issn = {18645909},
journal = {Evolutionary Intelligence},
keywords = {Evolution,Learning,Neural networks,evolutive,review},
mendeley-tags = {evolutive,review},
number = {1},
pages = {47--62},
publisher = {Springer},
title = {{Neuroevolution: From architectures to learning}},
volume = {1},
year = {2008}
}
@inproceedings{zhang2017very,
author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
organization = {IEEE},
pages = {4845--4849},
title = {{Very deep convolutional networks for end-to-end speech recognition}},
year = {2017}
}
@article{johnston2017improved,
author = {Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
journal = {arXiv preprint arXiv:1703.10114},
title = {{Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks}},
year = {2017}
}
@inproceedings{tuohy2006evolved,
author = {Tuohy, Daniel R and Potter, Walter D and Center, Artificial Intelligence},
booktitle = {ICMC},
title = {{An Evolved Neural Network/HC Hybrid for Tablature Creation in GA-based Guitar Arranging.}},
year = {2006}
}
@inproceedings{park2016analysis,
author = {Park, Sungheon and Kwak, Nojun},
booktitle = {Asian Conference on Computer Vision},
organization = {Springer},
pages = {189--204},
title = {{Analysis on the Dropout Effect in Convolutional Neural Networks}},
year = {2016}
}
@article{yadav2007time,
author = {Yadav, Ram N and Kalra, Prem Kumar and John, Joseph},
journal = {Applied soft computing},
number = {4},
pages = {1157--1163},
publisher = {Elsevier},
title = {{Time series prediction with single multiplicative neuron model}},
volume = {7},
year = {2007}
}
@article{stanley2007compositional,
author = {Stanley, Kenneth O},
journal = {Genetic programming and evolvable machines},
number = {2},
pages = {131--162},
publisher = {Springer},
title = {{Compositional pattern producing networks: A novel abstraction of development}},
volume = {8},
year = {2007}
}
@book{bishop06,
author = {Bishop, Christopher M},
publisher = {springer},
title = {{Pattern recognition and machine learning}},
year = {2006}
}
@inproceedings{wu2016investigating,
author = {Wu, Zhizheng and King, Simon},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2016 IEEE International Conference on},
organization = {IEEE},
pages = {5140--5144},
title = {{Investigating gated recurrent networks for speech synthesis}},
year = {2016}
}
@inproceedings{southall2016automatic,
author = {Southall, Carl and Stables, Ryan and Hockman, Jason},
booktitle = {ISMIR},
pages = {591--597},
title = {{Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks.}},
year = {2016}
}
@inproceedings{zhang2016colorful,
author = {Zhang, Richard and Isola, Phillip and Efros, Alexei A},
booktitle = {European Conference on Computer Vision},
organization = {Springer},
pages = {649--666},
title = {{Colorful image colorization}},
year = {2016}
}
@article{leshno93,
author = {Leshno, Moshe and Lin, Vladimir Ya and Pinkus, Allan and Schocken, Shimon},
journal = {Neural networks},
number = {6},
pages = {861--867},
publisher = {Elsevier},
title = {{Multilayer feedforward networks with a nonpolynomial activation function can approximate any function}},
volume = {6},
year = {1993}
}
@inproceedings{zhu2016generative,
author = {Zhu, Jun-Yan and Kr{\"{a}}henb{\"{u}}hl, Philipp and Shechtman, Eli and Efros, Alexei A},
booktitle = {European Conference on Computer Vision},
organization = {Springer},
pages = {597--613},
title = {{Generative visual manipulation on the natural image manifold}},
year = {2016}
}
@article{yang2017tensor,
author = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
journal = {arXiv preprint arXiv:1707.01786},
title = {{Tensor-Train Recurrent Neural Networks for Video Classification}},
year = {2017}
}
@article{xu2017deep,
author = {Xu, Lamei and Lin, Jin and Wang, Lina and Yin, Chunyong and Wang, Jin},
title = {{Deep Convolutional Neural Network based Approach for Aspect-based Sentiment Analysis}},
year = {2017}
}
@inproceedings{boulanger2013high,
author = {Boulanger-Lewandowski, Nicolas and Bengio, Yoshua and Vincent, Pascal},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
organization = {IEEE},
pages = {3178--3182},
title = {{High-dimensional sequence transduction}},
year = {2013}
}
@article{choi2016automatic,
author = {Choi, Keunwoo and Fazekas, George and Sandler, Mark},
journal = {arXiv preprint arXiv:1606.00298},
title = {{Automatic tagging using deep convolutional neural networks}},
year = {2016}
}
@inproceedings{mollahosseini2016going,
author = {Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H},
booktitle = {Applications of Computer Vision (WACV), 2016 IEEE Winter Conference on},
organization = {IEEE},
pages = {1--10},
title = {{Going deeper in facial expression recognition using deep neural networks}},
year = {2016}
}
@book{Rosenblatt57,
author = {Rosenblatt, Frank},
publisher = {Cornell Aeronautical Laboratory},
title = {{The perceptron, a perceiving and recognizing automaton Project Para}},
year = {1957}
}
@article{zagoruyko2016wide,
author = {Zagoruyko, Sergey and Komodakis, Nikos},
journal = {arXiv preprint arXiv:1605.07146},
title = {{Wide residual networks}},
year = {2016}
}
@article{hou2015blind,
author = {Hou, Weilong and Gao, Xinbo and Tao, Dacheng and Li, Xuelong},
journal = {IEEE transactions on neural networks and learning systems},
number = {6},
pages = {1275--1286},
publisher = {IEEE},
title = {{Blind image quality assessment via deep learning}},
volume = {26},
year = {2015}
}
@article{sak2015fast,
author = {Sak, Ha$\backslash$csim and Senior, Andrew and Rao, Kanishka and Beaufays, Fran{\c{c}}oise},
journal = {arXiv preprint arXiv:1507.06947},
title = {{Fast and accurate recurrent neural network acoustic models for speech recognition}},
year = {2015}
}
@inproceedings{veit2016residual,
author = {Veit, Andreas and Wilber, Michael J and Belongie, Serge},
booktitle = {Advances in Neural Information Processing Systems},
pages = {550--558},
title = {{Residual networks behave like ensembles of relatively shallow networks}},
year = {2016}
}
@article{socher2014recursive,
author = {Socher, Richard},
publisher = {Citeseer},
title = {{Recursive deep learning for natural language processing and computer vision}},
year = {2014}
}
@inproceedings{lee2016generalizing,
author = {Lee, Chen-Yu and Gallagher, Patrick W and Tu, Zhuowen},
booktitle = {Artificial Intelligence and Statistics},
pages = {464--472},
title = {{Generalizing pooling functions in convolutional neural networks: Mixed, gated, and tree}},
year = {2016}
}
@inproceedings{deng2013new,
author = {Deng, Li and Hinton, Geoffrey and Kingsbury, Brian},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on},
organization = {IEEE},
pages = {8599--8603},
title = {{New types of deep neural network learning for speech recognition and related applications: An overview}},
year = {2013}
}
@article{janai2017computer,
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
journal = {arXiv preprint arXiv:1704.05519},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
year = {2017}
}
@inproceedings{zen2015unidirectional,
author = {Zen, Heiga and Sak, Ha$\backslash$csim},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on},
organization = {IEEE},
pages = {4470--4474},
title = {{Unidirectional long short-term memory recurrent neural network with recurrent output layer for low-latency speech synthesis}},
year = {2015}
}
@article{ludermir2006optimization,
author = {Ludermir, Teresa B and Yamazaki, Akio and Zanchettin, Cleber},
journal = {IEEE Transactions on Neural Networks},
number = {6},
pages = {1452--1459},
publisher = {IEEE},
title = {{An optimization methodology for neural network weights and architectures}},
volume = {17},
year = {2006}
}
@inproceedings{choi2017convolutional,
author = {Choi, Keunwoo and Fazekas, Gy{\"{o}}rgy and Sandler, Mark and Cho, Kyunghyun},
booktitle = {Acoustics, Speech and Signal Processing (ICASSP), 2017 IEEE International Conference on},
organization = {IEEE},
pages = {2392--2396},
title = {{Convolutional recurrent neural networks for music classification}},
year = {2017}
}
@article{sigtia2016end,
author = {Sigtia, Siddharth and Benetos, Emmanouil and Dixon, Simon},
journal = {IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP)},
number = {5},
pages = {927--939},
publisher = {IEEE Press},
title = {{An end-to-end neural network for polyphonic piano music transcription}},
volume = {24},
year = {2016}
}
@article{oord2016pixel,
author = {van den Oord, Aaron and Kalchbrenner, Nal and Kavukcuoglu, Koray},
journal = {arXiv preprint arXiv:1601.06759},
title = {{Pixel recurrent neural networks}},
year = {2016}
}
@inproceedings{bell2016inside,
author = {Bell, Sean and {Lawrence Zitnick}, C and Bala, Kavita and Girshick, Ross},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2874--2883},
title = {{Inside-outside net: Detecting objects in context with skip pooling and recurrent neural networks}},
year = {2016}
}
@article{ojha2017metaheuristic,
author = {Ojha, Varun Kumar and Abraham, Ajith and Sn{\'{a}}{\v{s}}el, V{\'{a}}clav},
journal = {Engineering Applications of Artificial Intelligence},
pages = {97--116},
publisher = {Elsevier},
title = {{Metaheuristic design of feedforward neural networks: A review of two decades of research}},
volume = {60},
year = {2017}
}
@article{palmes2005mutation,
author = {Palmes, Paulito P and Hayasaka, Taichi and Usui, Shiro},
journal = {IEEE Transactions on Neural Networks},
number = {3},
pages = {587--600},
publisher = {IEEE},
title = {{Mutation-based genetic neural network}},
volume = {16},
year = {2005}
}
@article{mao2000probabilistic,
author = {Mao, Ke Zhi and Tan, K-C and Ser, Wee},
journal = {IEEE Transactions on neural networks},
number = {4},
pages = {1009--1016},
publisher = {IEEE},
title = {{Probabilistic neural-network structure determination for pattern classification}},
volume = {11},
year = {2000}
}
@article{goldberg2016primer,
author = {Goldberg, Yoav},
journal = {J. Artif. Intell. Res.(JAIR)},
pages = {345--420},
title = {{A Primer on Neural Network Models for Natural Language Processing.}},
volume = {57},
year = {2016}
}
@article{tsai2006tuning,
author = {Tsai, Jinn-Tsong and Chou, Jyh-Horng and Liu, Tung-Kuan},
journal = {IEEE Transactions on Neural Networks},
number = {1},
pages = {69--80},
publisher = {IEEE},
title = {{Tuning the structure and parameters of a neural network by using hybrid Taguchi-genetic algorithm}},
volume = {17},
year = {2006}
}
@article{egmont2002image,
author = {Egmont-Petersen, Michael and de Ridder, Dick and Handels, Heinz},
journal = {Pattern recognition},
number = {10},
pages = {2279--2301},
publisher = {Elsevier},
title = {{Image processing with neural networks—a review}},
volume = {35},
year = {2002}
}
@article{arik2017deep,
author = {Arik, Sercan O and Chrzanowski, Mike and Coates, Adam and Diamos, Gregory and Gibiansky, Andrew and Kang, Yongguo and Li, Xian and Miller, John and Raiman, Jonathan and Sengupta, Shubho and Others},
journal = {arXiv preprint arXiv:1702.07825},
title = {{Deep Voice: Real-time neural text-to-speech}},
year = {2017}
}
@inproceedings{kasabov2012neucube,
author = {Kasabov, Nikola},
booktitle = {ANNPR},
organization = {Springer},
pages = {225--243},
title = {{NeuCube EvoSpike Architecture for Spatio-temporal Modelling and Pattern Recognition of Brain Signals.}},
year = {2012}
}
@inproceedings{he2015multimodal,
author = {He, Lang and Jiang, Dongmei and Yang, Le and Pei, Ercheng and Wu, Peng and Sahli, Hichem},
booktitle = {Proceedings of the 5th International Workshop on Audio/Visual Emotion Challenge},
organization = {ACM},
pages = {73--80},
title = {{Multimodal affective dimension prediction using deep bidirectional long short-term memory recurrent neural networks}},
year = {2015}
}
@article{jiang1999image,
author = {Jiang, J},
journal = {Signal Processing: Image Communication},
number = {9},
pages = {737--760},
publisher = {Elsevier},
title = {{Image compression with neural networks--a survey}},
volume = {14},
year = {1999}
}
@article{santurkar2017generative,
author = {Santurkar, Shibani and Budden, David and Shavit, Nir},
journal = {arXiv preprint arXiv:1703.01467},
title = {{Generative compression}},
year = {2017}
}
@article{araque2017enhancing,
author = {Araque, Oscar and Corcuera-Platas, Ignacio and S{\'{a}}nchez-Rada, J Fernando and Iglesias, Carlos A},
journal = {Expert Systems with Applications},
pages = {236--246},
publisher = {Elsevier},
title = {{Enhancing deep learning sentiment analysis with ensemble techniques in social applications}},
volume = {77},
year = {2017}
}
@article{sato2015apac,
author = {Sato, Ikuro and Nishimura, Hiroki and Yokoi, Kensuke},
journal = {arXiv preprint arXiv:1505.03229},
title = {{Apac: Augmented pattern classification with neural networks}},
year = {2015}
}
@article{isola2016image,
author = {Isola, Phillip and Zhu, Jun-Yan and Zhou, Tinghui and Efros, Alexei A},
journal = {arXiv preprint arXiv:1611.07004},
title = {{Image-to-image translation with conditional adversarial networks}},
year = {2016}
}
@article{huang2017learning,
author = {Huang, Ke-Kun and Dai, Dao-Qing and Ren, Chuan-Xian and Lai, Zhao-Rong},
journal = {IEEE transactions on neural networks and learning systems},
number = {5},
pages = {1082--1094},
publisher = {IEEE},
title = {{Learning kernel extended dictionary for face recognition}},
volume = {28},
year = {2017}
}
@inproceedings{collobert2004links,
author = {Collobert, Ronan and Bengio, Samy},
booktitle = {Proceedings of the twenty-first international conference on Machine learning},
organization = {ACM},
pages = {23},
title = {{Links between perceptrons, MLPs and SVMs}},
year = {2004}
}
@article{hinton2012deep,
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Others},
journal = {IEEE Signal Processing Magazine},
number = {6},
pages = {82--97},
publisher = {IEEE},
title = {{Deep neural networks for acoustic modeling in speech recognition: The shared views of four research groups}},
volume = {29},
year = {2012}
}
@article{costa2017evaluation,
author = {Costa, Yandre M G and Oliveira, Luiz S and Silla, Carlos N},
journal = {Applied Soft Computing},
pages = {28--38},
publisher = {Elsevier},
title = {{An evaluation of Convolutional Neural Networks for music classification using spectrograms}},
volume = {52},
year = {2017}
}
@article{yao1999evolving,
author = {Yao, Xin},
journal = {Proceedings of the IEEE},
number = {9},
pages = {1423--1447},
publisher = {IEEE},
title = {{Evolving artificial neural networks}},
volume = {87},
year = {1999}
}
@article{hwangimage,
author = {Hwang, Jeff and Zhou, You},
title = {{Image Colorization with Deep Convolutional Neural Networks}},
year = {2016}
}
@inproceedings{gregor2016towards,
author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
booktitle = {Advances In Neural Information Processing Systems},
pages = {3549--3557},
title = {{Towards conceptual compression}},
year = {2016}
}
@article{salama2015learning,
author = {Salama, Khalid M and Abdelbar, Ashraf M},
journal = {Swarm Intelligence},
number = {4},
pages = {229--265},
publisher = {Springer},
title = {{Learning neural network structures with ant colony algorithms}},
volume = {9},
year = {2015}
}
@article{balle2015density,
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P},
journal = {arXiv preprint arXiv:1511.06281},
title = {{Density modeling of images using a generalized normalization transformation}},
year = {2015}
}
@article{hutchings2017talking,
author = {Hutchings, P},
journal = {arXiv preprint arXiv:1706.09558},
title = {{Talking Drums: Generating drum grooves with neural networks}},
year = {2017}
}
@article{ritchie2003optimizationof,
author = {Ritchie, Marylyn D and White, Bill C and Parker, Joel S and Hahn, Lance W and Moore, Jason H},
journal = {BMC bioinformatics},
number = {1},
pages = {28},
publisher = {BioMed Central},
title = {{Optimizationof neural network architecture using genetic programming improvesdetection and modeling of gene-gene interactions in studies of humandiseases}},
volume = {4},
year = {2003}
}
@inproceedings{goodfellow2014generative,
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
booktitle = {Advances in neural information processing systems},
pages = {2672--2680},
title = {{Generative adversarial nets}},
year = {2014}
}
@inproceedings{gatys2016image,
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages = {2414--2423},
title = {{Image style transfer using convolutional neural networks}},
year = {2016}
}
@article{Wu2015,
abstract = {In this paper, a drum transcription algorithm using partially fixed non-negative matrix factorization is presented. The proposed method allows users to identify percussive events in complex mixtures with a minimal training set. The algorithm decomposes the music signal into two parts: percussive part with pre-defined drum templates and harmonic part with undefined entries. The harmonic part is able to adapt to the music content, allowing the algorithm to work in polyphonic mixtures. Drum event times can be simply picked from the percussive activation matrix with onset detection. The system is efficient and robust even with a minimal training set. The recognition rates for the ENST dataset vary from 56.7 to 78.9{\%} for three percussive instruments extracted from polyphonic music.},
author = {Wu, Chih Wei and Lerch, Alexander},
doi = {10.1109/EUSIPCO.2015.7362590},
isbn = {9780992862633},
journal = {2015 23rd European Signal Processing Conference, EUSIPCO 2015},
keywords = {Automatic Music Transcription,Drum Transcription,MIR,NMF,music transcription,tesis},
mendeley-tags = {music transcription,tesis},
pages = {1281--1285},
title = {{Drum transcription using partially fixed non-negative matrix factorization}},
year = {2015}
}
@article{Stanley2002a,
abstract = {The ventral visual pathway implements object recognition and categorization in a hierarchy of processing areas with neuronal selectivities of increasing complexity. The presence of massive feedback connections within this hierarchy raises the possibility that normal visual processing relies on the use of computational loops. It is not known, however, whether object recognition can be performed at all without such loops (i.e., in a purely feed-forward mode). By analyzing the time course of reaction times in a masked natural scene categorization paradigm, we show that the human visual system can generate selective motor responses based on a single feed-forward pass. We confirm these results using a more constrained letter discrimination task, in which the rapid succession of a target and mask is actually perceived as a distractor. We show that a masked stimulus presented for only 26 msecand often not consciously perceivedcan fully determine the earliest selective motor responses: The neural representations of the stimulus and mask are thus kept separated during a short period corresponding to the feedforward sweep. Therefore, feedback loops do not appear to be mandatory for visual processing. Rather, we found that such loops allow the masked stimulus to reverberate in the visual system and affect behavior for nearly 150 msec after the feed-forward sweep.},
archivePrefix = {arXiv},
arxivId = {1407.0576},
author = {Stanley, Kenneth O. and Miikkulainen, Risto},
doi = {10.1162/106365602320169811},
eprint = {1407.0576},
isbn = {1063-6560},
issn = {1063-6560},
journal = {Evolutionary Computation},
keywords = {evolutive},
mendeley-tags = {evolutive},
number = {2},
pages = {99--127},
pmid = {12180173},
title = {{Evolving Neural Networks through Augmenting Topologies}},
url = {http://www.mitpressjournals.org/doi/10.1162/106365602320169811},
volume = {10},
year = {2002}
}
@article{Yadav2007a,
abstract = {Single neuron models are typical functional replica of the biological neuron that are derived using their individual and group responses in networks. In recent past, a lot of work in this area has produced advanced neuron models for both analog and binary data patterns. Popular among these are the higher-order neurons, fuzzy neurons and other polynomial neurons. In this paper, we propose a new neuron model based on a polynomial architecture. Instead of considering all the higher-order terms, a simple aggregation function is used. The aggregation function is considered as a product of linear functions in different dimensions of the space. The functional mapping capability of the proposed neuron model is demonstrated through some well known time series prediction problems and is compared with the standard multilayer neural network. ?? 2006 Elsevier B.V. All rights reserved.},
author = {Yadav, R. N. and Kalra, P. K. and John, J.},
doi = {10.1016/j.asoc.2006.01.003},
issn = {15684946},
journal = {Applied Soft Computing Journal},
keywords = {Capacity of single neuron,Financial time series prediction,Mackey-Glass time series,Multiplicative neuron model,Time series prediction,forecasting},
mendeley-tags = {forecasting},
number = {4},
pages = {1157--1163},
title = {{Time series prediction with single multiplicative neuron model}},
volume = {7},
year = {2007}
}
@article{Zweig2016,
abstract = {This paper advances the design of CTC-based all-neural (or end-to-end) speech recognizers. We propose a novel symbol inventory, and a novel iterated-CTC method in which a second system is used to transform a noisy initial output into a cleaner version. We present a number of stabilization and initialization methods we have found useful in training these networks. We evaluate our system on the commonly used NIST 2000 conversational telephony test set, and significantly exceed the previously published performance of similar systems, both with and without the use of an external language model and decoding technology.},
archivePrefix = {arXiv},
arxivId = {1609.05935},
author = {Zweig, Geoffrey and Yu, Chengzhu and Droppo, Jasha and Stolcke, Andreas},
doi = {10.1109/ICASSP.2017.7953069},
eprint = {1609.05935},
isbn = {9781509041176},
issn = {15206149},
journal = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
keywords = {CTC,end-to-end training,recurrent neural network,speech recognition},
mendeley-tags = {speech recognition},
pages = {4805--4809},
title = {{Advances in all-neural speech recognition}},
url = {http://arxiv.org/abs/1609.05935},
year = {2017}
}
@article{Zhang2016b,
abstract = {Sequence-to-sequence models have shown success in end-to-end speech recognition. However these models have only used shallow acoustic encoder networks. In our work, we successively train very deep convolutional networks to add more expressive power and better generalization for end-to-end ASR models. We apply network-in-network principles, batch normalization, residual connections and convolutional LSTMs to build very deep recurrent and convolutional structures. Our models exploit the spectral structure in the feature space and add computational depth without overfitting issues. We experiment with the WSJ ASR task and achieve 10.5$\backslash${\%} word error rate without any dictionary or language using a 15 layer deep network.},
archivePrefix = {arXiv},
arxivId = {1610.03022},
author = {Zhang, Yu and Chan, William and Jaitly, Navdeep},
eprint = {1610.03022},
isbn = {9781509041176},
keywords = {speech recognition,state of the art},
mendeley-tags = {speech recognition,state of the art},
pages = {10--14},
title = {{Very Deep Convolutional Networks for End-to-End Speech Recognition}},
url = {http://arxiv.org/abs/1610.03022},
year = {2016}
}
@article{Hinton2012,
abstract = {Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition.},
archivePrefix = {arXiv},
arxivId = {1207.0580},
author = {Hinton, Geoffrey and Deng, Li and Yu, Dong and Dahl, George E and Mohamed, Abdel-rahman and Jaitly, Navdeep and Senior, Andrew and Vanhoucke, Vincent and Nguyen, Patrick and Sainath, Tara N and Kingsbury, Brian},
doi = {10.1109/MSP.2012.2205597},
eprint = {1207.0580},
isbn = {1053-5888},
issn = {1053-5888},
journal = {IEEE Signal Processing Magazine},
keywords = {speech recognition},
mendeley-tags = {speech recognition},
number = {November},
pages = {82--97},
pmid = {13057166},
title = {{Deep Neural Networks for Acoustic Modeling in Speech Recognition}},
year = {2012}
}
@article{Wang2017a,
abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {\textless}text, audio{\textgreater} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
archivePrefix = {arXiv},
arxivId = {1703.10135},
author = {Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
eprint = {1703.10135},
keywords = {speech synthesis,state of the art},
mendeley-tags = {speech synthesis,state of the art},
pages = {1--10},
title = {{Tacotron: Towards End-to-End Speech Synthesis}},
url = {http://arxiv.org/abs/1703.10135},
year = {2017}
}
@article{Stanley2007,
abstract = {Natural DNA can encode complexity on an enormous scale. Researchers are attempting to achieve the same representational efficiency in computers by implementing developmental encodings, i.e. encodings that map the genotype to the phenotype through a process of growth from a small starting point to a mature form. A major challenge in in this effort is to find the right level of abstraction of biological development to capture its essential properties without introducing unnecessary inefficiencies. In this paper, a novel abstraction of natural development, called Compositional Pattern Producing Networks (CPPNs), is proposed. Unlike currently accepted abstractions such as iterative rewrite systems and cellular growth simulations, CPPNs map to the phenotype without local interaction, that is, each individual component of the phenotype is determined independently of every other component. Results produced with CPPNs through interactive evolution of two dimensional images show that such an encoding can nevertheless produce structural motifs often attributed to more conventional developmental abstractions, suggesting that local interaction may not be essential to the desirable properties of natural encoding in the way that is usually assumed.},
author = {Stanley, Kenneth O.},
doi = {10.1007/s10710-007-9028-8},
isbn = {1389-2576},
issn = {13892576},
journal = {Genetic Programming and Evolvable Machines},
keywords = {Artificial embryogeny,Complexity,Developmental encoding,Evolutionary computation,Generative systems,Indirect encoding,Representation},
number = {2},
pages = {131--162},
title = {{Compositional pattern producing networks: A novel abstraction of development}},
volume = {8},
year = {2007}
}
@article{Mollahosseini2015a,
abstract = {Automated Facial Expression Recognition (FER) has remained a challenging and interesting problem. Despite efforts made in developing various methods for FER, existing approaches traditionally lack generalizability when applied to unseen images or those that are captured in wild setting. Most of the existing approaches are based on engineered features (e.g. HOG, LBPH, and Gabor) where the classifier's hyperparameters are tuned to give best recognition accuracies across a single database, or a small collection of similar databases. Nevertheless, the results are not significant when they are applied to novel data. This paper proposes a deep neural network architecture to address the FER problem across multiple well-known standard face datasets. Specifically, our network consists of two convolutional layers each followed by max pooling and then four Inception layers. The network is a single component architecture that takes registered facial images as the input and classifies them into either of the six basic or the neutral expressions. We conducted comprehensive experiments on seven publically available facial expression databases, viz. MultiPIE, MMI, CK+, DISFA, FERA, SFEW, and FER2013. The results of proposed architecture are comparable to or better than the state-of-the-art methods and better than traditional convolutional neural networks and in both accuracy and training time.},
archivePrefix = {arXiv},
arxivId = {1511.04110},
author = {Mollahosseini, Ali and Chan, David and Mahoor, Mohammad H.},
doi = {10.1109/WACV.2016.7477450},
eprint = {1511.04110},
keywords = {facial expression},
mendeley-tags = {facial expression},
title = {{Going Deeper in Facial Expression Recognition using Deep Neural Networks}},
url = {http://arxiv.org/abs/1511.04110{\%}0Ahttp://dx.doi.org/10.1109/WACV.2016.7477450},
year = {2015}
}
@article{Lin2013b,
abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
archivePrefix = {arXiv},
arxivId = {1312.4400},
author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
doi = {10.1109/ASRU.2015.7404828},
eprint = {1312.4400},
isbn = {9781479972913},
issn = {03029743},
keywords = {image classification},
mendeley-tags = {image classification},
pages = {1--10},
pmid = {24356345},
title = {{Network In Network}},
url = {http://arxiv.org/abs/1312.4400},
year = {2013}
}
@article{Saxena2016a,
abstract = {Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.},
archivePrefix = {arXiv},
arxivId = {1606.02492},
author = {Saxena, Shreyas and Verbeek, Jakob},
eprint = {1606.02492},
issn = {10495258},
keywords = {evolutive},
mendeley-tags = {evolutive},
title = {{Convolutional Neural Fabrics}},
url = {http://arxiv.org/abs/1606.02492},
year = {2016}
}
@article{Barrow2016a,
abstract = {This paper evaluates k-fold and Monte Carlo cross-validation and aggregation (crogging) for combining neural network autoregressive forecasts. We introduce Monte Carlo crogging which combines bootstrapping and cross-validation (CV) in a single approach through repeated random splitting of the original time series into mutually exclusive datasets for training. As the training/validation split is independent of the number of folds, the algorithm offers more flexibility in the size, and number of training samples compared to k-fold cross-validation. The study also provides for crogging and bagging: (1) the first systematic evaluation across time series length and combination size, (2) a bias and variance decomposition of the forecast errors to understand improvement gains, and (3) a comparison to established benchmarks of model averaging and selection. Crogging can easily be extended to other autoregressive models. Results on real and simulated series demonstrate significant improvements in forecasting accuracy especially for short time series and long forecast horizons.},
author = {Barrow, Devon K. and Crone, Sven F.},
doi = {10.1016/j.ijforecast.2015.12.011},
issn = {01692070},
journal = {International Journal of Forecasting},
keywords = {Bootstrapping,Cross-validation,Forecast combination,Monte Carlo,Time series,forecasting},
mendeley-tags = {forecasting},
number = {4},
pages = {1120--1137},
publisher = {Elsevier B.V.},
title = {{Cross-validation aggregation for combining autoregressive neural network forecasts}},
url = {http://dx.doi.org/10.1016/j.ijforecast.2015.12.011},
volume = {32},
year = {2016}
}
@article{Omar2017,
abstract = {This study explores the effectiveness of an Artificial Neural Network (ANN) in predicting fraudulent financial reporting in small market capitalization companies in Malaysia. Design/methodology/approach Based on the concepts of ANN, a mathematical model is developed to compare non-fraud and fraud companies selected from among small market capitalization companies in Malaysia; the fraud companies had already been charged by the Securities Commission for the falsification of financial statements. Ten financial ratios are used as fraud risk indicators to predict fraudulent financial reporting using ANN. Findings Indicate that the proposed ANN methodology outperforms other statistical techniques widely used for predicting fraudulent financial reporting. Originality/value The study is one of few to adopt the ANN approach to the prediction of financial reporting fraud.},
author = {Omar, Normah and Johari, Zulaikha 'Amirah and Smith, Malcolm},
doi = {10.1108/13590791011082797},
isbn = {1359079051062},
issn = {1359-0790},
journal = {Journal of Financial Crime Iss},
keywords = {security},
mendeley-tags = {security},
number = {2},
title = {{Predicting fraudulent financial reporting using artificial neural network}},
url = {http://dx.doi.org/10.1108/eb025814{\%}5Cnhttp://},
volume = {24},
year = {2017}
}
@article{Janai2017a,
abstract = {Recent years have witnessed amazing progress in AI related fields such as computer vision, machine learning and autonomous vehicles. As with any rapidly growing field, however, it becomes increasingly difficult to stay up-to-date or enter the field as a beginner. While several topic specific survey papers have been written, to date no general survey on problems, datasets and methods in computer vision for autonomous vehicles exists. This paper attempts to narrow this gap by providing a state-of-the-art survey on this topic. Our survey includes both the historically most relevant literature as well as the current state-of-the-art on several specific topics, including recognition, reconstruction, motion estimation, tracking, scene understanding and end-to-end learning. Towards this goal, we first provide a taxonomy to classify each approach and then analyze the performance of the state-of-the-art on several challenging benchmarking datasets including KITTI, ISPRS, MOT and Cityscapes. Besides, we discuss open problems and current research challenges. To ease accessibility and accommodate missing references, we will also provide an interactive platform which allows to navigate topics and methods, and provides additional information and project links for each paper.},
archivePrefix = {arXiv},
arxivId = {1704.05519},
author = {Janai, Joel and G{\"{u}}ney, Fatma and Behl, Aseem and Geiger, Andreas},
eprint = {1704.05519},
keywords = {and lowers the,autonomous vehicles,autonomous vision,by providing an exhaustive,computer vision,entry barrier for beginners,field of autonomous vision,for researchers in the,image classification,manner 1,object detection,overview,review,survey will become a,useful tool,we hope that our},
mendeley-tags = {image classification,object detection,review},
title = {{Computer Vision for Autonomous Vehicles: Problems, Datasets and State-of-the-Art}},
url = {http://arxiv.org/abs/1704.05519},
year = {2017}
}
@article{Aljumah2016,
author = {Aljumah, Abdullah and Ahamad, Tariq},
keywords = {security},
mendeley-tags = {security},
number = {12},
pages = {132--138},
title = {{A Novel Approach for Detecting DDoS using Artificial Neural}},
volume = {16},
year = {2016}
}
@article{Raczynski2013,
author = {Raczy{\'{n}}ski, Stanis{\l}aw A. and Vincent, Emmanuel and Sagayama, Shigeki},
doi = {10.1109/TASL.2013.2258012},
journal = {IEEE Transactions on Audio, Speech, and Language Processing},
keywords = {music transcription},
mendeley-tags = {music transcription},
number = {9},
pages = {1830 -- 1840},
title = {{Dynamic Bayesian networks for symbolic polyphonic pitch modeling}},
volume = {21},
year = {2013}
}
@article{Bengio2007,
abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), rea- soning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, withmin- imal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally lim- ited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very ineffi- cient in terms of required number of computational elements and examples. Sec- ond, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learn- ing) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more ab- stract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence. 1},
author = {Bengio, Yoshua and {\{}LeCun{\}}, Yann and Lecun, Yann},
doi = {10.1.1.72.4580},
isbn = {1002620262},
issn = {00099104},
journal = {Large Scale Kernel Machines},
keywords = {theory},
mendeley-tags = {theory},
number = {1},
pages = {321--360},
pmid = {11359439},
title = {{Scaling Learning Algorithms towards AI}},
year = {2007}
}
@article{Clune2013,
abstract = {A central biological question is how natural organisms are so evolvable (capable of quickly adapting to new environments). A key driver of evolvability is the widespread modularity of biological networks--their organization as functional, sparsely connected subunits--but there is no consensus regarding why modularity itself evolved. Although most hypotheses assume indirect selection for evolvability, here we demonstrate that the ubiquitous, direct selection pressure to reduce the cost of connections between network nodes causes the emergence of modular networks. Computational evolution experiments with selection pressures to maximize network performance and minimize connection costs yield networks that are significantly more modular and more evolvable than control experiments that only select for performance. These results will catalyse research in numerous disciplines, such as neuroscience and genetics, and enhance our ability to harness evolution for engineering purposes.},
archivePrefix = {arXiv},
arxivId = {1207.2743v1},
author = {Clune, J. and Mouret, J.-B. and Lipson, H.},
doi = {10.1098/rspb.2012.2863},
eprint = {1207.2743v1},
isbn = {1471-2954 (Electronic)$\backslash$n0962-8452 (Linking)},
issn = {0962-8452},
journal = {Proceedings of the Royal Society B: Biological Sciences},
keywords = {computational biology,evolution},
number = {1755},
pages = {20122863--20122863},
pmid = {23363632},
title = {{The evolutionary origins of modularity}},
url = {http://rspb.royalsocietypublishing.org/cgi/doi/10.1098/rspb.2012.2863},
volume = {280},
year = {2013}
}
@misc{Rosenblatt1957,
abstract = {First publication about the perceptron},
author = {Rosenblatt, F.},
booktitle = {Report 85, Cornell Aeronautical Laboratory},
doi = {85-460-1},
keywords = {seminal},
mendeley-tags = {seminal},
pages = {460--1},
title = {{The Perceptron - A Perceiving and Recognizing Automaton}},
year = {1957}
}
@article{Yang2017,
abstract = {The Recurrent Neural Networks and their vari-ants have shown promising performances in se-quence modeling tasks such as Natural Language Processing. These models, however, turn out to be impractical and difficult to train when exposed to very high-dimensional inputs due to the large input-to-hidden weight matrix. This may have prevented RNNs' large-scale application in tasks that involve very high input dimensions such as video modeling; current approaches reduce the input dimensions using various feature extrac-tors. To address this challenge, we propose a new, more general and efficient approach by fac-torizing the input-to-hidden weight matrix using Tensor-Train decomposition which is trained si-multaneously with the weights themselves. We test our model on classification tasks using mul-tiple real-world video datasets and achieve com-petitive performances with state-of-the-art mod-els, even though our model architecture is or-ders of magnitude less complex. We believe that the proposed approach provides a novel and fundamental building block for modeling high-dimensional sequential data with RNN architec-tures and opens up many possibilities to transfer the expressive and advanced architectures from other domains such as NLP to modeling high-dimensional sequential data.},
archivePrefix = {arXiv},
arxivId = {1707.01786},
author = {Yang, Yinchong and Krompass, Denis and Tresp, Volker},
eprint = {1707.01786},
keywords = {video classification},
mendeley-tags = {video classification},
title = {{Tensor-Train Recurrent Neural Networks for Video Classification}},
url = {https://arxiv.org/pdf/1707.01786.pdf},
year = {2017}
}
@article{McCulloch1943,
abstract = {Because of the "all-or-none" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {McCulloch, Warren S. and Pitts, Walter},
doi = {10.1007/BF02478259},
eprint = {arXiv:1011.1669v3},
isbn = {0007-4985},
issn = {00074985},
journal = {The Bulletin of Mathematical Biophysics},
keywords = {seminal},
mendeley-tags = {seminal},
number = {4},
pages = {115--133},
pmid = {2185863},
title = {{A logical calculus of the ideas immanent in nervous activity}},
volume = {5},
year = {1943}
}
@article{Rosenblatt1958,
abstract = {To answer the questions of how information about the physical world is sensed, in what form is information remembered, and how does information retained in memory influence recognition and behavior, a theory is developed for a hypothetical nervous system called a perceptron. The theory serves as a bridge between biophysics and psychology. It is possible to predict learning curves from neurological variables and vice versa. The quantitative statistical approach is fruitful in the understanding of the organization of cognitive systems. 18 references.},
author = {Rosenblatt, F.},
doi = {10.1037/h0042519},
isbn = {0033-295X},
issn = {0033-295X},
journal = {Psychological Review},
keywords = {seminal},
mendeley-tags = {seminal},
number = {6},
pages = {386--408},
pmid = {13602029},
title = {{The perceptron: A probabilistic model for information storage and organization in the brain.}},
url = {http://content.apa.org/journals/rev/65/6/386},
volume = {65},
year = {1958}
}
@article{Xu2015a,
author = {Xu, Wenduan and Auli, Michael and Clark, Stephen},
isbn = {9781941643730},
journal = {Acl-2015},
keywords = {sentence classification},
mendeley-tags = {sentence classification},
number = {2014},
pages = {250--255},
title = {{CCG Supertagging with a Recurrent Neural Network}},
year = {2015}
}
@article{Araque2017a,
abstract = {Deep learning techniques for Sentiment Analysis have become very popular. They provide automatic feature extraction and both richer representation capabilities and better performance than traditional feature based techniques (i.e., surface methods). Traditional surface approaches are based on complex manually extracted features, and this extraction process is a fundamental question in feature driven methods. These long-established approaches can yield strong baselines, and their predictive capabilities can be used in conjunction with the arising deep learning methods. In this paper we seek to improve the performance of deep learning techniques integrating them with traditional surface approaches based on manually extracted features. The contributions of this paper are sixfold. First, we develop a deep learning based sentiment classifier using a word embeddings model and a linear machine learning algorithm. This classifier serves as a baseline to compare to subsequent results. Second, we propose two ensemble techniques which aggregate our baseline classifier with other surface classifiers widely used in Sentiment Analysis. Third, we also propose two models for combining both surface and deep features to merge information from several sources. Fourth, we introduce a taxonomy for classifying the different models found in the literature, as well as the ones we propose. Fifth, we conduct several experiments to compare the performance of these models with the deep learning baseline. For this, we use seven public datasets that were extracted from the microblogging and movie reviews domain. Finally, as a result, a statistical study confirms that the performance of these proposed models surpasses that of our original baseline on F1-Score.},
author = {Araque, Oscar and Corcuera-Platas, Ignacio and S{\'{a}}nchez-Rada, J. Fernando and Iglesias, Carlos A.},
doi = {10.1016/j.eswa.2017.02.002},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep learning,Ensemble,Machine learning,Natural language processing,Sentiment analysis,sentiment analysis},
mendeley-tags = {sentiment analysis},
pages = {236--246},
publisher = {Elsevier Ltd},
title = {{Enhancing deep learning sentiment analysis with ensemble techniques in social applications}},
volume = {77},
year = {2017}
}
@article{Leshno1991,
author = {Leshno, Moshe and Schocken, Shimon},
keywords = {theory},
mendeley-tags = {theory},
number = {21},
pages = {1--16},
title = {{Multilayer Feedforward Networks with Non-Polynomial Activation Functions Can Approximate Any Function}},
url = {https://archive.nyu.edu/bitstream/2451/14384/1/IS-91-26.pdf},
year = {1991}
}
@article{Widrob1990,
abstract = {Fundamental developments in feedforward artificial neural networks from the past thirty years are reviewed. The history, origination, operating characteristics, and basic theory of several supervised neural-network training algorithms (including the perceptron rule, the least-mean-square algorithm, three Madaline rules, and the backpropagation technique) are described. The concept underlying these iterative adaptation algorithms is the minimal disturbance principle, which suggests that during training it is advisable to inject new information into a network in a manner that disturbs stored information to the smallest extent possible. The two principal kinds of online rules that have developed for altering the weights of a network are examined for both single-threshold elements and multielement networks. They are error-correction rules, which alter the weights of a network to correct error in the output response to the present input pattern, and gradient rules, which alter the weights of a network during each pattern presentation by gradient descent with the objective of reducing mean-square error (averaged over all training patterns)},
author = {Widrob, B and Widrob, B and Lehr, Michael A. and Lehr, Michael A. and Widrow, Bernard and Lehr, Michael A.},
doi = {10.1109/5.58323},
isbn = {0018-9219},
issn = {15582256},
journal = {Proceedings of the IEEE},
keywords = {review},
mendeley-tags = {review},
number = {9},
pages = {1415--1442},
title = {{30 years of adaptive neural networks: perceptron, Madaline, andbackpropagation}},
volume = {78},
year = {1990}
}
@article{Fritzke1994a,
abstract = {We present a new self-organizing neural network model that has two variants. The first variant performs unsupervised learning and can be used for data visualization, clustering, and vector quantization. The main advantage over existing approaches (e.g., the Kohonen feature map) is the ability of the model to automatically find a suitable network structure and size. This is achieved through a controlled growth process that also includes occasional removal of units. The second variant of the model is a supervised learning method that results from the combination of the above-mentioned self-organizing network with the radial basis function (RBF) approach. In this model it is possible—in contrast to earlier approaches—to perform the positioning of the RBF units and the supervised training of the weights in parallel. Therefore, the current classification error can be used to determine where to insert new RBF units. This leads to small networks that generalize very well. Results on the two-spirals benchmark and a vowel classification problem are presented that are better than any results previously published.},
author = {Fritzke, Bernd},
doi = {10.1016/0893-6080(94)90091-4},
isbn = {0893-6080},
issn = {08936080},
journal = {Neural Networks},
keywords = {--self-organization,1,as proposed by,classification,clustering,data visualization,evolutive,feature map,i n t r,incremental learning,o d u c,pattern,radial basis function,self-organizing neural network models,seminal,t i o n,two-spiral problem},
mendeley-tags = {evolutive,seminal},
number = {9},
pages = {1441--1460},
title = {{Growing cell structures—A self-organizing network for unsupervised and supervised learning}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0893608094900914},
volume = {7},
year = {1994}
}
@article{M??ller2001,
abstract = {This paper provides an introduction to support vector machines, kernel Fisher discriminant analysis, and kernel principal component analysis, as examples for successful kernel-based learning methods. We first give a short background about Vapnik-Chervonenkis theory and kernel feature spaces and then proceed to kernel based learning in supervised and unsupervised scenarios including practical and algorithmic considerations. We illustrate the usefulness of kernel algorithms by discussing applications such as optical character recognition and DNA analysis.},
author = {M??ller, Klaus Robert and Mika, Sebastian and R??tsch, Gunnar and Tsuda, Koji and Sch??lkopf, Bernhard},
doi = {10.1109/72.914517},
isbn = {1045-9227},
issn = {10459227},
journal = {IEEE Transactions on Neural Networks},
keywords = {Boosting,Fisher's discriminant,Kernel PCA,Kernel methods,Mathematical programming machines,Mercer kernels,Principal component analysis (PCA),Single-class classification,Support vector machines (SVMs),seminal},
mendeley-tags = {seminal},
number = {2},
pages = {181--201},
pmid = {18244377},
title = {{An introduction to kernel-based learning algorithms}},
volume = {12},
year = {2001}
}
@article{Deng2016,
abstract = {With the emergence of online social networks, the social network-based recommendation approach is popularly used. The major benefit of this approach is the ability of dealing with the problems with cold-start users. In addition to social networks, user trust information also plays an important role to obtain reliable recommendations. Although matrix factorization (MF) becomes dominant in recommender systems, the recommendation largely relies on the initialization of the user and item latent feature vectors. Aiming at addressing these challenges, we develop a novel trust-based approach for recommendation in social networks. In particular, we attempt to leverage deep learning to determinate the initialization in MF for trust-aware social recommendations and to differentiate the community effect in user's trusted friendships. A two-phase recommendation process is proposed to utilize deep learning in initialization and to synthesize the users' interests and their trusted friends' interests together with the impact of community effect for recommendations. We perform extensive experiments on real-world social network data to demonstrate the accuracy and effectiveness of our proposed approach in comparison with other state-of-the-art methods.},
author = {Deng, Shuiguang and Huang, Longtao and Xu, Guandong and Wu, Xindong and Wu, Zhaohui},
doi = {10.1109/TNNLS.2016.2514368},
isbn = {2162-2388 (Electronic)$\backslash$r2162-237X (Linking)},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {social recommendations},
mendeley-tags = {social recommendations},
number = {February},
pmid = {26915135},
title = {{On Deep Learning for Trust-Aware Recommendations in Social Networks}},
year = {2016}
}
@article{Tang2014,
abstract = {In this paper, we propose a joint segmenta- tion and classification framework for sen- timent analysis. Existing sentiment clas- sification algorithms typically split a sen- tence as a word sequence, which does not effectively handle the inconsistent senti- ment polarity between a phrase and the words it contains, such as "not bad" and "a great deal of ". We address this issue by developing a joint segmentation and classification framework (JSC), which si- multaneously conducts sentence segmen- tation and sentence-level sentiment classi- fication. Specifically, we use a log-linear model to score each segmentation candi- date, and exploit the phrasal information of top-ranked segmentations as features to build the sentiment classifier. A marginal log-likelihood objective function is de- vised for the segmentation model, which is optimized for enhancing the sentiment classification performance. The joint mod- el is trained only based on the annotat- ed sentiment polarity of sentences, with- out any segmentation annotations. Experi- ments on a benchmark Twitter sentimen- t classification dataset in SemEval 2013 show that, our joint model performs com- parably with the state-of-the-art methods.},
author = {Tang, Duyu and Wei, Furu and Qin, Bing and Dong, Li and Liu, Ting and Zhou, Ming},
doi = {10.1109/TASLP.2015.2449071},
isbn = {2329-9290 VO - 23},
issn = {2329-9290},
journal = {Proceedings of the 2014 Conferenve on Empirical Methods in Natural Language Processing (EMNLP)},
keywords = {sentiment analysis},
mendeley-tags = {sentiment analysis},
number = {2002},
pages = {477--487},
title = {{A Joint Segmentation and Classification Framework for Sentiment Analysis}},
volume = {23},
year = {2014}
}
@article{Chen2017a,
abstract = {Different types of sentences express sentiment in very different ways. Traditional sentence-level sentiment classification research focuses on one-technique-fits-all solution or only centers on one special type of sentences. In this paper, we propose a divide-and-conquer approach which first classifies sentences into different types, then performs sentiment analysis separately on sentences from each type. Specifically, we find that sentences tend to be more complex if they contain more sentiment targets. Thus, we propose to first apply a neural network based sequence model to classify opinionated sentences into three types according to the number of targets appeared in a sentence. Each group of sentences is then fed into a one-dimensional convolutional neural network separately for sentiment classification. Our approach has been evaluated on four sentiment classification datasets and compared with a wide range of baselines. Experimental results show that: (1) sentence type classification can improve the performance of sentence-level sentiment analysis; (2) the proposed approach achieves state-of-the-art results on several benchmarking datasets.},
author = {Chen, Tao and Xu, Ruifeng and He, Yulan and Wang, Xuan},
doi = {10.1016/j.eswa.2016.10.065},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Deep neural network,Natural language processing,Sentiment analysis,sentence classification,state of the art},
mendeley-tags = {sentence classification,state of the art},
pages = {221--230},
publisher = {Elsevier Ltd},
title = {{Improving sentiment analysis via sentence type classification using BiLSTM-CRF and CNN}},
url = {http://dx.doi.org/10.1016/j.eswa.2016.10.065},
volume = {72},
year = {2017}
}
@article{Tang2015,
abstract = {— Extreme learning machine (ELM) is an emerging learning algorithm for the generalized single hidden layer feedforward neural networks, of which the hidden node parame-ters are randomly generated and the output weights are analyti-cally computed. However, due to its shallow architecture, feature learning using ELM may not be effective for natural signals (e.g., images/videos), even with a large number of hidden nodes. To address this issue, in this paper, a new ELM-based hierarchical learning framework is proposed for multilayer perceptron. The proposed architecture is divided into two main components: 1) self-taught feature extraction followed by supervised feature classification and 2) they are bridged by random initialized hidden weights. The novelties of this paper are as follows: 1) unsupervised multilayer encoding is conducted for feature extraction, and an ELM-based sparse autoencoder is developed via 1 constraint. By doing so, it achieves more compact and meaningful feature representations than the original ELM; 2) by exploiting the advantages of ELM random feature mapping, the hierarchically encoded outputs are randomly projected before final decision making, which leads to a better generalization with faster learning speed; and 3) unlike the greedy layerwise training of deep learning (DL), the hidden layers of the proposed framework are trained in a forward manner. Once the previous layer is established, the weights of the current layer are fixed without fine-tuning. Therefore, it has much better learning efficiency than the DL. Extensive experiments on various widely used classification data sets show that the proposed algorithm achieves better and faster convergence than the existing state-of-the-art hierarchical learning methods. Furthermore, multiple applications in computer vision further confirm the generality and capability of the proposed learning scheme. Index Terms— Deep learning (DL), deep neural network (DNN), extreme learning machine (ELM), multilayer perceptron (MLP), random feature mapping.},
author = {Tang, Jiexiong and ChenweiDeng and Huang, Guang-Bin},
doi = {10.1109/TNNLS.2015.2424995},
isbn = {2162-2388 (Electronic) 2162-237X (Linking)},
issn = {2162-237X},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {ELM},
mendeley-tags = {ELM},
pages = {1--13},
pmid = {25966483},
title = {{Extreme Learning Machine for Multilayer Perceptron}},
year = {2015}
}
@article{Mao2000,
author = {Mao, K Z and Tan, K C and Ser, W},
journal = {IEEE Transactions on Neural Networks},
keywords = {evolutive,seminal},
mendeley-tags = {evolutive,seminal},
number = {4},
pages = {1009--1016},
title = {{Probabilistic neural network structure determination for pattern classification}},
volume = {11},
year = {2000}
}
@article{Karpathy2014a,
abstract = {Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new...},
archivePrefix = {arXiv},
arxivId = {1412.0767},
author = {Karpathy, Andrej and Toderici, George and Shetty, Sachin and Leung, Tommy and Sukthankar, Rahul and Fei-Fei, Li},
doi = {10.1109/CVPR.2014.223},
eprint = {1412.0767},
isbn = {978-1-4799-5118-5},
issn = {978-1-4799-5118-5},
journal = {2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
keywords = {video classification},
mendeley-tags = {video classification},
pages = {1725--1732},
title = {{Large-Scale Video Classification with Convolutional Neural Networks}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6909619{\%}0Apapers3://publication/doi/10.1109/CVPR.2014.223},
year = {2014}
}
@article{Schmidhuber2015,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks.},
archivePrefix = {arXiv},
arxivId = {1404.7828},
author = {Schmidhuber, J??rgen},
doi = {10.1016/j.neunet.2014.09.003},
eprint = {1404.7828},
isbn = {0893-6080},
issn = {18792782},
journal = {Neural Networks},
keywords = {Deep learning,Evolutionary computation,Reinforcement learning,Supervised learning,Unsupervised learning,review},
mendeley-tags = {review},
pages = {85--117},
pmid = {25462637},
title = {{Deep Learning in neural networks: An overview}},
volume = {61},
year = {2015}
}
@article{Ritchie2003,
abstract = {BACKGROUND: Appropriate definition of neural network architecture prior to data analysis is crucial for successful data mining. This can be challenging when the underlying model of the data is unknown. The goal of this study was to determine whether optimizing neural network architecture using genetic programming as a machine learning strategy would improve the ability of neural networks to model and detect nonlinear interactions among genes in studies of common human diseases. RESULTS: Using simulated data, we show that a genetic programming optimized neural network approach is able to model gene-gene interactions as well as a traditional back propagation neural network. Furthermore, the genetic programming optimized neural network is better than the traditional back propagation neural network approach in terms of predictive ability and power to detect gene-gene interactions when non-functional polymorphisms are present. CONCLUSION: This study suggests that a machine learning strategy for optimizing neural network architecture may be preferable to traditional trial-and-error approaches for the identification and characterization of gene-gene interactions in common, complex human diseases.},
author = {Ritchie, M D and White, B C and Parker, J S and Hahn, L W and Moore, J H},
doi = {10.1186/1471-2105-4-28},
isbn = {1471-2105 (Electronic)$\backslash$r1471-2105 (Linking)},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {*Epistasis,*Models,*Neural Networks (Computer),*Software,Algorithms,Artificial Intelligence,Gene Expression Regulation/*genetics,Genetic,Humans,Molecular Epidemiology/*methods/*trends,Polymorphism,Predictive Value of Tests,Research Design,Single Nucleotide/genetics,Software Validation,evolutive},
mendeley-tags = {evolutive},
pages = {28},
pmid = {12846935},
title = {{Optimization of neural network architecture using genetic programming improves detection and modeling of gene-gene interactions in studies of human diseases}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12846935},
volume = {4},
year = {2003}
}
@article{Shin2016,
abstract = {Remarkable progress has been made in image recognition, primarily due to the availability of large-scale annotated datasets and the revival of deep CNN. CNNs enable learning data-driven, highly representative, layered hierarchical image features from sufficient training data. However, obtaining datasets as comprehensively annotated as ImageNet in the medical imaging domain remains a challenge. There are currently three major techniques that successfully employ CNNs to medical image classification: training the CNN from scratch, using off-the-shelf pre-trained CNN features, and conducting unsupervised CNN pre-training with supervised fine-tuning. Another effective method is transfer learning, i.e., fine-tuning CNN models pre-trained from natural image dataset to medical image tasks. In this paper, we exploit three important, but previously understudied factors of employing deep convolutional neural networks to computer-aided detection problems. We first explore and evaluate different CNN architectures. The studied models contain 5 thousand to 160 million parameters, and vary in numbers of layers. We then evaluate the influence of dataset scale and spatial image context on performance. Finally, we examine when and why transfer learning from pre-trained ImageNet (via fine-tuning) can be useful. We study two specific computer-aided detection (CADe) problems, namely thoraco-abdominal lymph node (LN) detection and interstitial lung disease (ILD) classification. We achieve the state-of-the-art performance on the mediastinal LN detection, with 85{\%} sensitivity at 3 false positive per patient, and report the first five-fold cross-validation classification results on predicting axial CT slices with ILD categories. Our extensive empirical evaluation, CNN model analysis and valuable insights can be extended to the design of high performance CAD systems for other medical imaging tasks.},
archivePrefix = {arXiv},
arxivId = {1602.03409},
author = {Shin, Hoo Chang and Roth, Holger R. and Gao, Mingchen and Lu, Le and Xu, Ziyue and Nogues, Isabella and Yao, Jianhua and Mollura, Daniel and Summers, Ronald M.},
doi = {10.1109/TMI.2016.2528162},
eprint = {1602.03409},
isbn = {0278-0062 VO - 35},
issn = {1558254X},
journal = {IEEE Transactions on Medical Imaging},
keywords = {Biomedical imaging,computer aided diagnosis,image analysis,image classification,machine learning,neural networks,optimization},
mendeley-tags = {image classification,optimization},
number = {5},
pages = {1285--1298},
pmid = {26886976},
title = {{Deep Convolutional Neural Networks for Computer-Aided Detection: CNN Architectures, Dataset Characteristics and Transfer Learning}},
volume = {35},
year = {2016}
}
@article{Bell2015a,
abstract = {It is well known that contextual and multi-scale representations are important for accurate visual recognition. In this paper we present the Inside-Outside Net (ION), an object detector that exploits information both inside and outside the region of interest. Contextual information outside the region of interest is integrated using spatial recurrent neural networks. Inside, we use skip pooling to extract information at multiple scales and levels of abstraction. Through extensive experiments we evaluate the design space and provide readers with an overview of what tricks of the trade are important. ION improves state-of-the-art on PASCAL VOC 2012 object detection from 73.9{\%} to 76.4{\%} mAP. On the new and more challenging MS COCO dataset, we improve state-of-art-the from 19.7{\%} to 33.1{\%} mAP. In the 2015 MS COCO Detection Challenge, our ION model won the Best Student Entry and finished 3rd place overall. As intuition suggests, our detection results provide strong evidence that context and multi-scale representations improve small object detection.},
archivePrefix = {arXiv},
arxivId = {1512.04143},
author = {Bell, Sean and Zitnick, C. Lawrence and Bala, Kavita and Girshick, Ross},
doi = {10.1109/CVPR.2016.314},
eprint = {1512.04143},
isbn = {978-1-4673-8851-1},
issn = {978-953-7619-08-4},
keywords = {object detection},
mendeley-tags = {object detection},
pages = {2874--2883},
pmid = {21803542},
title = {{Inside-Outside Net: Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1512.04143},
year = {2015}
}
@article{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different " thinned " networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
archivePrefix = {arXiv},
arxivId = {1102.4807},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
doi = {10.1214/12-AOS1000},
eprint = {1102.4807},
isbn = {1532-4435},
issn = {15337928},
journal = {Journal of Machine Learning Research},
keywords = {deep learning,model combination,neural networks,regularization,seminal},
mendeley-tags = {seminal},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@article{Goodfellow2014,
abstract = {We propose a new framework for estimating generative models via an adversar- ial process; in which we simultaneously train two models: a generative model G that captures the data distribution; and a discriminative model D that estimates the probability that a sample came from the training data rather thanG. The train- ing procedure for G is to maximize the probability of D making a mistake. This framework corresponds to a minimax two-player game. In the space of arbitrary functions G and D; a unique solution exists; with G recovering the training data distribution andD equal to 1 2 everywhere. In the case where G andD are defined by multilayer perceptrons; the entire system can be trained with backpropagation. There is no need for any Markov chains or unrolled approximate inference net- works during either training or generation of samples. Experiments demonstrate the potential of the framework through qualitative and quantitative evaluation of the generated samples. 1},
archivePrefix = {arXiv},
arxivId = {arXiv:1406.2661v1},
author = {Goodfellow, Ian and Pouget-Abadie, Jean and Mirza, Mehdi and Xu, Bing and Warde-Farley, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
doi = {10.1001/jamainternmed.2016.8245},
eprint = {arXiv:1406.2661v1},
isbn = {1406.2661},
issn = {10495258},
journal = {Advances in Neural Information Processing Systems 27},
keywords = {seminal},
mendeley-tags = {seminal},
pages = {2672--2680},
title = {{Generative Adversarial Nets (NIPS version)}},
url = {http://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf},
year = {2014}
}
@article{Hayes2013,
author = {Hayes, Brian},
number = {April},
title = {{First Links in the Markov Chain Brian}},
year = {2013}
}
@article{Yosinski2014,
abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
archivePrefix = {arXiv},
arxivId = {1411.1792},
author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
eprint = {1411.1792},
issn = {10495258},
keywords = {theory},
mendeley-tags = {theory},
pages = {1--9},
title = {{How transferable are features in deep neural networks?}},
url = {http://arxiv.org/abs/1411.1792},
year = {2014}
}
@article{Graves2014,
abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
archivePrefix = {arXiv},
arxivId = {1410.5401},
author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
doi = {10.3389/neuro.12.006.2007},
eprint = {1410.5401},
isbn = {0028-0836},
issn = {2041-1723},
keywords = {other},
mendeley-tags = {other},
pages = {1--26},
pmid = {18958277},
title = {{Neural Turing Machines}},
url = {http://arxiv.org/abs/1410.5401},
year = {2014}
}
@article{Wu2017,
abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57{\%} error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28{\%} relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC {\&} COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
archivePrefix = {arXiv},
arxivId = {1512.03385},
author = {Wu, Songtao and Zhong, Shenghua and Liu, Yan},
doi = {10.1007/s11042-017-4440-4},
eprint = {1512.03385},
isbn = {978-1-4673-6964-0},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {Convolutional neural networks,Image steganalysis,Residual learning,object detection},
mendeley-tags = {object detection},
pages = {1--17},
pmid = {23554596},
title = {{Deep residual learning for image Recognition}},
year = {2017}
}
@article{LeCun1998a,
abstract = {Multilayer neural networks trained with the back-propagation$\backslash$nalgorithm constitute the best example of a successful gradient based$\backslash$nlearning technique. Given an appropriate network architecture,$\backslash$ngradient-based learning algorithms can be used to synthesize a complex$\backslash$ndecision surface that can classify high-dimensional patterns, such as$\backslash$nhandwritten characters, with minimal preprocessing. This paper reviews$\backslash$nvarious methods applied to handwritten character recognition and$\backslash$ncompares them on a standard handwritten digit recognition task.$\backslash$nConvolutional neural networks, which are specifically designed to deal$\backslash$nwith the variability of 2D shapes, are shown to outperform all other$\backslash$ntechniques. Real-life document recognition systems are composed of$\backslash$nmultiple modules including field extraction, segmentation recognition,$\backslash$nand language modeling. A new learning paradigm, called graph transformer$\backslash$nnetworks (GTN), allows such multimodule systems to be trained globally$\backslash$nusing gradient-based methods so as to minimize an overall performance$\backslash$nmeasure. Two systems for online handwriting recognition are described.$\backslash$nExperiments demonstrate the advantage of global training, and the$\backslash$nflexibility of graph transformer networks. A graph transformer network$\backslash$nfor reading a bank cheque is also described. It uses convolutional$\backslash$nneural network character recognizers combined with global training$\backslash$ntechniques to provide record accuracy on business and personal cheques.$\backslash$nIt is deployed commercially and reads several million cheques per day$\backslash$n},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {LeCun, Yann and Bottou, L??on and Bengio, Yoshua and Haffner, Patrick},
doi = {10.1109/5.726791},
eprint = {1102.0183},
isbn = {0018-9219},
issn = {00189219},
journal = {Proceedings of the IEEE},
keywords = {Convolutional neural networks,Document recognition,Finite state transducers,Gradient-based learning,Graph transformer networks,Machine learning,Neural networks,Optical character recognition (OCR),seminal},
mendeley-tags = {seminal},
number = {11},
pages = {2278--2323},
pmid = {15823584},
title = {{Gradient-based learning applied to document recognition}},
volume = {86},
year = {1998}
}
@article{Santurkar2017a,
abstract = {Traditional image and video compression algorithms rely on hand-crafted encoder/decoder pairs (codecs) that lack adaptability and are agnostic to the data being compressed. Here we describe the concept of generative compression, the compression of data using generative models, and show its potential to produce more accurate and visually pleasing reconstructions at much deeper compression levels for both image and video data. We also demonstrate that generative compression is orders-of-magnitude more resilient to bit error rates (e.g. from noisy wireless channels) than traditional variable-length entropy coding schemes.},
archivePrefix = {arXiv},
arxivId = {1703.01467},
author = {Santurkar, Shibani and Budden, David and Shavit, Nir},
eprint = {1703.01467},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
title = {{Generative Compression}},
url = {http://arxiv.org/abs/1703.01467},
year = {2017}
}
@article{Johnston2017a,
abstract = {We propose a method for lossy image compression based on recurrent, convolutional neural networks that outperforms BPG (4:2:0 ), WebP, JPEG2000, and JPEG as measured by MS-SSIM. We introduce three improvements over previous research that lead to this state-of-the-art result. First, we show that training with a pixel-wise loss weighted by SSIM increases reconstruction quality according to several metrics. Second, we modify the recurrent architecture to improve spatial diffusion, which allows the network to more effectively capture and propagate image information through the network's hidden state. Finally, in addition to lossless entropy coding, we use a spatially adaptive bit allocation algorithm to more efficiently use the limited number of bits to encode visually complex image regions. We evaluate our method on the Kodak and Tecnick image sets and compare against standard codecs as well recently published methods based on deep neural networks.},
archivePrefix = {arXiv},
arxivId = {1703.10114},
author = {Johnston, Nick and Vincent, Damien and Minnen, David and Covell, Michele and Singh, Saurabh and Chinen, Troy and Hwang, Sung Jin and Shor, Joel and Toderici, George},
eprint = {1703.10114},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
title = {{Improved Lossy Image Compression with Priming and Spatially Adaptive Bit Rates for Recurrent Networks}},
url = {http://arxiv.org/abs/1703.10114},
year = {2017}
}
@article{Iizuka2016a,
abstract = {We present a novel technique to automatically colorize grayscale images that combines both global priors and local image features. Based on Convolutional Neural Networks, our deep network fea- tures a fusion layer that allows us to elegantly merge local information dependent on small image patches with global priors computed using the entire image. The entire framework, including the global and local priors as well as the colorization model, is trained in an end-to-end fashion. Furthermore, our architecture can process images of any resolution, unlike most existing approaches based on CNN. We leverage an existing large-scale scene classification data-base to train our model, exploiting the class labels of the dataset to more efficiently and discriminatively learn the global priors. We validate our approach with a user study and compare against the state of the art, where we show significant improvements. Further- more, we demonstrate our method extensively on many different types of images, including black-and-white photography from over a hundred years ago, and show realistic colorizations.},
author = {Iizuka, Satoshi and Simo-Serra, Edgar and Ishikawa, Hiroshi},
doi = {10.1145/2897824.2925974},
isbn = {9781450342797},
issn = {07300301},
journal = {ACM Transactions on Graphics},
keywords = {colorization,computing methodologies,convolutional neural network concepts,image processing,image synthesis,neural net-},
mendeley-tags = {image synthesis},
number = {4},
pages = {1--11},
title = {{Let there be color! :Joint End-to-end Learning of Global and Local Image Priors for Automatic Image Colorization with Simultaneous Classification}},
url = {http://dl.acm.org/citation.cfm?doid=2897824.2925974},
volume = {35},
year = {2016}
}
@article{Pang2017a,
abstract = {Network in Netwrok (NiN) is an effective instance and an important extension of Convolutional Neural Network (CNN) consisting of alternating convolutional layers and pooling layers. Instead of using a linear filter for convolution, NiN utilizes shallow MultiLayer Perceptron (MLP), a nonlinear function, to replace the linear filter. Because of the powerfulness of MLP and {\$} 1\backslashtimes 1 {\$} convolutions in spatial domain, NiN has stronger ability of feature representation and hence results in better recognition rate. However, MLP itself consists of fully connected layers which give rise to a large number of parameters. In this paper, we propose to replace dense shallow MLP with sparse shallow MLP. One or more layers of the sparse shallow MLP are sparely connected in the channel dimension or channel-spatial domain. The proposed method is implemented by applying unshared convolution across the channel dimension and applying shared convolution across the spatial dimension in some computational layers. The proposed method is called CiC. Experimental results on the CIFAR10 dataset, augmented CIFAR10 dataset, and CIFAR100 dataset demonstrate the effectiveness of the proposed CiC method.},
archivePrefix = {arXiv},
arxivId = {1603.06759},
author = {Pang, Yanwei and Sun, Manli and Jiang, Xiaoheng and Li, Xuelong},
doi = {10.1109/TNNLS.2017.2676130},
eprint = {1603.06759},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {image classification},
mendeley-tags = {image classification},
pages = {1--11},
title = {{Convolution in Convolution for Network in Network}},
year = {2017}
}
@article{Theis2015a,
abstract = {Modeling the distribution of natural images is challenging, partly because of strong statistical dependencies which can extend over hundreds of pixels. Recurrent neural networks have been successful in capturing long-range dependencies in a number of problems but only recently have found their way into generative image models. We here introduce a recurrent image model based on multi-dimensional long short-term memory units which are particularly suited for image modeling due to their spatial structure. Our model scales to images of arbitrary size and its likelihood is computationally tractable. We find that it outperforms the state of the art in quantitative comparisons on several image datasets and produces promising results when used for texture synthesis and inpainting.},
archivePrefix = {arXiv},
arxivId = {1506.03478},
author = {Theis, Lucas and Bethge, Matthias},
eprint = {1506.03478},
issn = {10495258},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {1--9},
title = {{Generative Image Modeling Using Spatial LSTMs}},
url = {http://arxiv.org/abs/1506.03478},
year = {2015}
}
@article{Jiang1999,
abstract = {Apart from the existing technology on image compression represented by series of JPEG, MPEG and H.26x standards, new technology such as neural networks and genetic algorithms are being developed to explore the future of image coding. Successful applications of neural networks to vector quantization have now become well established, and other aspects of neural network involvement in this area are stepping up to play significant roles in assisting with those traditional technologies. This paper presents an extensive survey on the development of neural networks for image compression which covers three categories: direct image compression by neural networks; neural network implementation of existing techniques, and neural network based technology which provide improvement over traditional algorithms.},
author = {Jiang, J.},
doi = {10.1016/S0923-5965(98)00041-1},
isbn = {0923-5965},
issn = {09235965},
journal = {Signal Processing: Image Communication},
keywords = {image compression and coding,image compression- lossy,neural network},
mendeley-tags = {image compression- lossy},
number = {9},
pages = {737--760},
title = {{Image compression with neural networks - a survey}},
volume = {14},
year = {1999}
}
@article{Wang2015a,
abstract = {Image compression technology is to compress the redundancy between the pixels to reduce the transmission broadband and storage space by using the correlation of the image pixels. Fuzzy neural network effectively integrates neural network technology and fuzzy technology; combines learning, self- adaptivity, imagination and identity and uses rule-based reasoning and fuzzy information processing in the nodes; thus greatly improving the transparency of fuzzy neural network. This paper mainly investigates the applications of fuzzy neural network in image compression and realizes the image compression and reconstruction of fuzzy neural network. It is demonstrated in the simulation experiment that the image compression algorithm based on fuzzy neural network has significant advantages in training speed, compression quality and robustness.},
author = {Wang, Bo and Gao, Yubin},
doi = {10.12928/telkomnika.v13i1.1270},
issn = {2302-9293},
journal = {TELKOMNIKA (Telecommunication Computing Electronics and Control)},
keywords = {fuzzy theory,image compression,image compression- lossy,neural network},
mendeley-tags = {image compression- lossy},
number = {1},
pages = {137},
title = {{An Image Compression Scheme Based on Fuzzy Neural Network}},
url = {http://journal.uad.ac.id/index.php/TELKOMNIKA/article/view/1270},
volume = {13},
year = {2015}
}
@article{Gregor2016a,
abstract = {We introduce a simple recurrent variational auto-encoder architecture that significantly improves image modeling. The system represents the state-of-the-art in latent variable models for both the ImageNet and Omniglot datasets. We show that it naturally separates global conceptual information from lower level details, thus addressing one of the fundamentally desired properties of unsupervised learning. Furthermore, the possibility of restricting ourselves to storing only global information about an image allows us to achieve high quality 'conceptual compression'.},
archivePrefix = {arXiv},
arxivId = {1604.08772},
author = {Gregor, Karol and Besse, Frederic and Rezende, Danilo Jimenez and Danihelka, Ivo and Wierstra, Daan},
eprint = {1604.08772},
issn = {10495258},
keywords = {image compression - conceptual,state of the art},
mendeley-tags = {image compression - conceptual,state of the art},
number = {Nips},
title = {{Towards Conceptual Compression}},
url = {http://arxiv.org/abs/1604.08772},
year = {2016}
}
@article{Toderici2016a,
abstract = {This paper presents a set of full-resolution lossy image compression methods based on neural networks. Each of the architectures we describe can provide variable compression rates during deployment without requiring retraining of the network: each network need only be trained once. All of our architectures consist of a recurrent neural network (RNN)-based encoder and decoder, a binarizer, and a neural network for entropy coding. We compare RNN types (LSTM, associative LSTM) and introduce a new hybrid of GRU and ResNet. We also study "one-shot" versus additive reconstruction architectures and introduce a new scaled-additive framework. We compare to previous work, showing improvements of 4.3{\%}-8.8{\%} AUC (area under the rate-distortion curve), depending on the perceptual metric used. As far as we know, this is the first neural network architecture that is able to outperform JPEG at image compression across most bitrates on the rate-distortion curve on the Kodak dataset images, with and without the aid of entropy coding.},
archivePrefix = {arXiv},
arxivId = {1608.05148},
author = {Toderici, George and Vincent, Damien and Johnston, Nick and Hwang, Sung Jin and Minnen, David and Shor, Joel and Covell, Michele},
doi = {10.4135/9781412985277},
eprint = {1608.05148},
isbn = {9780761914402},
issn = {08936080},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
pmid = {21655600},
title = {{Full Resolution Image Compression with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1608.05148},
year = {2016}
}
@article{Risi2017,
abstract = {This paper surveys research on applying neuroevolution (NE) to games. In neuroevolution, artificial neural networks are trained through evolutionary algorithms, taking inspiration from the way biological brains evolved. We analyse the application of NE in games along five different axes, which are the role NE is chosen to play in a game, the different types of neural networks used, the way these networks are evolved, how the fitness is determined and what type of input the network receives. The article also highlights important open research challenges in the field.},
archivePrefix = {arXiv},
arxivId = {1410.7326},
author = {Risi, Sebastian and Togelius, Julian},
doi = {10.1109/TCIAIG.2015.2494596},
eprint = {1410.7326},
isbn = {1943-068X VO  - PP},
issn = {1943068X},
journal = {IEEE Transactions on Computational Intelligence and AI in Games},
keywords = {Evolutionary algorithms,evolutive,games,neural networks,neuroevolution,review},
mendeley-tags = {evolutive,games,review},
number = {1},
pages = {25--41},
title = {{Neuroevolution in Games: State of the Art and Open Challenges}},
volume = {9},
year = {2017}
}
@article{Toderici2015a,
abstract = {A large fraction of Internet traffic is now driven by requests from mobile devices with relatively small screens and often stringent bandwidth requirements. Due to these factors, it has become the norm for modern graphics-heavy websites to transmit low-resolution, low-bytecount image previews (thumbnails) as part of the initial page load process to improve apparent page responsiveness. Increasing thumbnail compression beyond the capabilities of existing codecs is therefore a current research focus, as any byte savings will significantly enhance the experience of mobile device users. Toward this end, we propose a general framework for variable-rate image compression and a novel architecture based on convolutional and deconvolutional LSTM recurrent networks. Our models address the main issues that have prevented autoencoder neural networks from competing with existing image compression algorithms: (1) our networks only need to be trained once (not per-image), regardless of input image dimensions and the desired compression rate; (2) our networks are progressive, meaning that the more bits are sent, the more accurate the image reconstruction; and (3) the proposed architecture is at least as efficient as a standard purpose-trained autoencoder for a given number of bits. On a large-scale benchmark of 32{\$}\backslashtimes{\$}32 thumbnails, our LSTM-based approaches provide better visual quality than (headerless) JPEG, JPEG2000 and WebP, with a storage size that is reduced by 10{\%} or more.},
archivePrefix = {arXiv},
arxivId = {1511.06085},
author = {Toderici, George and O'Malley, Sean M. and Hwang, Sung Jin and Vincent, Damien and Minnen, David and Baluja, Shumeet and Covell, Michele and Sukthankar, Rahul},
eprint = {1511.06085},
keywords = {image compression- lossy},
mendeley-tags = {image compression- lossy},
pages = {1--12},
title = {{Variable Rate Image Compression with Recurrent Neural Networks}},
url = {http://arxiv.org/abs/1511.06085},
year = {2015}
}
@article{Balle2016,
abstract = {We describe an image compression method, consisting of a nonlinear analysis transformation, a uniform quantizer, and a nonlinear synthesis transformation. The transforms are constructed in three successive stages of convolutional linear filters and nonlinear activation functions. Unlike most convolutional neural networks, the joint nonlinearity is chosen to implement a form of local gain control, inspired by those used to model biological neurons. Using a variant of stochastic gradient descent, we jointly optimize the entire model for rate-distortion performance over a database of training images, introducing a continuous proxy for the discontinuous loss function arising from the quantizer. Under certain conditions, the relaxed loss function may be interpreted as the log likelihood of a generative model, as implemented by a variational autoencoder. Unlike these models, however, the compression model must operate at any given point along the rate-distortion curve, as specified by a trade-off parameter. Across an independent set of test images, we find that the optimized method generally exhibits better rate-distortion performance than the standard JPEG and JPEG 2000 compression methods. More importantly, we observe a dramatic improvement in visual quality for all images at all bit rates, which is supported by objective quality estimates using MS-SSIM.},
archivePrefix = {arXiv},
arxivId = {1611.01704},
author = {Ball{\'{e}}, Johannes and Laparra, Valero and Simoncelli, Eero P.},
doi = {10.1016/S0197-3975(03)00059-6},
eprint = {1611.01704},
isbn = {0197-3975},
issn = {01973975},
keywords = {image compression},
mendeley-tags = {image compression},
pmid = {16508805},
title = {{End-to-end Optimized Image Compression}},
url = {http://arxiv.org/abs/1611.01704},
year = {2016}
}
@article{Egmont-Petersen2002,
abstract = {We review more than 200 applications of neural networks in image processing and discuss the present and possible future role of neural networks, especially feed-forward neural networks, Kohonen feature maps and Hopfield neural networks. The various applications are categorised into a novel two-dimensional taxonomy for image processing algorithms. One dimension specifies the type of task performed by the algorithm: preprocessing, data reduction/feature extraction, segmentation, object recognition, image understanding and optimisation. The other dimension captures the abstraction level of the input data processed by the algorithm: pixel-level, local feature-level, structure-level, object-level, object-set-level and scene characterisation. Each of the six types of tasks poses specific constraints to a neural-based approach. These specific conditions are discussed in detail. A synthesis is made of unresolved problems related to the application of pattern recognition techniques in image processing and specifically to the application of neural networks. Finally, we present an outlook into the future application of neural networks and relate them to novel developments. ?? 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Egmont-Petersen, M. and {De Ridder}, D. and Handels, H.},
doi = {10.1016/S0031-3203(01)00178-9},
isbn = {0031-3203},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Digital image processing,Feature extraction,Image compression,Image understanding,Invariant pattern recognition,Neural networks,Object recognition,Optimization,Preprocessing,Segmentation,image classification,object detection,review},
mendeley-tags = {image classification,object detection,review},
number = {10},
pages = {2279--2301},
title = {{Image processing with neural networks- A review}},
volume = {35},
year = {2002}
}
@article{Huang2016,
author = {Huang, Ke Kun and Dai, Dao Qing and Ren, Chuan Xian and Lai, Zhao Rong},
doi = {10.1109/TNNLS.2016.2522431},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {image classification},
mendeley-tags = {image classification},
number = {March},
title = {{Learning Kernel Extended Dictionary for Face Recognition}},
year = {2016}
}
@article{Kulkarni2015a,
abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
archivePrefix = {arXiv},
arxivId = {1503.03167},
author = {Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
doi = {10.1063/1.4914407},
eprint = {1503.03167},
issn = {10897550},
keywords = {image synthesis,tesis},
mendeley-tags = {image synthesis,tesis},
pages = {1--10},
title = {{Deep Convolutional Inverse Graphics Network}},
url = {http://arxiv.org/abs/1503.03167},
year = {2015}
}
@article{Krishna2016,
author = {Krishna, Tushar and Emer, Joel and Sze, Vivienne and Conference, International Solid-state Circuits and Francisco, San and Chen, Yu-hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne},
keywords = {hardware},
mendeley-tags = {hardware},
title = {{Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks The MIT Faculty has made this article openly available . Please share Citation " Eyeriss : An Energy-Efficient Reconfigurable Accelerator for Publisher Version}},
year = {2016}
}
@article{Silver2016,
abstract = {The game of Go has long been viewed as the most challenging of classic games for artificial intelligence owing to its enormous search space and the difficulty of evaluating board positions and moves. Here we introduce a new approach to computer Go that uses ‘value networks' to evaluate board positions and ‘policy networks' to select moves. These deep neural networks are trained by a novel combination of supervised learning from human expert games, and reinforcement learning from games of self-play. Without any lookahead search, the neural networks play Go at the level of state-of-the-art Monte Carlo tree search programs that simulate thousands of random games of self-play. We also introduce a new search algorithm that combines Monte Carlo simulation with value and policy networks. Using this search algorithm, our program AlphaGo achieved a 99.8{\%} winning rate against other Go programs, and defeated the human European Go champion by 5 games to 0. This is the first time that a computer program has defeated a human professional player in the full-sized game of Go, a feat previously thought to be at least a decade away.},
archivePrefix = {arXiv},
arxivId = {1610.00633},
author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
doi = {10.1038/nature16961},
eprint = {1610.00633},
isbn = {1476-4687 (Electronic)$\backslash$r0028-0836 (Linking)},
issn = {0028-0836},
journal = {Nature},
keywords = {games},
mendeley-tags = {games},
number = {7587},
pages = {484--489},
pmid = {26819042},
publisher = {Nature Publishing Group},
title = {{Mastering the game of Go with deep neural networks and tree search}},
url = {http://www.nature.com/doifinder/10.1038/nature16961},
volume = {529},
year = {2016}
}
@article{Hayashi2017,
abstract = {Today's economy, production activity, and our life are sustained by social and technological network infrastructures, while new threats of network attacks by destructing loops have been found recently in network science. We inversely take into account the weakness, and propose a new design principle for incrementally growing robust networks. The networks are self-organized by enhancing interwoven long loops. In particular, we consider the range-limited approximation of linking by intermediations in a few hops, and show the strong robustness in the growth without degrading efficiency of paths. Moreover, we demonstrate that the tolerance of connectivity is reformable even from extremely vulnerable real networks according to our proposed growing process with some investment. These results may indicate a prospective direction to the future growth of our network infrastructures.},
archivePrefix = {arXiv},
arxivId = {1706.03910},
author = {Hayashi, Yukio},
eprint = {1706.03910},
keywords = {coexistence of efficiency and,evolutive,interwoven,long-distance relations,loops,onion-like structure,robustness,unselfish self-organization},
mendeley-tags = {evolutive},
title = {{A new design principle of robust onion-like networks self-organized in growth}},
url = {http://arxiv.org/abs/1706.03910},
year = {2017}
}
@article{Khorrami2016a,
abstract = {We consider the task of dimensional emotion recognition on video data using deep learning. While several previous methods have shown the benefits of training temporal neural network models such as recurrent neural networks (RNNs) on hand-crafted features, few works have considered combining convolutional neural networks (CNNs) with RNNs. In this work, we present a system that performs emotion recognition on video data using both CNNs and RNNs, and we also analyze how much each neural network component contributes to the system's overall performance. We present our findings on videos from the Audio/Visual+Emotion Challenge (AV+EC2015). In our experiments, we analyze the effects of several hyperparameters on overall performance while also achieving superior performance to the baseline and other competing methods.},
archivePrefix = {arXiv},
arxivId = {1602.07377},
author = {Khorrami, Pooya and Paine, Tom Le and Brady, Kevin and Dagli, Charlie and Huang, Thomas S.},
eprint = {1602.07377},
keywords = {sentiment analysis,video classification},
mendeley-tags = {sentiment analysis,video classification},
title = {{How Deep Neural Networks Can Improve Emotion Recognition on Video Data}},
url = {http://arxiv.org/abs/1602.07377},
year = {2016}
}
@article{Hausknecht2017,
abstract = {This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional-integral-derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator's inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.},
author = {Hausknecht, Matthew and Li, Wen Ke and Mauk, Michael and Stone, Peter},
doi = {10.1109/TNNLS.2015.2512838},
isbn = {8750141007},
issn = {21622388},
journal = {IEEE Transactions on Neural Networks and Learning Systems},
keywords = {Cerebellar pattern recognition,MNIST handwritten digit recognition,Robot balance,cerebellum,health,inverted pendulum balancing (cart-pole),proportional-integral-derivative (PID) control},
mendeley-tags = {health},
number = {3},
pages = {510--522},
pmid = {26829807},
title = {{Machine Learning Capabilities of a Simulated Cerebellum}},
volume = {28},
year = {2017}
}
@article{Bennasar2015,
abstract = {Feature selection is used in many application areas relevant to expert and intelligent systems, such as data mining and machine learning, image processing, anomaly detection, bioinformatics and natural language processing. Feature selection based on information theory is a popular approach due its computational efficiency, scalability in terms of the dataset dimensionality, and independence from the classifier. Common drawbacks of this approach are the lack of information about the interaction between the features and the classifier, and the selection of redundant and irrelevant features. The latter is due to the limitations of the employed goal functions leading to overestimation of the feature significance. To address this problem, this article introduces two new nonlinear feature selection methods, namely Joint Mutual Information Maximisation (JMIM) and Normalised Joint Mutual Information Maximisation (NJMIM); both these methods use mutual information and the 'maximum of the minimum' criterion, which alleviates the problem of overestimation of the feature significance as demonstrated both theoretically and experimentally. The proposed methods are compared using eleven publically available datasets with five competing methods. The results demonstrate that the JMIM method outperforms the other methods on most tested public datasets, reducing the relative average classification error by almost 6{\%} in comparison to the next best performing method. The statistical significance of the results is confirmed by the ANOVA test. Moreover, this method produces the best trade-off between accuracy and stability.},
author = {Bennasar, Mohamed and Hicks, Yulia and Setchi, Rossitza},
doi = {10.1016/j.eswa.2015.07.007},
isbn = {0957-4174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classification,Conditional mutual information,Dimensionality reduction,Feature selection,Feature selection stability,Joint mutual information,Mutual information,Subset feature selection,other},
mendeley-tags = {other},
number = {22},
pages = {8520--8532},
publisher = {Elsevier Ltd.},
title = {{Feature selection using Joint Mutual Information Maximisation}},
url = {http://dx.doi.org/10.1016/j.eswa.2015.07.007},
volume = {42},
year = {2015}
}
@article{Shizhou2016a,
abstract = {In this paper, we choose to learn useful cues from object recognition mechanisms of the human vi-sual cortex, and propose a DCNN performance im-provement method without the need for increasing the network complexity. Inspired by the category-selective property of the neuron population in the IT layer of the human visual cortex, we enforce the neuron responses at the top DCNN layer to be category selective. To achieve this, we propose the Sparse Category-Selective Objective Function (SCSOF) to modulate the neuron outputs of the top DCNN layer. The proposed method is generic and can be applied to any DCNN models. As ex-perimental results show, when applying the pro-posed method to the " Quick " model and NIN models, image classification performances are re-markably improved on four widely used bench-mark datasets: CIFAR-10, CIFAR-100, MNIST and SVHN, which demonstrate the effectiveness of the presented method.},
author = {Shizhou, Zhang and Gong, Yihong and Jinjun, Wang},
issn = {10450823},
journal = {IJCAI International Joint Conference on Artificial Intelligence},
keywords = {Machine Learning,image synthesis},
mendeley-tags = {image synthesis},
pages = {2343--2349},
title = {{Improving DCNN performance with sparse category-selective objective function}},
volume = {2016-Janua},
year = {2016}
}
@article{Park2017a,
author = {Park, Sungheon and Kwak, Nojun},
doi = {10.1007/978-3-319-54184-6_12},
isbn = {9783319541839},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {optimization},
mendeley-tags = {optimization},
pages = {189--204},
title = {{Analysis on the dropout effect in convolutional neural networks}},
volume = {10112 LNCS},
year = {2017}
}
@article{Phumrattanaprapin2016,
author = {Phumrattanaprapin, Khanittha;Punyaphol Horata},
keywords = {ELM,chine,hierarchical extreme learning ma-,hierarchical learning,multilayer perceptron},
mendeley-tags = {ELM},
number = {2},
pages = {196--204},
title = {{Extended Hierarchical Extreme Learning Machine with Multilayer Perceptron}},
volume = {10},
year = {2016}
}
@article{Liao2016a,
abstract = {Deep feedforward neural networks with piecewise linear activations are currently producing the state-of-the-art results in several public datasets. The combination of deep learning models and piecewise linear activation functions allows for the estimation of exponentially complex functions with the use of a large number of subnetworks specialized in the classification of similar input examples. During the training process, these subnetworks avoid overfitting with an implicit regularization scheme based on the fact that they must share their parameters with other subnetworks. Using this framework, we have made an empirical observation that can improve even more the performance of such models. We notice that these models assume a balanced initial distribution of data points with respect to the domain of the piecewise linear activation function. If that assumption is violated, then the piecewise linear activation units can degenerate into purely linear activation units, which can result in a significant reduction of their capacity to learn complex functions. Furthermore, as the number of model layers increases, this unbalanced initial distribution makes the model ill-conditioned. Therefore, we propose the introduction of batch normalisation units into deep feedforward neural networks with piecewise linear activations, which drives a more balanced use of these activation units, where each region of the activation function is trained with a relatively large proportion of training samples. Also, this batch normalisation promotes the pre-conditioning of very deep learning models. We show that by introducing maxout and batch normalisation units to the network in network model results in a model that produces classification results that are better than or comparable to the current state of the art in CIFAR-10, CIFAR-100, MNIST, and SVHN datasets.},
archivePrefix = {arXiv},
arxivId = {1508.00330},
author = {Liao, Zhibin and Carneiro, Gustavo},
doi = {10.1109/WACV.2016.7477624},
eprint = {1508.00330},
isbn = {9781509006410},
journal = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
keywords = {image classification,optimization,theory},
mendeley-tags = {image classification,optimization,theory},
title = {{On the importance of normalisation layers in deep learning with piecewise linear activation units}},
year = {2016}
}
@article{Huang2015,
abstract = {Abstract—The emergent machine learning technique - Ex- treme Learning Machines (ELMs) - has become a hot area of research over the past years, which is attributed to the growing research activities and significant contributions made by numerous researchers around the world. Recently, it has come to our attention that a number of misplaced notions and misunderstandings are being dissipated on the relationships between ELM and some earlier works. This paper wishes to clarify that i) ELM theories manage to address the open problem which has puzzled the neural networks, machine learning and neuroscience communities for 60 years: whether hidden nodes / neurons need to be tuned in learning, and proved that in contrast to the common knowledge and conventional neural network learning tenets, hidden nodes / neurons do not need to be iteratively tuned in wide types of neural networks and learning models (Fourier series, biological learning, etc). Unlike ELM theories, none of those earlier works provides theoretical foundations on feedforward neural networks with random hidden nodes; ii) ELM is proposed for both generalized single hidden layer feedfoward network and multi hidden layers feedforward networks; iii) Homogeneous architecture based ELM is proposed for feature learning, clustering, regression and (binary / multi- class) classification. iv) Compared to ELM, SVM and LS-SVM tend to provide suboptimal solutions, and SVM and LS-SVM do not consider feature representations in hidden layers of multi layers of networks either.},
author = {Huang, Guang Bin},
doi = {10.1007/s12559-015-9333-0},
isbn = {1866-9956},
issn = {18669964},
journal = {Cognitive Computation},
keywords = {ELM,Extreme learning machine,Feedforward neural network,QuickNet,Radial basis function network,Random vector functional link,Randomness},
mendeley-tags = {ELM},
number = {3},
pages = {263--278},
publisher = {Springer US},
title = {{What are Extreme Learning Machines? Filling the Gap Between Frank Rosenblatt's Dream and John von Neumann's Puzzle}},
volume = {7},
year = {2015}
}
@article{Sigtia2016,
abstract = {We present a supervised neural network model for polyphonic piano music transcription. The architecture of the proposed model is analogous to speech recognition systems and comprises an acoustic model and a music language model. The acoustic model is a neural network used for estimating the probabilities of pitches in a frame of audio. The language model is a recurrent neural network that models the correlations between pitch combinations over time. The proposed model is general and can be used to transcribe polyphonic music without imposing any constraints on the polyphony. The acoustic and language model predictions are combined using a probabilistic graphical model. Inference over the output variables is performed using the beam search algorithm. We perform two sets of experiments. We investigate various neural network architectures for the acoustic models and also investigate the effect of combining acoustic and music language model predictions using the proposed architecture. We compare performance of the neural network based acoustic models with two popular unsupervised acoustic models. Results show that convolutional neural network acoustic models yields the best performance across all evaluation metrics. We also observe improved performance with the application of the music language models. Finally, we present an efficient variant of beam search that improves performance and reduces run-times by an order of magnitude, making the model suitable for real-time applications.},
archivePrefix = {arXiv},
arxivId = {1508.01774},
author = {Sigtia, Siddharth and Benetos, Emmanouil and DIxon, Simon},
doi = {10.1109/TASLP.2016.2533858},
eprint = {1508.01774},
issn = {23299290},
journal = {IEEE/ACM Transactions on Audio Speech and Language Processing},
keywords = {Automatic music transcription,Deep learning,Music language models,Recurrent neural networks},
number = {5},
pages = {927--939},
title = {{An end-to-end neural network for polyphonic piano music transcription}},
volume = {24},
year = {2016}
}
@article{Southall2016a,
abstract = {Automatic drum transcription (ADT) systems attempt to generate a symbolic music notation for percussive in-struments in audio recordings. Neural networks have al-ready been shown to perform well in fields related to ADT such as source separation and onset detection due to their utilisation of time-series data in classification. We pro-pose the use of neural networks for ADT in order to ex-ploit their ability to capture a complex configuration of fea-tures associated with individual or combined drum classes. In this paper we present a bi-directional recurrent neu-ral network for offline detection of percussive onsets from specified drum classes and a recurrent neural network suit-able for online operation. In both systems, a separate net-work is trained to identify onsets for each drum class under observation—that is, kick drum, snare drum, hi-hats, and combinations thereof. We perform four evaluations utilis-ing the IDMT-SMT-Drums and ENST minus one datasets, which cover solo percussion and polyphonic audio respec-tively. The results demonstrate the effectiveness of the pre-sented methods for solo percussion and a capacity for iden-tifying snare drums, which are historically the most diffi-cult drum class to detect.},
author = {Southall, Carl and Stables, Ryan and Hockman, Jason},
isbn = {978-0-692-75506-8},
journal = {Proceedings of International Society for Music Information Retrieval Conference (ISMIR)},
keywords = {music transcription,tesis},
mendeley-tags = {music transcription,tesis},
title = {{Automatic Drum Transcription Using Bi-Directional Recurrent Neural Networks}},
year = {2016}
}
@article{Floreano2008a,
abstract = {Artificial neural networks are applied to many$\backslash$nreal-world problems, ranging from pattern classification$\backslash$nto robot control. In order to design a neural network for$\backslash$na particular task, the choice of an architecture$\backslash$n(including the choice of a neuron model), and the choice$\backslash$nof a learning algorithm have to be addressed.$\backslash$nEvolutionary search methods can provide an automatic$\backslash$nsolution to these problems. New insights in both$\backslash$nneuroscience and evolutionary biology have led to the$\backslash$ndevelopment of increasingly powerful neuroevolution$\backslash$ntechniques over the last decade. This paper gives an$\backslash$noverview of the most prominent methods for evolving$\backslash$nartificial neural networks with a special focus on recent$\backslash$nadvances in the synthesis of learning architectures.},
author = {Floreano, Dario and D??rr, Peter and Mattiussi, Claudio},
doi = {10.1007/s12065-007-0002-4},
isbn = {1206500700},
issn = {18645909},
journal = {Evolutionary Intelligence},
keywords = {Evolution,Learning,Neural networks,evolutive,review},
mendeley-tags = {evolutive,review},
number = {1},
pages = {47--62},
title = {{Neuroevolution: From architectures to learning}},
volume = {1},
year = {2008}
}
@article{Deng2015,
abstract = {Extreme learning machine (ELM), as a new learning framework, draws increasing attractions in the areas of large-scale computing, high-speed signal processing, artificial intelligence, and so on. ELM aims to break the barriers between the conventional artificial learning techniques and biological learning mechanism and represents a suite of machine learning techniques in which hidden neurons need not to be tuned. ELM theories and algorithms argue that "random hidden neurons" capture the essence of some brain learning mechanisms as well as the intuitive sense that the efficiency of brain learning need not rely on computing power of neurons. Thus, compared with traditional neural networks and support vector machine, ELM offers significant advantages such as fast learning speed, ease of implementation, and minimal human intervention. Due to its remarkable generalization performance and implementation efficiency, ELM has been applied in various applications. In this paper, we first provide an overview of newly derived ELM theories and approaches. On the other hand, with the ongoing development of multilayer feature representation, some new trends on ELM-based hierarchical learning are discussed. Moreover, we also present several interesting ELM applications to showcase the practical advances on this subject.},
author = {Deng, ChenWei and Huang, GuangBin and Xu, Jia and Tang, JieXiong},
doi = {10.1007/s11432-014-5269-3},
issn = {1674-733X},
journal = {Science China Information Sciences},
keywords = {ELM,review},
mendeley-tags = {ELM,review},
number = {2},
pages = {1--16},
title = {{Extreme learning machines: new trends and applications}},
url = {http://link.springer.com/10.1007/s11432-014-5269-3},
volume = {58},
year = {2015}
}
@article{KenOHanlonandMarkD.PlumbleyQueenMaryUniversityofLondon2014,
author = {{Ken O ' Hanlon and Mark D . Plumbley Queen Mary University of London}},
number = {May},
pages = {3136--3140},
title = {{POLYPHONIC PIANO TRANSCRIPTION USING NON-NEGATIVE MATRIX FACTORISATION WITH GROUP SPARSITY}},
volume = {1},
year = {2014}
}
@article{Tuohy2006,
abstract = {In this paper we describe a technique for creating guitar tablature using a neural network. Training data was parsed from an online repository of human-created tablatures. The contents of both the input layer and the set of training data have been optimized through genetic search in order to maximize the accuracy of the network. The output of the network is im- proved upon with a local heuristic hill-climber (HC). We implement this model in an existing system for generating guitar arrangements via genetic algorithm (GA). When compared to the original system for generating tablature, we note modest improvement in tablature quality and drastic improvements in execution time.},
author = {Tuohy, D R and Potter, W D},
isbn = {{\%}(},
journal = {Procs. of the International Computer Music Conference (ICMC06)},
keywords = {GA,fingering{\_}prediction,guitar,music transcription},
mendeley-tags = {GA,music transcription},
number = {January 2006},
pages = {576--579},
title = {{An Evolved Neural Network/HC Hybrid for Tablature Creation in GA- based Guitar Arranging}},
url = {http://quod.lib.umich.edu/cgi/p/pod/dod-idx?c=icmc;idno=bbp2372.2006.119},
year = {2006}
}
@article{Frans2017a,
abstract = {When creating digital art, coloring and shading are often time consuming tasks that follow the same general patterns. A solution to automatically colorize raw line art would have many practical applications. We propose a setup utilizing two networks in tandem: a color prediction network based only on outlines, and a shading network conditioned on both outlines and a color scheme. We present processing methods to limit information passed in the color scheme, improving generalization. Finally, we demonstrate natural-looking results when colorizing outlines from scratch, as well as from a messy, user-defined color scheme.},
archivePrefix = {arXiv},
arxivId = {1704.08834},
author = {Frans, Kevin},
eprint = {1704.08834},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
title = {{Outline Colorization through Tandem Adversarial Networks}},
url = {http://arxiv.org/abs/1704.08834},
year = {2017}
}
@article{Larsson2016b,
abstract = {We develop a fully automatic image colorization system. Our approach leverages recent advances in deep networks, exploiting both low-level and semantic representations. As many scene elements naturally appear according to multimodal color distributions, we train our model to predict per-pixel color histograms. This intermediate output can be used to automatically generate a color image, or further manipulated prior to image formation. On both fully and partially automatic colorization tasks, we outperform existing methods. We also explore colorization as a vehicle for self-supervised visual representation learning.},
archivePrefix = {arXiv},
arxivId = {1603.06668},
author = {Larsson, Gustav and Maire, Michael and Shakhnarovich, Gregory},
doi = {10.1007/978-3-319-46493-0_35},
eprint = {1603.06668},
isbn = {9783319464923},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {image synthesis},
mendeley-tags = {image synthesis},
pages = {577--593},
title = {{Learning representations for automatic colorization}},
volume = {9908 LNCS},
year = {2016}
}
@article{Leite2016,
abstract = {The systematic innovation has been considered one of the most important practices in the business environment, increasingly uncertain and changeable. Understanding how changes they are occurring in society motivated by advances in information technology have impacted the innovation process, through the lens of startups. it is essential for the economic growth of a country, since most new products come these processes. The aim of this study is to develop a new model of development products in startups and small organizations seeking to develop physical products from innovations based on creativity.},
author = {Leite, M.L.G. and Purcidonio, P.M. and Tarjano, C.},
issn = {07981015},
journal = {Espacios},
keywords = {Innovation,Product development,Startups},
number = {7},
title = {{The process of product development for startups based on creative innovation}},
volume = {37},
year = {2016}
}
@article{Weinman2014,
author = {Weinman, Jaime J},
file = {::},
number = {May 2012},
pages = {261277},
title = {{Top of page}},
year = {2014}
}
@article{Estelles-Arolas2012,
abstract = {Crowdsourcing is a relatively recent concept that encompasses many practices. This diversity leads to the blurring of the limits of crowdsourcing that may be identified virtually with any type of Internet-based collaborative activity, such as co-creation or user innovation. Varying definitions of crowdsourcing exist and therefore, some authors present certain specific examples of crowdsourcing as paradigmatic, while others present the same examples as the opposite. In this paper, existing definitions of crowdsourcing are analyzed to extract common elements and to establish the basic characteristics of any crowdsourcing initiative. Based on these existing definitions, an exhaustive and consistent definition for crowdsourcing is presented and contrasted in eleven cases.},
author = {Estelles-Arolas, E. and Gonzalez-Ladron-de-Guevara, F.},
doi = {10.1177/016555150000000},
isbn = {0165551500000},
journal = {Journal of Information Science},
keywords = {crowdsourcing,definition,innovation},
number = {2},
pages = {189--200},
title = {{Towards an integrated crowdsourcing definition}},
volume = {38},
year = {2012}
}
@article{Berthon2007,
abstract = {Creative consumers (defined as customers who adapt, modify, or transform a proprietary offering) represent an intriguing paradox for business. On one hand, they can signify a black hole for future revenue, with breach of copyright and intellectual property. On the other hand, they represent a gold mine of ideas and business opportunities. Central to business is the need to create and capture value, and creative consumers demand a shift in the mindsets and business models of how firms accomplish both. Based upon their attitude and action toward customer innovation, we develop a typology of firms' stances toward creative consumers. We then consider the implications of the stances model for corporate strategy and examine a three-step approach to dealing with creative consumers: awareness, analysis, and response. ?? 2006 Kelley School of Business, Indiana University.},
author = {Berthon, Pierre R. and Pitt, Leyland F. and McCarthy, Ian and Kates, Steven M.},
doi = {10.1016/j.bushor.2006.05.005},
issn = {00076813},
journal = {Business Horizons},
keywords = {Creative customers,Diagnostics,Firm stance,Strategic response},
month = {jan},
number = {1},
pages = {39--47},
title = {{When customers get clever: Managerial approaches to dealing with creative consumers}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0007681306000796},
volume = {50},
year = {2007}
}
@article{Angell,
author = {Angell, Ian},
title = {{Winners and Losers in the Information Age}}
}
@article{Index2013,
author = {Index, Industrial},
pages = {2013--2014},
title = {{Technology Is Wiping Out Companies Faster than Ever}},
year = {2013}
}
@article{Pisano2008,
abstract = {Nowadays, virtually no companies innovate alone. Firms team up with a variety of partners, in a wide number of ways, to create new technologies, products, and services. But what is the best way to leverage the power of outsiders? To help executives answer that question, Pisano, of Harvard Business School, and Verganti, of Politecnico di Milano, developed a simple framework focused on two questions: Given your strategy, how open or closed should your network of collaborators be? And who should decide which problems to tackle and which solutions to adopt? There are four basic modes of collaboration, say the authors. An elite circle is a closed network with a hierarchical governance: One company selects the participants, defines the problem, and chooses the solution. For instance, Alessi, an Italian home-products company, invited 200 outside experts in postmodern architecture to contribute ideas for new home-product designs. An innovation mall is hierarchical but open: Anyone can post a problem or propose solutions in it, but the company posting the problem chooses the solution. An example is InnoCentive.com, an eBay-like site where companies post scientific challenges. An innovation community is open and decentralized: Anyone can propose problems, offer solutions, and decide which ideas to use -- as happens in the Linux open-source software community. A consortium is a private group of participants that operate as equals and jointly select problems, decide how to conduct work, and choose solutions. IBM has set up a number of consortia with other companies to develop next-generation semiconductor technologies. No one approach is superior; each involves strategic trade-offs. When choosing among modes, firms must weigh their advantages and challenges, and assess which will work best with their strategy, capabilities, structure, and assets.  INSETS: The Four Ways to Collaborate;How to Choose the Best Mode of Collaboration.},
author = {Pisano, Gary P. and Verganti, Roberto},
journal = {Harvard Business Review},
number = {12},
pages = {78--86},
title = {{Which Kind of Collaboration Is Right for You?}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=buh{\&}AN=35387060{\&}site=ehost-live{\%}5Cnhttp://content.ebscohost.com/ContentServer.asp?T=P{\&}P=AN{\&}K=35387060{\&}S=R{\&}D=buh{\&}EbscoContent=dGJyMMvl7ESep7Q4wtvhOLCmr0uep65Ssqu4TLGWxWXS{\&}ContentCustomer=dGJyMPGotk{\%}2BxrLZQuePfgeyx44Dt6fIA},
volume = {86},
year = {2008}
}
@article{Piller2006,
abstract = {Research has shown that many innovations originate not in the manufacturer but the user domain. Internet-based toolkits for idea competitions (TIC) are a novel way for manufacturers to access innovative ideas and solutions from users. Idea competitions build on the nature of competition as a means to encourage users to participate at an open innovation process, to inspire their creativity, and to increase the quality of the submissions. When the contest ends, submissions are evaluated by an expert panel. Users whose submissions score highest receive an award from the manufacturer, which is often granted in exchange for the right to exploit the solution in its domain. Following the idea of evolutionary prototyping, we developed a TIC in cooperation with a manufacturer of sports goods. The TIC was launched as a pilot in one of the company's markets. Submissions were evaluated using the consensual assessment technique. The evaluation of this study provides suggestions for further research, but also implications for managers willing to explore TIC in their organization.},
author = {Piller, Frank T. and Walcher, Dominik},
journal = {R{\&}D Management},
number = {3},
pages = {307--318},
title = {{Toolkits for idea competitions: a novel method to integrate users in new product development}},
url = {http://doi.wiley.com/10.1111/j.1467-9310.2006.00432.x},
volume = {36},
year = {2006}
}
@article{Poetz2012,
abstract = {Generating ideas for new products used to be the exclusive domain of marketers, engineers, and/or designers. Users have only recently been recognized as an alternative source of new product ideas. Whereas some have attributed great potential to outsourcing idea generation to the "crowd" of users ("crowdsourcing"), others have clearly been more skeptical. The authors join this debate by presenting a real-world comparison of ideas actually generated by a firm's professionals with those generated by users in the course of an idea generation contest. Both professionals and users provided ideas to solve an effective and relevant problem in the consumer goods market for baby products. Executives from the underlying company evaluated all ideas (blind to their source) in terms of key quality dimensions including novelty, customer benefit, and feasibility. The study reveals that the crowdsourcing process generated user ideas that score significantly higher in terms of novelty and customer benefit, and somewhat lower in terms of feasibility. However, the average values for feasibility—in sharp contrast to novelty and customer benefit—tended to be relatively high overall, meaning that feasibility did not constitute a narrow bottleneck in this study. Even more interestingly, it is found that user ideas are placed more frequently than expected among the very best in terms of novelty and customer benefit. These findings, which are quite counterintuitive from the perspective of classic new product development (NPD) literature, suggest that, at least under certain conditions, crowdsourcing might constitute a promising method to gather user ideas that can complement those of a firm's professionals at the idea generation stage in NPD.},
author = {Poetz, Marion K. and Schreier, Martin},
doi = {10.1111/j.1540-5885.2011.00893.x},
issn = {07376782},
journal = {Journal of Product Innovation Management},
month = {mar},
number = {2},
pages = {245--256},
title = {{The Value of Crowdsourcing: Can Users Really Compete with Professionals in Generating New Product Ideas?}},
url = {http://doi.wiley.com/10.1111/j.1540-5885.2011.00893.x},
volume = {29},
year = {2012}
}
@article{Ledford2013,
author = {Ledford, B Y Heidi},
file = {::},
title = {{START-UP}},
year = {2013}
}
@article{Dodgson2006,
author = {Dodgson, Mark and Gann, David and Salter, Ammon},
pages = {333--346},
title = {{The role of technology in the shift towards open innovation : the case}},
year = {2006}
}
@article{Maeda,
author = {Maeda, John},
file = {::},
title = {l aw s o f}
}
@article{Edmunds2000a,
abstract = {This paper reviews the literature on the problem of information overload, with particular reference to business organisations. The literature reveals that although the problem of information overload has existed for many years, in recent years the problem has become more widely recognised and experienced. Both perceptions and the actual effects of information overload have been exacerbated by the rapid advances made in information and communication technology, although it is not clear cut as to whether the Internet has worsened or improved the situation. A theme stressed in the literature is the paradoxical situation that, although there is an abundance of information available, it is often difficult to obtain useful, relevant information when it is needed. Some solutions put forward to reduce information overload are: a reduction in the duplication of information found in the professional literature; the adoption of personal information management strategies, together with the integration of software solutions such as push technology and intelligent agents; and the provision of value-added information (filtered by software or information specialists). An emphasis is placed on technology as a tool and not the driver, while increased information literacy may provide the key to reducing information overload.},
author = {Edmunds, Angela and Morris, Anne},
journal = {International Journal of Information Management},
keywords = {infoglut,information fatigue syndrome,information overload},
number = {1},
pages = {17--28},
title = {{The problem of information overload in business organisations: a review of the literature}},
volume = {20},
year = {2000}
}
@article{Alberts1997,
author = {Alberts, David S and Papp, Daniel S},
title = {{The Information Age : An Anthology on Its Impact and Consequences Table of Contents}},
year = {1997}
}
@article{DIppolito2014a,
abstract = {Scholars dedicated increasing attention towards appreciating how design has changed individuals׳ perception of new products, firms׳ understanding and formulation of strategy, or other relevant actors׳ approach to innovation and technology management. By emphasising the importance of design for the definition of consumers׳ needs, the restructuring of firms׳ organisational structures and strategies, and the evolution of firms׳ value creation processes, this review paper identifies relevant research gaps and questions that would benefit from future scholarly attention. In particular, it is suggested that such effort should address the analysis of how design consumption can help better comprehend consumers׳ needs; what are the implications of design thinking on the skill sets of design professionals; the organisational structure of firms, including the reconfiguration of other business functions, and their strategy; and whether and how design thinking can shape firms׳ value creation processes and contribute to the formalisation of design tasks.},
author = {D'Ippolito, Beatrice},
doi = {10.1016/j.technovation.2014.01.007},
issn = {01664972},
journal = {Technovation},
keywords = {Literature Review Process},
month = {feb},
pages = {1--15},
publisher = {Elsevier},
title = {{The importance of design for firms׳ competitiveness: A review of the literature}},
url = {http://www.sciencedirect.com/science/article/pii/S016649721400008X},
year = {2014}
}
@book{Norman,
author = {Norman, Don},
isbn = {9780465050659},
title = {{of EVERYDAY THINGS THE DESIGN OF EVERYDAY}}
}
@article{Fini2010a,
abstract = {Research and public policy on academic entrepreneurship are largely based on the assumption that faculty members start businesses to commercialize inventions that have been disclosed to university administrators and have been patented. In this paper, we analyze a sample of 11,572 professors and find that much academic entrepreneurship occurs outside the university intellectual property system. Specifically, about 2/3 of businesses started by academics are not based on disclosed and patented inventions. Moreover, we show that individual characteristics, departmental and organizational affiliations, and time allocation of academics that have started business outside the IP system are different from those of academics that have started businesses to exploit disclosed and patented inventions. We discuss the implications for research on and the practice of academic entrepreneurship. ?? 2010 Elsevier B.V. All rights reserved.},
author = {Fini, Riccardo and Lacetera, Nicola and Shane, Scott},
doi = {10.1016/j.respol.2010.05.014},
issn = {00487333},
journal = {Research Policy},
keywords = {Academic entrepreneurship,Business creation,Knowledge transfer},
month = {oct},
number = {8},
pages = {1060--1069},
title = {{Inside or outside the IP system? Business creation in academia}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0048733310001381},
volume = {39},
year = {2010}
}
@article{Lee-mortimer1994,
author = {Lee-mortimer, Andrew},
number = {2},
pages = {31--34},
title = {{Strategic Design}},
volume = {1},
year = {1994}
}
@article{Dewick2002a,
author = {Dewick, Paul and Green, Ken and Miozzo, Marcela},
number = {January},
title = {{Tyndall ˚ Centre and the Environment Technological Change , Industry Structure and the Environment}},
year = {2002}
}
@article{Hutchins2013,
author = {Hutchins, Aaron},
file = {::},
pages = {649206},
title = {{Top of page}},
year = {2013}
}
@article{,
file = {::},
title = {{No Title}}
}
@article{Schumpeter1994,
author = {Schumpeter, Joseph A and Mccraw, Thomas and Mirowski, Phillip},
pages = {1--8},
title = {{Thomas K . McCraw , Cambridge : Harvard University Press , 719 pages ,}},
year = {1994}
}
@article{Sampler1998,
abstract = {We are entering a new competitive age in which the basis of competition is being fundamentally altered through the introduction of advanced information technologies and public communication infrastructures, such as the Internet. In these environments, the nature and locus of competition will radically alter, as information becomes an increasingly important resource. This paper develops ideas around the strategic characteristics of information-information separability and industry concentration, related diversification, and innovation for firms competing in the Information Age.},
author = {Sampler, J.},
journal = {Strategic Management Journal},
keywords = {industry boundary,industry structure,information,information age,information separability},
number = {4},
pages = {343--355},
publisher = {Wiley Online Library},
title = {{Redefining industry structure for the information age}},
url = {http://onlinelibrary.wiley.com/doi/10.1002/(SICI)1097-0266(199804)19:4{\%}3C343::AID-SMJ975{\%}3E3.0.CO;2-G/abstract},
volume = {19},
year = {1998}
}
@article{Wang2011,
abstract = {With the development of computer-aided design (CAD) technology and increasing demands of customized footwear, shoe-lasts are requested to be designed rapidly so as to speed-up the process of footwear manufacturing. Thus, this study presents a CAD system for shoe-last rapid customized design based on the piecewise reconstruction to realize the interactive deformation and separate/global shoe-last form reuse. First, piecewise remodeling method is proposed based on the multi-layer parametric definition and contour curves are extracted from the mesh. Then, five types of proper constraints to support surface manipulation are proposed, and the draft-driven deformation by the contour curve bending can realize the interactive local surface design in free angle of view. Finally, shoe-last styles can be saved and reused globally or separately to share design results between different shoe-lasts. Experimental examples show that customized shoe-lasts can be easily and rapidly generated by adopting the parametric design methods.},
author = {Wang, Jin and Zhang, Haining and Lu, Guodong and Liu, Zheng},
doi = {10.1007/s00170-010-3144-y},
issn = {0268-3768},
journal = {The International Journal of Advanced Manufacturing Technology},
keywords = {deformation,form reuse,interactive,parametric design,shoe-last},
month = {jan},
number = {1-4},
pages = {173--186},
title = {{Rapid parametric design methods for shoe-last customization}},
url = {http://link.springer.com/10.1007/s00170-010-3144-y},
volume = {54},
year = {2011}
}
@article{Moscaritoloa,
author = {Moscaritolo, Autores and Angelamoscaritolopcmagcom, Angela},
title = {{Pebble Smartwatch Sells Out , Collects {\$} 10 Million on Kickstarter}}
}
@article{Agerfalk2008a,
abstract = {This paper presents a psychological contract perspective on the use of the open source development model as a global sourcing strategy -- opensourcing, as the authors termed it -- whereby commercial companies and open source communities collaborate on development of software of commercial interest to the company. Building on previous research on information systems outsourcing, a theoretical framework for exploring the opensourcing phenomenon is derived. The first phase of the research concerned qualitative case studies involving three commercial organizations (IONA Technologies, Philips Medical Systems, and Telefonica) that had "liberated" what had hitherto been proprietary software and sought to grow a global open source community around their product. Overall the key watchwords for opensourcing are openness, trust, tact, professionalism, transparency, and complementariness. The study reveals an ongoing shift from open source software (OSS) as a community of individual developers to OSS as a community of commercial organizations, primarily small to medium-sized enterprises.},
author = {{\AA}gerfalk, P. and Fitzgerald, B.},
journal = {MIS Quarterly},
keywords = {crowdsourcing,global software development,multi-,offshoring,open source,opensourcing,out-,sourcing},
number = {2},
pages = {385},
shorttitle = {OUTSOURCING TO AN UNKNOWN WORKFORCE},
title = {{Outsourcing to an Unknown Workforce: Exploring Opensourcing as a Global Sourcing Strategy}},
volume = {32},
year = {2008}
}
@misc{,
title = {{Poverty and profits in the information age.pdf}}
}
@book{,
isbn = {9781111221294},
title = {{Licensed to : CengageBrain User Licensed to : CengageBrain User}}
}
@article{The,
author = {The, I S},
file = {::},
number = {March 2002},
title = {{DEAD ?}}
}
@article{Noble2011,
abstract = {While the popular understanding of the influence of design is growing, academic research has largely been restricted to considering consumer-level responses to design elements. This paper reviews this past work and proposes a more strategic research agenda for the field, with the potential to explicate linkages between design elements and strategies and outcomes related to innovation and corporate performance.},
author = {Noble, Charles H},
journal = {Journal of Product Innovation Management},
number = {3},
pages = {389--393},
publisher = {Blackwell Publishing Inc},
title = {{On Elevating Strategic Design Research*}},
url = {http://dx.doi.org/10.1111/j.1540-5885.2011.00808.x},
volume = {28},
year = {2011}
}
@misc{,
title = {{Leaps and bounds.pdf}}
}
@article{Bagozzi2006a,
abstract = {We conceptualize participation in Linux user groups (LUGs) in terms of group-referent intentional actions and investigate cognitive (attitudes, perceived behavioral control, identification with the open source movement), affective (positive and negative anticipated emotions), and social (social identity) determinants of participation and its consequences on Linux-related behaviors of users. This survey-based study, conducted with 402 active LUG members representing 191 different LUGs from 23 countries and employing structural equation modeling methodology, supports the proposed model. Furthermore, we find that the Linux user's experience level moderates the extent of the LUG's social influence and its impact on the user's participation. We conclude with a consideration of the managerial and research implications of the study's findings.},
author = {Bagozzi, Richard P. and Dholakia, Utpal M.},
doi = {10.1287/mnsc.1060.0545},
issn = {0025-1909},
journal = {Management Science},
keywords = {2004,accepted by eric von,anticipated emotions,for 3 revisions,hippel and georg von,history,krogh,linux,model of goal-directed behavior,novice versus experienced users,open source software,received september 1,social identity,special issue editors,the authors 4 months,this paper was with,virtual communities,we-intentions},
month = {jul},
number = {7},
pages = {1099--1115},
title = {{Open Source Software User Communities: A Study of Participation in Linux User Groups}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/mnsc.1060.0545},
volume = {52},
year = {2006}
}
@article{,
title = {{Copyright {\textcopyright}2000. All Rights Reserved.}},
year = {2000}
}
@article{Arthur1875,
author = {Arthur, By W Brian},
title = {{Is the Information Revolution Dead ?}},
year = {1875}
}
@article{Stilgoe2013,
abstract = {The governance of emerging science and innovation is a major challenge for contemporary democracies. In this paper we present a framework for understanding and supporting efforts aimed at 'responsible innovation'. The framework was developed in part through work with one of the first major research projects in the controversial area of geoengineering, funded by the UK Research Councils. We describe this case study, and how this became a location to articulate and explore four integrated dimensions of responsible innovation: anticipation, reflexivity, inclusion and responsiveness. Although the framework for responsible innovation was designed for use by the UK Research Councils and the scientific communities they support, we argue that it has more general application and relevance. ?? 2013 Elsevier B.V. All rights reserved.},
author = {Stilgoe, Jack and Owen, Richard and Macnaghten, Phil},
doi = {10.1016/j.respol.2013.05.008},
issn = {00487333},
journal = {Research Policy},
keywords = {Emerging technologies,Ethics,Geoengineering,Governance,Responsible innovation},
month = {nov},
number = {9},
pages = {1568--1580},
publisher = {Elsevier B.V.},
title = {{Developing a framework for responsible innovation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0048733313000930},
volume = {42},
year = {2013}
}
@article{Scott1987,
author = {Scott, Mel and Bruce, Richard},
number = {3},
title = {{Five Stages of Growth Business in Small}},
volume = {20},
year = {1987}
}
@misc{,
title = {{Democratizing innovation.pdf}}
}
@article{Brabham2008,
abstract = {Crowdsourcing is an online, distributed problem-solving and production model that has emerged in recent years. Notable examples of the model include Threadless, iStockphoto, Inno- Centive, the Goldcorp Challenge, and user-generated advertising contests. This article provides an introduction to crowdsourcing, both its theoretical grounding and exemplar cases, taking care to distinguish crowdsourcing from open source production. This article also explores the possibilities for the model, its potential to exploit a crowd of innovators, and its potential for use beyond for- profit sectors. Finally, this article proposes an agenda for research into crowdsourcing.},
author = {Brabham, D. C.},
doi = {10.1177/1354856507084420},
isbn = {1354856507084},
issn = {1354-8565},
journal = {Convergence: The International Journal of Research into New Media Technologies},
keywords = {about human ingenuity,challenge,collective intelligence,crowdsourcing,designer,distributed problem solving,goldcorp,innocentive,is going on,istockphoto,its unfolding is to,of client,open source,reject the binary notion,right now,story to be told,the first step to,the next step is,there is an incredible,threadless,to look to what,wisdom of crowds},
month = {feb},
number = {1},
pages = {75--90},
title = {{Crowdsourcing as a Model for Problem Solving: An Introduction and Cases}},
url = {http://con.sagepub.com/cgi/doi/10.1177/1354856507084420},
volume = {14},
year = {2008}
}
@article{Ordanini2011,
abstract = {Purpose – The purpose of this paper is to analyze the emerging crowd-funding phenomenon, that is a collective effort by consumers who network and pool their money together, usually via the internet, in order to invest in and support efforts initiated by other people or organizations. Successful service businesses that organize crowd-funding and act as intermediaries are emerging, attesting to the viability of this means of attracting investment. Design/methodology/approach – The research employs a "grounded theory" approach, performing an in-depth qualitative analysis of three cases involving crowd-funding initiatives: SellaBand in the music business, Trampoline in financial services, and Kapipal in non-profit services. These cases were selected to represent a diverse set of crowd-funding operations that vary in terms of risk/return for the investor and the type of payoff associated to the investment. Findings – The research addresses two research questions: how and why do consumers turn into crowd-funding participants? and how and why do service providers set up a crowd-funding initiative? Concerning the first research question, the authors' findings reveal purposes, characteristics, roles and tasks, and investment size of crowd-funding activity from the consumer's point of view. Regarding the second research question, the authors' analysis reveals purposes, service roles, and network effects of crowd-funding activity investigated from the point of view of the service organization that set up the initiative. Practical implications – The findings also have implications for service managers interested in launching and/or managing crowd-funding initiatives. Originality/value – The paper addresses an emerging phenomenon and contributes to service theory in terms of extending the consumer's role from co-production and co-creation to investment.},
author = {Ordanini, Andrea and Miceli, Lucia and Pizzetti, Marta and Parasuraman, a.},
doi = {10.1108/09564231111155079},
issn = {1757-5818},
journal = {Journal of Service Management},
keywords = {crowd-funding,customer-investors,customers,investments,paper type research paper,service innovation},
number = {4},
pages = {443--470},
title = {{Crowd-funding: transforming customers into investors through innovative service platforms}},
url = {http://www.emeraldinsight.com/10.1108/09564231111155079},
volume = {22},
year = {2011}
}
@article{Bettis-Outland2012,
abstract = {Although an abundance of academic literature positions organizational information processing as antecedent to decision making, little attention is paid to the possibility that decision making can be antecedent to certain elements of organizational information processing. Specifically, does the decision making process impact the type of organizational learning that takes place? Do different approaches to decision making alter the amount and variety of information made available to the organization, that is, the level of information overload? This paper examines incremental and comprehensive decision making to understand the effects of different decision making types on organizational learning and information overload. Incrementalism suggests that decision making should take place in small steps or increments. This approach analyzes only a few scenarios to make decisions resulting in few, if any, major organizational changes. However, comprehensive decision making requires the consideration of all possible scenarios and potential outcomes, resulting in a major overhaul of traditions and procedures within the organization. Consequently, each decision making approach has a different impact on organizational learning and information overload. (C) 2011 Elsevier Inc. All rights reserved.},
author = {Bettis-Outland, H},
doi = {10.1016/j.jbusres.2010.12.021},
issn = {01482963},
journal = {Journal of Business Research},
keywords = {Decision making,Incrementalism,Information overload,Organizational learning},
month = {jun},
number = {6},
pages = {814--820},
publisher = {Elsevier Inc.},
title = {{Decision-making's impact on organizational learning and information overload}},
url = {http://www.sciencedirect.com/science/article/pii/S0148296310002845},
volume = {65},
year = {2012}
}
@book{Thea,
author = {The, Rossing},
file = {::},
isbn = {0060189878},
title = {{No Title}}
}
@article{Elliot2013,
abstract = {In today's global business environment, where multinational companies are pressed to increase revenues in order to survive, creativity may hold the key to ensuring their new product development (NPD) efforts lead to innovations with worldwide appeal, such as Apple's iPad and Gillette's Fusion Razor. To leverage creativity for effective global NPD, businesses want to know how cultures differ in their concepts of creativity and the impact of those differences on approaches to developing new products. Because global new products are increasingly developed in, by, and for multiple cultures, a particular need is for a culturally reflective understanding, or conceptualization, of creativity. While creativity is believed to be culturally tied, the dominant framework of creativity used in business and management assumes that creativity is culturally indifferent or insensitive. This knowledge gap is addressed by studying the role of creativity in NPD practices in a cross-cultural or global context. The study begins by first developing a culturally anchored conceptualization of creativity. Called cross-cultural creativity, the concept draws on creativity insights from the field of art and aesthetics. The concept specifies two modes of creativity, neither of which is superior to the other, called the spontaneous or S route and the divergent or D route. The S route emphasizes adaptiveness, processes, intuitiveness, and metamorphism, while the D route focuses on disruptiveness, results, rationality, and literalism. Next, this new concept is applied to NPD by positing how creativity in distinct cultures may shape NPD practices, as illustrated by Japanese and U.S. firms. Research propositions are formulated to capture these patterns, and thereafter, theoretical and practical implications of the framework and propositions are discussed. The implications center on global NPD, which is a complex enterprise involving typically more than one culture to design and develop new products for several geographic markets. The study is of interest to researchers needing a globally situated, culturally attached framework of creativity for international NPD studies, and managers seeking to exploit creativity in multinational and multicultural innovation projects.},
author = {Elliot, E.a. and Nakata, C.},
doi = {10.1111/jpim.12066},
issn = {07376782},
journal = {Journal of Product Innovation Management},
month = {dec},
pages = {110--125},
title = {{Cross-Cultural Creativity: Conceptualization and propositions for global new product development}},
url = {http://doi.wiley.com/10.1111/jpim.12066},
volume = {30},
year = {2013}
}
@book{,
file = {::},
isbn = {9780983648703},
title = {{No Title}}
}
@article{Foster2012,
author = {Foster, Richard N},
keywords = {Kodak, Radio Shack, Bear Stearns},
title = {{Creative Destruction Whips through Corporate America}},
year = {2012}
}
@article{,
file = {::},
pages = {1--9},
title = {{No Title}},
year = {1942}
}
@book{Adler,
author = {Adler, Isabel K},
isbn = {9788565424004},
title = {{Design Thinking Design Thinking Inova{\c{c}}{\~{a}}o em neg{\'{o}}cios}}
}
@misc{,
title = {{A revis{\~{a}}o da bibliografia em teses e disserta{\c{c}}{\~{o}}es.pdf}}
}
@article{Anderson2012,
abstract = {The Information Age has a surfeit of information received relative to what is processed. We model multiple sectors competing for consumer attention, with competition in price within each sector. Sector advertising levels follow a constant elasticity of substitution (CES) form, and within-sector prices are dispersed with a truncated Pareto distribution. The "information hump" shows highest ad levels for intermediate attention levels. Overall, advertising is excessive, although the allocation across sectors is optimal. The blame for information overload falls most on product categories with low information transmission costs and low profits.},
author = {Anderson, Simon P. and de Palma, Andr{\'{e}}},
journal = {The RAND Journal of Economics},
number = {1},
pages = {1--25},
title = {{Competition for attention in the Information (overload) Age}},
url = {http://doi.wiley.com/10.1111/j.1756-2171.2011.00155.x},
volume = {43},
year = {2012}
}
@article{Sawhney2005,
abstract = {In the networked world, firms are recognizing the power of the Internet as a platform for co-creating value with customers.We focus on how the Internet has impacted the process of collaborative innovation—a key process in value co-creation.We outline the distinctive capabilities of the Internet as a platform for customer engagement, including interactivity, enhanced reach, persistence, speed, and flexibility, and suggest that firms can use these capabilities to engage customers in collaborative product innovation through a variety of Internet-based mechanisms.We discuss how these mechanisms can facilitate collaborative innovation at different stages of the New Product Development process (back end vs. front end stages) and for differing levels of customer involvement (high reach vs. high richness).We present two detailed exploratory case studies to illustrate the integrated and systematic usage of Internetbased collaborative innovation mechanisms—Ducati from the motorbike industry and Eli Lilly from the pharmaceutical industry.We derive implications for managerial practice and academic research on collaborative innovation.},
author = {Sawhney, Mohanbir and Verona, Gianmario and Prandelli, Emanuela},
doi = {10.1002/dir.20046},
issn = {10949968},
journal = {Journal of Interactive Marketing},
month = {jan},
number = {4},
pages = {4--17},
publisher = {Elsevier},
title = {{Collaborating to create: The Internet as a platform for customer engagement in product innovation}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S1094996805700785},
volume = {19},
year = {2005}
}
@article{Dewick2006,
abstract = {Future disruptive, pervasive technologies will have important consequences for industrial structure, economic growth and the environment. Drawing on theories of technological diffusion, industrial evolution and long-term technological change this paper explores the effect of the development and diffusion of two future pervasive technologies on five industrial sectors in three regions during the 21st century in terms of their effect on economic structural change. Through semi-structured interviews with over 100 experts in the two technologies, the paper quantifies the effects of future biotechnologies and nanotechnologies on the industrial structure of the EU, USA and China in 2020 and 2050. The paper finds that as a result of the development and diffusion of future biotechnologies and nanotechnologies, some industries grow whilst others decline and some new ones emerge. The evidence suggests that the effect is different across countries and time; whereas the experts commonly believe that effect of the technologies on the industrial structure of the EU and US is likely to be similar, the effect in China is considered to be less by 2020 but the same as in the EU and US by 2050. This finding has important implications for the location of production, economic growth and energy demand in the future. ?? 2006 Elsevier Inc. All rights reserved.},
author = {Dewick, Paul and Green, Ken and Fleetwood, Toby and Miozzo, Marcela},
doi = {10.1016/j.techfore.2006.04.002},
issn = {00401625},
journal = {Technological Forecasting and Social Change},
keywords = {Biotechnologies,Energy,Industrial structure,Nanotechnologies,Technological diffusion},
month = {nov},
number = {9},
pages = {1084--1106},
title = {{Modelling creative destruction: Technological diffusion and industrial structure change to 2050}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0040162506000862},
volume = {73},
year = {2006}
}
@inproceedings{zen_deep_2014,
location = {Florence, Italy},
title = {Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis},
isbn = {978-1-4799-2893-4},
url = {http://ieeexplore.ieee.org/document/6854321/},
doi = {10.1109/ICASSP.2014.6854321},
abstract = {Statistical parametric speech synthesis ({SPSS}) using deep neural networks ({DNNs}) has shown its potential to produce naturally-sounding synthesized speech. However, there are limitations in the current implementation of {DNN}-based acoustic modeling for speech synthesis, such as the unimodal nature of its objective function and its lack of ability to predict variances. To address these limitations, this paper investigates the use of a mixture density output layer. It can estimate full probability density functions over real-valued output features conditioned on the corresponding input features. Experimental results in objective and subjective evaluations show that the use of the mixture density output layer improves the prediction accuracy of acoustic features and the naturalness of the synthesized speech.},
eventtitle = {{ICASSP} 2014 - 2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
pages = {3844--3848},
booktitle = {2014 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
publisher = {{IEEE}},
author = {Zen, Heiga and Senior, Andrew},
urldate = {2021-01-15},
date = {2014-05},
langid = {english},
file = {2014 Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis - Zen and Senior.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2014 Deep mixture density networks for acoustic modeling in statistical parametric speech synthesis - Zen and Senior.pdf:application/pdf}
}
@article{lyubimov_mathematical_2016,
title = {Mathematical model of acoustic speech production with mobile walls of the vocal tract},
volume = {62},
issn = {1063-7710, 1562-6865},
url = {http://link.springer.com/10.1134/S1063771016020093},
doi = {10.1134/S1063771016020093},
abstract = {A mathematical speech production model is considered that describes acoustic oscillation prop agation in a vocal tract with mobile walls. The wave field function satisfies the Helmholtz equation with boundary conditions of the third kind (impedance type). The impedance mode corresponds to a three parameter pendulum oscillation model. The experimental research demonstrates the nonlinear character of how the mobility of the vocal tract walls influence the spectral envelope of a speech signal.},
pages = {225--234},
number = {2},
journaltitle = {Acoustical Physics},
shortjournal = {Acoust. Phys.},
author = {Lyubimov, N. A. and Zakharov, E. V.},
urldate = {2021-01-15},
date = {2016-03},
langid = {english},
file = {2016 Mathematical model of acoustic speech production with mobile walls of the vocal tract - Lyubimov and Zakharov.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2016 Mathematical model of acoustic speech production with mobile walls of the vocal tract - Lyubimov and Zakharov.pdf:application/pdf}
}
@inproceedings{li_acoustic_2017,
title = {Acoustic Modeling for Google Home},
url = {http://www.isca-speech.org/archive/Interspeech_2017/abstracts/0234.html},
doi = {10.21437/Interspeech.2017-234},
abstract = {This paper describes the technical and system building advances made to the Google Home multichannel speech recognition system, which was launched in November 2016. Technical advances include an adaptive dereverberation frontend, the use of neural network models that do multichannel processing jointly with acoustic modeling, and Grid-{LSTMs} to model frequency variations. On the system level, improvements include adapting the model using Google Home speciﬁc data. We present results on a variety of multichannel sets. The combination of technical and system advances result in a reduction of {WER} of 8-28\% relative compared to the current production system.},
eventtitle = {Interspeech 2017},
pages = {399--403},
booktitle = {Interspeech 2017},
publisher = {{ISCA}},
author = {Li, Bo and Sainath, Tara N. and Narayanan, Arun and Caroselli, Joe and Bacchiani, Michiel and Misra, Ananya and Shafran, Izhak and Sak, Haşim and Pundak, Golan and Chin, Kean and Sim, Khe Chai and Weiss, Ron J. and Wilson, Kevin W. and Variani, Ehsan and Kim, Chanwoo and Siohan, Olivier and Weintraub, Mitchel and {McDermott}, Erik and Rose, Richard and Shannon, Matt},
urldate = {2021-01-15},
date = {2017-08-20},
langid = {english},
file = {2017 Acoustic Modeling for Google Home - Li et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2017 Acoustic Modeling for Google Home - Li et al..pdf:application/pdf}
}
@article{maestre_creating_2019,
title = {Creating Virtual Acoustic Replicas of Real Violins},
abstract = {We provide an overview of a current research project on measuring, modeling, and virtually recreating the sound radiation characteristics of real acoustic violins. Our general approach is based on measuring the directivity of an acoustic violin, and designing a digital ﬁlter structure that mimics the observed directivity while allowing interactive operation. The digital ﬁlter structure is fed by the electrical signal coming from a silent electric violin as played by a musician. In a hemi-anechoic chamber, we use a microphone array to characterize the frequency-dependent directivity transfer function of a real violin by exciting the bridge with an impact hammer and measuring the acoustic pressure at 4320 points on a sphere surrounding the instrument. From the input force and output pressure signals obtained from the real violin measurements, we use deconvolution to estimate 4320 impulse responses each corresponding to a radiation direction. With such impulse responses, we use State Wave Synthesis to model the observed directivity in time-varying conditions and efﬁciently render directional wavefronts in a virtual environment. We characterize the silent violin transfer function by exciting the bridge with an impact hammer and measuring the electrical signal at its output, leading to an impulse response that we use to design an inverse ﬁlter to recover the force excitation at the bridge.},
pages = {7},
author = {Maestre, Esteban and Scavone, Gary},
date = {2019},
langid = {english},
file = {2019 Creating Virtual Acoustic Replicas of Real Violins - Maestre and Scavone.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2019 Creating Virtual Acoustic Replicas of Real Violins - Maestre and Scavone.pdf:application/pdf}
}
@article{kapralova_big_nodate,
title = {A big data approach to acoustic model training corpus selection},
abstract = {Deep neural networks ({DNNs}) have recently become the state of the art technology in speech recognition systems. In this paper we propose a new approach to constructing large high quality unsupervised sets to train {DNN} models for large vocabulary speech recognition. The core of our technique consists of two steps. We ﬁrst redecode speech logged by our production recognizer with a very accurate (and hence too slow for real-time usage) set of speech models to improve the quality of ground truth transcripts used for training alignments. Using conﬁdence scores, transcript length and transcript ﬂattening heuristics designed to cull salient utterances from three decades of speech per language, we then carefully select training data sets consisting of up to 15K hours of speech to be used to train acoustic models without any reliance on manual transcription. We show that this approach yields models with approximately 18K context dependent states that achieve 10\% relative improvement in large vocabulary dictation and voice-search systems for Brazilian Portuguese, French, Italian and Russian languages.},
pages = {5},
author = {Kapralova, Olga and Alex, John and Weinstein, Eugene and Moreno, Pedro and Siohan, Olivier},
langid = {english},
file = {A big data approach to acoustic model training corpus selection - Kapralova et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\A big data approach to acoustic model training corpus selection - Kapralova et al..pdf:application/pdf}
}
@inproceedings{regnier_singing_2009,
location = {Taipei, Taiwan},
title = {Singing voice detection in music tracks using direct voice vibrato detection},
isbn = {978-1-4244-2353-8},
url = {http://ieeexplore.ieee.org/document/4959926/},
doi = {10.1109/ICASSP.2009.4959926},
abstract = {In this paper we investigate the problem of locating singing voice in music tracks. As opposed to most existing methods for this task, we rely on the extraction of the characteristics speciﬁc to singing voice. In our approach we suppose that the singing voice is characterized by harmonicity, formants, vibrato and tremolo. In the present study we deal only with the vibrato and tremolo characteristics. For this, we ﬁrst extract sinusoidal partials from the musical audio signal . The frequency modulation (vibrato) and amplitude modulation (tremolo) of each partial are then studied to determine if the partial corresponds to singing voice and hence the corresponding segment is supposed to contain singing voice. For this we estimate for each partial the rate (frequency of the modulations) and the extent (amplitude of modulation) of both vibrato and tremolo. A partial selection is then operated based on these values. A second criteria based on harmonicity is also introduced. Based on this, each segment can be labelled as singing or non-singing. Post-processing of the segmentation is then applied in order to remove short-duration segments. The proposed method is then evaluated on a large manually annotated test-set. The results of this evaluation are compared to the one obtained with a usual machine learning approach ({MFCC} and {SFM} modeling with {GMM}). The proposed method achieves very close results to the machine learning approach : 76.8\% compared to 77.4\% F-measure (frame classiﬁcation). This result is very promising, since both approaches are orthogonal and can then be combined.},
eventtitle = {{ICASSP} 2009 - 2009 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
pages = {1685--1688},
booktitle = {2009 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
publisher = {{IEEE}},
author = {Regnier, L. and Peeters, G.},
urldate = {2021-01-15},
date = {2009-04},
langid = {english},
file = {2009 Singing voice detection in music tracks using direct voice vibrato detection - Regnier and Peeters.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2009 Singing voice detection in music tracks using direct voice vibrato detection - Regnier and Peeters.pdf:application/pdf}
}
@inproceedings{huang_singing-voice_2012,
location = {Kyoto, Japan},
title = {Singing-voice separation from monaural recordings using robust principal component analysis},
isbn = {978-1-4673-0046-9 978-1-4673-0045-2 978-1-4673-0044-5},
url = {http://ieeexplore.ieee.org/document/6287816/},
doi = {10.1109/ICASSP.2012.6287816},
abstract = {Separating singing voices from music accompaniment is an important task in many applications, such as music information retrieval, lyric recognition and alignment. Music accompaniment can be assumed to be in a low-rank subspace, because of its repetition structure; on the other hand, singing voices can be regarded as relatively sparse within songs. In this paper, based on this assumption, we propose using robust principal component analysis for singing-voice separation from music accompaniment. Moreover, we examine the separation result by using a binary time-frequency masking method. Evaluations on the {MIR}-1K dataset show that this method can achieve around 1∼1.4 {dB} higher {GNSDR} compared with two state-of-the-art approaches without using prior training or requiring particular features.},
eventtitle = {{ICASSP} 2012 - 2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing},
pages = {57--60},
booktitle = {2012 {IEEE} International Conference on Acoustics, Speech and Signal Processing ({ICASSP})},
publisher = {{IEEE}},
author = {Huang, Po-Sen and Chen, Scott Deeann and Smaragdis, Paris and Hasegawa-Johnson, Mark},
urldate = {2021-01-15},
date = {2012-03},
langid = {english},
file = {2012 Singing-voice separation from monaural recordings using robust principal component analysis - Huang et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2012 Singing-voice separation from monaural recordings using robust principal component analysis - Huang et al..pdf:application/pdf}
}
@article{jarvelainen_audibility_2001,
title = {Audibility of the timbral effects of inharmonicity in stringed instrument tones},
volume = {2},
issn = {1529-7853},
url = {http://asa.scitation.org/doi/10.1121/1.1374756},
doi = {10.1121/1.1374756},
pages = {79--84},
number = {3},
journaltitle = {Acoustics Research Letters Online},
shortjournal = {Acoustics Research Letters Online},
author = {Järveläinen, Hanna and Välimäki, Vesa and Karjalainen, Matti},
urldate = {2021-01-15},
date = {2001-07},
langid = {english},
file = {2001 Audibility of the timbral effects of inharmonicity in stringed instrument tones - Järveläinen et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2001 Audibility of the timbral effects of inharmonicity in stringed instrument tones - Järveläinen et al..pdf:application/pdf}
}
@thesis{davidsson_structure-acoustic_2004,
location = {Lund},
title = {Structure-acoustic analysis: finite element modelling and reduction methods},
shorttitle = {Structure-acoustic analysis},
institution = {Univ.},
type = {phdthesis},
author = {Davidsson, Peter},
date = {2004},
langid = {english},
note = {{ISBN}: 9789162861766
{OCLC}: 186476406},
keywords = {test},
file = {2004 Structure-acoustic analysis finite element modelling and reduction methods - Davidsson.pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\2004 Structure-acoustic analysis finite element modelling and reduction methods - Davidsson.pdf:application/pdf}
}
@article{kartofelev_modeling_nodate,
title = {{MODELING} A {VIBRATING} {STRING} {TERMINATED} {AGAINST} A {BRIDGE} {WITH} {ARBITRARY} {GEOMETRY}},
pages = {7},
author = {Kartofelev, Dmitri and Stulov, Anatoli and Lehtonen, Heidi-Maria and Va, Vesa},
langid = {english},
file = {MODELING A VIBRATING STRING TERMINATED AGAINST A BRIDGE WITH ARBITRARY GEOMETRY - Kartofelev et al..pdf:C\:\\Users\\tesse\\Desktop\\Files\\Dropbox\\BIBrep\\Articles\\Acoustics\\MODELING A VIBRATING STRING TERMINATED AGAINST A BRIDGE WITH ARBITRARY GEOMETRY - Kartofelev et al..pdf:application/pdf}
}
@InProceedings{kimoto1990stock,
author        = {Kimoto, Takashi and Asakawa, Kazuo and Yoda, Morio and Takeoka, Masakazu},
title         = {Stock market prediction system with modular neural networks},
booktitle     = {1990 IJCNN international joint conference on neural networks},
year          = {1990},
pages         = {1--6},
organization  = {IEEE},
__markedentry = {[tesse:1]},
}
@Article{saad1998comparative,
author        = {Saad, Emad W and Prokhorov, Danil V and Wunsch, Donald C},
title         = {Comparative study of stock trend prediction using time delay, recurrent and probabilistic neural networks},
journal       = {IEEE Transactions on neural networks},
year          = {1998},
volume        = {9},
number        = {6},
pages         = {1456--1470},
__markedentry = {[tesse:1]},
publisher     = {IEEE},
}
@Article{chen2003application,
author        = {Chen, An-Sing and Leung, Mark T and Daouk, Hazem},
title         = {Application of neural networks to an emerging financial market: forecasting and trading the Taiwan Stock Index},
journal       = {Computers \& Operations Research},
year          = {2003},
volume        = {30},
number        = {6},
pages         = {901--923},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{kim2003financial,
author        = {Kim, Kyoung-jae},
title         = {Financial time series forecasting using support vector machines},
journal       = {Neurocomputing},
year          = {2003},
volume        = {55},
number        = {1-2},
pages         = {307--319},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{2005HuangForecasting,
author        = {Wei Huang and Yoshiteru Nakamori and Shou-Yang Wang},
title         = {Forecasting stock market movement direction with support vector machine},
journal       = {Computers {\&} Operations Research},
year          = {2005},
volume        = {32},
number        = {10},
pages         = {2513--2522},
month         = {oct},
__markedentry = {[tesse:1]},
doi           = {10.1016/j.cor.2004.03.016},
publisher     = {Elsevier {BV}},
}
@Article{enke2005use,
author        = {Enke, David and Thawornwong, Suraphan},
title         = {The use of data mining and neural networks for forecasting stock market returns},
journal       = {Expert Systems with applications},
year          = {2005},
volume        = {29},
number        = {4},
pages         = {927--940},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{zhang2009stock,
author        = {Zhang, Yudong and Wu, Lenan},
title         = {Stock market prediction of S\&P 500 via combination of improved BCO approach and BP neural network},
journal       = {Expert systems with applications},
year          = {2009},
volume        = {36},
number        = {5},
pages         = {8849--8854},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{atsalakis2009surveying,
author        = {Atsalakis, George S and Valavanis, Kimon P},
title         = {Surveying stock market forecasting techniques--Part II: Soft computing methods},
journal       = {Expert Systems with Applications},
year          = {2009},
volume        = {36},
number        = {3},
pages         = {5932--5941},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{atsalakis2010surveying,
author        = {Atsalakis, George S and Valavanis, Kimon P},
title         = {Surveying stock market forecasting techniques-Part I: Conventional methods},
journal       = {Journal of Computational Optimization in Economics and Finance},
year          = {2010},
volume        = {2},
number        = {1},
pages         = {45--92},
__markedentry = {[tesse:1]},
}
@Article{kara2011predicting,
author        = {Kara, Yakup and Boyacioglu, Melek Acar and Baykan, {\"O}mer Kaan},
title         = {Predicting direction of stock price index movement using artificial neural networks and support vector machines: The sample of the Istanbul Stock Exchange},
journal       = {Expert systems with Applications},
year          = {2011},
volume        = {38},
number        = {5},
pages         = {5311--5319},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{guresen2011using,
author        = {Guresen, Erkam and Kayakutlu, Gulgun and Daim, Tugrul U},
title         = {Using artificial neural network models in stock market index prediction},
journal       = {Expert Systems with Applications},
year          = {2011},
volume        = {38},
number        = {8},
pages         = {10389--10397},
__markedentry = {[tesse:1]},
publisher     = {Elsevier},
}
@Article{mingers2015review,
author    = {Mingers, John and Leydesdorff, Loet},
title     = {A review of theory and practice in scientometrics},
journal   = {European journal of operational research},
year      = {2015},
volume    = {246},
number    = {1},
pages     = {1--19},
publisher = {Elsevier},
}
@InProceedings{tan1995probabilistic,
author    = {Tan, Hong and Prokhorov, D and Wunsch, D},
title     = {Probabilistic and time-delay neural-network techniques for conservative short-term stock trend prediction},
booktitle = {Proc. World Congr. Neural Networks},
year      = {1995},
}
@Article{kreesuradej1994time,
author    = {Kreesuradej, W and Wunsch, Donald C and Lane, M},
title     = {Time delay neural network for small time series data sets},
year      = {1994},
publisher = {Lawrence Erlbaum Associates},
}
@InProceedings{saad1996advanced,
author       = {Saad, Emad W and Prokhorov, Danil V and Wunsch, Donald C},
title        = {Advanced neural network training methods for low false alarm stock trend prediction},
booktitle    = {Proceedings of International Conference on Neural Networks (ICNN'96)},
year         = {1996},
volume       = {4},
pages        = {2021--2026},
organization = {IEEE},
}
@InProceedings{han2015learning,
author        = {Han, Song and Pool, Jeff and Tran, John and Dally, William},
title         = {Learning both weights and connections for efficient neural network},
booktitle     = {Advances in neural information processing systems},
year          = {2015},
pages         = {1135--1143},
__markedentry = {[tesse:1]},
}
@Article{han2015deep,
author        = {Han, Song and Mao, Huizi and Dally, William J},
title         = {Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding},
journal       = {arXiv preprint arXiv:1510.00149},
year          = {2015},
__markedentry = {[tesse:1]},
}
@InProceedings{hanson1989comparing,
author        = {Hanson, Stephen Jos{\'e} and Pratt, Lorien Y},
title         = {Comparing biases for minimal network construction with back-propagation},
booktitle     = {Advances in neural information processing systems},
year          = {1989},
pages         = {177--185},
__markedentry = {[tesse:1]},
}
@InProceedings{mozer1989skeletonization,
author        = {Mozer, Michael C and Smolensky, Paul},
title         = {Skeletonization: A technique for trimming the fat from a network via relevance assessment},
booktitle     = {Advances in neural information processing systems},
year          = {1989},
pages         = {107--115},
__markedentry = {[tesse:1]},
}
@InProceedings{lecun1990optimal,
author        = {LeCun, Yann and Denker, John S and Solla, Sara A},
title         = {Optimal brain damage},
booktitle     = {Advances in neural information processing systems},
year          = {1990},
pages         = {598--605},
__markedentry = {[tesse:1]},
}
@Article{reed1993pruning,
author        = {Reed, Russell},
title         = {Pruning algorithms-a survey},
journal       = {IEEE transactions on Neural Networks},
year          = {1993},
volume        = {4},
number        = {5},
pages         = {740--747},
__markedentry = {[tesse:1]},
publisher     = {IEEE},
}
@InProceedings{blum1989training,
author    = {Blum, Avrim and Rivest, Ronald L},
title     = {Training a 3-node neural network is NP-complete},
booktitle = {Advances in neural information processing systems},
year      = {1989},
pages     = {494--501},
}
@InProceedings{livni2014computational,
author    = {Livni, Roi and Shalev-Shwartz, Shai and Shamir, Ohad},
title     = {On the computational efficiency of training neural networks},
booktitle = {Advances in neural information processing systems},
year      = {2014},
pages     = {855--863},
}
@Article{2018SilverGeneral,
author    = {David Silver and Thomas Hubert and Julian Schrittwieser and Ioannis Antonoglou and Matthew Lai and Arthur Guez and Marc Lanctot and Laurent Sifre and Dharshan Kumaran and Thore Graepel and Timothy Lillicrap and Karen Simonyan and Demis Hassabis},
title     = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
journal   = {Science},
year      = {2018},
volume    = {362},
number    = {6419},
pages     = {1140--1144},
month     = {dec},
doi       = {10.1126/science.aar6404},
publisher = {American Association for the Advancement of Science ({AAAS})},
}
@InProceedings{2018TobingEvaluation,
author    = {Patrick Lumban Tobing and Tomoki Hayashi and Yi-Chiao Wu and Kazuhiro Kobayashi and Tomoki Toda},
title     = {An Evaluation of Deep Spectral Mappings and {WaveNet} Vocoder for Voice Conversion},
booktitle = {2018 {IEEE} Spoken Language Technology Workshop ({SLT})},
year      = {2018},
month     = {dec},
publisher = {{IEEE}},
doi       = {10.1109/slt.2018.8639608},
}
